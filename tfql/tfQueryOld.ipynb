{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# Search\n",
    "\n",
    "Do we need search in TF, like MQL?\n",
    "\n",
    "Yes, it is convenient to have a more declarative way of getting a set of interesting nodes to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query syntax\n",
    "\n",
    "## General\n",
    "We have these kinds of lines:\n",
    "\n",
    "* white-space lines (everywhere allowed, will be always ignored)\n",
    "* relation line: **name operator name**.\n",
    "  Indents and spacing are ignored, but there must be space around the operator.\n",
    "* atom line: **indent otype features**. The indent is significant.\n",
    "* feature line: **features**. Indent is not significant. Only allowed after an atom line or after\n",
    "  a feature line. \n",
    "  Feature lines are continuations of the features of an atom, handy in those cases where the\n",
    "  features occupy a lot of space.\n",
    "\n",
    "## Features\n",
    "\n",
    "A white-space separated list of *key*=*values*.\n",
    "\n",
    "* there may be no space around the `=`.\n",
    "* *key* must be a feature name that exists in the dataset.\n",
    "  If it is not yet loaded, it will be loaded.\n",
    "* *values* must be a `|` separated list of feature values, no quotes.\n",
    "  No spaces around the `|`.\n",
    "  If you need a space or `|` or `\\\\` in a value, escape it by a `\\\\`.\n",
    "  Escape tabs and newlines as `\\\\t` and `\\\\n`.\n",
    "\n",
    "## Operators\n",
    "\n",
    "### Node comparison\n",
    "* `=`: is equal (meaning the same node, a clause and a verse that occupy the same slots are still unequal)\n",
    "* `#`: is unequal (meaning a different node, a clause and a verse that occupy the same slots are still unequal)\n",
    "* `<` `>`: before and after (in the *canonical ordering*)\n",
    "\n",
    "### Slot comparison\n",
    "* `==`: occupy the same slots (identical slot sets)\n",
    "* `&&`: overlap (the intersection of both slot sets is not empty)\n",
    "* `##`: occupy different slots (but they may overlap, the set of slots of the two are different as sets)\n",
    "* `||`: occupy disjoint slots (no slot occupied by the one is also occupied by the other)\n",
    "* `[[` `]]`: embeds and contains (slot set inclusion, in both directions)\n",
    "* `<<` `>>`: before and after (with respect to the slots occupied: left ends before right starts and vv)\n",
    "\n",
    "### Edge features\n",
    "* `-`*name*`>` `<`*name*`-`: connected by the edge feature *name*, in both directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Query \n",
    "\n",
    "We receive the nodes and edges of a query, give the nodes a number,\n",
    "and replace the *from*- and *to*-nodes\n",
    "of the edges by the numbers of those nodes.\n",
    "\n",
    "We also build a list of otypes of the nodes, in the same order as the nodes themselves.\n",
    "\n",
    "Then we are ready to perform all kinds of operations with this query.\n",
    "\n",
    "## Atoms\n",
    "\n",
    "The first stage of running a query is to run the individual nodes as filters in their associated\n",
    "object types.\n",
    "\n",
    "A node is specified by an `otype` and a features dict.\n",
    "The features specifiy any number of features, with an allowed value or set of allowed values for each of them.\n",
    "\n",
    "Running an atom means to filter the set of all nodes in its `otype` by means of its `features`.\n",
    "\n",
    "The result list is delivered in a list of result lists, corresponding to the nodes list.\n",
    "\n",
    "## Edges\n",
    "\n",
    "Once the atoms have done their work, it is time to work out the constraints posed by edges.\n",
    "\n",
    "An edge from query node `nF` to query node `nT` means\n",
    "* that the text nodes in the result of `nF` should embed a text node in the result of `nT`,\n",
    "or equivalently,\n",
    "* that the text nodes in the result of `nT` should be embedded in a text node in the result of `nF`.\n",
    "\n",
    "This will reduce the amount of nodes in both `nF` and `nT`.\n",
    "\n",
    "### Note\n",
    "> In general, one pass over all edges will not be enough.\n",
    "We have to repeat the process until nothing changes anymore.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "The challenge is now to run the edges in an optimal sequence.\n",
    "\n",
    "The basic intuition is this.\n",
    "\n",
    "* some query nodes filter strongly, others hardly, i.e. some atom results are small compared to the total\n",
    "  number of nodes in their otype, other atom results are nearly as big as the total otype.\n",
    "* if an edge connects a strongly filtering node with a weakly filtering node, we expect a big reduction\n",
    "* if we work within the strongest filtering query nodes, we do not have to do much work and when we reach the\n",
    "  weaker filters, they will decrease rapidly\n",
    "* so we postpone to touch the bigger sets as long as possible, and when we touch them, they are expected to decrease\n",
    "  quickly\n",
    "\n",
    "We are going to rank query nodes by how strong they have filtered their otype so far.\n",
    "\n",
    "* let $o_n$ be the otype associated with query node $n$\n",
    "* let $r_n$ be the current result set associated with query node $n$\n",
    "\n",
    "\n",
    "Then the **query fraction** $q(n)$ is the a proportion between\n",
    "the number of text nodes in the current result:\n",
    "and\n",
    "the total number of text nodes in the otype:\n",
    "\n",
    "$$q(n) = {{|r_n|}\\over {|o_n|}}$$\n",
    "\n",
    "Then we rank the edges by combined query fraction of the nodes:\n",
    "\n",
    "$$ r(f,t) = q(f)^2 + q(t)^2$$\n",
    "\n",
    "By squaring both query fractions, we strongly give precedence to edges involving few results.\n",
    "\n",
    "## Showing\n",
    "\n",
    "We want to see intermediate yarns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Query(object):\n",
    "    def __init__(self, rawNodes, rawEdges):\n",
    "        self.good = True\n",
    "        otypes = []\n",
    "        features = []\n",
    "        edges = []\n",
    "        nodeIndex = {}\n",
    "        for (q, node) in enumerate(rawNodes):\n",
    "            fNode = freeze(node)\n",
    "            nodeIndex[fNode] = q\n",
    "            otypes.append(node[0])\n",
    "            features.append(node[1])\n",
    "        for (nodeFrom, relation, nodeTo) in rawEdges:\n",
    "            (fNodeFrom, fNodeTo) = (freeze(nodeFrom), freeze(nodeTo))\n",
    "            if fNodeFrom not in nodeIndex:\n",
    "                error('\"from\"-node  in edge {} is not found: {}'.format(i, nodeFrom))\n",
    "                self.good = False\n",
    "                continue\n",
    "            if fNodeTo not in nodeIndex:\n",
    "                error('\"to\"-node in edge {} is not a node: {}'.format(i, nodeTo))\n",
    "                self.good = False\n",
    "                continue\n",
    "            fromI = nodeIndex[fNodeFrom]\n",
    "            toI = nodeIndex[fNodeTo]\n",
    "            edges.append((fromI, relation, toI))\n",
    "        self.otypes = otypes\n",
    "        self.features = features\n",
    "        self.qnodes = list(range(len(otypes)))\n",
    "        self.edges = edges\n",
    "        self.yarns = {}\n",
    "        self.spreads = {}\n",
    "        self.spreadsC = {}\n",
    "        self.uptodate = {}\n",
    "        self.connectedness()\n",
    "        self.verbose = None\n",
    "    \n",
    "    def connectedness(self):\n",
    "        componentIndex = dict(((q, {q}) for q in range(len(self.otypes))))\n",
    "        for (f, rela, t) in self.edges:\n",
    "            if f != t:\n",
    "                componentIndex[f] |= componentIndex[t]\n",
    "                for u in componentIndex[f] - {f}:\n",
    "                    componentIndex[u] = componentIndex[f]\n",
    "        components = sorted(set(frozenset(c) for c in componentIndex.values()))\n",
    "        componentIndex = {}\n",
    "        for c in components:\n",
    "            for q in c:\n",
    "                componentIndex[q] = c\n",
    "        componentEdges = {}\n",
    "        for (e, (f, rela, t)) in enumerate(self.edges):\n",
    "            c = componentIndex[f]\n",
    "            componentEdges.setdefault(c, []).append(e)\n",
    "        self.components = []\n",
    "        for c in components:\n",
    "            self.components.append([\n",
    "                sorted(c),\n",
    "                componentEdges[c]\n",
    "            ])\n",
    "        indent(level=0)\n",
    "        lComps = len(self.components)\n",
    "        if lComps == 0:\n",
    "            error('Query without instructions. Tell me what to look for.')\n",
    "            self.good = False\n",
    "        elif lComps > 1:\n",
    "            error('More than one connected components ({}):'.format(len(self.components)))\n",
    "            error('Either run the subqueries one by one, or connect the components by a relation',tm=False)\n",
    "            self.good = False\n",
    "        else:\n",
    "            info('Query OK: {} query nodes and {} query edges'.format(len(self.qnodes), len(self.edges)))\n",
    "                \n",
    "    def showNode(self, q):\n",
    "        info('node {:>2}-{:<13} ({:>6} in yarn)'.format(\n",
    "            q, self.otypes[q], len(self.yarns[q]),\n",
    "        ), tm=False)\n",
    "\n",
    "\n",
    "    def showEdge(self, e, dir):\n",
    "        otypes = self.otypes\n",
    "        es = self.edges\n",
    "        spreads = self.spreads\n",
    "        spreadsC = self.spreadsC\n",
    "        (f, rela, t) = es[e]\n",
    "        if dir == -1: (f, t) = (t, f)\n",
    "        info('edge {:>2}-{:<13} ={:>2}=> {:>2}-{:<13} @{:8.3g}'.format(\n",
    "            f, otypes[f], dir, t, otypes[t],\n",
    "            spreads.get(e, '?') if dir == 1 else spreadsC.get(e, '?'),\n",
    "        ), tm=False)\n",
    "\n",
    "    def showYarns(self):\n",
    "        indent(level=0)\n",
    "        indent(level=1)\n",
    "        for q in self.qnodes:\n",
    "            self.showNode(q)\n",
    "\n",
    "    def showCandidate(self, title, qs, esO):\n",
    "        info('{} with {} nodes and {} edges'.format(title, len(qs), len(esO)), tm=False)\n",
    "        for q in qs: self.showNode(q)\n",
    "        for e in esO: self.showEdge(*e)\n",
    "\n",
    "    def spinAtom(self, q):\n",
    "        otype = self.otypes[q]\n",
    "        features = self.features[q]\n",
    "        featureList = sorted(features.items())\n",
    "        yarn = set()\n",
    "        for n in F.otype.s(otype):\n",
    "            good = True\n",
    "            for (ft, val) in featureList:\n",
    "                fval = Fs(ft).v(n)\n",
    "                if type(val) is str or type(val) is int:\n",
    "                    if fval != val:\n",
    "                        good = False\n",
    "                        break\n",
    "                else:\n",
    "                    if fval not in val:\n",
    "                        good = False\n",
    "                        break\n",
    "            if good: yarn.add(n)\n",
    "        self.yarns[q] = yarn\n",
    "\n",
    "    def spinAtoms(self):\n",
    "        qs = self.qnodes\n",
    "        indent(level=0)\n",
    "        info('Spinning principal yarn for {} atoms'.format(len(qs)))\n",
    "        for q in qs:\n",
    "            self.spinAtom(q)\n",
    "        self.showYarns()\n",
    "\n",
    "    def estimateSpreads(self, both=False):\n",
    "        otypes = self.otypes\n",
    "        es = self.edges\n",
    "        yarns = self.yarns\n",
    "\n",
    "        self.spreadsC = {}\n",
    "        self.spreads = {}\n",
    "\n",
    "        for (e, (f, rela, t)) in enumerate(es):\n",
    "            tasks = [(f, rela, t, self.spreads)]\n",
    "            if both:\n",
    "                tasks.append((t, converse[rela], f, self.spreadsC))\n",
    "            for (tf, trela, tt, dest) in tasks:\n",
    "                yarnF = list(yarns[tf])\n",
    "                yarnT = yarns[tt]\n",
    "                function = types.FunctionType\n",
    "                totalSpread = 0\n",
    "                if len(yarnF) < 100:\n",
    "                    tries = yarnF\n",
    "                else:\n",
    "                    tries = set(yarnF[randrange(len(yarnF))] for n in range(100))\n",
    "                if len(tries) == 0:\n",
    "                    dest[e] = 0\n",
    "                else:\n",
    "                    r = relations[trela](otypes[tf], otypes[tt])\n",
    "                    for n in tries:\n",
    "                        image = r(n)\n",
    "                        if type(image) is function:\n",
    "                            nRela = set(m for m in yarnT if image(m))\n",
    "                            totalSpread += len(nRela)\n",
    "                        else:\n",
    "                            nRela = set(image) & yarnT\n",
    "                            totalSpread += len(nRela)\n",
    "                    dest[e] = totalSpread / len(tries)\n",
    "        indent(level=0)\n",
    "        info('Estimated spreads:', tm=False)\n",
    "        indent(level=1)\n",
    "        for e in range(len(es)):\n",
    "            self.showEdge(e, 1)\n",
    "            if both: self.showEdge(e, -1)\n",
    "\n",
    "    def chooseEdge(self):\n",
    "        yarnFractionNode = {}\n",
    "        qs = self.qnodes\n",
    "        es = self.edges\n",
    "        otypes = self.otypes\n",
    "        spreads = self.spreads\n",
    "        for q in qs:\n",
    "            otype = otypes[q]\n",
    "            (begin, end) = F.otype.sInterval(otype)\n",
    "            nOtype = 1 + end - begin\n",
    "            nYarn = len(self.yarns[q])\n",
    "            yf = nYarn / nOtype\n",
    "            yarnFractionNode[q] = yf * yf\n",
    "        yarnFractionEdge = {}\n",
    "        for (e, (f, rela, t)) in enumerate(es):\n",
    "            if self.uptodate[e]: continue\n",
    "            yarnFractionEdge[e] = yarnFractionNode[f] + yarnFractionNode[t] + spreads[e]\n",
    "        indent(level=0)\n",
    "        firstEdge = sorted(yarnFractionEdge.items(), key=lambda x: x[1])[0][0]\n",
    "        return firstEdge\n",
    "            \n",
    "    def spinEdge(self, e):\n",
    "        otypes = self.otypes\n",
    "        yarns = self.yarns\n",
    "        qs = self.qnodes\n",
    "        es = self.edges\n",
    "        uptodate = self.uptodate\n",
    "        \n",
    "        (f, rela, t) = es[e]\n",
    "        yarnF = yarns[f]\n",
    "        yarnT = yarns[t]\n",
    "        indent(level=1, reset=True)\n",
    "        self.showEdge(e, 1)\n",
    "        self.showNode(f)\n",
    "        self.showNode(t)\n",
    "\n",
    "        newYarnF = set()\n",
    "        newYarnT = set()\n",
    "        function = types.FunctionType\n",
    "        \n",
    "        r = relations[rela](otypes[f], otypes[t])\n",
    "        for n in yarnF:\n",
    "            image = r(n)\n",
    "            if type(image) is function:\n",
    "                nRela = set(m for m in yarnT if image(m))\n",
    "                if len(nRela):\n",
    "                    newYarnT |= nRela\n",
    "                    newYarnF.add(n)\n",
    "            else:\n",
    "                nRela = set(image) & yarnT\n",
    "                if len(nRela):\n",
    "                    newYarnT |= nRela\n",
    "                    newYarnF.add(n)\n",
    "\n",
    "        affectedF = len(newYarnF) != len(yarns[f])\n",
    "        affectedT = len(newYarnT) != len(yarns[t])\n",
    "\n",
    "        uptodate[e] = True\n",
    "        for (oe, (of, orela, ot)) in enumerate(es):\n",
    "            if oe == e: continue\n",
    "            if (affectedF and f in {of, ot}) or (affectedT and t in {of, ot}):\n",
    "                self.uptodate[oe] = False\n",
    "        self.yarns[f] = newYarnF\n",
    "        self.yarns[t] = newYarnT      \n",
    "        self.showNode(f)\n",
    "        self.showNode(t)\n",
    "\n",
    "    def spinEdges(self):\n",
    "        qs = self.qnodes\n",
    "        es = self.edges\n",
    "        yarns = self.yarns\n",
    "        uptodate = self.uptodate\n",
    "\n",
    "        indent(level=0)\n",
    "        info('Spinning with {} edges'.format(len(es)))\n",
    "        for e in range(len(es)):\n",
    "            uptodate[e] = False\n",
    "        it = 0\n",
    "        while True:\n",
    "            if min(len(yarns[q]) for q in qs) == 0: break\n",
    "            if reduce(\n",
    "                lambda y,z: y and z, \n",
    "                (uptodate[e] for e in range(len(es))),\n",
    "                True,\n",
    "            ): break\n",
    "            e = self.chooseEdge()\n",
    "            self.spinEdge(e)\n",
    "            it += 1\n",
    "        indent(level=0)\n",
    "        info('Done {} iterations'.format(it))\n",
    "        self.showYarns()\n",
    "\n",
    "    def makeS1(self):\n",
    "        qs = self.qnodes\n",
    "        es = self.edges\n",
    "        s1Edges = []\n",
    "        for (e, (f, rela, t)) in enumerate(es):\n",
    "            if self.spreads[e] <= 1:\n",
    "                s1Edges.append((e, 1))\n",
    "            if self.spreadsC[e] <= 1:\n",
    "                s1Edges.append((e, -1))\n",
    "        # s1Edges contain all edges with spread <= 1, or whose converse has spread <= 1\n",
    "        # now we want to build the largest graph with the original nodes and these edges,\n",
    "        # such that you can walk from a starting point over directed s1 edges to every other point\n",
    "        # we initialize candidate graphs: for each node: singletons graph, no edges.\n",
    "        candidates = []\n",
    "        # add s1 edges and nodes to all candidates\n",
    "        for q in qs: \n",
    "            cnodes = {q}\n",
    "            cedges = set()\n",
    "            cedgesOrder = []\n",
    "            while True:\n",
    "                added = False\n",
    "                for (e, dir) in s1Edges:\n",
    "                    (f, rela, t) = es[e]\n",
    "                    if dir == -1: (t,f) = (f,t)\n",
    "                    if f in cnodes:\n",
    "                        if t not in cnodes:\n",
    "                            cnodes.add(t)\n",
    "                            added = True\n",
    "                        if (e, dir) not in cedges:\n",
    "                            cedges.add((e, dir))\n",
    "                            cedgesOrder.append((e, dir))\n",
    "                            added = True\n",
    "                if not added: break\n",
    "            candidates.append((cnodes, cedgesOrder))\n",
    "\n",
    "        # pick the biggest graph (nodes and edges count for 1)\n",
    "        startS1 = sorted(candidates, key=lambda x: len(x[0])+len(x[1]))[-1]\n",
    "        # remove spurious edges: if we have both the 1 and -1 version of an edge,\n",
    "        # we can leave out the one that we encounter in the second place\n",
    "        newCedges = set()\n",
    "        newCedgesOrder = []\n",
    "        for (e, dir) in startS1[1]:\n",
    "            if e not in newCedges:\n",
    "                newCedgesOrder.append((e, dir))\n",
    "                newCedges.add(e)\n",
    "        startS1 = (startS1[0], newCedgesOrder)\n",
    "        self.showCandidate('startS1', *startS1)\n",
    "        return startS1\n",
    "\n",
    "    def stitchPlan(self, startS1):\n",
    "        es = self.edges\n",
    "\n",
    "        newNodes = startS1[0]\n",
    "        newEdges = startS1[1]\n",
    "        doneEdges = set(e[0] for e in newEdges)\n",
    "        # we add all edges that are not yet in our startS1.\n",
    "        # we add them two-fold: also with converse, and we sort the result by spread\n",
    "        # then we start a big loop:\n",
    "        # in every iteration, we take the edge with smallest spread that can be connected\n",
    "        # to the graph under construction\n",
    "        # then we start a new iteration, because the graph has grown, and and new edges might\n",
    "        # have become connectable by that\n",
    "\n",
    "        remainingEdges = set()\n",
    "        for e in range(len(es)):\n",
    "            if e not in doneEdges:\n",
    "                remainingEdges.add((e, 1))\n",
    "                remainingEdges.add((e, -1))\n",
    "        remainingEdgesO = sorted(\n",
    "            remainingEdges,\n",
    "            key=lambda e: self.spreads[e[0]] if e[1] == 1 else self.spreadsC[e[0]],\n",
    "        )\n",
    "\n",
    "        info('Making a Plan. {} untreated edges so far:'.format(\n",
    "                len(set(e[0] for e in remainingEdgesO))),\n",
    "            tm=False)\n",
    "        for e in remainingEdgesO: self.showEdge(*e)\n",
    "\n",
    "        while True:\n",
    "            added = False\n",
    "            for (e, dir) in remainingEdgesO:\n",
    "                if e in doneEdges: continue\n",
    "                (f, rela, t) = es[e]\n",
    "                if dir == -1: (f, t) = (t, f)\n",
    "                if f in newNodes:\n",
    "                    newNodes.add(t)\n",
    "                    newEdges.append((e, dir))\n",
    "                    doneEdges.add(e)\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added: break\n",
    "        # conjecture: we have all edges and all nodes now\n",
    "        # reason: we work in a connected component, so all nodes are reachable\n",
    "        # by edges or inverses\n",
    "        self.showCandidate('Plan', newNodes, newEdges)\n",
    "        return (newNodes, newEdges)\n",
    "        \n",
    "    def stitch(self):\n",
    "        qs = self.qnodes\n",
    "        es = self.edges\n",
    "        yarns = self.yarns\n",
    "\n",
    "        self.estimateSpreads(both=True)\n",
    "        indent(level=0)\n",
    "        info('Stitching {} yarns with {} stitches. Total yarn length {}'.format(\n",
    "            len(qs), len(es),\n",
    "            sum(len(yarns[q]) for q in qs)\n",
    "        ))\n",
    "        startS1 = self.makeS1()\n",
    "        plan = self.stitchPlan(startS1)\n",
    "        deliver = self.stitchResults(plan)\n",
    "        indent(level=0, verbose=-2)\n",
    "        info('Stitching done')\n",
    "        \n",
    "    def stitchResults(self, plan):\n",
    "        otypes = self.otypes\n",
    "        es = self.edges\n",
    "        yarns = self.yarns\n",
    "        function = types.FunctionType\n",
    "\n",
    "        planEdges = plan[1]\n",
    "        if len(planEdges) == 0:\n",
    "            # no edges, hence a single node (because of connectedness,\n",
    "            # hence we must deliver everything of its yarn\n",
    "            yarn = yarns[0]\n",
    "            def deliver():\n",
    "                for n in yarn: yield n\n",
    "            return deliver\n",
    "                \n",
    "        (firstE, firstDir) = planEdges[0]\n",
    "        (firstF, firstRela, firstT) = es[firstE]\n",
    "        if firstDir == -1:\n",
    "            (firstF, firstT) = (firstT, firstF)\n",
    "            firstRela = converse[firstRela]\n",
    "        firstR = relations[firstRela](otypes[firstF], otypes[firstT])\n",
    "        yarnFirstF = yarns[firstF]\n",
    "        yarnFirstT = yarns[firstT]\n",
    "\n",
    "        def deliver():\n",
    "            def stitchOn(stitch, ie):\n",
    "                if ie >= len(planEdges):\n",
    "                    yield stitch\n",
    "                    return\n",
    "                (e, dir) = planEdges[ie]\n",
    "                (f, rela, t) = es[e]\n",
    "                if dir == -1:\n",
    "                    (f, t) = (t, f)\n",
    "                    rela = converse[rela]\n",
    "                r = relations[rela](otypes[f], otypes[t])\n",
    "                yarnF = yarns[f]\n",
    "                yarnT = yarns[t]\n",
    "                n = stitch[-1]\n",
    "                image = r(n)\n",
    "                if type(image) is function:\n",
    "                    nRela = set(m for m in yarnT if image(m))\n",
    "                else:\n",
    "                    nRela = set(image) & yarnT\n",
    "                for m in nRela:\n",
    "                    for s in stitchOn(stitch+(m,), ie+1):\n",
    "                        yield s\n",
    "            \n",
    "            for n in yarnFirstF:\n",
    "                image = firstR(n)\n",
    "                if type(image) is function:\n",
    "                    nRela = set(m for m in yarnFirstT if image(m))\n",
    "                else:\n",
    "                    nRela = set(image) & yarnFirstT\n",
    "                if len(nRela) == 0: # no stitch here\n",
    "                    continue\n",
    "                for m in nRela:\n",
    "                    for s in stitchOn((n, m), 1):\n",
    "                        yield s\n",
    "\n",
    "        self.deliver = deliver\n",
    "                   \n",
    "    def runQuery(self):\n",
    "        if not self.good: return None\n",
    "        indent(level=0, reset=True)\n",
    "        self.spinAtoms()\n",
    "        self.estimateSpreads()\n",
    "        self.spinEdges()\n",
    "        self.stitch()\n",
    "        return self.deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But should it be MQL?\n",
    "\n",
    "Experience shows that MQL may give you a very good first try, \n",
    "until you realize that you may not have queried for all cases.\n",
    "You forgot to query for some elements in a different order.\n",
    "You have not reckoned with gaps.\n",
    "And the query does not give you interesting things from the context with the results.\n",
    "Also, MQL does not work nicely with object types that are scattered through other object types, such as the\n",
    "[*lexeme*](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/otype.html)\n",
    "type.\n",
    "\n",
    "Look in SHEBANQ for examples how unwieldy MQL queries may become.\n",
    "\n",
    "Here is a good example:\n",
    "\n",
    "[Dirk Roorda: Yesh](https://shebanq.ancient-data.org/hebrew/query?version=4b&id=556)\n",
    "\n",
    "```\n",
    "select all objects where\n",
    "[book [chapter [verse\n",
    "[clause\n",
    "    [clause_atom\n",
    "        [phrase\n",
    "            [phrase_atom\n",
    "                [word focus lex=\"JC/\" OR lex=\">JN/\"]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "]]]\n",
    "```\n",
    "\n",
    "Well, this is not too complicated, but the query misses results.\n",
    "See [here](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_mql.html)\n",
    "to see what would be needed to make it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I wonder: do we want a new language?\n",
    "Suppose we make a TFQL, then we need a parser for it,\n",
    "we need to define a syntax, we need to refine the syntax, update the parser, etc.\n",
    "It will become a cumbersome straight-jacket.\n",
    "\n",
    "In our case, we do not have the requirement that non-coders should be able to use TFQL in a stand-alone manner.\n",
    "\n",
    "On the contrary, TFQL should live in a programming environment, and we can take advantage of that.\n",
    "\n",
    "Here are initial thought for **tfQuery**, a query *mechanism* inside TF, not a *language*.\n",
    "\n",
    "* tfQuery defines queries as data structures in Python, more precisely: as a graph\n",
    "* it does not matter how you build up a query, tfQuery processses the value of a datastructure\n",
    "  that you pass to it. The surface syntax will not be seen by tfQuery\n",
    "* a query is a graph representation where the nodes are things like\n",
    "  \n",
    "  `('phrase', dict(det='und'))`\n",
    "  \n",
    "  or\n",
    "  \n",
    "  `('word', dict(sp='verb', gn='f', ps='3f'))`\n",
    "\n",
    "* the edges specify relations between the nodes, like: *is contained in*, *follows*,\n",
    "  *precedes*\n",
    "  \n",
    "In MQL you also specify a graph, by means of a template, but this template forces you to *overspecify*: the template often implies more constraints then you really want.\n",
    "\n",
    "So how do we specify edges? As constraints.\n",
    "\n",
    "Let us formulate a query for\n",
    "\n",
    "* clauses that are object clauses\n",
    "* containing two phrases (both undetermined)\n",
    "* one of which contains a verb in the third person feminine\n",
    "* and the other phrase contains a feminine, plural noun\n",
    "\n",
    "In MQL\n",
    "\n",
    "```\n",
    "[clause rela='Objc'\n",
    "    [phrase det='und'\n",
    "        [word sp='verb' AND gn='f' AND ps='p3']\n",
    "    ]\n",
    "    [phrase det='und'\n",
    "        [word sp='subs' AND gn='f' AND nu='pl']\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "Here is how we are going to do it,\n",
    "and note that we are going to write executable code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ('clause', dict(rela='Objc'))\n",
    "p1 = ('phrase', dict(det='und'))\n",
    "p2 = ('phrase', dict(det='und'))\n",
    "w1 = ('word', dict(sp='verb', gn='f', ps='p3'))\n",
    "w2 = ('word', dict(sp='subs', gn='f', nu='pl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes = [c, p1, p2, w1, w2]\n",
    "edges = [\n",
    "    (c, [p1,p2]),\n",
    "    (p1, [w1]),\n",
    "    (p2, [w2]),\n",
    "    (p1, p2),\n",
    "]\n",
    "\n",
    "query = (nodes, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An edge of like `(x, [y,z])` means that `y` and `z` are embedded in `x`, but does not mean\n",
    "that `y` comes before `z`.\n",
    "\n",
    "An edge like `(x, y)` means that `x` comes before `y`.\n",
    "\n",
    "## Increased flexibility\n",
    "\n",
    "Note that it is very easy to remove the `(p1, p2)` condition, which states that the first\n",
    "phrase comes before the second one.\n",
    "\n",
    "If we wanted to do that in MQL, the query would become:\n",
    "\n",
    "```\n",
    "[clause rela='Objc'\n",
    "    [phrase det='und'\n",
    "        [word sp='verb' AND gn='f' AND ps='p3']\n",
    "    ]\n",
    "    [phrase det='und'\n",
    "        [word sp='subs' AND gn='f' AND nu='pl']\n",
    "    ]\n",
    "    OR\n",
    "    [phrase det='und'\n",
    "        [word sp='subs' AND gn='f' AND nu='pl']\n",
    "    ]\n",
    "    [phrase det='und'\n",
    "        [word sp='verb' AND gn='f' AND ps='p3']\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "This goes quickly out of hand, see e.g.\n",
    "[Dirk Roorda: Object clauses of verbless mothers](https://shebanq.ancient-data.org/hebrew/query?id=984) and accompanying\n",
    "[notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/shebanq/VerblessMothers.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query results\n",
    "\n",
    "What should we return as query results?\n",
    "Do we want every instantiation of the nodes that satisfy the criteria?\n",
    "\n",
    "That can become overwhelming. \n",
    "If for example you search for a word in a book, an other word in the same book, and a third word in the same book without further constraints, then for a book with 10,000 words you'll get 10,000 * 10,000 * 10,000 results or 1 Tera results, which is, even for a computer, a bit much.\n",
    "\n",
    "This is why Ulrik invented the sheaf.\n",
    "\n",
    "Our way of solving this problem could be like this:\n",
    "\n",
    "* we return a collection of node lists: for each node in the query we return the \n",
    "  corresponding node list;\n",
    "* these lists consist of TF nodes which are guaranteed to occur in at least one\n",
    "  instantatiation of the whole graph;\n",
    "* we return nothing else.\n",
    "\n",
    "It is up to the user to pick one of these nodesets and to further process them.\n",
    "If he needs needs context around the result nodes, he can easily draw the info from there.\n",
    "\n",
    "It is even possible to generate the code to get full results from the original query graph.\n",
    "\n",
    "So the real result is two things:\n",
    "\n",
    "* a collection of node lists\n",
    "* a function to look up other result nodes in the context of a given result node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "How could we implement this search efficiently?\n",
    "\n",
    "First idea:\n",
    "\n",
    "* build for each query graph node\n",
    "  (which corresponds to a local feature condition on an object)\n",
    "  the set of nodes that satisfy the condition.\n",
    "  This is the easy part:\n",
    "  A single walk over all nodes could construct these sets in one go, in a fraction\n",
    "  of a second;\n",
    "* then work through all edges, where every edge is an instruction to weed out non-results\n",
    "  from the earlier obtained sets.\n",
    "  \n",
    "How would that work, filtering along an edge in the graph?\n",
    "\n",
    "Suppose there is an edge from node1 to node2 (in the query graph).\n",
    "This edge specifies a relationship between nodes in the result nodeset of n1 and nodes \n",
    "in the result nodeset of n2.\n",
    "\n",
    "In English: such an edge says: hey TF node in result set of n1: \n",
    "do you have a parent (or child, or older brother or younger sister)\n",
    "that occurs in the result of n2?\n",
    "\n",
    "If so: you can stay. If not: you're OUT.\n",
    "So this weeds out TF nodes from the result set of n1.\n",
    "\n",
    "But we can also reduce the nodes in the result set of n2.\n",
    "Every TF node in the result set of n2 that does not figure as the parent\n",
    "(or child or brother/sister) of a TF node in the result set of n1 is also out.\n",
    "\n",
    "In this way we can take every edge, one by one, and perform the filtering.\n",
    "This is also a fast operation, provided we can make the elementary relationship checks\n",
    "quickly. (And we can, in TF, thanks to precomputing).\n",
    "\n",
    "When we have done all edges, we probably have to iterate again over all edges.\n",
    "\n",
    "Because an edge between n2 and n3 could have weeded out additional TF nodes from the \n",
    "result set of n2, and this influences the validity of the TF nodes in the result set of n1.\n",
    "\n",
    "Probably we have to repeat until the result sets do not change anymore.\n",
    "\n",
    "Or we can find a way to order the edges, so that we can do them in one or two passes.\n",
    "\n",
    "Maybe we need a bit of math here.\n",
    "\n",
    "My gut feeling is that this is all very doable and that it corresponds to people's query\n",
    "needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "We specify our algorithm in a strategy-independent way and prove the correctness of it.\n",
    "In order to do that, we need a bit of language.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "### Query graph\n",
    "A query is a graph of query nodes and query edges.\n",
    "We refer to the query nodes as\n",
    "$$q_1, ... , q_n$$\n",
    "and to the query edges as\n",
    "$$e_1, ... , e_k$$\n",
    "where every\n",
    "$$e_j = (R_j, q_f, q_t)$$\n",
    "for some $f$ and $t$ in $1, ..., n$, \n",
    "where $R_j$ is a relationship between text nodes.\n",
    "That means, if $s$ and $u$ are text nodes, $sR_ju$ is either *true* or *false*.\n",
    "\n",
    "### Query node\n",
    "A query node $q_i$ consists of an\n",
    "[otype](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/otype)\n",
    "of text nodes and a \n",
    "[features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html)\n",
    "dict.\n",
    "The features dict specifies any number of data features, with an allowed value or set of allowed values for each of them.\n",
    "\n",
    "### Wool and yarn\n",
    "The set of all text nodes of an\n",
    "[otype](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/otype)\n",
    "is called the *wool* of that otype.\n",
    "\n",
    "The wool of a query node, is the wool of the otype that is associated with that query\n",
    "node.\n",
    "\n",
    "If we filter the wool of a node by some process, we call the result set a *yarn*\n",
    "of that query node.\n",
    "\n",
    "The *principal* yarn of a query node consists of the wool of the query node,\n",
    "filtered by the features dict of the query node.\n",
    "\n",
    "### Spinning and proper yarns\n",
    "If we have an edge $e_j = (R_j, q_f, q_t)$ and if we have yarns $y_f, y_t$ for\n",
    "$q_f, $q_t, then we *turn* the edge $e_j$ \n",
    "to *spin* the yarns $y_f$, $y_t$ further.\n",
    "\n",
    "The result of this spinning are two new yarns, $z_f, z_t$, obtained as follows:\n",
    "\n",
    "* we iterate through the text nodes $s$ in yarn $y_i$:\n",
    "  * we compute the set of text nodes $u$ in $y_j$ such that $sRu$ holds;\n",
    "  * if this set is non-empty, we:\n",
    "    * add those $u$ text nodes to $z_j$\n",
    "    * add the $s$ text node to $z_i$\n",
    "  Otherwise we do not add nodes to $z_i$ nor to $z_j$.\n",
    "\n",
    "So spinning means thinning out the yarns of two query nodes that are connected to a\n",
    "query edge, in such a way, that afterwards the relationship that is associated\n",
    "with the edge, can be realized for every text node in both yarns.\n",
    "In other words: after spinning, every text node in a yarn involved, has at least one\n",
    "counterpart text node in the other yarn such that both are in relationship which each other.\n",
    "That is, the relationship belonging to the edge.\n",
    "\n",
    "So, every *spinning* action is caused by a *turn* of the edge, which acts as the spinning\n",
    "wheel.\n",
    "\n",
    "A *proper yarn* of a query node is either its *principal yarn* or any yarn that can be spun \n",
    "from an other proper yarn.\n",
    "So if we start with the principal yarn of a query node, and start spinning, we produce\n",
    "proper yarns for that query node.\n",
    "\n",
    "### Stitching results\n",
    "\n",
    "A *result* of a query is a list of text nodes\n",
    "$$s_1, ... , s_n$$\n",
    "satisfying the following conditions:\n",
    "\n",
    "* for every $i$ in $1, ... , n$: $s_i$ is in the principal yarn of $q_i$\n",
    "* for every $e_j = (R_j, q_f, q_t)$: $s_fRs_t$\n",
    "\n",
    "If this is the case, we call text node $s_i$ a *stitch* for query node $q_i$. \n",
    "\n",
    "A result can be seen as a stitching together of the principal yarns of the query nodes,\n",
    "in such a way, that all constraints specified by the query edges are satisfied.\n",
    "The stitches are those nodes in the text, one for each yarn, that together constitute\n",
    "a result.\n",
    "\n",
    "### Overspun and underspun yards\n",
    "\n",
    "If a yarn of a query node has become so thin, that it fails to contain stitches of\n",
    "of the result of the query,\n",
    "we call the yarn *overspun*.\n",
    "\n",
    "If a yarn of a query node contains nodes that are not stitches of any result of the query,\n",
    "we call the yarn *underspun*.\n",
    "\n",
    "### Employ the terminology\n",
    "\n",
    "We can now use this fabric language to formulate our query algorithm and prove essential properties of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial\n",
    "\n",
    "Here is the preparation for running a sequence of iterations.\n",
    "In those iterations we *turn* the edges to *spin* yarns.\n",
    "\n",
    "First of all, for every query node, we collect its wool, and spin its principal yarn.\n",
    "\n",
    "> we compute edge constraints and filter the result sets of text nodes.\n",
    "\n",
    "> we collect all text nodes of the otype associated with the query node\n",
    "(the wool), and we apply the criteria defined by the features of the query node\n",
    "(from wool to principal yarn).\n",
    "\n",
    "We give every edge a the status: *not up-to-date*.\n",
    "The intention is to give an edge the status *up-to-date* if it has just been turned,\n",
    "and no turnings of other edges have since interfered with the yarns involved,\n",
    "so the yarns still reflect the effects of turning the edge.\n",
    "\n",
    "Initially, no edge has been turned, so every edge starts out as not up-to-date.\n",
    "\n",
    "\n",
    "## Iterations\n",
    "When query edges are computed, yarns are spun,\n",
    "some edges become up-to-date, others get *not* up-to-date.\n",
    "\n",
    "* we select a query edge by means of some strategy,\n",
    "* we turn the edge,\n",
    "* we set the status of the edge to up-to-date,\n",
    "* for each of both query nodes of the edge:\n",
    "  * if its yarn has changed:\n",
    "    * for each query edge leading to or from such a query node:\n",
    "      * we set its status to not up-to-date\n",
    "\n",
    "## Final\n",
    "Here is when we stop turning the edges.\n",
    "We stop when continuing does not make sense anymore, and that happens if one\n",
    "of the following conditions occur. \n",
    "Whether we have correct and complete results if we stop, is something that\n",
    "remains to be seen.\n",
    "We'll prove it later.\n",
    "\n",
    "### Stop on empty yarn\n",
    "If a yarn becomes empty, we can stop: there are no results at all. \n",
    "The combined turning of the edges has spun the yard into nothing: the thread has broken.\n",
    "Every yarn must be part of the result, so if a yarn breaks, the result is gone.\n",
    "\n",
    "> The constraints expressed by the query edges are such, that one of the query nodes\n",
    "cannot be instantiated by a text node. Since every query result must instantiate all\n",
    "query nodes by a text node, we do not have a result.\n",
    "\n",
    "### Stop on no change\n",
    "If all edges are up-to-date, it does not make sense to turn edges anymore.\n",
    "So we stop.\n",
    "\n",
    "> Turning an up-to-date edge means recomputing the constraints posed by that edge\n",
    "on node sets on which it has already been computed. There will be no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness and completeness\n",
    "\n",
    "The main questions are: \n",
    "* does the computation stop at all in all cases\n",
    "* and when it stops, have we got what we want?\n",
    "\n",
    "This fundamental question can be split into lesser ones.\n",
    "\n",
    "### Note about strategy\n",
    "The validity of the results delivered should not dependent on\n",
    "the strategy of selecting edges for computation. \n",
    "But it is easy to come up with a stupid strategy that will not terminate\n",
    "or not arrive at desired results.\n",
    "For example, if we always select the first edge,\n",
    "it is clear that this is in general a very unreliable strategy.\n",
    "\n",
    "So we suppose that our strategy of edge selection for computation satisfies a minimal\n",
    "sanity requirement. \n",
    "We can formulate that exactly:\n",
    "\n",
    "A strategy is *thorough* if it always selects a non-up-to-date edge if there is one.\n",
    "\n",
    "This is really a minimal criterion, since selecting an up-to-date edge is always a waste\n",
    "of time.\n",
    "And if there are no up-to-date edges, it is time to stop anyway.\n",
    "\n",
    "### Note on results\n",
    "The algorithm as defined above does not deliver a list of *results*, \n",
    "results being *stitches*. \n",
    "It only delivers the yarns to be stitched.\n",
    "\n",
    "> The algorithm yields a list of text node sets\n",
    "instantiations of all query nodes by text nodes in such a way that all query edges\n",
    "are satisfied. Which combinations of these text nodes constitute results, remains to \n",
    "be established.\n",
    "\n",
    "Instead, we assert that after the process, every yarn is *fully spun*, so\n",
    "none of them is *underspun* and none of them is *overspun*.\n",
    "\n",
    "> The claim is that every node set has only nodes that occur in the real results, and that\n",
    "all nodes that occur in the real results, do occur in the proper node sets.\n",
    "\n",
    "## Lemma 1: Termination\n",
    "**Under every *thorough* strategy, the iteration always terminates.**\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "If an edge is turned, no yarn becomes longer. It is possible, however, that no yarn\n",
    "shrinks.\n",
    "\n",
    "For every turn of an edge there are two cases to consider:\n",
    "\n",
    "a. No yarn becomes shorter:\n",
    "   Then the amount of up-to-date edges increases by one.\n",
    "   This is so, because the recently turned edge becomes up-to-date, and no other edges    \n",
    "   become non-up-to-date.\n",
    "b. One or more yarns become shorter.\n",
    "\n",
    "Now suppose that that the sequence of edge turnings never stops.\n",
    "Mark every step as `a` if case a. applies and as `b` if case b. applies.\n",
    "Our infinite turning is then an infinite sequence of `a`s and `b`s.\n",
    "\n",
    "In that sequence there cannot be infinitely many `b`s, because every `b` corresponds to\n",
    "a shortening of the total amount of yarn, and the total length of yarn is finite.\n",
    "\n",
    "So there must be infinitely many `a`s.\n",
    "That means that from some point onwards, we see only `a`s and never a `b` anymore.\n",
    "But every `a` decreases the number of non-up-to-date edges, so a sequence of `a`s is\n",
    "bound to stop.\n",
    "\n",
    "Ergo: the sequence cannot be infinite and the computation terminates.\n",
    "\n",
    "## Lemma 2: No overspinning\n",
    "**Turning of edges does not cause overspinning: no yarn gets overspun.**\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Suppose none of the yarns is overspun yet, and we turn an edge\n",
    "$e = (R_j, q_f, q_t)$ on yarns $y_f, y_t$ resulting in yarns $z_f, z_t$.\n",
    "\n",
    "We assume $y_f$ and $y_t$ are not overspun.\n",
    "\n",
    "**Case a: Suppose $z_f$ is overspun.**\n",
    "\n",
    "(i) Then there must be a result text node $s$ of query node $q_f$ in $y_f$ \n",
    "that does not reappear in $z_f$.\n",
    "\n",
    "Because $s$ is in the result, there must be a text node $u$ in the result\n",
    "of query node $q_t$, such that $sR_ju$.\n",
    "\n",
    "That means that $u$ belongs to every yarn of $q_t$ that is not overspun.\n",
    "By hypothesis, $y_t$ is not overspun, so $u$ is member of $y_t$.\n",
    "\n",
    "By following the definition of turning an edge,\n",
    "we see that the existence of this $u$ will cause $s$ to be added to $z_f$\n",
    "which conflicts with (i).\n",
    "\n",
    "So case a. does not happen.\n",
    "\n",
    "**Case b: Suppose $z_t$ is overspun.**\n",
    "\n",
    "(ii) Then there must be a result text node $u$ of query node $q_t$ in $y_t$ \n",
    "that does not reappear in $z_t$.\n",
    "\n",
    "Because $u$ is in the result, there must be a text node $s$ in the result\n",
    "of query node $q_f$, such that $sR_ju$.\n",
    "\n",
    "That means that $s$ belongs to every yarn of $q_f$ that is not overspun.\n",
    "By hypothesis, $y_f$ is not overspun, so $s$ is member of $y_f$.\n",
    "\n",
    "By following the definition of turning an edge,\n",
    "we see that the existence of this $s$ will cause $u$ to be added to $z_t$\n",
    "which conflicts with (ii).\n",
    "\n",
    "**No more cases: $z_f$ and $z_t$ are not overspun. QED**\n",
    "\n",
    "## Corollary 3: Empty yarn means no results\n",
    "**If, after some turning, a yarn becomes empty, the query as a whole has no results.**\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "If at some point a yarn, say $y_i$ becomes empty, we know by lemma 2, that it is\n",
    "still not overspun.\n",
    "That means that all nodes that occur in a result and that correspond with query node\n",
    "$q_i$, are contained in this empty yarn $y_i$. \n",
    "\n",
    "Hence there are no results, QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition: Connected components\n",
    "A connected component of a graph is a subgraph satisfying two conditions:\n",
    "* **connectedness:** \n",
    "  there is a bridge of edges between each pair of nodes, meaning that for ech pair of\n",
    "  nodes $q_0$ and $q_{n+1}$ there is a set of nodes\n",
    "  $q_1, ... , q_n$ in the subgraph such that for each $i$ in $0, ..., n$:\n",
    "  * there is either an edge from $q_i$ to $q_{i+1}$ or the other way round;\n",
    "* **maximal:**\n",
    "  every other subgraph that properly contains this subgraph, does not have the\n",
    "  **connectedness** property.\n",
    "  \n",
    "## Lemma 3: Decomposition\n",
    "Every graph can be divided into a number of disjunct connected components.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "See the literature on graph theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma 4: Decomposing queries\n",
    "**The results of a query is essentially the set consisting of the cartesian product of the \n",
    "query results of its connected component queries.**\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "We prove first:\n",
    "\n",
    "### Lemma 4a: \n",
    "**If a query can be split into two subparts between which there are no edges,\n",
    "then the results of the whole query can be seen as the cartesian product of \n",
    "the results of the two subparts.**\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Say the query is a graph $G$ consisting of\n",
    "$G_1 = (Q_1, E_1)$ and $G_2 = (Q_2, E_2)$,\n",
    "with no edges betweem $Q_1$ and $Q_2$ and\n",
    "with result sets $R_1$ and $R_2$.\n",
    "\n",
    "Say there are $m_1$ nodes in $Q_1$ and $m_2$ in $Q_2$.\n",
    "\n",
    "Then $R_1$ is a set of $m_1$ tuples of text nodes,\n",
    "and $R_2$ is a set of $m_2$ tuples of text nodes.\n",
    "\n",
    "Every $r_1$ in $R_1$ is a $m_1$-tuple of text nodes satisfying $G_1$.\n",
    "\n",
    "Every $r_2$ in $R_2$ is a $m_2$-tuple of text nodes satisfying $G_2$.\n",
    "\n",
    "Then, for every combination of such an $r_1$ and $r_2$,\n",
    "the concatenation of $r_1$ and $r_2$ a $m_1+m_2$-tuple of text nodes.\n",
    "\n",
    "The first $m_1$ text nodes instantiate the query nodes of $G_1$\n",
    "and satisfy all edge constraints of $G_1$. \n",
    "\n",
    "They do not have to satisfy the query nodes in $G_2$\n",
    "in order to be valid for $G$ as a whole,\n",
    "because they do not correspond to them.\n",
    "\n",
    "They are not influenced by the constraints of the edges in $G_2$,\n",
    "because these edges do not reach the nodes of $G_1$.\n",
    "\n",
    "Analogous for the last $m_2$ nodes in such a combination.\n",
    "\n",
    "Hence the combination is a result of the whole graph.\n",
    "\n",
    "Conversely, every result of the whole graph is a tuple that can be decomposed in an $r_1$\n",
    "which is a result of $G_1$ and an $r_2$ which is a result of $G_2$.\n",
    "\n",
    "**QED (lemma 4a)**\n",
    "\n",
    "Now we can prove by induction on the number of connected components of a query\n",
    "that its results can be seen as the cartesian product \n",
    "of the results of its connected components.\n",
    "\n",
    "**Case a:**\n",
    "\n",
    "The query consists of just one connected component: the lemma is trivially true.\n",
    "\n",
    "**Case b:**\n",
    "The query consists of $n+2$ connected components, $n > 0$.\n",
    "\n",
    "We assume by way of induction hypothesis\n",
    "that the lemma holds for all cases up to $n+1$.\n",
    "\n",
    "Say $G = (G_1 + ... + G_{n+2}, E_1, ... , E_{n+2})$\n",
    "with result set $R$, \n",
    "where each \n",
    "$(G_i, E_i)$ is a connected component with result set $R_i$.\n",
    "\n",
    "Say $H = (G_1 + ... + G_{n+1}, E_1, ... , E_{n+1})$\n",
    "then we know by induction hypothesis that the results of H are\n",
    "$R_1 \\times ... \\times R_{n+1}$.\n",
    "\n",
    "Now $H$ and $G_{n+2}$ satisfy the conditions of lemma 4a, to the results of\n",
    "\n",
    "$G = H + (G_{n+2}, E_{n+2})$\n",
    "\n",
    "is $(R_1 \\times ... \\times R_{n+1}) \\times R_{n+2}$\n",
    "\n",
    "is $R_1 \\times ... \\times R_{n+1} \\times R_{n+2}$\n",
    "\n",
    "**QED (lemma 4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Cyclic queries\n",
    "A *cycle* in a query is a list of query nodes connected by edges, where the first node is equal to\n",
    "the last node. \n",
    "Here the edges are taken in the directional sense.\n",
    "\n",
    "### Examples\n",
    "Here is a graph with a cycle:\n",
    "$$G = (\\{q_1, q_2\\}, \\{(R, q_1, q_2), (S, q_2, q_1)\\})$$\n",
    "But this is not a cycle:\n",
    "$$G = (\\{q_1, q_2\\}, \\{(R, q_1, q_2), (S, q_1, q_2)\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma 5: Eventual full-spinning, with conditions\n",
    "**For queries consisting of just one connected component:**\n",
    "\n",
    "**if the query does not contain cycles:**\n",
    "\n",
    "**if, after some spinning, all edges are up-to-date, then all yarns are fully spun.**\n",
    "\n",
    "**Proof:**\n",
    "Because the yarns are never overspun by lemma 2,\n",
    "we must prove that when all edges are up-to-date,\n",
    "none of the yarns are *underspun*.\n",
    "\n",
    "Suppose we are in the situation that all edges are up-to-date.\n",
    "Suppose that in this situation there is an underspun yarn $y_i$, associated\n",
    "with query node $q_i$.\n",
    "That means that $y_i$ contains a text node $s$ that does not belong to any final result.\n",
    "\n",
    "Let us have a closer look at how $q_i$ lies hooked up in the query graph.\n",
    "Because the query is a connected component, we have only these possibilities:\n",
    "\n",
    "**Case a: $q_i$ does not belong to an edge.**\n",
    "\n",
    "In this case, $q_i$ must the only node, because if there were other nodes,\n",
    "there would have been bridges from $q_i$ to those other nodes, and hence\n",
    "$q_i$ would be involved in edges.\n",
    "\n",
    "So, the query is just one node without edges, hence $y_i$ is the primary yarn,\n",
    "and hence all nodes in it are results. So this $s$ cannot exist.\n",
    "\n",
    "**Case b: $q_i$ has an out-going edge or an incoming edge.**\n",
    "\n",
    "For all $q_i$-outgoing edges $(R_j, q_i, q_t)$ it holds that there\n",
    "is a $u$ in the yarn $y_t$ of $q_t$ such that $sR_ju$.\n",
    "This is because all edges are up-to-date.\n",
    "\n",
    "Like wise, for all $q_i$-incoming edges $(R_k, q_f, q_i)$ it holds that\n",
    "there is a $u$ in the yarn $y_f$ of $q_f$ such that $uR_ks$.\n",
    "\n",
    "If all $u$s found in this way belonged to the result, then $s$ would also belong\n",
    "to the result, because all its edge constraints would be satisfied.\n",
    "\n",
    "So, if, as we assumed, $s$ is not in the result, then at least one of those $u$s\n",
    "does not belong to the result.\n",
    "\n",
    "Then we can repeat the same argument for this $u$, and find a $v$ that does not belong\n",
    "to the result.\n",
    "\n",
    "Since there are finitely many nodes, we find a cycle $s_1, ... , s_k$ of text\n",
    "nodes in yarns $y_1, ... , y_k$ that do not belong to the result.\n",
    "\n",
    "This contradicts the assumption. QED.\n",
    "\n",
    "## Example 5b: Cycles can prevent full spinning.\n",
    "In order to show that the condition of *no cycles* in Lemma 5 cannot be missed,\n",
    "we show a small example of a graph with a cycle that will not become fully spun\n",
    "by the algorithm.\n",
    "\n",
    "$$G = (\\{q_1, q_2\\}, \\{(R, a, c), (R, b, d), S(c, b), S(d, a)\\})$$\n",
    "\n",
    "with primary yarns $y_1, y_2$ for $q_1, q_2$ as follows:\n",
    "\n",
    "$$y_1 = \\{a, b\\}$$\n",
    "$$y_2 = \\{c, d\\}$$\n",
    "\n",
    "If we spin the edges $R$ and $S$, nothing happens, because\n",
    "the elements $a, b \\in y_1$ are happily in relationship $R$ with elements $c, d$ in $y_2$,\n",
    "and\n",
    "the elements $c, d \\in y_2$ are happily in relationship $S$ with elements $a, b$ in $y_2$.\n",
    "\n",
    "Yet, none of them are part of any result, because there are no results, because the equation\n",
    "\n",
    "$$xRySx$$\n",
    "\n",
    "does not have any solutions.\n",
    "\n",
    "## Analysis\n",
    "The trouble with a cycle is, that it creates a long distance dependency in the final results\n",
    "that cannot be \"felt\" by spinning.\n",
    "\n",
    "You can also have long distance dependencies by confluence, but these will be eventually solved by just spinning.\n",
    "\n",
    "For example: if we have to solve both of the following\n",
    "\n",
    "$$uRvSwTxUk$$\n",
    "$$aGbHcKmUk$$\n",
    "\n",
    "then we could solve both chains separately, and if both have a solution, we now that the combined solution is a complete solution.\n",
    "Hence, if there is no solution, one of the two will have no solution.\n",
    "\n",
    "In case of the cycle, we are essentially trying to solve\n",
    "\n",
    "$$xRySz$$\n",
    "$$x = z$$\n",
    "\n",
    "where $x = z$ is the long distance dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result fetching\n",
    "\n",
    "Spinning is just part of the solution.\n",
    "It thins out the space in which solutions exist, and does a better job for graphs without\n",
    "cycles than for graphs with cycles.\n",
    "\n",
    "But after spinning, we have to find the valid stitches between the yarns.\n",
    "\n",
    "That is a separate search process. \n",
    "The challenge is to wade through all possibilities in an efficient way.\n",
    "\n",
    "For graphs without cycles we know that if we start with any text node in any yarn, it will\n",
    "be part of a result.\n",
    "And if we start stitching that node to a node in the next yarn, we are sure that the\n",
    "pair is also part of a result. And so on. So in this case we can stitch arbitrarily (note\n",
    "that stitching implies that we hop from one yarn to the other by following the appropriate\n",
    "relationships between text nodes), and always generate results.\n",
    "\n",
    "In this case, it does not matter much in what order we stitch, because we need to make all stitchings anyway to generate results.\n",
    "\n",
    "If the graph does have cycles, then we will discover that certain partial stitichings will\n",
    "lead to no valid results.\n",
    "Because of that strategy becomes important, because if we are not smart, we could spend a lot\n",
    "of time in rejecting millions of stitchings, before arriving at the first valid stitiching.\n",
    "\n",
    "Here is an example, and let us become a bit more concrete.\n",
    "\n",
    "Suppose our graph has nodes: *Sentence*, *Word1*, *Word2*, all without feature restrictions.\n",
    "So the primary yarns are all sentences, all words, all words, respectively.\n",
    "Suppose we have 100,000 sentences, all 10 words, so 1 million words in total.\n",
    "\n",
    "Suppose our graph has edges:\n",
    "\n",
    "* word1 is in sentence\n",
    "* word2 is in sentence\n",
    "* word1 comes before word2\n",
    "\n",
    "A solution is a tuple of text nodes $s$, $w_1$, $w_2$, such that $w_1$ is a word\n",
    "in sentence $s$, $w_2$ is a word in the same sentence $s$, and $w_1$ comes before $w_2$.\n",
    "\n",
    "Let's start stitching by first picking word1, then word2, then sentence.\n",
    "\n",
    "We start to pick the first word $w_1$ in the yarn of word1, which is all words>\n",
    "\n",
    "Then we pick a word $w_2$ in the yarn of word2, such that $w_2$ comes after $w_1$.\n",
    "\n",
    "Note that we have 999,999 possibilities for $w_2$, of which only the first 10 are part of\n",
    "a result. All others are not in the same sentence.\n",
    "\n",
    "So if we try out all these $w_2$s, we get 10 results fairly quickly, and then 999,990 spurious tries, before we try a new node in the yarn of *Word1* and get new results.\n",
    "\n",
    "By the time we have collected all results,\n",
    "we have visited a million times on average half a million words, or\n",
    "$1,000,000 \\times 500,000 = 500,000,000,000$ words.\n",
    "\n",
    "We could have done it in an other way: after picking $w_1$ from the yarn of *Word1*,\n",
    "we pick a sentence $s_1$ from the yarn of *Sentence* (only one possibility), and from\n",
    "there we pick a $w_2$ from the yarn of *Word2* such that $w_2$ is in $s_1$ (only 10 possibilities). Of those 10 possibilities, one will be rejected, (where $w_2 = w_1$).\n",
    "\n",
    "After this, we move to the next $w_1$ in the yarn of $Word1$, which is the second word in the same sentence, pick the same $s_1$ in the sentence yarn, pick the same words in the *Word2*\n",
    "yarn, reject 2 of them, and deliver 8 results.\n",
    "And so on.\n",
    "When we have gone through the whole of $s_1$, we have tried 100 words, and rejected 50 of them. \n",
    "And so it goes for all sentences.\n",
    "In the end we find all 100,000 * 50 = 5,000,000 solutions by visiting 10,000,000 words.\n",
    "\n",
    "This is a 50,000 fold improvement!\n",
    "\n",
    "So how can we build a strategy from this observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitching strategy\n",
    "\n",
    "Consider the relationships by which we hop from one yarn to another, while stitching.\n",
    "Some of them are functional, in the sense that coming from node, they leave only one possibility for the next node in the stitching.\n",
    "\n",
    "Take for example the relation: *sentence of*. Coming from a word, this relationship leaves\n",
    "us but one choice: the one sentence of which that word is a part.\n",
    "\n",
    "Other relationships are virtually the opposite of functional: they leave very many options.\n",
    "\n",
    "Take for example the relation: *comes after* between words. For any word you have on average the choice of half the total amount of words.\n",
    "\n",
    "Some relationships that are not functional are still functional in the opposite direction.\n",
    "\n",
    "Take for example: *contains* between sentences and words. A sentence contains multiple words,\n",
    "so *contains* is not functional. But going into the other direction, is the precisely the\n",
    "relation: *sentence of*, which is functional.\n",
    "\n",
    "Between functional and non-functional relations there is a spectrum of *functionalness*.\n",
    "Let us call this notion the *spread* of a relation.\n",
    "\n",
    "### Definition: spread\n",
    "\n",
    "**The spread of a relation $R$ is the average number of $t$ that satisfies the\n",
    "equation $fRt$ for each $f$.**\n",
    "\n",
    "In other words, for every *from* node, compute to how many *to* nodes $R$ brings you.\n",
    "Take the average, and that is your spread.\n",
    "\n",
    "## Back to stichting\n",
    "\n",
    "When stitching, we want to follow the query graph in such a way that we hop from yarn to\n",
    "yarn by edges with relations with lesser spread first.\n",
    "\n",
    "This could be a way:\n",
    "\n",
    "1. compute the spread of all relations in the query graph, and all their converses too\n",
    "2. for every relation, consider whether it or its converse has the lesser spread\n",
    "   and if so, replace the relation by its converse in the opposite direction\n",
    "3. for each node, define the its spread as the sum of the spreads of its outgoing\n",
    "   relations\n",
    "4. start with a node that has minimal spread among nodes with outgoing edges\n",
    "5. go from that node via outgoing edge to a node with minimum spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contained = 0\n",
    "embeds = 1\n",
    "before = 2\n",
    "after = 3\n",
    "unequal = 4\n",
    "overlap = 6\n",
    "\n",
    "bk = ('book', dict())\n",
    "ch = ('chapter', dict())\n",
    "vs = ('verse', dict())\n",
    "cl = ('clause', dict())\n",
    "ca = ('clause_atom', dict())\n",
    "ph = ('phrase', dict())\n",
    "pa = ('phrase_atom', dict())\n",
    "w1 = ('word', dict(lex={'JC/', '>JN/'}))\n",
    "w2 = ('word', dict(sp='verb'))\n",
    "\n",
    "p2 = ('phrase', dict(det='und'))\n",
    "w3 = ('word', dict(sp='subs', gn='f', nu='pl'))\n",
    "w4 = ('word', dict(sp='adjv', gn='f', nu='pl'))\n",
    "\n",
    "c2 = ('clause', dict())\n",
    "p3 = ('phrase', dict())\n",
    "p4 = ('phrase', dict())\n",
    "w5 = ('word', dict())\n",
    "w6 = ('word', dict())\n",
    "w7 = ('word', dict())\n",
    "\n",
    "nodes1 = [bk, ch, vs, cl, ca, ph, pa, w1, w2]\n",
    "edges1 = [\n",
    "    [1, contained, 0],\n",
    "    [2, contained, 1],\n",
    "    [3, contained, 2],\n",
    "    [4, contained, 3],\n",
    "    [5, contained, 4],\n",
    "    [6, contained, 5],\n",
    "    [7, contained, 6],\n",
    "    [8, after, 7],\n",
    "    [8, contained, 3],\n",
    "]\n",
    "\n",
    "nodes2 = [p2, w3, w4]\n",
    "edges2 = [\n",
    "    [1, contained, 0],\n",
    "    [2, contained, 0],\n",
    "    [1, before, 2],\n",
    "]\n",
    "nodes3 = [p3, p4, w5, w6, w7, c2]\n",
    "edges3 = [\n",
    "    [2, contained, 0],\n",
    "    [4, contained, 0],\n",
    "    [3, contained, 1],\n",
    "    [2, before, 3],\n",
    "    [3, before, 4],\n",
    "    [0, contained, 5],\n",
    "    [1, contained, 5],\n",
    "    [0, unequal, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Search template OK: 6 search nodes and 8 search edges\n",
      "plan with 6 nodes and 8 edges\n",
      "node  0-phrase        (253174 in yarn)\n",
      "node  1-phrase        (253174 in yarn)\n",
      "node  2-word          (426581 in yarn)\n",
      "node  3-word          (426581 in yarn)\n",
      "node  4-word          (426581 in yarn)\n",
      "node  5-clause        ( 88000 in yarn)\n",
      "edge  4-word          = in  =>  0-phrase        @       1\n",
      "edge  0-phrase        = in  =>  5-clause        @       1\n",
      "edge  0-phrase        = has =>  2-word          @     2.4\n",
      "edge  5-clause        = has =>  1-phrase        @       3\n",
      "edge  0-phrase        = <>  =>  1-phrase        @2.53e+05\n",
      "edge  1-phrase        = has =>  3-word          @     1.8\n",
      "edge  3-word          = <<  =>  4-word          @1.69e+05\n",
      "edge  3-word          = >>  =>  2-word          @ 2.3e+05\n",
      "  6.98s Search prepared: total amount of yarn: 1874091\n"
     ]
    }
   ],
   "source": [
    "S.runSearch(nodes3, edges3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('phrase', 'phrase', 'word', 'word', 'word', 'clause')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605793, 605794, 1159, 1160, 1161, 426799)\n",
      "(605793, 605794, 1159, 1160, 1162, 426799)\n",
      "(605793, 605794, 1159, 1160, 1163, 426799)\n",
      "(605793, 605794, 1159, 1160, 1164, 426799)\n",
      "(606150, 606151, 1720, 1721, 1722, 426921)\n",
      "(606150, 606151, 1720, 1721, 1723, 426921)\n",
      "(607746, 607747, 4819, 4824, 4825, 427418)\n",
      "(607746, 607747, 4819, 4821, 4825, 427418)\n",
      "(607746, 607747, 4819, 4822, 4825, 427418)\n",
      "(607746, 607747, 4819, 4823, 4825, 427418)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for s in S.results():\n",
    "    i += 1\n",
    "    print(s)\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s fetching results\n",
      "(605793, 605794, 1159, 1160, 1161, 426799)\n",
      "(606150, 606151, 1720, 1721, 1722, 426921)\n",
      "(607746, 607747, 4819, 4824, 4825, 427418)\n",
      "(607746, 607747, 4819, 4821, 4825, 427418)\n",
      "(607746, 607747, 4819, 4822, 4825, 427418)\n",
      "(607746, 607747, 4819, 4823, 4825, 427418)\n",
      "(608322, 608323, 5803, 5805, 5807, 427601)\n",
      "(608322, 608323, 5803, 5806, 5807, 427601)\n",
      "(608369, 608370, 5868, 5869, 5871, 427616)\n",
      "(608369, 608370, 5868, 5870, 5871, 427616)\n",
      " 1m 40s 7699 condensed to 751 results\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "seen = set()\n",
    "indent(reset=True)\n",
    "info('fetching results')\n",
    "for s in S.results():\n",
    "    if s[3] not in seen:\n",
    "        seen.add(s[3])\n",
    "        if len(seen) <= 10:\n",
    "            print(s)\n",
    "    i += 1\n",
    "info('{} condensed to {} results'.format(i, len(seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20s Getting gapped phrases\n",
      "    25s 373 results\n",
      "(605793, 605794, 1159, 1160, 1164, 426799)\n",
      "(606150, 606151, 1720, 1721, 1723, 426921)\n",
      "(607746, 607747, 4819, 4821, 4828, 427418)\n",
      "(608322, 608323, 5803, 5805, 5809, 427601)\n",
      "(608369, 608370, 5868, 5869, 5875, 427616)\n",
      "(608705, 608706, 6515, 6521, 6530, 427723)\n",
      "(609286, 609287, 7431, 7432, 7437, 427917)\n",
      "(609997, 609998, 8502, 8507, 8520, 428159)\n",
      "(609997, 609999, 8502, 8508, 8520, 428159)\n",
      "(610379, 610380, 9127, 9129, 9133, 428286)\n"
     ]
    }
   ],
   "source": [
    "info('Getting gapped phrases')\n",
    "results = []\n",
    "for c in F.otype.s('clause'):\n",
    "    ps = L.d(c, 'phrase')\n",
    "    for p in ps:\n",
    "        words = L.d(p, 'word')\n",
    "        (bp, ep) = (words[0], words[-1])\n",
    "        for q in ps:\n",
    "            if p == q: continue\n",
    "            bq = L.d(q, 'word')[0]\n",
    "            if bp < bq and bq < ep:\n",
    "                results.append((p, q, bp, bq, ep, c))\n",
    "info('{} results'.format(len(results)))\n",
    "for r in results[0:10]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "escapes = (\n",
    "    '\\\\\\\\',\n",
    "    '\\\\ ',\n",
    "    '\\\\t',\n",
    "    '\\\\n',\n",
    "    '\\\\|',\n",
    "    '\\\\=',\n",
    ")\n",
    "def esc(x):\n",
    "    for (i, c) in enumerate(escapes):\n",
    "        x = x.replace(c, chr(i))\n",
    "    return x\n",
    "def unesc(x):\n",
    "    for (i, c) in enumerate(escapes):\n",
    "        x = x.replace(chr(i), c[1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def syntax(search):\n",
    "    lines = search.split('\\n')\n",
    "    tokens = tokenize(lines)\n",
    "    if tokens != None:\n",
    "        for t in tokens:\n",
    "            print(t)\n",
    "        return tokens\n",
    "    else:\n",
    "        for (i, line) in enumerate(lines):\n",
    "            print('{:>2} {}'.format(i, line))\n",
    "        return None\n",
    "\n",
    "def tokenize(lines):            \n",
    "    operators = '''\n",
    "    = # < > \n",
    "    == && ## || [[ ]] << >>\n",
    "'''.strip().split()\n",
    "    edges = '''\n",
    "    -\\S+> <\\S+-\n",
    "'''.strip().split()\n",
    "\n",
    "    opPat = '|'.join(\n",
    "        tuple(o.\\\n",
    "                replace('&', '\\\\&').\\\n",
    "                replace('|', '\\\\|').\\\n",
    "                replace('[', '\\\\[').\\\n",
    "                replace(']', '\\\\]')\\\n",
    "            for o in operators\n",
    "        )+\\\n",
    "        tuple(edges)\n",
    "    )\n",
    "    atomPat = '(\\s*)([^ \\t=]+)(?:(?:\\s*\\Z)|(?:\\s+(.*)))$'\n",
    "    namePat = '[A-Za-z0-9_-]+'\n",
    "    atomRe = re.compile(atomPat)\n",
    "    relPat = '^\\s*({nm})\\s+({op})\\s+({nm})\\s*$'.format(nm=namePat, op=opPat)\n",
    "    \n",
    "    nameRe = re.compile('^{}$'.format(namePat))\n",
    "    relRe = re.compile(relPat)\n",
    "    whiteRe = re.compile('^\\s*$')\n",
    "    \n",
    "    def getFeatures(x):\n",
    "        features = {}\n",
    "        wrongs = []\n",
    "        featureList = (x if x != None else '').split()\n",
    "        for feat in featureList:\n",
    "            featComps = feat.split('=', 1)\n",
    "            if len(featComps) != 2:\n",
    "                wrongs.append(unesc(feat))\n",
    "                continue\n",
    "            featName = unesc(featComps[0])\n",
    "            featValList = featComps[1]\n",
    "            featVals = set(unesc(featVal) for featVal in featValList.split('|'))\n",
    "            features[featName] = featVals\n",
    "        return (features, wrongs)\n",
    "    \n",
    "    tokens = []\n",
    "    allGood = True\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if line.startswith('#') or whiteRe.match(line): continue\n",
    "        good = False\n",
    "        for x in [True]:\n",
    "            match = relRe.match(line)\n",
    "            if match:\n",
    "                tokens.append((i, 'rel', match.group(1), match.group(2), match.group(3)))\n",
    "                good = True\n",
    "                break\n",
    "            match = atomRe.match(esc(line))\n",
    "            if match:\n",
    "                (indent, atom, features) = match.groups()\n",
    "                atomComps = atom.split(':', 1)\n",
    "                if len(atomComps) == 1:\n",
    "                    name = ''\n",
    "                else:\n",
    "                    name = unesc(atomComps[0])\n",
    "                    atom = unesc(atomComps[1])\n",
    "                    mt = nameRe.match(name)\n",
    "                    if not mt:\n",
    "                        error('Illegal name at line {}: \"{}\"'.format(i, name))\n",
    "                        good = False\n",
    "                (features, wrongs) = getFeatures(features)\n",
    "                if len(wrongs):\n",
    "                    for wrong in wrongs:\n",
    "                        error('Illegal feature specification at line {}: \"{}\"'.format(i, wrong))\n",
    "                    good = False\n",
    "                    break                \n",
    "                tokens.append((i, 'atom', len(indent), name, atom, features))\n",
    "                good = True\n",
    "                break\n",
    "            (features, wrongs) = getFeatures(esc(line))\n",
    "            if len(wrongs):\n",
    "                for wrong in wrongs:\n",
    "                    error('Illegal feature specification at line {}: \"{}\"'.format(i, wrong))\n",
    "                good = False\n",
    "                break                \n",
    "            tokens.append((i, 'feat', features))\n",
    "            good = True\n",
    "            break\n",
    "        if not good: allGood = False\n",
    "    return tokens if allGood else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def semantics(tokens):\n",
    "    prevKind = None\n",
    "    good = True\n",
    "    meaning = []\n",
    "    qnames = {}\n",
    "    qnodes = []\n",
    "    qedges = []\n",
    "    edgeLine = {}\n",
    "    nodeLine = {}\n",
    "    tokens = sorted(tokens, key=lambda t: (len(tokens)+t[0]) if t[1] == 'rel' else t[0])\n",
    "\n",
    "    # atomStack is a stack of qnodes with there indent levels\n",
    "    # such that every next member is one level deeper\n",
    "    # and every member is the last qnode encountered at that level\n",
    "    # The stack is implemented as a dict, keyed by the indent, and valued by the qnode\n",
    "    atomStack = {}\n",
    "    \n",
    "    for (i, kind, *fields) in tokens:\n",
    "        if kind == 'atom':\n",
    "            (indent, name, otype, features) = fields\n",
    "            qnodes.append((otype, features))\n",
    "            q = len(qnodes) - 1\n",
    "            nodeLine[q] = i\n",
    "            name = ':{}'.format(i) if name == '' else name\n",
    "            qnames[name] = q\n",
    "            if len(atomStack) == 0:\n",
    "                if indent > 0:\n",
    "                    error('Unexpected indent at line {}: {}, expected {}'.format(i, indent, 0))\n",
    "                    good = False\n",
    "                atomStack[0] = q\n",
    "            else:\n",
    "                atomNest = sorted(atomStack.items(), key=lambda x: x[0])\n",
    "                top = atomNest[-1]\n",
    "                if indent == top[0]:  \n",
    "                    # sibling of previous atom\n",
    "                    if len(atomNest) > 1:\n",
    "                        # take the qnode of the subtop of the atomStack, if there is one\n",
    "                        qedges.append((q, ']]', atomNest[-2][1]))\n",
    "                        edgeLine[len(qedges) - 1] = i\n",
    "                elif indent > top[0]:\n",
    "                    # child of previous atom\n",
    "                    qedges.append((q, ']]', top[1]))\n",
    "                    edgeLine[len(qedges) - 1] = i\n",
    "                else:\n",
    "                    # outdent action: look up the proper parent in the stack\n",
    "                    if indent not in atomStack:\n",
    "                        # parent cannot be found: indentation error\n",
    "                        error('Unexpected indent at line {}: {}, expected one of {}'.format(\n",
    "                            i, indent,\n",
    "                            ','.join(str(at[0]) for at in atomNest if at[0] < indent),\n",
    "                        ))\n",
    "                        good = False\n",
    "                    else:\n",
    "                        parents = [at[1] for at in atomNest if at[0] < indent]\n",
    "                        if len(parents) != 0: # if not already at outermost level\n",
    "                            qedges.append((q, ']]', parents[-1]))\n",
    "                            edgeLine[len(qedges) - 1] = i\n",
    "                        removeKeys = [at[0] for at in atomNest if at[0] > indent]\n",
    "                        for rk in removeKeys: del atomStack[rk]\n",
    "                atomStack[indent] = q\n",
    "        elif kind == 'feat':\n",
    "            features = fields[0]\n",
    "            if prevKind != None and prevKind != 'atom':\n",
    "                error('Features without atom at line {}: \"{}\"'.format(i, features))\n",
    "                good = False\n",
    "            else:\n",
    "                qnodes[-1][1].update(features)\n",
    "        elif kind == 'rel':\n",
    "            (fName, opName, tName) = fields\n",
    "            f = qnames.get(fName, None)\n",
    "            t = qnames.get(tName, None)\n",
    "            namesGood = True\n",
    "            for (q, n) in ((f, fName), (t, tName)):\n",
    "                if q == None:\n",
    "                    error('Relation with undefined name at line {}: \"{}\"'.format(i, n))\n",
    "                    namesGood = False\n",
    "            if not namesGood:\n",
    "                good = False\n",
    "            else:\n",
    "                qedges.append((f, opName, t))                \n",
    "                edgeLine[len(qedges) - 1] = i\n",
    "        prevKind = kind\n",
    "    if not good:\n",
    "        return None\n",
    "    return (qnames, qnodes, qedges, nodeLine, edgeLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(search):\n",
    "    tokens = syntax(search)\n",
    "    if tokens == None:\n",
    "        return None\n",
    "    meaning = semantics(tokens)\n",
    "    if meaning == None:\n",
    "        for (i, line) in enumerate(search.split('\\n')):\n",
    "            print('{:>2} {}'.format(i, line))\n",
    "        return None\n",
    "    return meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'atom', 0, '', 'clause', {})\n",
      "(4, 'atom', 4, 'p1', 'phrase', {'det': {'und'}})\n",
      "(5, 'atom', 8, 'w1', 'word', {'lex': {'WS[', 'JC/'}})\n",
      "(6, 'feat', {'gn': {'f'}, 'ps': {'p1', 'p2', 'p3'}})\n",
      "(7, 'atom', 8, 'w3', 'word', {})\n",
      "(8, 'rel', 'w1', '<', 'w3')\n",
      "(10, 'atom', 4, 'p2', 'phrase', {})\n",
      "(11, 'atom', 8, 'w2', 'word', {})\n",
      "(12, 'rel', 'w1', '<', 'w2')\n",
      "(13, 'rel', 'w2', '<', 'w3')\n",
      "(15, 'rel', 'p2', '<mother-', 'p1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({':2': 0, 'p1': 1, 'p2': 4, 'w1': 2, 'w2': 5, 'w3': 3},\n",
       " [('clause', {}),\n",
       "  ('phrase', {'det': {'und'}}),\n",
       "  ('word', {'gn': {'f'}, 'lex': {'JC/', 'WS['}, 'ps': {'p1', 'p2', 'p3'}}),\n",
       "  ('word', {}),\n",
       "  ('phrase', {}),\n",
       "  ('word', {})],\n",
       " [(1, ']', 1, 0),\n",
       "  (2, ']', 1, 1),\n",
       "  (3, ']', 1, 1),\n",
       "  (4, ']', 1, 0),\n",
       "  (5, ']', 1, 4),\n",
       "  (2, '<', 1, 3),\n",
       "  (2, '<', 1, 5),\n",
       "  (5, '<', 1, 3),\n",
       "  (4, 'mother', -1, 1)],\n",
       " {0: 2, 1: 4, 2: 5, 3: 7, 4: 10, 5: 11},\n",
       " {0: 4, 1: 5, 2: 7, 3: 10, 4: 11, 5: 8, 6: 12, 7: 13, 8: 15})"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(search1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
