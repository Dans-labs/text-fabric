{
    "docs": [
        {
            "location": "/",
            "text": "Text-Fabric is a Python3 package for Text plus Annotations.\n\n\nIt provides a data model, a text file format, and a binary format for (ancient) text plus\n(linguistic) annotations.\n\n\nThe emphasis of this all is on:\n\n\n\n\ndata processing\n\n\nsharing data\n\n\ncontributing modules\n\n\n\n\nA defining characteristic is that Text-Fabric does not make use of XML or JSON,\nbut stores text as a bunch of plain text files.\nEach of these files contains the data of a single feature,\naligned with a \ngraph\n of nodes and edges, which make up the\nabstract fabric of the text.\n\n\nInstall\n\u00b6\n\n\nHave \nPython3\n on your system.\n\n\nOptionally install \nJupyter\n as well:\n\n\n1\npip3 install jupyter\n\n\n\n\n\n\nInstall Text-Fabric:\n\n\n1\npip3 install text-fabric\n\n\n\n\n\n\nCorpora\n\u00b6\n\n\nThere are a few corpora in Text-Fabric that are being supported\nwith extra modules.\n\n\nHebrew Bible\n\u00b6\n\n\nGet the corpus:\n\n\n1\n2\ncd\n ~/github/etcbc\ngit clone https://github.com/etcbc/bhsa\n\n\n\n\n\n\nCuneiform tablets from Uruk\n\u00b6\n\n\nGet the corpus:\n\n\n1\n2\ncd\n ~/github/Nino-cunei\ngit clone https://github.com/Nino-cunei/uruk\n\n\n\n\n\n\nMore\n\u00b6\n\n\nWe have example corpora (Greek, Sanskrit, Babylonian),\nbut these are not supported by extra modules.\n\n\n1\n2\ncd\n ~/github\ngit clone https://github.com/Dans-labs/text-fabric-data\n\n\n\n\n\n\nGetting started\n\u00b6\n\n\nStart programming: write a python script or code in the Jupyter notebook\n\n\n1\n2\ncd\n somewhere-else\njupyter notebook\n\n\n\n\n\n\nEnter the following text in a code cell\n\n\n1\n2\n3\n4\nfrom\n \ntf.fabric\n \nimport\n \nFabric\n\n\nTF\n \n=\n \nFabric\n(\nmodules\n=\n[\n'my/dataset'\n])\n\n\napi\n \n=\n \nTF\n.\nload\n(\n'sp lex'\n)\n\n\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\n\nMaybe you have to tell Text-Fabric exactly where your data is.\nIf you have the data in a directory \ntext-fabric-data\n\nunder your home directory  or under \n~/github\n, Text-Fabric can find it.\nIn your \nmodules\n argument you then specify one or more subdirectories of\n\ntext-fabric-data\n.\n\n\nUsing Hebrew data\n\u00b6\n\n\nTo get started with the Hebrew corpus, use its tutorial in the BHSA repo:\n\nstart\n.\n\n\nOr go straight to the\n\nbhsa-api-docs\n.\n\n\nUsing Cuneiform data\n\u00b6\n\n\nTo get started with the Uruk corpus, use its tutorial in the Nino-cunei repo:\n\nstart\n.\n\n\nOr go straight to the\n\ncunei-api-docs\n.\n\n\nHistory\n\u00b6\n\n\nMost ideas derive from an earlier project, \n\nLAF-Fabric\n.\nWe have taken out everything that makes LAF-Fabric complicated and\nall things that are not essential for the sake of raw data processing.",
            "title": "Home"
        },
        {
            "location": "/#install",
            "text": "Have  Python3  on your system.  Optionally install  Jupyter  as well:  1 pip3 install jupyter   Install Text-Fabric:  1 pip3 install text-fabric",
            "title": "Install"
        },
        {
            "location": "/#corpora",
            "text": "There are a few corpora in Text-Fabric that are being supported\nwith extra modules.",
            "title": "Corpora"
        },
        {
            "location": "/#hebrew-bible",
            "text": "Get the corpus:  1\n2 cd  ~/github/etcbc\ngit clone https://github.com/etcbc/bhsa",
            "title": "Hebrew Bible"
        },
        {
            "location": "/#cuneiform-tablets-from-uruk",
            "text": "Get the corpus:  1\n2 cd  ~/github/Nino-cunei\ngit clone https://github.com/Nino-cunei/uruk",
            "title": "Cuneiform tablets from Uruk"
        },
        {
            "location": "/#more",
            "text": "We have example corpora (Greek, Sanskrit, Babylonian),\nbut these are not supported by extra modules.  1\n2 cd  ~/github\ngit clone https://github.com/Dans-labs/text-fabric-data",
            "title": "More"
        },
        {
            "location": "/#getting-started",
            "text": "Start programming: write a python script or code in the Jupyter notebook  1\n2 cd  somewhere-else\njupyter notebook   Enter the following text in a code cell  1\n2\n3\n4 from   tf.fabric   import   Fabric  TF   =   Fabric ( modules = [ 'my/dataset' ])  api   =   TF . load ( 'sp lex' )  api . makeAvailableIn ( globals ())    Maybe you have to tell Text-Fabric exactly where your data is.\nIf you have the data in a directory  text-fabric-data \nunder your home directory  or under  ~/github , Text-Fabric can find it.\nIn your  modules  argument you then specify one or more subdirectories of text-fabric-data .",
            "title": "Getting started"
        },
        {
            "location": "/#using-hebrew-data",
            "text": "To get started with the Hebrew corpus, use its tutorial in the BHSA repo: start .  Or go straight to the bhsa-api-docs .",
            "title": "Using Hebrew data"
        },
        {
            "location": "/#using-cuneiform-data",
            "text": "To get started with the Uruk corpus, use its tutorial in the Nino-cunei repo: start .  Or go straight to the cunei-api-docs .",
            "title": "Using Cuneiform data"
        },
        {
            "location": "/#history",
            "text": "Most ideas derive from an earlier project,  LAF-Fabric .\nWe have taken out everything that makes LAF-Fabric complicated and\nall things that are not essential for the sake of raw data processing.",
            "title": "History"
        },
        {
            "location": "/News/",
            "text": "Changes\n\u00b6\n\n\nConsult the tutorials after changes\nWhen we change the API, we make sure that the tutorials shows off\nall possibilities:\n\nbhsa\n\n\ncunei\n4.1.2\n\u00b6\n\n\n2018-05-17\n\n\nBhsa and Cunei APIs:\n\n\n\n\ncustom highlight colors also work for condensed results.\n\n\nyou can pass the \nhighlights\n parameter also to \nshow\n and \nprettyTuple\n\n\n\n\n4.1.1\n\u00b6\n\n\n2018-05-16\n\n\nBhsa API: you can customize the features that are shown in pretty displays.\n\n\n4.1\n\u00b6\n\n\n2018-05-16\n\n\nBhsa and Cunei APIs: you can customize the highlighting of search results:\n\n\n\n\ndifferent colors for different parts of the results\n\n\nyou can choose your colors freely from all that CSS has to offer.\n\n\n\n\nSee the updated search tutorials.\n\n\n4.0.3\n\u00b6\n\n\n2018-05-11\n\n\nNo changes, just quirks in the update process to get a new version of TF out.\n\n\n4.0.1\n\u00b6\n\n\n2018-05-11\n\n\nDocumentation updates.\n\n\n4.0.0\n\u00b6\n\n\n2018-05-11\n\n\n\n\nAdditions to Search.\n    You can now include the values of edges in your search templates.\n\n\nF.\nfeature\n.freqList()\n accepts a new parameter: \nnodeTypes\n. It will restrict its results to nodes in\n    one of the types in \nnodeTypes\n. \n\n\nYou can now also do \nE.\nfeature\n.freqList()\n.\n    It will count the number of edges if the edge is declared to be without values, \n    or it will give a frequency list of the edges by value if the edge has values.\n    Like \nF.freqList\n, you can pass parameters to constrain the frequency list to certain node types.\n    You can constrain the node types from which the edges start (\nnodeTypesFrom\n) and where they arrive\n    (\nnodeTypesTo\n).\n\n\nNew documentation system based on \nmkdocs\n.\n\n\n\n\n3.4.12\n\u00b6\n\n\n2018-05-02\n\n\nThe Cunei and Bhsa APIs show the version of Text-Fabric that is being called.\n\n\n3.4.11\n\u00b6\n\n\n2018-05-01\n\n\nCunei\n\n\n\n\ncases are divided horizontally and vertically, alternatingly with their\n    nesting level;\n\n\ncases have a feature \ndepth\n now, indicating at which level of nesting they\n    are.\n\n\n\n\n3.4.8-9-10\n\u00b6\n\n\n2018-04-30\n\n\nVarious small fixes, such as:\n\n\n\n\n\n\nBhsa: Lexeme links in pretty displays.\n\n\n\n\n\n\nCunei: Prevented spurious \n</div>\n in nbviewer.\n\n\n\n\n\n\n3.4.7\n\u00b6\n\n\nCunei: Modified local image names\n\n\n3.4.6\n\u00b6\n\n\nSmall tweaks in search.\n\n\n3.4.5\n\u00b6\n\n\n2018-04-28\n\n\nBhsa API:\n\n\n\n\nnew functions \nplain()\n and \ntable()\n for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by \npretty()\n and\n    \nshow()\n.\n\n\n\n\n3.4.4\n\u00b6\n\n\n2018-04-27\n\n\nCunei API:\n\n\n\n\nnew functions \nplain()\n and \ntable()\n for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by \npretty()\n and\n    \nshow()\n.\n\n\n\n\n3.4.2\n\u00b6\n\n\n2018-04-26\n\n\nBetter search documentation.\n\n\nCunei API: small fixes.\n\n\n3.4.1\n\u00b6\n\n\n2018-04-25\n\n\nBhsa API:\n\n\n\n\nSearch/show: you can now show results condensed: i.e. a list of passages with\n    highlighted results is returned. This is how SHEBANQ represents the results of\n    a query. If you have two results in the same verse, with \ncondensed=True\n you\n    get one verse display with two highlights, with \ncondensed=False\n you get two\n    verse displays with one highlight each.\n\n\n\n\nCunei API:\n\n\n\n\nSearch/show: the \npretty\n, \nprettyTuple\n, \nshow\n functions of the Bhsa API\n    have bee translated to the Cunei API. You can now get \nvery\n pretty displays\n    of search results.\n\n\n\n\n3.4\n\u00b6\n\n\n2018-04-23\n\n\nSearch\n:\n\n\n\n\nYou can use regular expressions to specifify feature values in queries.\n\n\nYou could already search for nodes which have a non-None value for a certain\n    feature. Now you can also search for the complement: nodes that do not have a\n    certain feature.\n\n\n\n\nBhsa API:\n\n\nThe display of query results also works with lexeme nodes.\n\n\n3.3.4\n\u00b6\n\n\n2018-04-20\n\n\nCunei API: Better height and width control for images. Leaner captions.\n\n\n3.3.3\n\u00b6\n\n\n2018-04-19\n\n\nCunei API: \ncasesByLevel()\n returns case nodes in corpus order.\n\n\n3.3.2\n\u00b6\n\n\n2018-04-18\n\n\nChange in the cunei api reflecting that undivided lines have no cases now (was:\nthey had a single case with the same material as the line). Also: the feature\n\nfullNumber\n on cases is now called \nnumber\n, and contains the full hierarchical\npart leading to a case. There is an extra feature \nterminal\n on lines and cases\nif they are not subdivided.\n\n\nChanges in Cunei and Bhsa api:\n\n\n\n\nfixed a bug that occurred when working outside a github repository.\n\n\n\n\n3.3.1\n\u00b6\n\n\n2018-04-18\n\n\nChange in the cunei api. \ncasesByLevel()\n now takes an optional argument\n\nterminal\n instead of \nwithChildren\n, with opposite values.\n\n\nwithChildren=False\n is ambiguous: will it deliver only cases that have no\nchildren (intended), or will it deliver cases and their children (understood,\nbut not intended).\n\n\nterminal=True\n: delivers only cases that are terminal.\n\n\nterminal=False\n: delivers all cases at that level.\n\n\n3.3\n\u00b6\n\n\n2018-04-14\n\n\nSmall fix in the bhsa api.\n\n\nBumped the version number because of the inclusion of corpus specific APIs.\n\n\n3.2.6\n\u00b6\n\n\n2018-04-14\n\n\n\n\nText-Fabric now contains corpus specific extras:\n\n\nbhsa.py\n for the Hebrew Bible (BHSA)\n\n\ncunei.py\n for the Proto-cuneiform corpus Uruk\n\n\n\n\n\n\nThe \nFabric(locations=locations, modules=modules)\n constructor now uses \n['']\n\n    as default value for modules. Now you can use the \nlocations\n parameter on its\n    own to specify the search paths for TF features, leaving the \nmodules\n\n    parameter undefined, if you wish.\n\n\n\n\n3.2.5\n\u00b6\n\n\n2018-03-23\n\n\nEnhancement in search templates: you can now test for the presence of features.\nTill now, you could only test for one or more concrete values of features. So,\nin addition to things like\n\n\n1\nword number=plural tense=yiqtol\n\n\n\n\n\n\nyou can also say things like\n\n\n1\nword number=plural tense\n\n\n\n\n\n\nand it will give you words in the plural that have a tense.\n\n\n3.2.4\n\u00b6\n\n\n2018-03-20\n\n\nThe short API names \nF\n, \nT\n, \nL\n etc. have been aliased to longer names:\n\nFeature\n, \nText\n, \nLocality\n, etc.\n\n\n3.2.2\n\u00b6\n\n\n2018-02-27\n\n\nRemoved the sub module \ncunei.py\n. It is better to keep corpus dependent modules\nin outside the TF package.\n\n\n3.2.1\n\u00b6\n\n\n2018-02-26\n\n\nAdded a sub module \ncunei.py\n, which contains methods to produce ATF\ntranscriptions for nodes of certain types.\n\n\n3.2\n\u00b6\n\n\n2018-02-19\n\n\nAPI change\n Previously, the functions \nL.d()\n and \nL.u()\n took rank into\naccount. In the Hebrew Bible, that means that \nL.d(sentence)\n will not return a\nverse, even if the verse is contained in the sentence.\n\n\nThis might be handy for sentences and verses, but in general this behaviour\ncauses problems. It also disturbs the expectiation that with these functions you\nget \nall\n embedders and embeddees.\n\n\nSo we have lifted this restriction. Still, the results of the \nL\n functions have\nan ordering that takes into account the levels of the returned nodes.\n\n\nEnhancement\n Previously, Text-Fabric determined the levels of node types\nautomatically, based on the average slot-size of nodes within the node types. So\nbooks get a lower level than chapters than verses than phrases, etc.\n\n\nHowever, working with cuneiform tablets revealed that containing node types may\nhave a smaller average size than contained node types. This happens when a\ncontainer type only contains small instances of the contained type and not the\nbigger ones.\n\n\nNow you can override the computation by text-fabric by means of a key-value in\nthe \notext\n feature. See the \napi\n.\n\n\n3.1.5\n\u00b6\n\n\n2018-02-15\n\n\nFixed a small problem in \nsectionFromNode(n)\n when \nn\n is a node within a\nprimary section but outside secundary/tertiary sections.\n\n\n3.1.4\n\u00b6\n\n\n2018-02-15\n\n\nSmall fix in the Text API. If your data set does not have language dependent\nfeatures, for section level 1 headings, such as \nbook@en\n, \nbook@sw\n, the Text\nAPI will not break, and the plain \nbook\n feature will be taken always.\n\n\nWe also reformatted all code with a pep8 code formatter.\n\n\n3.1.3\n\u00b6\n\n\n2018-01-29\n\n\nSmall adaptions in conversion from MQL to TF, it can now also convert the MQL\ncoming from CALAP dataset (Syriac).\n\n\n3.1.2\n\u00b6\n\n\n2018-01-27\n\n\nNothing changed, only the names of some variables and the text of some messages.\nThe terminology has been maed more consistent with the fabric metaphor, in\nparticular, \ngrid\n has been replaced by \nwarp\n.\n\n\n3.1.1\n\u00b6\n\n\n2017-10-21\n\n\nThe \nexportMQL()\n function now generates one single enumeration type that serves\nfor all enum features. That makes it possible to compare values of different\nenum features with each other, such as \nps\n and \nprs_ps\n.\n\n\n3.1\n\u00b6\n\n\n2017-10-20\n\n\nThe \nexportMQL()\n function now generates enumeration types for features, if\ncertain conditions are fulfilled. That makes it possible to query those features\nwith the \nIN\n relationship of MQL, like \n[chapter book IN (Genesis, Exodus)]\n.\n\n\n3.0.8\n\u00b6\n\n\n2017-10-07\n\n\nWhen reading edges with values, also the edges without a value are taken in.\n\n\n3.0.7\n\u00b6\n\n\n2017-10-07\n\n\nEdges with edge values did not allow for the absence of values. Now they do.\n\n\n3.0.6\n\u00b6\n\n\n2017-10-05\n\n\nA major tweak in the \nimportMQL()\n function so that it can\nhandle gaps in the monad sequence. The issue arose when converting MQL for\nversion 3 of the \nBHSA\n. In that version there\nare somewhat arbitrary gaps in the monad sequence between the books of the\nHebrew Bible. I transform a gapped sequence of monads into a continuous sequence\nof slots.\n\n\n3.0.5\n\u00b6\n\n\n2017-10-05\n\n\nAnother little tweak in the \nimportMQL()\n function so that it\ncan handle more patterns in the MQL dump file. The issue arose when converting\nMQL for version 3 of the \nBHSA\n.\n\n\n3.0.4\n\u00b6\n\n\n2017-10-04\n\n\nLittle tweak in the \nimportMQL()\n function so that it can handle\nmore patterns in the MQL dump file. The issue arose when converting MQL for\n\nextrabiblical\n material.\n\n\n3.0.2, 3.0.3\n\u00b6\n\n\n2017-10-03\n\n\nNo changes, only an update of the package metadata, to reflect that Text-Fabric\nhas moved from \nETCBC\n to\n\nDans-labs\n.\n\n\n3.0.1\n\u00b6\n\n\n2017-10-02\n\n\nBug fix in reading edge features with values.\n\n\n3.0.0\n\u00b6\n\n\n2017-10-02\n\n\nMQL! You can now convert MQL data into a TF dataset:\n\nimportMQL()\n. We had already \nexportMQL()\n.\n\n\nThe consequence is that we can operate with much agility between the worlds of\nMQL and TF.\n\n\nWe can start with source data in MQL, convert it to TF, combine it with other TF\ndata sources, compute additional stuff and add it, and then finally export it as\nenriched MQL, so that the enriched data can be queried by MQL.\n\n\n2.3.15\n\u00b6\n\n\n2017-09-29\n\n\nCompletion: TF defines the concept of\n\nedges\n that\ncarry a value. But so far we have not used them. It turned out that it was\nimpossible to let TF know that an edge carries values, when\n\nsaving\n data\nas a new feature. Now it is possible.\n\n\n2.3.14\n\u00b6\n\n\n2017-09-29\n\n\nBug fix: it was not possible to get\n\nT.nodeFromSection(('2_Chronicles', 36, 23))\n, the last verse in the Bible.\n\n\nThis is the consequence of a bug in precomputing the sections\n\nsections\n. The\npreparation step used\n\n\n1\nrange\n(\nfirstVerse\n,\n \nlastVerse\n)\n\n\n\n\n\n\n\nsomewhere, which should of course have been\n\n\n1\nrange\n(\nfirstVerse\n,\n \nlastVerse\n \n+\n \n1\n)\n\n\n\n\n\n\n\n2.3.13\n\u00b6\n\n\n2017-09-28\n\n\nLoading TF was not completely silent if \nsilent=True\n was passed. Better now.\n\n\n2.3.12\n\u00b6\n\n\n2017-09-18\n\n\n\n\n\n\nSmall fix in\n    \nTF.save()\n.\n    The spec says that the metadata under the empty key will be inserted into all\n    features, but in fact this did not happen. Instead it was used as a default\n    when some feature did not have metadata specified.\n\n\nFrom now on, that metadata will spread through all features.\n\n\n\n\n\n\nNew API function \nexplore\n, to get a list of all known\n    features in a dataset.\n\n\n\n\n\n\n2.3.11\n\u00b6\n\n\n2017-09-18\n\n\n\n\nSmall fix in Search: the implementation of the relation operator \n||\n\n    (disjoint slot sets) was faulty. Repaired.\n\n\nThe\n    \nsearch tutorial\n\n    got an extra example: how to look for gaps. Gaps are not a primitive in the TF\n    search language. Yet the language turns out to be powerful enough to look for\n    gaps. This answers a question by Cody Kingham.\n\n\n\n\n2.3.10\n\u00b6\n\n\n2017-08-24\n\n\nWhen defining text formats in the \notext.tf\n feature, you can now include\nnewlines and tabs in the formats. Enter them as \n\\n\n and \n\\t\n.\n\n\n2.3.9\n\u00b6\n\n\n2017-07-24\n\n\nTF has a list of default locations to look for data sources: \n~/Downloads\n,\n\n~/github\n, etc. Now \n~/Dropbox\n has been added to that list.\n\n\n2.3.8\n\u00b6\n\n\n2017-07-24\n\n\nThe section levels (book, chapter, verse) were supposed to be customizable\nthrough the \notext\n feature. But in\nfact, up till version 2.3.7 this did not work. From now on the names of the\nsection types and the features that name/number them, are given in the \notext\n\nfeature. It is still the case that exactly three levels must be specified,\notherwise it does not work.\n\n\n2.3.7\n\u00b6\n\n\n2017-05-12\n\n\nFixes. Added an extra default location for looking for text-fabric-data sources,\nfor the benefit of running text-fabric within a shared notebook service.\n\n\n2.3.5, 2.3.6\n\u00b6\n\n\n2017-03-01\n\n\nBug fix in Search. Spotted by Cody Kingham. Relational operators between atoms\nin the template got discarded after an outdent.\n\n\n2.3.4\n\u00b6\n\n\n2017-02-12\n\n\nAlso the \nFabric()\n call can be made silent now.\n\n\n2.3.3\n\u00b6\n\n\n2017-02-11\n\n\nImprovements:\n\n\n\n\nyou can load features more silently. See \nTF.load()\n;\n\n\nyou can search more silently. See \nS.study()\n;\n\n\nyou can search more concisely. See the new \nS.search()\n;\n\n\nwhen fetching results, the \namount\n parameter of\n    \nS.fetch()\n has been renamed to \nlimit\n;\n\n\nthe tutorial notebooks (see links on top) have been updated.\n\n\n\n\n2.3.2\n\u00b6\n\n\n2017-02-03\n\n\nBug fix: the results of \nF.feature.s()\n, \nE.feature.f()\n, and \nE.features.t()\n\nare now all tuples. They were a mixture of tuples and lists.\n\n\n2.3.1\n\u00b6\n\n\n2017-01-23\n\n\nBug fix: when searching simple queries with only one query node, the result\nnodes were delivered as integers, instead of 1-tuples of integers.\n\n\n2.3\n\u00b6\n\n\n2017-01-13\n\n\nWe start archiving releases of Text-Fabric at \nZenodo\n.\n\n\n2.2.1\n\u00b6\n\n\n2017-01-09\n\n\nSmall fixes.\n\n\n2.2.0\n\u00b6\n\n\n2017-01-06\n\n\nNew: sortKey\n\u00b6\n\n\nThe API has a new member: \nsortKey\n\n\nNew relationships in templates: \nnearness\n. See for\nexamples the end of the\n\nsearchTutorial\n.\nThanks to James Cu\u00e9nod for requesting nearness operators.\n\n\nFixes\n\u00b6\n\n\n\n\nin \nS.glean()\n word nodes were not printed;\n\n\nthe check whether the search graph consists of a single connected component\n    did not handle the case of one node without edges well;\n\n\n\n\n2.1.3\n\u00b6\n\n\n2017-01-04\n\n\nVarious fixes.\n\n\n2.1.0\n\u00b6\n\n\n2017-01-04\n\n\nNew: relations\n\u00b6\n\n\nSome relations have been added to search templates:\n\n\n\n\n=:\n and \n:=\n and \n::\n: \nstart at same slot\n, \nend at same slot\n, \nstart at\n    same slot and end at same slot\n\n\n<:\n and \n:>\n: \nadjacent before\n and \nadjacent next\n.\n\n\n\n\nThe latter two can also be used through the \nL\n-api: \nL.p()\n and \nL.n()\n.\n\n\nThe data that feeds them is precomputed and available as \nC.boundary\n.\n\n\nNew: enhanced search templates\n\u00b6\n\n\nYou can now easily make extra constraints in search templates without naming\natoms.\n\n\nSee the\n\nsearchTutorial\n\nfor an updated exposition on searching.\n\n\n2.0.0\n\u00b6\n\n\n2016-12-23\n\n\nNew: Search\n\u00b6\n\n\n\n\nWant to feel cosy with Christmas? Put your laptop on your lap, update\nText-Fabric, and start playing with search. Your laptop will spin itself warm\nwith your queries!\n\n\nText-Fabric just got a powerful search facility, based on (graph)-templates.\n\n\nIt is still very fresh, and more experimentation will be needed. Feedback is\nwelcome.\n\n\nStart with the\n\ntutorial\n.\n\n\nThe implementation of this search engine can be nicely explained with a textile\nmetaphor: spinning wool into yarn and then stitching the yarns together.\n\n\nThat will be explained further in a document that I'll love to write during\nXmas.\n\n\n1.2.7\n\u00b6\n\n\n2016-12-14\n\n\nNew\n\u00b6\n\n\nF.otype.sInterval()\n\n\n1.2.6\n\u00b6\n\n\n2016-12-14\n\n\nbug fix\n\u00b6\n\n\nThere was an error in computing the order of nodes. One of the consequences was\nthat objects that occupy the same slots were not ordered properly. And that had\nas consequence that you could not go up from words in one-word phrases to their\ncontaining phrase.\n\n\nIt has been remedied.\n\n\nNote\nYour computed data needs to be refreshed. This can be done by calling a new\nfunction \nTF.clearCache()\n. When you use TF after\nthis, you will see it working quite hard to recompute a bunch of data.\n1.2.5\n\u00b6\n\n\n2016-12-13\n\n\nDocumentation update\n\n\n1.2.0\n\u00b6\n\n\n2016-12-08\n\n\nNote\nData update needed\nNew\n\u00b6\n\n\nFrequency lists\n\u00b6\n\n\nF.feature.freqList()\n: get a sorted frequency list for any\nfeature. Handy as a first step in exploring a feature.\n\n\nExport to MQL\n\u00b6\n\n\nTF.exportMQL()\n: export a whole dataset as a MQL database.\nIncluding all modules that you have loaded with it.\n\n\nChanged\n\u00b6\n\n\nThe slot numbers start at 0, no longer at 1. Personally I prefer the zero\nstarting point, but Emdros insists on positive monads and objects ids. Most\nimportant is that users do not have to add/subtract one from the numbers they\nsee in TF if they want to use it in MQL and vice versa.\n\n\nBecause of this you need to update your data too:\n\n\n1\n2\n    \ncd\n ~/github/text-fabric-data\n    git pull origin master",
            "title": "News"
        },
        {
            "location": "/News/#changes",
            "text": "Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off\nall possibilities: bhsa  cunei",
            "title": "Changes"
        },
        {
            "location": "/News/#412",
            "text": "2018-05-17  Bhsa and Cunei APIs:   custom highlight colors also work for condensed results.  you can pass the  highlights  parameter also to  show  and  prettyTuple",
            "title": "4.1.2"
        },
        {
            "location": "/News/#411",
            "text": "2018-05-16  Bhsa API: you can customize the features that are shown in pretty displays.",
            "title": "4.1.1"
        },
        {
            "location": "/News/#41",
            "text": "2018-05-16  Bhsa and Cunei APIs: you can customize the highlighting of search results:   different colors for different parts of the results  you can choose your colors freely from all that CSS has to offer.   See the updated search tutorials.",
            "title": "4.1"
        },
        {
            "location": "/News/#403",
            "text": "2018-05-11  No changes, just quirks in the update process to get a new version of TF out.",
            "title": "4.0.3"
        },
        {
            "location": "/News/#401",
            "text": "2018-05-11  Documentation updates.",
            "title": "4.0.1"
        },
        {
            "location": "/News/#400",
            "text": "2018-05-11   Additions to Search.\n    You can now include the values of edges in your search templates.  F. feature .freqList()  accepts a new parameter:  nodeTypes . It will restrict its results to nodes in\n    one of the types in  nodeTypes .   You can now also do  E. feature .freqList() .\n    It will count the number of edges if the edge is declared to be without values, \n    or it will give a frequency list of the edges by value if the edge has values.\n    Like  F.freqList , you can pass parameters to constrain the frequency list to certain node types.\n    You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive\n    ( nodeTypesTo ).  New documentation system based on  mkdocs .",
            "title": "4.0.0"
        },
        {
            "location": "/News/#3412",
            "text": "2018-05-02  The Cunei and Bhsa APIs show the version of Text-Fabric that is being called.",
            "title": "3.4.12"
        },
        {
            "location": "/News/#3411",
            "text": "2018-05-01  Cunei   cases are divided horizontally and vertically, alternatingly with their\n    nesting level;  cases have a feature  depth  now, indicating at which level of nesting they\n    are.",
            "title": "3.4.11"
        },
        {
            "location": "/News/#348-9-10",
            "text": "2018-04-30  Various small fixes, such as:    Bhsa: Lexeme links in pretty displays.    Cunei: Prevented spurious  </div>  in nbviewer.",
            "title": "3.4.8-9-10"
        },
        {
            "location": "/News/#347",
            "text": "Cunei: Modified local image names",
            "title": "3.4.7"
        },
        {
            "location": "/News/#346",
            "text": "Small tweaks in search.",
            "title": "3.4.6"
        },
        {
            "location": "/News/#345",
            "text": "2018-04-28  Bhsa API:   new functions  plain()  and  table()  for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by  pretty()  and\n     show() .",
            "title": "3.4.5"
        },
        {
            "location": "/News/#344",
            "text": "2018-04-27  Cunei API:   new functions  plain()  and  table()  for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by  pretty()  and\n     show() .",
            "title": "3.4.4"
        },
        {
            "location": "/News/#342",
            "text": "2018-04-26  Better search documentation.  Cunei API: small fixes.",
            "title": "3.4.2"
        },
        {
            "location": "/News/#341",
            "text": "2018-04-25  Bhsa API:   Search/show: you can now show results condensed: i.e. a list of passages with\n    highlighted results is returned. This is how SHEBANQ represents the results of\n    a query. If you have two results in the same verse, with  condensed=True  you\n    get one verse display with two highlights, with  condensed=False  you get two\n    verse displays with one highlight each.   Cunei API:   Search/show: the  pretty ,  prettyTuple ,  show  functions of the Bhsa API\n    have bee translated to the Cunei API. You can now get  very  pretty displays\n    of search results.",
            "title": "3.4.1"
        },
        {
            "location": "/News/#34",
            "text": "2018-04-23  Search :   You can use regular expressions to specifify feature values in queries.  You could already search for nodes which have a non-None value for a certain\n    feature. Now you can also search for the complement: nodes that do not have a\n    certain feature.   Bhsa API:  The display of query results also works with lexeme nodes.",
            "title": "3.4"
        },
        {
            "location": "/News/#334",
            "text": "2018-04-20  Cunei API: Better height and width control for images. Leaner captions.",
            "title": "3.3.4"
        },
        {
            "location": "/News/#333",
            "text": "2018-04-19  Cunei API:  casesByLevel()  returns case nodes in corpus order.",
            "title": "3.3.3"
        },
        {
            "location": "/News/#332",
            "text": "2018-04-18  Change in the cunei api reflecting that undivided lines have no cases now (was:\nthey had a single case with the same material as the line). Also: the feature fullNumber  on cases is now called  number , and contains the full hierarchical\npart leading to a case. There is an extra feature  terminal  on lines and cases\nif they are not subdivided.  Changes in Cunei and Bhsa api:   fixed a bug that occurred when working outside a github repository.",
            "title": "3.3.2"
        },
        {
            "location": "/News/#331",
            "text": "2018-04-18  Change in the cunei api.  casesByLevel()  now takes an optional argument terminal  instead of  withChildren , with opposite values.  withChildren=False  is ambiguous: will it deliver only cases that have no\nchildren (intended), or will it deliver cases and their children (understood,\nbut not intended).  terminal=True : delivers only cases that are terminal.  terminal=False : delivers all cases at that level.",
            "title": "3.3.1"
        },
        {
            "location": "/News/#33",
            "text": "2018-04-14  Small fix in the bhsa api.  Bumped the version number because of the inclusion of corpus specific APIs.",
            "title": "3.3"
        },
        {
            "location": "/News/#326",
            "text": "2018-04-14   Text-Fabric now contains corpus specific extras:  bhsa.py  for the Hebrew Bible (BHSA)  cunei.py  for the Proto-cuneiform corpus Uruk    The  Fabric(locations=locations, modules=modules)  constructor now uses  [''] \n    as default value for modules. Now you can use the  locations  parameter on its\n    own to specify the search paths for TF features, leaving the  modules \n    parameter undefined, if you wish.",
            "title": "3.2.6"
        },
        {
            "location": "/News/#325",
            "text": "2018-03-23  Enhancement in search templates: you can now test for the presence of features.\nTill now, you could only test for one or more concrete values of features. So,\nin addition to things like  1 word number=plural tense=yiqtol   you can also say things like  1 word number=plural tense   and it will give you words in the plural that have a tense.",
            "title": "3.2.5"
        },
        {
            "location": "/News/#324",
            "text": "2018-03-20  The short API names  F ,  T ,  L  etc. have been aliased to longer names: Feature ,  Text ,  Locality , etc.",
            "title": "3.2.4"
        },
        {
            "location": "/News/#322",
            "text": "2018-02-27  Removed the sub module  cunei.py . It is better to keep corpus dependent modules\nin outside the TF package.",
            "title": "3.2.2"
        },
        {
            "location": "/News/#321",
            "text": "2018-02-26  Added a sub module  cunei.py , which contains methods to produce ATF\ntranscriptions for nodes of certain types.",
            "title": "3.2.1"
        },
        {
            "location": "/News/#32",
            "text": "2018-02-19  API change  Previously, the functions  L.d()  and  L.u()  took rank into\naccount. In the Hebrew Bible, that means that  L.d(sentence)  will not return a\nverse, even if the verse is contained in the sentence.  This might be handy for sentences and verses, but in general this behaviour\ncauses problems. It also disturbs the expectiation that with these functions you\nget  all  embedders and embeddees.  So we have lifted this restriction. Still, the results of the  L  functions have\nan ordering that takes into account the levels of the returned nodes.  Enhancement  Previously, Text-Fabric determined the levels of node types\nautomatically, based on the average slot-size of nodes within the node types. So\nbooks get a lower level than chapters than verses than phrases, etc.  However, working with cuneiform tablets revealed that containing node types may\nhave a smaller average size than contained node types. This happens when a\ncontainer type only contains small instances of the contained type and not the\nbigger ones.  Now you can override the computation by text-fabric by means of a key-value in\nthe  otext  feature. See the  api .",
            "title": "3.2"
        },
        {
            "location": "/News/#315",
            "text": "2018-02-15  Fixed a small problem in  sectionFromNode(n)  when  n  is a node within a\nprimary section but outside secundary/tertiary sections.",
            "title": "3.1.5"
        },
        {
            "location": "/News/#314",
            "text": "2018-02-15  Small fix in the Text API. If your data set does not have language dependent\nfeatures, for section level 1 headings, such as  book@en ,  book@sw , the Text\nAPI will not break, and the plain  book  feature will be taken always.  We also reformatted all code with a pep8 code formatter.",
            "title": "3.1.4"
        },
        {
            "location": "/News/#313",
            "text": "2018-01-29  Small adaptions in conversion from MQL to TF, it can now also convert the MQL\ncoming from CALAP dataset (Syriac).",
            "title": "3.1.3"
        },
        {
            "location": "/News/#312",
            "text": "2018-01-27  Nothing changed, only the names of some variables and the text of some messages.\nThe terminology has been maed more consistent with the fabric metaphor, in\nparticular,  grid  has been replaced by  warp .",
            "title": "3.1.2"
        },
        {
            "location": "/News/#311",
            "text": "2017-10-21  The  exportMQL()  function now generates one single enumeration type that serves\nfor all enum features. That makes it possible to compare values of different\nenum features with each other, such as  ps  and  prs_ps .",
            "title": "3.1.1"
        },
        {
            "location": "/News/#31",
            "text": "2017-10-20  The  exportMQL()  function now generates enumeration types for features, if\ncertain conditions are fulfilled. That makes it possible to query those features\nwith the  IN  relationship of MQL, like  [chapter book IN (Genesis, Exodus)] .",
            "title": "3.1"
        },
        {
            "location": "/News/#308",
            "text": "2017-10-07  When reading edges with values, also the edges without a value are taken in.",
            "title": "3.0.8"
        },
        {
            "location": "/News/#307",
            "text": "2017-10-07  Edges with edge values did not allow for the absence of values. Now they do.",
            "title": "3.0.7"
        },
        {
            "location": "/News/#306",
            "text": "2017-10-05  A major tweak in the  importMQL()  function so that it can\nhandle gaps in the monad sequence. The issue arose when converting MQL for\nversion 3 of the  BHSA . In that version there\nare somewhat arbitrary gaps in the monad sequence between the books of the\nHebrew Bible. I transform a gapped sequence of monads into a continuous sequence\nof slots.",
            "title": "3.0.6"
        },
        {
            "location": "/News/#305",
            "text": "2017-10-05  Another little tweak in the  importMQL()  function so that it\ncan handle more patterns in the MQL dump file. The issue arose when converting\nMQL for version 3 of the  BHSA .",
            "title": "3.0.5"
        },
        {
            "location": "/News/#304",
            "text": "2017-10-04  Little tweak in the  importMQL()  function so that it can handle\nmore patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical  material.",
            "title": "3.0.4"
        },
        {
            "location": "/News/#302-303",
            "text": "2017-10-03  No changes, only an update of the package metadata, to reflect that Text-Fabric\nhas moved from  ETCBC  to Dans-labs .",
            "title": "3.0.2, 3.0.3"
        },
        {
            "location": "/News/#301",
            "text": "2017-10-02  Bug fix in reading edge features with values.",
            "title": "3.0.1"
        },
        {
            "location": "/News/#300",
            "text": "2017-10-02  MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already  exportMQL() .  The consequence is that we can operate with much agility between the worlds of\nMQL and TF.  We can start with source data in MQL, convert it to TF, combine it with other TF\ndata sources, compute additional stuff and add it, and then finally export it as\nenriched MQL, so that the enriched data can be queried by MQL.",
            "title": "3.0.0"
        },
        {
            "location": "/News/#2315",
            "text": "2017-09-29  Completion: TF defines the concept of edges  that\ncarry a value. But so far we have not used them. It turned out that it was\nimpossible to let TF know that an edge carries values, when saving  data\nas a new feature. Now it is possible.",
            "title": "2.3.15"
        },
        {
            "location": "/News/#2314",
            "text": "2017-09-29  Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible.  This is the consequence of a bug in precomputing the sections sections . The\npreparation step used  1 range ( firstVerse ,   lastVerse )    somewhere, which should of course have been  1 range ( firstVerse ,   lastVerse   +   1 )",
            "title": "2.3.14"
        },
        {
            "location": "/News/#2313",
            "text": "2017-09-28  Loading TF was not completely silent if  silent=True  was passed. Better now.",
            "title": "2.3.13"
        },
        {
            "location": "/News/#2312",
            "text": "2017-09-18    Small fix in\n     TF.save() .\n    The spec says that the metadata under the empty key will be inserted into all\n    features, but in fact this did not happen. Instead it was used as a default\n    when some feature did not have metadata specified.  From now on, that metadata will spread through all features.    New API function  explore , to get a list of all known\n    features in a dataset.",
            "title": "2.3.12"
        },
        {
            "location": "/News/#2311",
            "text": "2017-09-18   Small fix in Search: the implementation of the relation operator  || \n    (disjoint slot sets) was faulty. Repaired.  The\n     search tutorial \n    got an extra example: how to look for gaps. Gaps are not a primitive in the TF\n    search language. Yet the language turns out to be powerful enough to look for\n    gaps. This answers a question by Cody Kingham.",
            "title": "2.3.11"
        },
        {
            "location": "/News/#2310",
            "text": "2017-08-24  When defining text formats in the  otext.tf  feature, you can now include\nnewlines and tabs in the formats. Enter them as  \\n  and  \\t .",
            "title": "2.3.10"
        },
        {
            "location": "/News/#239",
            "text": "2017-07-24  TF has a list of default locations to look for data sources:  ~/Downloads , ~/github , etc. Now  ~/Dropbox  has been added to that list.",
            "title": "2.3.9"
        },
        {
            "location": "/News/#238",
            "text": "2017-07-24  The section levels (book, chapter, verse) were supposed to be customizable\nthrough the  otext  feature. But in\nfact, up till version 2.3.7 this did not work. From now on the names of the\nsection types and the features that name/number them, are given in the  otext \nfeature. It is still the case that exactly three levels must be specified,\notherwise it does not work.",
            "title": "2.3.8"
        },
        {
            "location": "/News/#237",
            "text": "2017-05-12  Fixes. Added an extra default location for looking for text-fabric-data sources,\nfor the benefit of running text-fabric within a shared notebook service.",
            "title": "2.3.7"
        },
        {
            "location": "/News/#235-236",
            "text": "2017-03-01  Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms\nin the template got discarded after an outdent.",
            "title": "2.3.5, 2.3.6"
        },
        {
            "location": "/News/#234",
            "text": "2017-02-12  Also the  Fabric()  call can be made silent now.",
            "title": "2.3.4"
        },
        {
            "location": "/News/#233",
            "text": "2017-02-11  Improvements:   you can load features more silently. See  TF.load() ;  you can search more silently. See  S.study() ;  you can search more concisely. See the new  S.search() ;  when fetching results, the  amount  parameter of\n     S.fetch()  has been renamed to  limit ;  the tutorial notebooks (see links on top) have been updated.",
            "title": "2.3.3"
        },
        {
            "location": "/News/#232",
            "text": "2017-02-03  Bug fix: the results of  F.feature.s() ,  E.feature.f() , and  E.features.t() \nare now all tuples. They were a mixture of tuples and lists.",
            "title": "2.3.2"
        },
        {
            "location": "/News/#231",
            "text": "2017-01-23  Bug fix: when searching simple queries with only one query node, the result\nnodes were delivered as integers, instead of 1-tuples of integers.",
            "title": "2.3.1"
        },
        {
            "location": "/News/#23",
            "text": "2017-01-13  We start archiving releases of Text-Fabric at  Zenodo .",
            "title": "2.3"
        },
        {
            "location": "/News/#221",
            "text": "2017-01-09  Small fixes.",
            "title": "2.2.1"
        },
        {
            "location": "/News/#220",
            "text": "2017-01-06",
            "title": "2.2.0"
        },
        {
            "location": "/News/#new-sortkey",
            "text": "The API has a new member:  sortKey  New relationships in templates:  nearness . See for\nexamples the end of the searchTutorial .\nThanks to James Cu\u00e9nod for requesting nearness operators.",
            "title": "New: sortKey"
        },
        {
            "location": "/News/#fixes",
            "text": "in  S.glean()  word nodes were not printed;  the check whether the search graph consists of a single connected component\n    did not handle the case of one node without edges well;",
            "title": "Fixes"
        },
        {
            "location": "/News/#213",
            "text": "2017-01-04  Various fixes.",
            "title": "2.1.3"
        },
        {
            "location": "/News/#210",
            "text": "2017-01-04",
            "title": "2.1.0"
        },
        {
            "location": "/News/#new-relations",
            "text": "Some relations have been added to search templates:   =:  and  :=  and  :: :  start at same slot ,  end at same slot ,  start at\n    same slot and end at same slot  <:  and  :> :  adjacent before  and  adjacent next .   The latter two can also be used through the  L -api:  L.p()  and  L.n() .  The data that feeds them is precomputed and available as  C.boundary .",
            "title": "New: relations"
        },
        {
            "location": "/News/#new-enhanced-search-templates",
            "text": "You can now easily make extra constraints in search templates without naming\natoms.  See the searchTutorial \nfor an updated exposition on searching.",
            "title": "New: enhanced search templates"
        },
        {
            "location": "/News/#200",
            "text": "2016-12-23",
            "title": "2.0.0"
        },
        {
            "location": "/News/#new-search",
            "text": "Want to feel cosy with Christmas? Put your laptop on your lap, update\nText-Fabric, and start playing with search. Your laptop will spin itself warm\nwith your queries!  Text-Fabric just got a powerful search facility, based on (graph)-templates.  It is still very fresh, and more experimentation will be needed. Feedback is\nwelcome.  Start with the tutorial .  The implementation of this search engine can be nicely explained with a textile\nmetaphor: spinning wool into yarn and then stitching the yarns together.  That will be explained further in a document that I'll love to write during\nXmas.",
            "title": "New: Search"
        },
        {
            "location": "/News/#127",
            "text": "2016-12-14",
            "title": "1.2.7"
        },
        {
            "location": "/News/#new",
            "text": "F.otype.sInterval()",
            "title": "New"
        },
        {
            "location": "/News/#126",
            "text": "2016-12-14",
            "title": "1.2.6"
        },
        {
            "location": "/News/#bug-fix",
            "text": "There was an error in computing the order of nodes. One of the consequences was\nthat objects that occupy the same slots were not ordered properly. And that had\nas consequence that you could not go up from words in one-word phrases to their\ncontaining phrase.  It has been remedied.  Note Your computed data needs to be refreshed. This can be done by calling a new\nfunction  TF.clearCache() . When you use TF after\nthis, you will see it working quite hard to recompute a bunch of data.",
            "title": "bug fix"
        },
        {
            "location": "/News/#125",
            "text": "2016-12-13  Documentation update",
            "title": "1.2.5"
        },
        {
            "location": "/News/#120",
            "text": "2016-12-08  Note Data update needed",
            "title": "1.2.0"
        },
        {
            "location": "/News/#new_1",
            "text": "",
            "title": "New"
        },
        {
            "location": "/News/#frequency-lists",
            "text": "F.feature.freqList() : get a sorted frequency list for any\nfeature. Handy as a first step in exploring a feature.",
            "title": "Frequency lists"
        },
        {
            "location": "/News/#export-to-mql",
            "text": "TF.exportMQL() : export a whole dataset as a MQL database.\nIncluding all modules that you have loaded with it.",
            "title": "Export to MQL"
        },
        {
            "location": "/News/#changed",
            "text": "The slot numbers start at 0, no longer at 1. Personally I prefer the zero\nstarting point, but Emdros insists on positive monads and objects ids. Most\nimportant is that users do not have to add/subtract one from the numbers they\nsee in TF if they want to use it in MQL and vice versa.  Because of this you need to update your data too:  1\n2      cd  ~/github/text-fabric-data\n    git pull origin master",
            "title": "Changed"
        },
        {
            "location": "/Api/General/",
            "text": "Text-Fabric API\n\u00b6\n\n\nTutorial\nThe tutorials for the\n\nHebrew Bible\n\nand the\n\nUruk Cuneiform Tablets\n\nput the Text-Fabric API on show for two distinguished (and vastly distinct) corpora.\nGeneric API\nThis is the API of Text-Fabric in general.\nText-Fabric has no baked in knowledge of particular corpora.\nHowever, Text-Fabric comes with several additions that make working\nwith specific corpora easier.\nHebrew Bible: \nBHSA\nProto-cuneiform tablets from Uruk: \nCunei\nLoading\n\u00b6\n\n\nTF=Fabric()\n1\n2\nfrom\n \ntf.fabric\n \nimport\n \nFabric\n\n\nTF\n \n=\n \nFabric\n(\nlocations\n=\ndirectories\n,\n \nmodules\n=\nsubdirectories\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nText-Fabric is initialized for a corpus. It will search a set of directories\nand catalog all \n.tf\n files it finds there.\nThese are the features you can subsequently load.\nHere \ndirectories\n and \nsubdirectories\n are strings with directory names\nseparated by newlines, or iterables of directories.\nlocations, modules\nThe directories specified in \nlocations\n will be searched for \nmodules\n, which\nare paths that will be appended to the paths in \nlocations\n.\nAll \n.tf\n files (non-recursively) in any \nmodule\n will be added to the feature\nset to be loaded in this session. The order in \nmodules\n is important, because\nif a feature occurs in multiple modules, the last one will be chosen. In this\nway you can easily override certain features in one module by features in an\nother module of your choice.\notext@ in modules\nIf modules contain features with a name starting with \notext@\n, then the format\ndefinitions in these features will be added to the format definitions in the\nregular \notext\n feature (which is a WARP feature). In this way, modules that\ndefine new features for text representation, also can add new formats to the\nText-API.\nDefaults\nThe \nlocations\n list has a few defaults:\n1\n2\n3\n~/Downloads/text-fabric-data\n~/text-fabric-data\n~/github/text-fabric-data\n\n\n\n\n\nSo if you have stored your main Text-Fabric dataset in\n\ntext-fabric-data\n in one of these directories\nyou do not have to pass a location to Fabric.\nThe \nmodules\n list defaults to \n['']\n. So if you leave it out, Text-Fabric will\njust search the paths specified in \nlocations\n.\nsilent\nIf \nsilent=True\n is passed, banners and normal progress messages are suppressed.\nTF.explore()\n1\n2\nfeatures\n \n=\n \nTF\n.\nexplore\n(\nsilent\n=\nFalse\n,\n \nshow\n=\nTrue\n)\n\n\nfeatures\n\n\n\n\n\n\nor\n1\n2\nTF\n.\nexplore\n(\nsilent\n=\nFalse\n,\n \nshow\n=\nFalse\n)\n\n\nTF\n.\nfeatureSets\n\n\n\n\n\n\nDescription\nThis will give you a dictionary of all available features by kind. The kinds\nare: \nnodes\n, \nedges\n, \nconfigs\n, \ncomputeds\n.\nsilent\nWith \nsilent=False\n a message containing the total numbers of features is issued.\nshow\nThe resulting dictionary is delivered in \nTF.featureSets\n, but if you say\n\nshow=True\n, the dictionary is returned as function result.\napi=TF.load()\n1\napi\n \n=\n \nTF\n.\nload\n(\nfeatures\n,\n \nadd\n=\nFalse\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReads the features indicated by \nfeatures\n and loads them in memory\nready to be used in the rest of the program.\nfeatures\nfeatures\n is a string containing space separated feature names, or an\niterable of feature names. The feature names are just the names of \n.tf\n files\nwithout directory information and without extension.\nadd\nIf later on you want load more features, you can either:\nadd the features to the original \nload()\n statement and just run it again\nmake a new statement: \nTF.load(newfeatures, add=True)\n. The new features will\n    be added to the same api, so you do not have to to call\n    \napi.makeAvailableIn(globals())\n again after this!\nsilent\nThe features will be loaded rather silently, most messages will be suppressed.\nTime consuming operations will always be announced, so that you know what\nText-Fabric is doing. If \nsilent=True\n is passed, all informational messages\nwill be suppressed. This is handy I you want to load data as part of other\nmethods, on-the-fly.\napi.makeAvailableIn(globals())\n1\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nDescription\nThis method will export every member of the API (such as \nN\n, \nF\n, \nE\n, \nL\n, \nT\n,\n\ninfo\n) to the global namespace. From now on, we will omit the \napi.\n in our\ndocumentation.\nContents of the API\nAfter having loaded the features by \napi = TF.load(...)\n, the \napi\n harbours\nyour Text-Fabric API. You can access node feature \nmydata\n by \napi.F.mydata.v(node)\n, edge\nfeature \nmylink\n by \napi.E.mylink.f(node)\n, and so on.\nIf you are working with a single data source in your program, it is a bit\ntedious to write the initial \napi.\n all the time.\nBy this methodd you can avoid that.\nLonger names\nThere are also longer names which can be used as aliases to the single capital\nletters. This might or might not improve the readability of your program.\nshort name\nlong name\nN\nNodes\nF\nFeature\nFs\nFeatureString\nFall\nAllFeatures\nE\nEdge\nEs\nEdgeString\nEall\nAllEdges\nC\nComputed\nCs\nComputedString\nCall\nAllComputeds\nL\nLocality\nT\nText\nS\nSearch\nignored\n1\napi\n.\nignored\n\n\n\n\n\n\nDescription\nIf you want to know which features were found but ignored (because the feature\nis also present in another, later, location), you can use this attribute\nto inspect the ignored features and their locations.\nloadLog()\n1\napi\n.\nloadlog\n()\n\n\n\n\n\n\nDescription\nAfter loading you can view all messages using this method.\nIt also shows the messages that have been suppressed due to \nsilent=True\n.\nNavigating nodes\n\u00b6\n\n\nN()\n1\n2\nfor\n \nn\n \nin\n \nN\n():\n\n    \naction\n\n\n\n\n\n\nDescription\nThe result of \nN()\n is a generator that walks through all nodes in the\n\ncanonical order\n (see below).\nIterating over \nN()\n delivers you all words and structural elements of\nyour corpus in a very natural order.\nWalking nodes\nMost processing boils down to walking through the nodes by visiting node sets in\na suitable order. Occasionally, during the walk you might want to visit\nembedding or embedded nodes to glean some feature information from them.\nMore ways of walking\nLater, under \nFeatures\n there is another convenient way to walk through\nnodes.\ncanonical order\nThe canonical order is a way to sort the nodes in your corpus in such a way\nthat you can enumerate all nodes in the order you encounter them if you\nwalk through your corpus.\nBriefly this means:\nembedder nodes come before the nodes that lie embedded in them;\nearlier stuff comes before later stuff,\nif a verse coincides with a sentence, the verse comes before the sentence,\n    because verses generally contain sentences and not the other way round;\nif two objects are intersecting, but none embeds the other, the one with the\n    smallest slot that does not occur in the other, comes first.\nfirst things first, big things first\nThat means, roughly, that you start with a\nbook node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis\n1:1, then a sentence node, then a clause node, a phrase node, and the first word\nnode. Then follow all word nodes in the first phrase, then the phrase node of\nthe second phrase, followed by the word nodes in that phrase. When ever you\nenter a higher structure, you will first get the node corresponding to that\nstructure, and after that the nodes corresponding to the building blocks of that\nstructure.\nThis concept follows the intuition that slot sets with smaller elements come\nbefore slot set with bigger elements, and embedding slot sets come before\nembedded slot sets. Hence, if you enumerate a set of nodes that happens to\nconstitute a tree hierarchy based on slot set embedding, and you enumerate those\nnodes in the slot set order, you will walk the tree in pre-order.\nThis order is a modification of the one as described in (Doedens 1994, 3.6.3).\n\nDoedens, Crist-Jan (1994), \nText Databases. One Database Model and Several\nRetrieval Languages\n, number 14 in Language and Computers, Editions Rodopi,\nAmsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1,\n\nhttp://books.google.nl/books?id=9ggOBRz1dO4C\n. The order as defined by\nDoedens corresponds to walking trees in post-order.\nFor a lot of processing, it is handy to have a the stack of embedding elements\navailable when working with an element. That is the advantage of pre-order over\npost-order. It is very much like SAX parsing in the XML world.\nsortNodes()\n1\nsortNodes\n(\nnodeSet\n)\n\n\n\n\n\n\nDescription\ndelivers an iterable of nodes as a tuple sorted by the \ncanonical ordering\n.\nnodeSet\nAn iterable of nodes to be sorted.\nsortKey\n1\nnodeList\n \n=\n \nsorted\n(\nnodes\n,\n \nkey\n=\nsortKey\n)\n\n\n\n\n\n\nDescription\nA function that provides for each node the key to be used to sort nodes in the\ncanonical ordering. That means that the following two pieces of code do the same\nthing:\nsortNodes(nodeSet)\n and \nsorted(nodeSet, key=sortKey)\n.\nSorting tuples of nodes\nHandy to sort things that are not nodes themselves, but data structures with\nnodes in it, e.g. search results: if \nresults\n is a list of tuples of nodes, we\ncould sort them in canonical order like this:\n1\nsorted\n(\nnodeSet\n,\n \nkey\n=\nlambda\n \nr\n:\n \nsortKey\n(\nr\n[\n0\n]))\n\n\n\n\n\n\nLocality\n\u00b6\n\n\nLocal navigation\nHere are the methods by which you can navigate easily from a node to its\nneighbours: parents and children, previous and next siblings.\nL\nThe Locality API is exposed as \nL\n or \nLocality\n.\notype parameter\nIn all of the following \nL\n-functions, if the \notype\n parameter is passed, the result is filtered and\nonly nodes with \notype=nodeType\n are retained.\nResults of the \nL.\n functions are tuples, not single nodes\nEven if an \nL\n-function returns a single node, it is packed in a \ntuple\n.\n  So to get the node itself, you have to dereference the tuple:\n1\nL\n.\nu\n(\nnode\n)[\n0\n]\n\n\n\n\n\n\nL.u()\n1\nL\n.\nu\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of nodes \nupward\n, i.e. embedder nodes.\nnode\nThe node whose embedder nodes will be delivered.\nThe result never includes \nnode\n itself.\nL.d()\n1\nL\n.\nd\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of nodes \ndownward\n, i.e. embedded nodes.\nnode\nThe node whose embedded nodes will be delivered.\nThe result never includes \nnode\n itself.\nL.n()\n1\nL\n.\nn\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of adjacent \nnext\n nodes.\nnode\nThe node whose right adjacent nodes will be delivered;\ni.e. the nodes whose first slot immediately follow the last slot \nof \nnode\n.\nThe result never includes \nnode\n itself.\nL.p()\n1\nL\n.\np\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of adjacent \nprevious\n nodes from \nnode\n, i.e. nodes\nwhose last slot just precedes the first slot of \nnode\n.\nDescription\nProduces an ordered tuple of adjacent \nprevious\n nodes.\nnode\nThe node whose lefy adjacent nodes will be delivered;\ni.e. the nodes whose last slot immediately precede the first slot \nof \nnode\n.\nLocality and levels\nHere is something that is very important to be aware of when using \nsortNodes\n\nand the \nL.d(n)\n and \nL.u(n)\n methods.\nWhen we order nodes and report on which nodes embed which other nodes, we do not\nonly take into account the sets of slots the nodes occupy, but also their\n\nlevel\n. See \nlevels\n and \ntext\n.\nBoth the \nL.d(n)\n and \nL.u(n)\n work as follows:\nL.d(n)\n returns nodes such that embedding nodes come before embedded nodes\n    words)\nL.u(n)\n returns nodes such that embedded nodes come before embedding nodes\n    books)\nN.B.:\n Suppose you have node types \nverse\n and \nsentence\n, and usually a\nverse has multiple sentences, but not vice versa. Then you expect that\nL.d(verseNode)\n will contain sentence nodes,\nL.d(sentenceNode)\n will \nnot\n contain verse nodes.\nBut if there is a verse with exactly one sentence, and both have exactly the\nsame words, then that is a case where:\nL.d(verseNode)\n will contain one sentence node,\nL.d(sentenceNode)\n will contain \none\n verse node.\nText\n\u00b6\n\n\nOverview\nHere are the functions that enable you to get the actual text in the dataset.\nThere are several things to accomplish here, such as\ngetting text given book, chapter, and verse;\ngiven a node, produce the book, chapter and verse indicators in which the node\n    is contained;\nhandle multilingual book names;\nswitch between various text representations.\nThe details of the Text API are dependent on the \nwarp\n feature \notext\n, which\nis a config feature.\nT\nThe Text API is exposed as \nT\n or \nText\n.\nSections\n\u00b6\n\n\nSection levels\nIn \notext\n the main section levels (usually \nbook\n, \nchapter\n, \nverse\n) can be\ndefined. It loads the features it needs (so you do not have to specify those\nfeatures, unless you want to use them via \nF\n). And finally, it makes some\nfunctions available by which you can make handy use of that information.\nSection levels are generic\nIn this documentation, we call the main section level \nbook\n, the second level\n\nchapter\n, and the third level \nverse\n. Text-Fabric, however, is completely\nagnostic about how these levels are called. It is prepared to distinguish three\nsection levels, but how they are called, must be configured in the dataset. The\ntask of the \notext\n feature is to declare which node type and feature correspond\nwith which section level. Text-Fabric assumes that the first section level may\nhave multilingual headings, but that section levels two and three have single\nlanguage headings (numbers of some kind).\nString versus number\nChapter and verse numbers will be considered to be strings or\nintegers, depending on whether your dataset has declared the corresponding\nfeature \nvalueType\n as \nstr\n or as \nint\n.\nConceivably, other works might have chapter and verse numbers\nlike \nXIV\n, '3A', '4.5', and in those cases these numbers are obviously not\n\nint\ns.\notext is optional\nIf \notext\n is missing, the Text API will not be build. If it exists, but\ndoes not specify sections, that part of the Text API will not be built. Likewise\nfor text representations.\nlevels of node types\nUsually, Text-Fabric computes the hierarchy of node types correctly, in the\nsense that node types that act as containers have a lower level than node types\nthat act as containees. So books have the lowest level, words the highest. See\n\nlevels\n. However, if this level assignment turns out to be wrong for\nyour dataset, you can configure the right order in the \notext\n feature, by means\nof a key \nlevels\n with value a comma separated list of levels. Example:\n1\n@levels=tablet,face,column,line,case,cluster,quad,comment,sign\n\n\n\n\n\nT.sectionFromNode()\n1\nT\n.\nsectionFromNode\n(\nnode\n,\n \nlastSlot\n=\nFalse\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\nReturns the book/chapter/verse indications that correspond to the reference\nnode, which is the first or last slot belonging \nn\n, dependent on \nlastSlot\n.\nThe result is a tuple, consisting of the book name (in language \nlang\n), the\nchapter number, and the verse number.\nnode\nThe node from which we obtain a section specification.\nlastSlot\nWhether the reference node will be the last slot contained by the \nnode\n argument\nor the first node.\nlang\nThe language to be used for the section parts, as far as they are language dependent.\ncrossing verse boundaries\nSometimes a sentence or clause in a verse continue into the next verse.\nIn those cases, this function will return a different results for\n\nlastSlot=False\n and \nlastSlot=True\n.\nnodes outside sections\nNodes that lie outside any book, chapter, verse will get a \nNone\n in the\ncorresponding members of the returned tuple.\nT.nodeFromSection()\n1\nT\n.\nnodeFromSection\n(\nsection\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\nGiven a \nsection\n tuple, return the node of it.\nsection\nsection\n consists of a book name (in language \nlang\n),\nand a chapter number and a verse\nnumber (both as strings or number depending on the value type of the\ncorresponding feature). The verse number may be left out, the result is then a\nchapter node. Both verse and chapter numbers may be left out, the result is then\na book node. If all three are present, de result is a verse node.\nlang\nThe language assumed for the section parts, as far as they are language dependent.\nBook names and languages\n\u00b6\n\n\nBook names and nodes\nThe names of the books may be available in multiple languages. The book names\nare stored in node features with names of the form \nbook@\nla\n, where \nla\n is\nthe \nISO 639\n two-letter code for that\nlanguage. Text-Fabric will always load these features.\nT.languages\n1\nT\n.\nlanguages\n\n\n\n\n\n\nDescription\nA dictionary of the languages that are available for book names.\nT.bookName()\n1\nT\n.\nbookName\n(\nnode\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\ngives the name of the book in which a node occurs.\nnode\nThe node in question.\nlang\nThe \nlang\n parameter is a two letter language code. The default is \nen\n\n(English).\nIf there is no feature data for the language chosen, the value of the ordinary\n\nbook\n feature of the dataset will be returned.\nWorks for all nodes\nn\n may or may not be a book node. If not, \nbookName()\n retrieves the\nembedding book node first.\nT.bookNode()\n1\nT\n.\nbookNode\n(\nname\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\ngives the node of the book identified by its name\nname\nThe name of the book.\nlang\nThe language in which the book name is supplied by the \nname\n parameter.\nIf \nlang\n can not be found, the value of the ordinary \nbook\n feature of the\ndataset will be used.\nIf \nname\n cannot be found in the specified language, \nNone\n will be returned.\nFunction name follows configured section level\nIf your dataset has configured section level one under an other name,\nsay \ntablet\n, then these two methods follow that name. Instead of \nT.bookName()\n\nand \nT.bookNode()\n we have then \nT.tabletName()\n and \nT.tabletNode()\n.\nText representation\n\u00b6\n\n\nText formats\nText can be represented in multiple ways. We provide a number of formats with\nstructured names.\nA format name is a string of keywords separated by \n-\n:\nwhat\n-\nhow\n-\nfullness\n-\nmodifier\nFor Hebrew any combination of the follwoing could be useful formats:\nkeyword\nvalue\nmeaning\nwhat\ntext\nwords as they belong to the text\nwhat\nlex\nlexemes of the words\nhow\norig\nin the original script (Hebrew, Greek, Syriac) (all Unicode)\nhow\ntrans\nin (latin) transliteration\nhow\nphono\nin phonetic/phonological transcription\nfullness\nfull\ncomplete with accents and all diacritical marks\nfullness\nplain\nwith accents and diacritical marks stripped, in Hebrew only the consonants are left\nmodifier\nketiv\n(Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default);\nThe default format is \ntext-orig-full\n, we assume that every TF dataset defines\nthis format.\nRemember that the formats are defined in the \notext\n warp config feature of your\nset, not by Text-Fabric.\nFreedom of names for formats\nThere is complete freedom of choosing names for text formats.\nThey do not have to complied with the above-mentioned scheme.\nT.formats\n1\nT\n.\nformats\n\n\n\n\n\n\nDescription\nShow the text representation formats that have been defined in your dataset.\nT.text()\n1\nT\n.\ntext\n(\nnodes\n,\n \nfmt\n=\nNone\n)\n\n\n\n\n\n\nDescription\nGives the text that corresponds to a bunch of nodes.\nnodes\nnodes\n can be an arbitrary iterable of nodes.\nNo attempt will be made to sort the nodes.\nIf you need order, it is\nbetter to order the nodes before you feed them to \nT.text()\n.\nfmt\nThe format of text-representation is given with \nfmt\n, with default \ntext-orig-full\n.\nIf the \nfmt\n\ncannot be found, the default is taken.\nIf the default format is not defined in the\n\notext\n feature of the dataset,\nthe node numbers will be output instead.\nThis function does not give error messages, because that could easily overwhelm\nthe output stream, especially in a notebook.\nNon slot nodes allowed\nIn most cases, the nodes fed to \nT.text()\n are slots, and the formats are\ntemplates that use features that are defined for slots.\nBut nothing prevents you to define a format for non-slot nodes, and use features\ndefined for a non-slot node type.\nIf, for example, your slot type is \nglyph\n, and you want a format that renders\nlexemes, which are not defined for glyphs but for words, you can just define a\nformat in terms of word features.\nIt is your responsibility to take care to use the formats for node types for\nwhich they make sense.\nEscape whitespace in formats\nWhen defining formats in \notext.tf\n, if you need a newline or tab in the format,\nspecify it as \n\\n\n and \n\\t\n.\nSearching\n\u00b6\n\n\nWhat is Text-Fabric Search?\nYou can query for graph like structures in your data set. The structure you are\ninterested in has to be written as a \nsearch template\n, offered to \nS.search()\n\nwhich returns the matching results as tuples of nodes.\nS\nThe Search API is exposed as \nS\n or \nSearch\n.\nSearch templates\n\u00b6\n\n\nSearch primer\nA search template consists of a bunch of lines, possibly indented, that specify\nobjects to look for. Here is a simple example:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        word pos=noun gender=feminine number=singular\n\n\n\n\n\nThis template looks for word combinations within a sentence within chapter 2 of\neither Genesis or Exodus, where one of the words is a verb and the other is a\nnoun. Both have a feminine inflection, but the verb is plural and the noun is\nsingular.\nThe indentation signifies embedding, i.e. containment. The two words are\ncontained in the same sentence, the sentence is contained in the chapter, the\nchapter in the book.\nThe conditions you specify on book, chapter, word are all conditions in terms of\n\nnode features\n. You can use all features in the corpus for\nthis.\nThe order of the two words is immaterial. If there are cases where the verb\nfollows the noun, they will be included in the results.\nAlso, the words do not have to be adjacent. If there are cases with words\nintervening between the noun and the verb, they will be included in the results.\nSpeaking of results: the \nS.search()\n function returns its results as tuples of\nnodes:\n1\n(\nbook\n,\n \nchapter\n,\n \nsentence\n,\n \nword1\n,\n \nword2\n)\n\n\n\n\n\n\nWith these nodes in hand, you can programmatically gather all information about\nthe results that the corpus provides.\nIf the order between the verb and the noun is important, you can specify that as\nan additional constraint. You can give the words a name, and state a relational\ncondition. Here we state that the noun precedes the verb.\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        vb:word pos=verb gender=feminine number=plural\n        nn:word pos=noun gender=feminine number=singular\nnn < vb\n\n\n\n\n\nThis can be written a bit more economically as:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        > word pos=noun gender=feminine number=singular\n\n\n\n\n\nIf you insist that the noun immediately precedes the verb, you can use a\ndifferent relational operator:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular\n\n\n\n\n\nThere are more kinds of relational operators.\nIf the noun must be the first word in the sentence, you can specify it as\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        w:word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\ns =: w\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        =: word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n\n\n\n\n\nIf the verb must be the last word in the sentence, you can specify it as\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: w:word pos=verb gender=feminine number=plural\ns := w\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n        :=\n\n\n\n\n\nYou can also use the \nedge features\n in the corpus as\nrelational operators as well.\nSuppose we have an edge feature \nsub\n between clauses, such that if main clause\n\nm\n has subordinate clauses \ns1\n, \ns2\n and \ns3\n, then\n1\nE.sub.f(m) = (s1, s2, s3)\n\n\n\n\n\nYou can use this relation in search. Suppose we want to find the noun verb pair\nin subordinate clauses only. We can use this template:\n1\n2\n3\n4\n5\n6\n7\nbook name=Genesis|Exodus\n   chapter number=2\n      m:clause\n        s:clause\n          word pos=verb gender=feminine number=plural\n          :> word pos=noun gender=feminine number=singular\nm -sub> s\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n  chapter number=2\n    clause\n      -sub> clause\n        word    pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular\n\n\n\n\n\nRead \nm -sub> s\n as: there is a \nsub\n-arrow from \nm\n to \ns\n.\nEdge features may have values.\nFor example, the\n\ncrossref feature\n\nis a set of edges between parallel verses, with the levels of confidence\nas values. This number is an integer between 0 and 100.\nWe can ask for parallel verses in an unqualified way:\n1\n2\nverse\n-crossref> verse\n\n\n\n\n\nBut we can also ask for the cases with a specific confidence:\n1\n2\nverse\n-crossref=90> verse\n\n\n\n\n\nor cases with a high confidence:\n1\n2\nverse\n-crossref>95> verse\n\n\n\n\n\nor cases with a low confidence:\n1\n2\nverse\n-crossref<80> verse\n\n\n\n\n\nAll feature conditions that you can assert on node features, you can also\nassert for edge features. If an edge feature is integer valued, such as \ncrossref\n\nyou can use comparisons; if it is string valued, you can use regular expressions.\nIn both cases you can also use the other constructs, such as\n1\n2\nverse\n-crossref=66|77> verse\n\n\n\n\n\nTo get a more specific introduction to search, consult the search tutorials for\n\nHebrew\n and\n\nCuneiform\n.\nSearch template reference\nGeneral\nWe have these kinds of lines in a template:\natom\n lines\n(simple): \nindent name:otype features\nvb:word pos=verb gender=feminine\nThe indent is significant. Indent is counted as the number of white space\n    characters, where tabs count for just 1. \nAvoid tabs!\n.\nThe \nname:\n part is optional.\n(with relop): \nindent op name:otype features\n<: word pos=verb gender=feminine\nThe relation operator specifies an extra constraint between a preceding atom\n    and this atom.\nThe preceding atom may be the parent, provided we are at its first child, or\n    it may the preceding sibling.\nYou can leave out the \nname:otype features\n bit. In that case, the\n    relation holds between the preceding atom and its parent.\nfeature\n lines: \nfeatures\nIndent is not significant. Continuation of feature constraints after a\n    preceding atom line or other feature line. This way you can divide lengthy\n    feature constraints over multiple lines.\nrelation\n lines: \nname operator name\ns := w\nm -sub> s\nm <sub- s\nIndents and spacing are ignored.\nThere must be white-space around the operator.\nOperators that come from edge features may be enriched with values.\n    See below.\nwhite-space or empty\n lines\nEverywhere allowed.\nAlways ignored.\nFeatures\nThe \nfeatures\n above is a specification of what features with which values to\nsearch for. This specification must be written as a white-space separated list\nof \nfeature specs\n.\nA \nfeature spec\n has the form \nname\n \nvalueSpec\n, with no space between the \nname\n\nand the \nvalueSpec\n.\nThe \nvalueSpec\n may have the following forms:\nform\nmeaning\nfeature \nname\n may have any value except \nNone\n!\nfeature \nname\n must have value \nNone\n (synonymous for: \nname\n has no value)\n=\nvalues\nfeature \nname\n has one of the values specified\n>\nvalue\nfeature \nname\n must be greater than \nvalue\n<\nvalue\nfeature \nname\n must be less than \nvalue\n~\nregular expression\nfeature \nname\n has a value and it matches \nregular expression\nAll these forms are also valid as \n-\nname\n \nform\n>\n and \n<\nname\n \nform\n-\n, in which case\nthey specify value constraints on edge features.\nThis is only meaningful if the edge feature is declared to have values (most edge features\ndo not have values).\nAdditional constraints\nThere may be no space around the \n=\n, nor the \n~\n.\nname\n must be a feature name that exists in the dataset. If it references a\n    feature that is not yet loaded, the feature will be loaded automatically.\nvalues\n must be a \n|\n separated list of feature values, no quotes. No spaces\n    around the \n|\n. If you need a space or \n|\n or \n\\\n in a value, escape it by a\n    \n\\\n. Escape tabs and newlines as \n\\t\n and \n\\n\n.\nWhen comparing values with \n<\n and \n>\n:\nvalue\n must be an integer (negative values allowed);\nYou can do numeric comparisons only on number-valued features, not on\n    string-valued features.\nregular expression\n must be a string that conforms to the Python\n    \nregular axpression syntax\nIf that syntax prescribes a\n\\\n, you have to write it twice: \n\\\n \n\\\n.\nIf you need a space in your regular expression, you have to escape it with a\n    \n\\\n.\nYou can do regular expressions only on string-valued features, not on\n    number-valued features.\nOperator lines\nNode comparison\n=\n: is equal (meaning the same node, a clause and a verse that occupy the\n    same slots are still unequal)\n    \n#\n: is unequal (meaning a different node, a clause and a verse that occupy\n    the same slots are still unequal)\n    \n<\n \n>\n: before and after (in the \ncanonical ordering\n)\n    \nSlot comparison\n==\n: occupy the same slots (identical slot sets)\n    \n&&\n: overlap (the intersection of both slot sets is not empty)\n    \n##\n: occupy different slots (but they may overlap, the set of slots of the\n    two are different as sets)\n    \n||\n: occupy disjoint slots (no slot occupied by the one is also occupied by\n    the other)\n    \n[[ ]]\n: embeds and contains (slot set inclusion, in both directions)\n    \n<<\n \n>>\n: before and after (with respect to the slots occupied: left ends\n    before right starts and vice versa)\n    \n<:\n \n:>\n: \nadjacent\n before and after (with respect to the slots occupied:\n    left ends immediately before right starts and vice versa)\n    \n=:\n left and right start at the same slot\n    \n:=\n left and right end at the same slot\n    \n::\n left and right start and end at the same slot\n    \nNearness comparison\nSome of the adjacency relations can actually be weakened. Instead of requiring\nthat one slot is equal to an other slot, you can require that they are \nk-near\n,\ni.e. they are at most \nk\n apart. Here are the relationships where you can do\nthat. Instead of typing the letter \nk\n, provide the actual number you want.\n<k:\n \n:k>\n: \nk\n-\nadjacent\n before and after (with respect to the slots\n    occupied: left ends \nk\n-near where right starts and vice versa)\n    \n=k:\n left and right start at \nk\n-near slots\n    \n:k=\n left and right end at \nk\n-near slots\n    \n:k:\n left and right start and end at \nk\n-near slots\n    \nBased on edge features\n-\nname\n>\n \n<\nname\n-\n: connected by the edge feature \nname\nin both directions;\nthese forms work for edges that do and do not have values;\n\n\n   \n-\nname\n \nvalueSpec\n>\n \n<\nname\n \nvalueSpec\n-\n: connected by the edge feature \nname*\nin both directions;\nthese forms work only for edges that do have values.\nS.relationsLegend\n1\nS\n.\nrelationsLegend\n\n\n\n\n\n\nDescription\nGives dynamic help about the basic relations that you can use in your search\ntemplate. It includes the edge features that are available in your dataset.\nS.search()\n1\nS\n.\nsearch\n(\nsearchTemplate\n,\n \nlimit\n=\nNone\n)\n\n\n\n\n\n\nDescription\nSearches for combinations of nodes that together match a search template.\nThis method returns a \ngenerator\n which yields the results one by one. One result\nis a tuple of nodes, where each node corresponds to an \natom\n-line in your\n\nsearch template\n.\nsearchTemplate\nThe search template is a string that conforms to the rules described above.\nlimit\nIf \nlimit\n is a number, it will fetch only that many results.\nGenerator versus tuple\nIf \nlimit\n is specified, the result is not a generator but a tuple of results.\nMore info on the search plan\nSearching is complex. The search template must be parsed, interpreted, and\ntranslated into a search plan. The following methods expose parts of the search\nprocess, and may provide you with useful information in case the search does not\ndeliver what you expect.\nsee the plan\nthe method \nS.showPlan()\n below shows you at a glance the correspondence\nbetween the nodes in each result tuple and your search template.\nS.study()\n1\nS\n.\nstudy\n(\nsearchTemplate\n,\n \nstrategy\n=\nNone\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nYour search template will be checked, studied, the search\nspace will be narrowed down, and a plan for retrieving the results will be set\nup.\nsearchTemplate\nThe search template is a string that conforms to the rules described above.\nstrategy\nIn order to tame the performance of search, the strategy by which results are fetched\nmatters a lot.\nThe search strategy is an implementation detail, but we bring\nit to the surface nevertheless.\nTo see the names of the available strategies, just call\n\nS.study('', strategy='x')\n and you will get a list of options reported to\nchoose from.\nFeel free to experiment. To see what the strategies do,\nsee the \ncode\n.\nsilent\nIf you want to suppress most of the output, say \nsilent=True\n.\nS.showPlan()\n1\nS\n.\nshowPlan\n(\ndetails\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nSearch results are tuples of nodes and the plan shows which part of the tuple\ncorresponds to which part of the search template.\ndetails\nIf you say \ndetails=True\n, you also get an overview of the search space and a\ndescription of how the results will be retrieved.\nafter S.study()\nThis function is only meaningful after a call to \nS.study()\n.\nSearch results\n\u00b6\n\n\nPreparation versus result fetching\nThe method \nS.search()\n above combines the interpretation of a given\ntemplate, the setting up of a plan, the constraining of the search space\nand the fetching of results.\nHere are a few methods that do actual result fetching.\nThey must be called after a previous \nS.search()\n or \nS.study()\n.\nS.count()\n1\nS\n.\ncount\n(\nprogress\n=\nNone\n,\n \nlimit\n=\nNone\n)\n\n\n\n\n\n\nDescription\nCounts the results, with progress messages, optionally up to a limit.\nprogress\nEvery so often it shows a progress message.\nThe frequency is \nprogress\n results, default every 100.\nlimit\nFetch results up to a given \nlimit\n, default 1000.\nSetting \nlimit\n to 0 or a negative value means no limit: all results will be\ncounted.\nwhy needed\nYou typically need this in cases where result fetching turns out to\nbe (very) slow.\ngenerator versus list\nlen(S.results())\n does not work, because \nS.results()\n is a generator\nthat delivers its results as they come.\nS.fetch()\n1\nS\n.\nfetch\n(\nlimit\n=\nNone\n)\n\n\n\n\n\n\nDescription\nFinally, you can retrieve the results. The result of \nfetch()\n is not a list of\nall results, but a \ngenerator\n. It will retrieve results as long as they are\nrequested and their are still results.\nlimit\nTries to get that many results and collects them in a tuple.\nSo if limit is not \nNone\n, the result is a tuple with a known length.\nIterating over the \nfetch()\n generator\nYou typically fetch results by saying:\n1\n2\n3\n4\ni\n \n=\n \n0\n\n\nfor\n \nr\n \nin\n \nS\n.\nresults\n():\n\n    \ndo_something\n(\nr\n[\n0\n])\n\n    \ndo_something_else\n(\nr\n[\n1\n])\n\n\n\n\n\n\nAlternatively, you can set the \nlimit\n parameter, to ask for just so many\nresults. They will be fetched, and when they are all collected, returned as a\ntuple.\nFetching a limited amount of results\n1\nS\n.\nfetch\n(\nlimit\n=\n10\n)\n\n\n\n\n\n\ngives you the first bunch of results quickly.\nS.glean()\n1\nS\n.\nglean\n(\nr\n)\n\n\n\n\n\n\nDescription\nA search result is just a tuple of nodes that correspond to your template, as\nindicated by \nshowPlan()\n. Nodes give you access to all information that the\ncorpus has about it.\nThe \nglean()\n function is here to just give you a first impression quickly.  \nr\nPass a raw result tuple \nr\n, and you get a string indicating where it occurs,\nin terms of sections, \nand what text is associated with the results.\nInspecting results\n1\n2\nfor\n \nresult\n \nin\n \nS\n.\nfetch\n(\nlimit\n=\n10\n):\n\n    \nprint\n(\nS\n.\nglean\n(\nresult\n))\n\n\n\n\n\n\nis a handy way to get an impression of the first bunch of results.\nUniversal\nThis function works on all tuples of nodes, whether they have been\nobtained by search or not.\nMore ways of showing results\nIf you work in one of the corpora for which the TF-API has been extended,\nyou will be provided with more powerful methods \nshow()\n and \ntable()\n\nto display your results. See \nCunei\n and \nBhsa\n.\nNode features\n\u00b6\n\n\nNode Features\nF\nThe node features API is exposed as \nF\n (\nFs\n) or \nFeature\n (\nFeatureString\n).\nFall() aka AllFeatures()\n1\n2\nFall\n()\n\n\nAllFeatures\n()\n\n\n\n\n\n\nDescription\nReturns a sorted list of all usable, loaded node feature names.\nF.\nfeature\n aka Feature.\nfeature\n1\n2\nF\n.\npart_of_speech\n\n\nFeature\n.\npart_of_speech\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in node features.\nIn this example, we assume there is a feature called\n\npart_of_speech\n.\nTricky feature names\nIf the feature name is not\na valid python identifier, you can not use this function,\nyou should use \nFs\n instead.\nFs(feature) aka FeatureString(feature)\n1\n2\n3\n4\nFs\n(\nfeature\n)\n\n\nFeatureString\n(\nfeature\n)\n\n\nFs\n(\n'part-of-speech'\n)\n\n\nFeatureString\n(\n'part-of-speech'\n)\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in node features.\nfeature\nIn this example, in line 1 and 2, the feature name is contained in\nthe variable \nfeature\n.\nIn lines 3 and 4, \nwe assume there is a feature called\n\npart-of-speech\n.\nNote that this is not a valid name in Python, yet we\ncan work with features with such names.\nBoth methods have identical results\nSuppose we have just issued \nfeature = 'pos'.\nThen the result of\nFs(feature)\nand\nF.pos` is identical.\nIn most cases \nF\n works just fine, but \nFs\n is needed in two cases:\nif we need to work with a feature whose name is not a valid\n  Python name;\nif we determine the feature we work with dynamically, at run time.\nSimple forms\nIn the sequel we'll give examples based on the simple form only.\nF.\nfeature\n.v(node)\n1\nF\n.\npart_of_speech\n.\nv\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the value of a \nfeature\n, such as \npart_of_speech\n for a node.\nnode\nThe node whose value for the feature is being retrieved.\nF.\nfeature\n.s(value)\n1\n2\nF\n.\npart_of_speech\n.\ns\n(\nvalue\n)\n\n\nF\n.\npart_of_speech\n.\ns\n(\n'noun'\n)\n\n\n\n\n\n\nDescription\nReturns a generator of all nodes in the canonical order with a given value for a given feature.\nThis is an other way to walk through nodes than using \nN()\n.\nvalue\nThe test value: all nodes with this value are yielded, the others pass through.\nnouns\nThe second line gives you all nodes which are nouns according to the corpus.\nF.`\nfeature\n.freqList()\n1\nF\n.\npart_of_speech\n.\nfreqList\n(\nnodeTypes\n=\nNone\n)\n\n\n\n\n\n\nDescription\nInspect the values of \nfeature\n (in this example: \npart_of_speech\n)\nand see how often they occur. The result is a\nlist of pairs \n(value, frequency)\n, ordered by \nfrequency\n, highest frequencies\nfirst.\nnodeTypes\nIf you pass a set of nodeTypes, only the values for nodes within those\ntypes will be counted.\nF.otype\notype\n is a special node feature and has additional capabilities.\nDescription\nF.otype.slotType\n is the node type that can fill the slots (usually: \nword\n)\nF.otype.maxSlot\n is the largest slot number\nF.otype.maxNode\n is the largest node number\nF.otype.all\n is a list of all \notypes\n from big to small (from books through\n    clauses to words)\nF.otype.sInterval(otype)\n is like \nF.otype.s(otype)\n, but instead of\n    returning you a range to iterate over, it will give you the starting and\n    ending nodes of \notype\n. This makes use of the fact that the data is so\n    organized that all node types have single ranges of nodes as members.\nEdge features\n\u00b6\n\n\nEdge Features\nE\nThe edge features API is exposed as \nE\n (\nEs\n) or \nEdge\n (\nEdgeString\n).\nEall() aka AllEdges()\n1\n2\nEall\n()\n\n\nAllEdges\n()\n\n\n\n\n\n\nDescription\nReturns a sorted list of all usable, loaded edge feature names.\nE.\nfeature\n aka Edge.\nfeature\n1\n2\nE\n.\nhead\n\n\nFeature\n.\nhead\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in edge features.\nIn this example, we assume there is a feature called\n\nhead\n.\nTricky feature names\nIf the feature name is not\na valid python identifier, you can not use this function,\nyou should use \nEs\n instead.\nEs(feature) aka EdgeString(feature)\n1\n2\n3\n4\nEs\n(\nfeature\n)\n\n\nEdgeString\n(\nfeature\n)\n\n\nEs\n(\n'head'\n)\n\n\nEdgeString\n(\n'head'\n)\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in edge features.\nfeature\nIn this example, in line 1 and 2, the feature name is contained in\nthe variable \nfeature\n.\nIn lines 3 and 4, \nwe assume there is a feature called\n\nhead\n.\nBoth methods have identical results\nSuppose we have just issued \nfeature = 'head'.\nThen the result of\nEs(feature)\nand\nE.pos` is identical.\nIn most cases \nE\n works just fine, but \nEs\n is needed in two cases:\nif we need to work with a feature whose name is not a valid\n  Python name;\nif we determine the feature we work with dynamically, at run time.\nSimple forms\nIn the sequel we'll give examples based on the simple form only.\nE.\nfeature\n.f(node)\n1\nE\n.\nhead\n.\nf\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the nodes reached by \nfeature\n-edges \nfrom\n a certain node.\nThese edges must be specified in \nfeature\n, in this case \nhead\n.\nThe result is an ordered tuple\n(again, in the \ncanonical order\n. The members of the\nresult are just nodes, if \nhead\n describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value.\nIf there are no edges from the node, the empty tuple is returned, rather than \nNone\n.\nnode\nThe node \nfrom\n which the edges in question start.\nE.\nfeature\n.t(node)\n1\nE\n.\nhead\n.\nt\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the nodes reached by \nfeature\n-edges \nto\n a certain node.\nThese edges must be specified in \nfeature\n, in this case \nhead\n.\nThe result is an ordered tuple\n(again, in the \ncanonical order\n. The members of the\nresult are just nodes, if \nfeature\n describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value.\nIf there are no edges to \nn\n, the empty tuple is returned, rather than \nNone\n.\nnode\nThe node \nto\n which the edges in question go.\nE.\nfeature\n.freqList()\n1\nE\n.\nop\n.\nfreqList\n(\nnodeTypesFrom\n=\nNone\n,\n \nnodeTypesTo\n=\nNone\n)\n\n\n\n\n\n\nDescription\nIf the edge feature has no values, simply return the number of node pairs\nbetween an edge of this kind exists.\nIf the edge feature does have values, we \ninspect them\nand see how often they occur. The result is a\nlist of pairs \n(value, frequency)\n, ordered by \nfrequency\n, highest frequencies\nfirst.\nnodeTypesFrom\nIf not \nNone\n,\nonly the values for edges that start from a node with type\nwithin \nnodeTypesFrom\n\nwill be counted.\nnodeTypesTo\nIf not \nNone\n,\nonly the values for edges that go to a node with type\nwithin \nnodeTypesTo\n\nwill be counted.\nE.oslots\noslots\n is a special edge feature and is mainly used to construct other parts\nof the API. It has less capabilities, and you will rarely need it. It does not\nhave \n.f\n and \n.t\n methods, but an \n.s\n method instead.\nDescription\nE.oslots.s(node)\n\nGives the sorted list of slot numbers linked to a node,\nor put otherwise: the slots that \nsupport\n that node.\nnode\nThe node whose slots are being delivered.\nMessaging\n\u00b6\n\n\nTimed messages\nError and informational messages can be issued, with a time indication.\ninfo(), error()\n1\ninfo\n(\nmsg\n,\n \ntm\n=\nTrue\n,\n \nnl\n=\nTrue\n)\n\n\n\n\n\n\nDescription\nSends a message to standard output, possibly with time and newline.\n\n if \ninfo()\n is being used, the message is sent to \nstdout\n;\n\n if \nerror()\n is being used, the message is sent to \nstderr\n;\nIn a Jupyter notebook, the standard error is displayed with\na reddish background colour.\ntm\nIf \nTrue\n, an indicator of the elapsed time will be prepended to the message.\nnl\nIf \nTrue\n a newline will be appended.\nindent()\n1\nindent\n(\nlevel\n=\nNone\n,\n \nreset\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nChanges the level of indentation of messages and possibly resets the time.\nlevel\nThe level of indentation, an integer.  Subsequent\n\ninfo()\n and \nerror()\n will display their messages with this indent.\nreset\nIf \nTrue\n, the elapsed time to will be reset to 0 at the given level.\nTimers at different levels are independent of each other.\nSaving features\n\u00b6\n\n\nTF.save()\n1\nTF\n.\nsave\n(\nnodeFeatures\n=\n{},\n \nedgeFeatures\n=\n{},\n \nmetaData\n=\n{},\n \nmodule\n=\nNone\n)\n\n\n\n\n\n\nDescription\nIf you have collected feature data in dictionaries, keyed by the\nnames of the features, and valued by their feature data,\nthen you can save that data to \n.tf\n feature files on disk.\nIt is this easy to export new data as features:\ncollect the data and metadata of\nthe features and \nfeed it in an orderly way to \nTF.save()\n and there you go.\nnodeFeatures\nThe data of a node feature is a dictionary with nodes as keys (integers!) and\nstrings or numbers as (feature) values.\nedgeFeatures\nThe data of an edge feature is a dictionary with nodes as keys, and sets or\ndictionaries as values. These sets should be sets of nodes (integers!), and\nthese dictionaries should have nodes as keys and strings or numbers as values.\nmetadata\nEvery feature will receive metadata from \nmetaData\n, which is a dictionary\nmapping a feature name to its metadata.\nvalue types\nThe type of the values should conform to \n@valueType\n (\nint\n or \nstr\n), which\nmust be stated in the metadata.\nedge values\nIf you save an edge feature, and there are values in that edge feature, you have\nto say so, by specifying \nedgeValues = True\n in the metadata for that feature.\ngeneric metadata\nmetaData\n may also contain fields under\n  the empty name. These fields will be added to all features in \nnodeFeatures\n and\n  \nedgeFeatures\n.\nconfig features\nIf you need to write the \nconfig\n feature \notext\n,\nwhich is a metadata-only feature, just\nadd the metadata under key \notext\n in \nmetaData\n and make sure\nthat \notext\n is not a key in \nnodeFeatures\n nor in\n\nedgeFeatures\n.\nThese fields will be written into the separate config feature \notext\n,\nwith no data associated.\nsave location\nThe (meta)data will be written to the very last module in the list of locations\nthat you specified when calling \nFabric()\n or to what you passed as \nmodule\n in\nthe same location. If that module does not exist, it will be created in the last\n\nlocation\n. If both \nlocations\n and \nmodules\n are empty, writing will take place\nin the current directory.\nClearing the cache\n\u00b6\n\n\nTF.clearCache()\n1\nTF\n.\nclearCache\n()\n\n\n\n\n\n\nDescription\nText-Fabric precomputes data for you, so that it can be loaded faster. If the\noriginal data is updated, Text-Fabric detects it, and will recompute that data.\nBut there are cases, when the algorithms of Text-Fabric have changed, without\nany changes in the data, where you might want to clear the cache of precomputed\nresults.\nCalling this function just does it, and it is equivalent with manually removing\nall \n.tfx\n files inside the hidden \n.tf\n directory inside your dataset.\nNo need to load\nIt is not needed to execute a \nTF.load()\n first.\nMQL\n\u00b6\n\n\nData interchange with MQL\nYou can interchange with MQL data. Text-Fabric can read and write MQL dumps. An\nMQL dump is a text file, like an SQL dump. It contains the instructions to\ncreate and fill a complete database.\nTF.exportMQL()\n1\nTF\n.\nexportMQL\n(\ndbName\n,\n \ndirName\n)\n\n\n\n\n\n\nDescription\nExports the complete TF dataset into single MQL database.\ndirName, dbName\nThe exported file will be written to \ndirName/dbName.mql\n. If \ndirName\n starts\nwith \n~\n, the \n~\n will be expanded to your home directory. Likewise, \n..\n will\nbe expanded to the parent of the current directory, and \n.\n to the current\ndirectory, both only at the start of \ndirName\n.\nCorrespondence TF and MQL\nThe resulting MQL database has the following properties with respect to the\nText-Fabric dataset it comes from:\nthe TF \nslots\n correspond exactly with the MQL \nmonads\n and have the same\n    numbers; provided the monad numbers in the MQL dump are consecutive. In MQL\n    this is not obligatory. Even if there gaps in the monads sequence, we will\n    fill the holes during conversion, so the slots are tightly consecutive;\nthe TF \nnodes\n correspond exactly with the MQL \nobjects\n and have the same\n    numbers\nNode features in MQL\nThe values of TF features are of two types, \nint\n and \nstr\n, and they translate\nto corresponding MQL types \ninteger\n and \nstring\n. The actual values do not\nundergo any transformation.\nThat means that in MQL queries, you use quotes if the feature is a string feature.\nOnly if the feature is a number feature, you may omit the quotes:\n1\n2\n[word sp='verb']\n[verse chapter=1 and verse=1]\n\n\n\n\n\nEnumeration types\nIt is attractive to use eumeration types for the values of a feature, whereever\npossible, because then you can query those features in MQL with \nIN\n and without\nquotes:\n1\n[chapter book IN (Genesis, Exodus)]\n\n\n\n\n\nWe will generate enumerations for eligible features.\nInteger values can already be queried like this, even if they are not part of an\nenumeration. So we restrict ourselves to node features with string values. We\nput the following extra restrictions:\nthe number of distinct values is less than 1000\nall values must be legal C names, in practice: starting with a letter,\n    followed by letters, digits, or \n_\n. The letters can only be plain ASCII\n    letters, uppercase and lowercase.\nFeatures that comply with these restrictions will get an enumeration type.\nCurrently, we provide no ways to configure this in more detail.\nMerged enumeration types\nInstead of creating separate enumeration types for individual features,\nwe collect all enumerated values for all those features into one\nbig enumeration type.\nThe reason is that MQL considers equal values in different types as\ndistinct values. If we had separate types, we could never compare\nvalues for different features.\nValues of edge features are ignored\nThere is no place for edge values in\nMQL. There is only one concept of feature in MQL: object features,\nwhich are node features.\nBut TF edges without values can be seen as node features: nodes are\nmapped onto sets of nodes to which the edges go. And that notion is supported by\nMQL:\nedge features are translated into MQL features of type \nLIST OF id_d\n,\ni.e. lists of object identifiers.\nLegal names in MQL\nMQL names for databases, object types and features must be valid C identifiers\n(yes, the computer language C). The requirements are:\nstart with a letter (ASCII, upper-case or lower-case)\nfollow by any sequence of ASCII upper/lower-case letters or digits or\n    underscores (\n_\n)\navoid being a reserved word in the C language\nSo, we have to change names coming from TF if they are invalid in MQL. We do\nthat by replacing illegal characters by \n_\n, and, if the result does not start\nwith a letter, we prepend an \nx\n. We do not check whether the name is a reserved\nC word.\nWith these provisos:\nthe given \ndbName\n correspond to the MQL \ndatabase name\nthe TF \notypes\n correspond to the MQL \nobjects\nthe TF \nfeatures\n correspond to the MQL \nfeatures\nFile size\nThe MQL export is usually quite massive (500 MB for the Hebrew Bible).\nIt can be compressed greatly, especially by the program \nbzip2\n.\nExisiting database\nIf you try to import an MQL file in Emdros, and there exists already a file or\ndirectory with the same name as the MQL database, your import will fail\nspectacularly. So do not do that. A good way to prevent it is:\nexport the MQL to outside your \ntext-fabric-data\n directory, e.g. to\n    \n~/Downloads\n;\nbefore importing the MQL file, delete the previous copy;\nDelete existing copy\n1\n2\ncd\n ~/Downloads\nrm dataset \n;\n mql -b \n3\n < dataset.mql\n\n\n\n\n\nTF.importMQL()\n1\nTF\n.\nimportMQL\n(\nmqlFile\n,\n \nslotType\n=\nNone\n,\n \notext\n=\nNone\n,\n \nmeta\n=\nNone\n)\n\n\n\n\n\n\nDescription\nConverts an MQL database dump to a Text-Fabric dataset.\nDestination directory\nIt is recommended to call this \nimportMQL\n on a TF instance called with\n1\nlocations\n=\ntargetDir\n,\n \nmodules\n=\n''\n\n\n\n\n\n\nThen the resulting features will be written in the targetDir.\nIn fact, the rules are exactly the same as for \nTF.save()\n.\nslotType\nYou have to tell which object type in the MQL file acts as the slot type,\nbecause TF cannot see that on its own.\notext\nYou can pass the information about sections and text formats as the parameter\n\notext\n. This info will end up in the \notext.tf\n feature. Pass it as a\ndictionary of keys and values, like so:\n1\n2\n3\n4\notext\n \n=\n \n{\n\n    \n'fmt:text-trans-plain'\n:\n \n'{glyphs}{trailer}'\n,\n\n    \n'sectionFeatures'\n:\n \n'book,chapter,verse'\n,\n\n\n}\n\n\n\n\n\n\nmeta\nLikewise, you can add a dictionary of keys and values that will added to the\nmetadata of all features. Handy to add provenance data here:\n1\n2\n3\n4\n5\nmeta\n \n=\n \ndict\n(\n\n    \ndataset\n=\n'DLC'\n,\n\n    \ndatasetName\n=\n'Digital Language Corpus'\n,\n\n    \nauthor\n=\n\"That 's me\"\n,\n\n\n)\n\n\n\n\n\n\nComputed data\n\u00b6\n\n\nPre-computing\nIn order to make the API work, Text-Fabric prepares some data and saves it in\nquick-load format. Most of this data are the features, but there is some extra\ndata needed for the special functions of the WARP features and the L-API.\nNormally, you do not use this data, but since it is there, it might be valuable,\nso we have made it accessible in the \nC\n-api, which we document here.\nC.levels.data\nDescription\nA sorted list of object types plus basic information about them.\nEach entry in the list has the shape\n1\n    \n(\notype\n,\n \naverageSlots\n,\n \nminNode\n,\n \nmaxNode\n)\n\n\n\n\n\n\nwhere \notype\n is the name of the node type, \naverageSlots\n the average size of\nobjects in this type, measured in slots (usually words). \nminNode\n is the first\nnode of this type, \nmaxNode\n the last, and the nodes of this node type are\nexactly the nodes between these two values (including).\nLevel computation and customization\nAll node types have a level, defined by the average amount of slots object of\nthat type usually occupy. The bigger the average object, the lower the levels.\nBooks have the lowest level, words the highest level.\nHowever, this can be overruled. Suppose you have a node type \nphrase\n and above\nit a node type \ncluster\n, i.e. phrases are contained in clusters, but not vice\nversa. If all phrases are contained in clusters, and some clusters have more\nthan one phrase, the automatic level ranking of node types works out well in\nthis case. But if clusters only have very small phrases, and the big phrases do\nnot occur in clusters, then the algorithm may assign a lower rank to clusters\nthan to phrases.\nIn general, it is too expensive to try to compute the levels in a sophisticated\nway. In order to remedy cases where the algorithm assigns wrong levels, you can\nadd a \n@levels\n key to the \notext\n config feature. See\n\ntext\n.\nC.order.data\nDescription\nAn \narray\n of all nodes in the correct order. This is the\norder in which \nN()\n alias \nNode()\n traverses all nodes.\nRationale\nTo order all nodes in the \ncanonical ordering\n is quite a bit of\nwork, and we need this ordering all the time.\nC.rank.data\nDescription\nAn \narray\n of all indices of all nodes in the canonical order\narray. It can be viewed as its inverse.\nOrder arbitrary node sets\nI we want to order a set of nodes in the canonical ordering, we need to know\nwhich position each node takes in the canonical order, in other words, at what\nindex we find it in the \nC.order.data\n array.\nC.levUp.data and C.levDown.data\nDescription\nThese tables feed the \nL.d()\n and \nL.u()\n functions.\nUse with care\nThey consist of a fair amount of megabytes, so they are heavily optimized.\nIt is not advisable to use them directly, it is far better to use the \nL\n functions.\nOnly when every bit of performance waste has to be squeezed out, this raw data\nmight be a deal.\nC.boundary.data\nDescription\nThese tables feed the \nL.n()\n and \nL.p()\n functions.\nIt is a tuple consisting of \nfirstSlots\n and \nlastSlots\n.\nThey are indexes for the first slot\nand last slot of nodes.\nSlot index\nFor each slot, \nfirstSlot\n gives all nodes (except\nslots) that start at that slot, and \nlastSlot\n gives all nodes (except slots)\nthat end at that slot.\nBoth \nfirstSlot\n and \nlastSlot\n are tuples, and the\ninformation for node \nn\n can be found at position \nn-MaxSlot-1\n.\nC.sections.data\nDescription\nLet us assume for the sake of clarity, that the node type of section level 1 is\n\nbook\n, that of level 2 is \nchapter\n, and that of level 3 is \nverse\n. And\nsuppose that we have features, named \nbookHeading\n, \nchapterHeading\n, and\n\nverseHeading\n that give the names or numbers of these.\nCustom names\nNote that the terms \nbook\n, \nchapter\n, \nverse\n are not baked into Text-Fabric.\nIt is the corpus data, especially the \notext\n config feature that\nspells out the names of the sections.\nThen \nC.section.data\n is a tuple of two mappings , let us call them \nchapters\n\nand \nverses\n.\nchapters\n is a mapping, keyed by \nbook\n \nnodes\n, and then by\nby chapter \nheadings\n, giving the corresponding\nchapter \nnode\ns as values.\nverses\n is a mapping, keyed by \nbook\n \nnodes\n, and then\nby chapter \nheadings\n, and then by verse \nheadings\n,\ngiving the corresponding verse \nnode\ns as values.\nSupporting the \nT\n-Api\nThe \nT\n-api is good in mapping nodes unto sections, such as books, chapters,\nverses and back. It knows how many chapters each book has, and how many verses\neach chapter.\nThe \nT\n api is meant to make your life easier when you have to find passage\nlabels by nodes or vice versa. That is why you probably never need to consult\nthe underlying data. But you can! That data is stored in\nMiscellaneous\n\u00b6\n\n\nTF.version\nDescription\nContains the version number of the Text-Fabric\nlibrary.\nTF.banner\nDescription\nContains the name and the version of the Text-Fabric\nlibrary.",
            "title": "Text-Fabric"
        },
        {
            "location": "/Api/General/#text-fabric-api",
            "text": "Tutorial The tutorials for the Hebrew Bible \nand the Uruk Cuneiform Tablets \nput the Text-Fabric API on show for two distinguished (and vastly distinct) corpora. Generic API This is the API of Text-Fabric in general.\nText-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several additions that make working\nwith specific corpora easier. Hebrew Bible:  BHSA Proto-cuneiform tablets from Uruk:  Cunei",
            "title": "Text-Fabric API"
        },
        {
            "location": "/Api/General/#loading",
            "text": "TF=Fabric() 1\n2 from   tf.fabric   import   Fabric  TF   =   Fabric ( locations = directories ,   modules = subdirectories ,   silent = False )    Description Text-Fabric is initialized for a corpus. It will search a set of directories\nand catalog all  .tf  files it finds there.\nThese are the features you can subsequently load. Here  directories  and  subdirectories  are strings with directory names\nseparated by newlines, or iterables of directories. locations, modules The directories specified in  locations  will be searched for  modules , which\nare paths that will be appended to the paths in  locations . All  .tf  files (non-recursively) in any  module  will be added to the feature\nset to be loaded in this session. The order in  modules  is important, because\nif a feature occurs in multiple modules, the last one will be chosen. In this\nway you can easily override certain features in one module by features in an\nother module of your choice. otext@ in modules If modules contain features with a name starting with  otext@ , then the format\ndefinitions in these features will be added to the format definitions in the\nregular  otext  feature (which is a WARP feature). In this way, modules that\ndefine new features for text representation, also can add new formats to the\nText-API. Defaults The  locations  list has a few defaults: 1\n2\n3 ~/Downloads/text-fabric-data\n~/text-fabric-data\n~/github/text-fabric-data   So if you have stored your main Text-Fabric dataset in text-fabric-data  in one of these directories\nyou do not have to pass a location to Fabric. The  modules  list defaults to  [''] . So if you leave it out, Text-Fabric will\njust search the paths specified in  locations . silent If  silent=True  is passed, banners and normal progress messages are suppressed. TF.explore() 1\n2 features   =   TF . explore ( silent = False ,   show = True )  features    or 1\n2 TF . explore ( silent = False ,   show = False )  TF . featureSets    Description This will give you a dictionary of all available features by kind. The kinds\nare:  nodes ,  edges ,  configs ,  computeds . silent With  silent=False  a message containing the total numbers of features is issued. show The resulting dictionary is delivered in  TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api   =   TF . load ( features ,   add = False ,   silent = False )    Description Reads the features indicated by  features  and loads them in memory\nready to be used in the rest of the program. features features  is a string containing space separated feature names, or an\niterable of feature names. The feature names are just the names of  .tf  files\nwithout directory information and without extension. add If later on you want load more features, you can either: add the features to the original  load()  statement and just run it again make a new statement:  TF.load(newfeatures, add=True) . The new features will\n    be added to the same api, so you do not have to to call\n     api.makeAvailableIn(globals())  again after this! silent The features will be loaded rather silently, most messages will be suppressed.\nTime consuming operations will always be announced, so that you know what\nText-Fabric is doing. If  silent=True  is passed, all informational messages\nwill be suppressed. This is handy I you want to load data as part of other\nmethods, on-the-fly. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ())    Description This method will export every member of the API (such as  N ,  F ,  E ,  L ,  T , info ) to the global namespace. From now on, we will omit the  api.  in our\ndocumentation. Contents of the API After having loaded the features by  api = TF.load(...) , the  api  harbours\nyour Text-Fabric API. You can access node feature  mydata  by  api.F.mydata.v(node) , edge\nfeature  mylink  by  api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit\ntedious to write the initial  api.  all the time.\nBy this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital\nletters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored    Description If you want to know which features were found but ignored (because the feature\nis also present in another, later, location), you can use this attribute\nto inspect the ignored features and their locations. loadLog() 1 api . loadlog ()    Description After loading you can view all messages using this method.\nIt also shows the messages that have been suppressed due to  silent=True .",
            "title": "Loading"
        },
        {
            "location": "/Api/General/#navigating-nodes",
            "text": "N() 1\n2 for   n   in   N (): \n     action    Description The result of  N()  is a generator that walks through all nodes in the canonical order  (see below).\nIterating over  N()  delivers you all words and structural elements of\nyour corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in\na suitable order. Occasionally, during the walk you might want to visit\nembedding or embedded nodes to glean some feature information from them. More ways of walking Later, under  Features  there is another convenient way to walk through\nnodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way\nthat you can enumerate all nodes in the order you encounter them if you\nwalk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence,\n    because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the\n    smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a\nbook node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis\n1:1, then a sentence node, then a clause node, a phrase node, and the first word\nnode. Then follow all word nodes in the first phrase, then the phrase node of\nthe second phrase, followed by the word nodes in that phrase. When ever you\nenter a higher structure, you will first get the node corresponding to that\nstructure, and after that the nodes corresponding to the building blocks of that\nstructure. This concept follows the intuition that slot sets with smaller elements come\nbefore slot set with bigger elements, and embedding slot sets come before\nembedded slot sets. Hence, if you enumerate a set of nodes that happens to\nconstitute a tree hierarchy based on slot set embedding, and you enumerate those\nnodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994),  Text Databases. One Database Model and Several\nRetrieval Languages , number 14 in Language and Computers, Editions Rodopi,\nAmsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, http://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by\nDoedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements\navailable when working with an element. That is the advantage of pre-order over\npost-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet )    Description delivers an iterable of nodes as a tuple sorted by the  canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList   =   sorted ( nodes ,   key = sortKey )    Description A function that provides for each node the key to be used to sort nodes in the\ncanonical ordering. That means that the following two pieces of code do the same\nthing: sortNodes(nodeSet)  and  sorted(nodeSet, key=sortKey) . Sorting tuples of nodes Handy to sort things that are not nodes themselves, but data structures with\nnodes in it, e.g. search results: if  results  is a list of tuples of nodes, we\ncould sort them in canonical order like this: 1 sorted ( nodeSet ,   key = lambda   r :   sortKey ( r [ 0 ]))",
            "title": "Navigating nodes"
        },
        {
            "location": "/Api/General/#locality",
            "text": "Local navigation Here are the methods by which you can navigate easily from a node to its\nneighbours: parents and children, previous and next siblings. L The Locality API is exposed as  L  or  Locality . otype parameter In all of the following  L -functions, if the  otype  parameter is passed, the result is filtered and\nonly nodes with  otype=nodeType  are retained. Results of the  L.  functions are tuples, not single nodes Even if an  L -function returns a single node, it is packed in a  tuple .\n  So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ]    L.u() 1 L . u ( node ,   otype = nodeType )    Description Produces an ordered tuple of nodes  upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered.\nThe result never includes  node  itself. L.d() 1 L . d ( node ,   otype = nodeType )    Description Produces an ordered tuple of nodes  downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered.\nThe result never includes  node  itself. L.n() 1 L . n ( node ,   otype = nodeType )    Description Produces an ordered tuple of adjacent  next  nodes. node The node whose right adjacent nodes will be delivered;\ni.e. the nodes whose first slot immediately follow the last slot \nof  node .\nThe result never includes  node  itself. L.p() 1 L . p ( node ,   otype = nodeType )    Description Produces an ordered tuple of adjacent  previous  nodes from  node , i.e. nodes\nwhose last slot just precedes the first slot of  node . Description Produces an ordered tuple of adjacent  previous  nodes. node The node whose lefy adjacent nodes will be delivered;\ni.e. the nodes whose last slot immediately precede the first slot \nof  node . Locality and levels Here is something that is very important to be aware of when using  sortNodes \nand the  L.d(n)  and  L.u(n)  methods. When we order nodes and report on which nodes embed which other nodes, we do not\nonly take into account the sets of slots the nodes occupy, but also their level . See  levels  and  text . Both the  L.d(n)  and  L.u(n)  work as follows: L.d(n)  returns nodes such that embedding nodes come before embedded nodes\n    words) L.u(n)  returns nodes such that embedded nodes come before embedding nodes\n    books) N.B.:  Suppose you have node types  verse  and  sentence , and usually a\nverse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode)  will contain sentence nodes, L.d(sentenceNode)  will  not  contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the\nsame words, then that is a case where: L.d(verseNode)  will contain one sentence node, L.d(sentenceNode)  will contain  one  verse node.",
            "title": "Locality"
        },
        {
            "location": "/Api/General/#text",
            "text": "Overview Here are the functions that enable you to get the actual text in the dataset.\nThere are several things to accomplish here, such as getting text given book, chapter, and verse; given a node, produce the book, chapter and verse indicators in which the node\n    is contained; handle multilingual book names; switch between various text representations. The details of the Text API are dependent on the  warp  feature  otext , which\nis a config feature. T The Text API is exposed as  T  or  Text .",
            "title": "Text"
        },
        {
            "location": "/Api/General/#sections",
            "text": "Section levels In  otext  the main section levels (usually  book ,  chapter ,  verse ) can be\ndefined. It loads the features it needs (so you do not have to specify those\nfeatures, unless you want to use them via  F ). And finally, it makes some\nfunctions available by which you can make handy use of that information. Section levels are generic In this documentation, we call the main section level  book , the second level chapter , and the third level  verse . Text-Fabric, however, is completely\nagnostic about how these levels are called. It is prepared to distinguish three\nsection levels, but how they are called, must be configured in the dataset. The\ntask of the  otext  feature is to declare which node type and feature correspond\nwith which section level. Text-Fabric assumes that the first section level may\nhave multilingual headings, but that section levels two and three have single\nlanguage headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or\nintegers, depending on whether your dataset has declared the corresponding\nfeature  valueType  as  str  or as  int . Conceivably, other works might have chapter and verse numbers\nlike  XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. otext is optional If  otext  is missing, the Text API will not be build. If it exists, but\ndoes not specify sections, that part of the Text API will not be built. Likewise\nfor text representations. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the\nsense that node types that act as containers have a lower level than node types\nthat act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for\nyour dataset, you can configure the right order in the  otext  feature, by means\nof a key  levels  with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign   T.sectionFromNode() 1 T . sectionFromNode ( node ,   lastSlot = False ,   lang = 'en' )    Description Returns the book/chapter/verse indications that correspond to the reference\nnode, which is the first or last slot belonging  n , dependent on  lastSlot .\nThe result is a tuple, consisting of the book name (in language  lang ), the\nchapter number, and the verse number. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the  node  argument\nor the first node. lang The language to be used for the section parts, as far as they are language dependent. crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse.\nIn those cases, this function will return a different results for lastSlot=False  and  lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a  None  in the\ncorresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section ,   lang = 'en' )    Description Given a  section  tuple, return the node of it. section section  consists of a book name (in language  lang ),\nand a chapter number and a verse\nnumber (both as strings or number depending on the value type of the\ncorresponding feature). The verse number may be left out, the result is then a\nchapter node. Both verse and chapter numbers may be left out, the result is then\na book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent.",
            "title": "Sections"
        },
        {
            "location": "/Api/General/#book-names-and-languages",
            "text": "Book names and nodes The names of the books may be available in multiple languages. The book names\nare stored in node features with names of the form  book@ la , where  la  is\nthe  ISO 639  two-letter code for that\nlanguage. Text-Fabric will always load these features. T.languages 1 T . languages    Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node ,   lang = 'en' )    Description gives the name of the book in which a node occurs. node The node in question. lang The  lang  parameter is a two letter language code. The default is  en \n(English). If there is no feature data for the language chosen, the value of the ordinary book  feature of the dataset will be returned. Works for all nodes n  may or may not be a book node. If not,  bookName()  retrieves the\nembedding book node first. T.bookNode() 1 T . bookNode ( name ,   lang = 'en' )    Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the  name  parameter. If  lang  can not be found, the value of the ordinary  book  feature of the\ndataset will be used. If  name  cannot be found in the specified language,  None  will be returned. Function name follows configured section level If your dataset has configured section level one under an other name,\nsay  tablet , then these two methods follow that name. Instead of  T.bookName() \nand  T.bookNode()  we have then  T.tabletName()  and  T.tabletNode() .",
            "title": "Book names and languages"
        },
        {
            "location": "/Api/General/#text-representation",
            "text": "Text formats Text can be represented in multiple ways. We provide a number of formats with\nstructured names. A format name is a string of keywords separated by  - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is  text-orig-full , we assume that every TF dataset defines\nthis format. Remember that the formats are defined in the  otext  warp config feature of your\nset, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats.\nThey do not have to complied with the above-mentioned scheme. T.formats 1 T . formats    Description Show the text representation formats that have been defined in your dataset. T.text() 1 T . text ( nodes ,   fmt = None )    Description Gives the text that corresponds to a bunch of nodes. nodes nodes  can be an arbitrary iterable of nodes.\nNo attempt will be made to sort the nodes.\nIf you need order, it is\nbetter to order the nodes before you feed them to  T.text() . fmt The format of text-representation is given with  fmt , with default  text-orig-full .\nIf the  fmt \ncannot be found, the default is taken.\nIf the default format is not defined in the otext  feature of the dataset,\nthe node numbers will be output instead. This function does not give error messages, because that could easily overwhelm\nthe output stream, especially in a notebook. Non slot nodes allowed In most cases, the nodes fed to  T.text()  are slots, and the formats are\ntemplates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features\ndefined for a non-slot node type. If, for example, your slot type is  glyph , and you want a format that renders\nlexemes, which are not defined for glyphs but for words, you can just define a\nformat in terms of word features. It is your responsibility to take care to use the formats for node types for\nwhich they make sense. Escape whitespace in formats When defining formats in  otext.tf , if you need a newline or tab in the format,\nspecify it as  \\n  and  \\t .",
            "title": "Text representation"
        },
        {
            "location": "/Api/General/#searching",
            "text": "What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are\ninterested in has to be written as a  search template , offered to  S.search() \nwhich returns the matching results as tuples of nodes. S The Search API is exposed as  S  or  Search .",
            "title": "Searching"
        },
        {
            "location": "/Api/General/#search-templates",
            "text": "Search primer A search template consists of a bunch of lines, possibly indented, that specify\nobjects to look for. Here is a simple example: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        word pos=noun gender=feminine number=singular   This template looks for word combinations within a sentence within chapter 2 of\neither Genesis or Exodus, where one of the words is a verb and the other is a\nnoun. Both have a feminine inflection, but the verb is plural and the noun is\nsingular. The indentation signifies embedding, i.e. containment. The two words are\ncontained in the same sentence, the sentence is contained in the chapter, the\nchapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for\nthis. The order of the two words is immaterial. If there are cases where the verb\nfollows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words\nintervening between the noun and the verb, they will be included in the results. Speaking of results: the  S.search()  function returns its results as tuples of\nnodes: 1 ( book ,   chapter ,   sentence ,   word1 ,   word2 )    With these nodes in hand, you can programmatically gather all information about\nthe results that the corpus provides. If the order between the verb and the noun is important, you can specify that as\nan additional constraint. You can give the words a name, and state a relational\ncondition. Here we state that the noun precedes the verb. 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        vb:word pos=verb gender=feminine number=plural\n        nn:word pos=noun gender=feminine number=singular\nnn < vb   This can be written a bit more economically as: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        > word pos=noun gender=feminine number=singular   If you insist that the noun immediately precedes the verb, you can use a\ndifferent relational operator: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular   There are more kinds of relational operators. If the noun must be the first word in the sentence, you can specify it as 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        w:word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\ns =: w   or a bit more economically: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        =: word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural   If the verb must be the last word in the sentence, you can specify it as 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: w:word pos=verb gender=feminine number=plural\ns := w   or a bit more economically: 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n        :=   You can also use the  edge features  in the corpus as\nrelational operators as well. Suppose we have an edge feature  sub  between clauses, such that if main clause m  has subordinate clauses  s1 ,  s2  and  s3 , then 1 E.sub.f(m) = (s1, s2, s3)   You can use this relation in search. Suppose we want to find the noun verb pair\nin subordinate clauses only. We can use this template: 1\n2\n3\n4\n5\n6\n7 book name=Genesis|Exodus\n   chapter number=2\n      m:clause\n        s:clause\n          word pos=verb gender=feminine number=plural\n          :> word pos=noun gender=feminine number=singular\nm -sub> s   or a bit more economically: 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n  chapter number=2\n    clause\n      -sub> clause\n        word    pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular   Read  m -sub> s  as: there is a  sub -arrow from  m  to  s . Edge features may have values.\nFor example, the crossref feature \nis a set of edges between parallel verses, with the levels of confidence\nas values. This number is an integer between 0 and 100.\nWe can ask for parallel verses in an unqualified way: 1\n2 verse\n-crossref> verse   But we can also ask for the cases with a specific confidence: 1\n2 verse\n-crossref=90> verse   or cases with a high confidence: 1\n2 verse\n-crossref>95> verse   or cases with a low confidence: 1\n2 verse\n-crossref<80> verse   All feature conditions that you can assert on node features, you can also\nassert for edge features. If an edge feature is integer valued, such as  crossref \nyou can use comparisons; if it is string valued, you can use regular expressions.\nIn both cases you can also use the other constructs, such as 1\n2 verse\n-crossref=66|77> verse   To get a more specific introduction to search, consult the search tutorials for Hebrew  and Cuneiform . Search template reference General We have these kinds of lines in a template: atom  lines (simple):  indent name:otype features vb:word pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space\n    characters, where tabs count for just 1.  Avoid tabs! . The  name:  part is optional. (with relop):  indent op name:otype features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom\n    and this atom. The preceding atom may be the parent, provided we are at its first child, or\n    it may the preceding sibling. You can leave out the  name:otype features  bit. In that case, the\n    relation holds between the preceding atom and its parent. feature  lines:  features Indent is not significant. Continuation of feature constraints after a\n    preceding atom line or other feature line. This way you can divide lengthy\n    feature constraints over multiple lines. relation  lines:  name operator name s := w m -sub> s m <sub- s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values.\n    See below. white-space or empty  lines Everywhere allowed. Always ignored. Features The  features  above is a specification of what features with which values to\nsearch for. This specification must be written as a white-space separated list\nof  feature specs . A  feature spec  has the form  name   valueSpec , with no space between the  name \nand the  valueSpec .\nThe  valueSpec  may have the following forms: form meaning feature  name  may have any value except  None ! feature  name  must have value  None  (synonymous for:  name  has no value) = values feature  name  has one of the values specified > value feature  name  must be greater than  value < value feature  name  must be less than  value ~ regular expression feature  name  has a value and it matches  regular expression All these forms are also valid as  - name   form >  and  < name   form - , in which case\nthey specify value constraints on edge features.\nThis is only meaningful if the edge feature is declared to have values (most edge features\ndo not have values). Additional constraints There may be no space around the  = , nor the  ~ . name  must be a feature name that exists in the dataset. If it references a\n    feature that is not yet loaded, the feature will be loaded automatically. values  must be a  |  separated list of feature values, no quotes. No spaces\n    around the  | . If you need a space or  |  or  \\  in a value, escape it by a\n     \\ . Escape tabs and newlines as  \\t  and  \\n . When comparing values with  <  and  > : value  must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on\n    string-valued features. regular expression  must be a string that conforms to the Python\n     regular axpression syntax If that syntax prescribes a \\ , you have to write it twice:  \\   \\ . If you need a space in your regular expression, you have to escape it with a\n     \\ . You can do regular expressions only on string-valued features, not on\n    number-valued features. Operator lines Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the\n    same slots are still unequal)\n     # : is unequal (meaning a different node, a clause and a verse that occupy\n    the same slots are still unequal)\n     <   > : before and after (in the  canonical ordering )\n     Slot comparison == : occupy the same slots (identical slot sets)\n     && : overlap (the intersection of both slot sets is not empty)\n     ## : occupy different slots (but they may overlap, the set of slots of the\n    two are different as sets)\n     || : occupy disjoint slots (no slot occupied by the one is also occupied by\n    the other)\n     [[ ]] : embeds and contains (slot set inclusion, in both directions)\n     <<   >> : before and after (with respect to the slots occupied: left ends\n    before right starts and vice versa)\n     <:   :> :  adjacent  before and after (with respect to the slots occupied:\n    left ends immediately before right starts and vice versa)\n     =:  left and right start at the same slot\n     :=  left and right end at the same slot\n     ::  left and right start and end at the same slot\n     Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring\nthat one slot is equal to an other slot, you can require that they are  k-near ,\ni.e. they are at most  k  apart. Here are the relationships where you can do\nthat. Instead of typing the letter  k , provide the actual number you want. <k:   :k> :  k - adjacent  before and after (with respect to the slots\n    occupied: left ends  k -near where right starts and vice versa)\n     =k:  left and right start at  k -near slots\n     :k=  left and right end at  k -near slots\n     :k:  left and right start and end at  k -near slots\n     Based on edge features - name >   < name - : connected by the edge feature  name in both directions; these forms work for edges that do and do not have values;      - name   valueSpec >   < name   valueSpec - : connected by the edge feature  name* in both directions; these forms work only for edges that do have values. S.relationsLegend 1 S . relationsLegend    Description Gives dynamic help about the basic relations that you can use in your search\ntemplate. It includes the edge features that are available in your dataset. S.search() 1 S . search ( searchTemplate ,   limit = None )    Description Searches for combinations of nodes that together match a search template.\nThis method returns a  generator  which yields the results one by one. One result\nis a tuple of nodes, where each node corresponds to an  atom -line in your search template . searchTemplate The search template is a string that conforms to the rules described above. limit If  limit  is a number, it will fetch only that many results. Generator versus tuple If  limit  is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and\ntranslated into a search plan. The following methods expose parts of the search\nprocess, and may provide you with useful information in case the search does not\ndeliver what you expect. see the plan the method  S.showPlan()  below shows you at a glance the correspondence\nbetween the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate ,   strategy = None ,   silent = False )    Description Your search template will be checked, studied, the search\nspace will be narrowed down, and a plan for retrieving the results will be set\nup. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched\nmatters a lot.\nThe search strategy is an implementation detail, but we bring\nit to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x')  and you will get a list of options reported to\nchoose from. Feel free to experiment. To see what the strategies do,\nsee the  code . silent If you want to suppress most of the output, say  silent=True . S.showPlan() 1 S . showPlan ( details = False )    Description Search results are tuples of nodes and the plan shows which part of the tuple\ncorresponds to which part of the search template. details If you say  details=True , you also get an overview of the search space and a\ndescription of how the results will be retrieved. after S.study() This function is only meaningful after a call to  S.study() .",
            "title": "Search templates"
        },
        {
            "location": "/Api/General/#search-results",
            "text": "Preparation versus result fetching The method  S.search()  above combines the interpretation of a given\ntemplate, the setting up of a plan, the constraining of the search space\nand the fetching of results. Here are a few methods that do actual result fetching.\nThey must be called after a previous  S.search()  or  S.study() . S.count() 1 S . count ( progress = None ,   limit = None )    Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message.\nThe frequency is  progress  results, default every 100. limit Fetch results up to a given  limit , default 1000.\nSetting  limit  to 0 or a negative value means no limit: all results will be\ncounted. why needed You typically need this in cases where result fetching turns out to\nbe (very) slow. generator versus list len(S.results())  does not work, because  S.results()  is a generator\nthat delivers its results as they come. S.fetch() 1 S . fetch ( limit = None )    Description Finally, you can retrieve the results. The result of  fetch()  is not a list of\nall results, but a  generator . It will retrieve results as long as they are\nrequested and their are still results. limit Tries to get that many results and collects them in a tuple.\nSo if limit is not  None , the result is a tuple with a known length. Iterating over the  fetch()  generator You typically fetch results by saying: 1\n2\n3\n4 i   =   0  for   r   in   S . results (): \n     do_something ( r [ 0 ]) \n     do_something_else ( r [ 1 ])    Alternatively, you can set the  limit  parameter, to ask for just so many\nresults. They will be fetched, and when they are all collected, returned as a\ntuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 )    gives you the first bunch of results quickly. S.glean() 1 S . glean ( r )    Description A search result is just a tuple of nodes that correspond to your template, as\nindicated by  showPlan() . Nodes give you access to all information that the\ncorpus has about it. The  glean()  function is here to just give you a first impression quickly.   r Pass a raw result tuple  r , and you get a string indicating where it occurs,\nin terms of sections, \nand what text is associated with the results. Inspecting results 1\n2 for   result   in   S . fetch ( limit = 10 ): \n     print ( S . glean ( result ))    is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been\nobtained by search or not. More ways of showing results If you work in one of the corpora for which the TF-API has been extended,\nyou will be provided with more powerful methods  show()  and  table() \nto display your results. See  Cunei  and  Bhsa .",
            "title": "Search results"
        },
        {
            "location": "/Api/General/#node-features",
            "text": "Node Features F The node features API is exposed as  F  ( Fs ) or  Feature  ( FeatureString ). Fall() aka AllFeatures() 1\n2 Fall ()  AllFeatures ()    Description Returns a sorted list of all usable, loaded node feature names. F. feature  aka Feature. feature 1\n2 F . part_of_speech  Feature . part_of_speech    Description Returns a sub-api for retrieving data that is stored in node features.\nIn this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not\na valid python identifier, you can not use this function,\nyou should use  Fs  instead. Fs(feature) aka FeatureString(feature) 1\n2\n3\n4 Fs ( feature )  FeatureString ( feature )  Fs ( 'part-of-speech' )  FeatureString ( 'part-of-speech' )    Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in\nthe variable  feature . In lines 3 and 4, \nwe assume there is a feature called part-of-speech .\nNote that this is not a valid name in Python, yet we\ncan work with features with such names. Both methods have identical results Suppose we have just issued  feature = 'pos'.\nThen the result of Fs(feature) and F.pos` is identical. In most cases  F  works just fine, but  Fs  is needed in two cases: if we need to work with a feature whose name is not a valid\n  Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .v(node) 1 F . part_of_speech . v ( node )    Description Get the value of a  feature , such as  part_of_speech  for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1\n2 F . part_of_speech . s ( value )  F . part_of_speech . s ( 'noun' )    Description Returns a generator of all nodes in the canonical order with a given value for a given feature.\nThis is an other way to walk through nodes than using  N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F.` feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None )    Description Inspect the values of  feature  (in this example:  part_of_speech )\nand see how often they occur. The result is a\nlist of pairs  (value, frequency) , ordered by  frequency , highest frequencies\nfirst. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those\ntypes will be counted. F.otype otype  is a special node feature and has additional capabilities. Description F.otype.slotType  is the node type that can fill the slots (usually:  word ) F.otype.maxSlot  is the largest slot number F.otype.maxNode  is the largest node number F.otype.all  is a list of all  otypes  from big to small (from books through\n    clauses to words) F.otype.sInterval(otype)  is like  F.otype.s(otype) , but instead of\n    returning you a range to iterate over, it will give you the starting and\n    ending nodes of  otype . This makes use of the fact that the data is so\n    organized that all node types have single ranges of nodes as members.",
            "title": "Node features"
        },
        {
            "location": "/Api/General/#edge-features",
            "text": "Edge Features E The edge features API is exposed as  E  ( Es ) or  Edge  ( EdgeString ). Eall() aka AllEdges() 1\n2 Eall ()  AllEdges ()    Description Returns a sorted list of all usable, loaded edge feature names. E. feature  aka Edge. feature 1\n2 E . head  Feature . head    Description Returns a sub-api for retrieving data that is stored in edge features.\nIn this example, we assume there is a feature called head . Tricky feature names If the feature name is not\na valid python identifier, you can not use this function,\nyou should use  Es  instead. Es(feature) aka EdgeString(feature) 1\n2\n3\n4 Es ( feature )  EdgeString ( feature )  Es ( 'head' )  EdgeString ( 'head' )    Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in\nthe variable  feature . In lines 3 and 4, \nwe assume there is a feature called head . Both methods have identical results Suppose we have just issued  feature = 'head'.\nThen the result of Es(feature) and E.pos` is identical. In most cases  E  works just fine, but  Es  is needed in two cases: if we need to work with a feature whose name is not a valid\n  Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .f(node) 1 E . head . f ( node )    Description Get the nodes reached by  feature -edges  from  a certain node.\nThese edges must be specified in  feature , in this case  head .\nThe result is an ordered tuple\n(again, in the  canonical order . The members of the\nresult are just nodes, if  head  describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than  None . node The node  from  which the edges in question start. E. feature .t(node) 1 E . head . t ( node )    Description Get the nodes reached by  feature -edges  to  a certain node.\nThese edges must be specified in  feature , in this case  head .\nThe result is an ordered tuple\n(again, in the  canonical order . The members of the\nresult are just nodes, if  feature  describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value. If there are no edges to  n , the empty tuple is returned, rather than  None . node The node  to  which the edges in question go. E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None ,   nodeTypesTo = None )    Description If the edge feature has no values, simply return the number of node pairs\nbetween an edge of this kind exists. If the edge feature does have values, we \ninspect them\nand see how often they occur. The result is a\nlist of pairs  (value, frequency) , ordered by  frequency , highest frequencies\nfirst. nodeTypesFrom If not  None ,\nonly the values for edges that start from a node with type\nwithin  nodeTypesFrom \nwill be counted. nodeTypesTo If not  None ,\nonly the values for edges that go to a node with type\nwithin  nodeTypesTo \nwill be counted. E.oslots oslots  is a special edge feature and is mainly used to construct other parts\nof the API. It has less capabilities, and you will rarely need it. It does not\nhave  .f  and  .t  methods, but an  .s  method instead. Description E.oslots.s(node) \nGives the sorted list of slot numbers linked to a node,\nor put otherwise: the slots that  support  that node. node The node whose slots are being delivered.",
            "title": "Edge features"
        },
        {
            "location": "/Api/General/#messaging",
            "text": "Timed messages Error and informational messages can be issued, with a time indication. info(), error() 1 info ( msg ,   tm = True ,   nl = True )    Description Sends a message to standard output, possibly with time and newline.  if  info()  is being used, the message is sent to  stdout ;  if  error()  is being used, the message is sent to  stderr ; In a Jupyter notebook, the standard error is displayed with\na reddish background colour. tm If  True , an indicator of the elapsed time will be prepended to the message. nl If  True  a newline will be appended. indent() 1 indent ( level = None ,   reset = False )    Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer.  Subsequent info()  and  error()  will display their messages with this indent. reset If  True , the elapsed time to will be reset to 0 at the given level.\nTimers at different levels are independent of each other.",
            "title": "Messaging"
        },
        {
            "location": "/Api/General/#saving-features",
            "text": "TF.save() 1 TF . save ( nodeFeatures = {},   edgeFeatures = {},   metaData = {},   module = None )    Description If you have collected feature data in dictionaries, keyed by the\nnames of the features, and valued by their feature data,\nthen you can save that data to  .tf  feature files on disk. It is this easy to export new data as features:\ncollect the data and metadata of\nthe features and \nfeed it in an orderly way to  TF.save()  and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and\nstrings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or\ndictionaries as values. These sets should be sets of nodes (integers!), and\nthese dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from  metaData , which is a dictionary\nmapping a feature name to its metadata. value types The type of the values should conform to  @valueType  ( int  or  str ), which\nmust be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have\nto say so, by specifying  edgeValues = True  in the metadata for that feature. generic metadata metaData  may also contain fields under\n  the empty name. These fields will be added to all features in  nodeFeatures  and\n   edgeFeatures . config features If you need to write the  config  feature  otext ,\nwhich is a metadata-only feature, just\nadd the metadata under key  otext  in  metaData  and make sure\nthat  otext  is not a key in  nodeFeatures  nor in edgeFeatures .\nThese fields will be written into the separate config feature  otext ,\nwith no data associated. save location The (meta)data will be written to the very last module in the list of locations\nthat you specified when calling  Fabric()  or to what you passed as  module  in\nthe same location. If that module does not exist, it will be created in the last location . If both  locations  and  modules  are empty, writing will take place\nin the current directory.",
            "title": "Saving features"
        },
        {
            "location": "/Api/General/#clearing-the-cache",
            "text": "TF.clearCache() 1 TF . clearCache ()    Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the\noriginal data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without\nany changes in the data, where you might want to clear the cache of precomputed\nresults. Calling this function just does it, and it is equivalent with manually removing\nall  .tfx  files inside the hidden  .tf  directory inside your dataset. No need to load It is not needed to execute a  TF.load()  first.",
            "title": "Clearing the cache"
        },
        {
            "location": "/Api/General/#mql",
            "text": "Data interchange with MQL You can interchange with MQL data. Text-Fabric can read and write MQL dumps. An\nMQL dump is a text file, like an SQL dump. It contains the instructions to\ncreate and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName ,   dirName )    Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to  dirName/dbName.mql . If  dirName  starts\nwith  ~ , the  ~  will be expanded to your home directory. Likewise,  ..  will\nbe expanded to the parent of the current directory, and  .  to the current\ndirectory, both only at the start of  dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the\nText-Fabric dataset it comes from: the TF  slots  correspond exactly with the MQL  monads  and have the same\n    numbers; provided the monad numbers in the MQL dump are consecutive. In MQL\n    this is not obligatory. Even if there gaps in the monads sequence, we will\n    fill the holes during conversion, so the slots are tightly consecutive; the TF  nodes  correspond exactly with the MQL  objects  and have the same\n    numbers Node features in MQL The values of TF features are of two types,  int  and  str , and they translate\nto corresponding MQL types  integer  and  string . The actual values do not\nundergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature.\nOnly if the feature is a number feature, you may omit the quotes: 1\n2 [word sp='verb']\n[verse chapter=1 and verse=1]   Enumeration types It is attractive to use eumeration types for the values of a feature, whereever\npossible, because then you can query those features in MQL with  IN  and without\nquotes: 1 [chapter book IN (Genesis, Exodus)]   We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an\nenumeration. So we restrict ourselves to node features with string values. We\nput the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter,\n    followed by letters, digits, or  _ . The letters can only be plain ASCII\n    letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type.\nCurrently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features,\nwe collect all enumerated values for all those features into one\nbig enumeration type. The reason is that MQL considers equal values in different types as\ndistinct values. If we had separate types, we could never compare\nvalues for different features. Values of edge features are ignored There is no place for edge values in\nMQL. There is only one concept of feature in MQL: object features,\nwhich are node features.\nBut TF edges without values can be seen as node features: nodes are\nmapped onto sets of nodes to which the edges go. And that notion is supported by\nMQL:\nedge features are translated into MQL features of type  LIST OF id_d ,\ni.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers\n(yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or\n    underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do\nthat by replacing illegal characters by  _ , and, if the result does not start\nwith a letter, we prepend an  x . We do not check whether the name is a reserved\nC word. With these provisos: the given  dbName  correspond to the MQL  database name the TF  otypes  correspond to the MQL  objects the TF  features  correspond to the MQL  features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible).\nIt can be compressed greatly, especially by the program  bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or\ndirectory with the same name as the MQL database, your import will fail\nspectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your  text-fabric-data  directory, e.g. to\n     ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1\n2 cd  ~/Downloads\nrm dataset  ;  mql -b  3  < dataset.mql   TF.importMQL() 1 TF . importMQL ( mqlFile ,   slotType = None ,   otext = None ,   meta = None )    Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this  importMQL  on a TF instance called with 1 locations = targetDir ,   modules = ''    Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for  TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type,\nbecause TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the  otext.tf  feature. Pass it as a\ndictionary of keys and values, like so: 1\n2\n3\n4 otext   =   { \n     'fmt:text-trans-plain' :   '{glyphs}{trailer}' , \n     'sectionFeatures' :   'book,chapter,verse' ,  }    meta Likewise, you can add a dictionary of keys and values that will added to the\nmetadata of all features. Handy to add provenance data here: 1\n2\n3\n4\n5 meta   =   dict ( \n     dataset = 'DLC' , \n     datasetName = 'Digital Language Corpus' , \n     author = \"That 's me\" ,  )",
            "title": "MQL"
        },
        {
            "location": "/Api/General/#computed-data",
            "text": "Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in\nquick-load format. Most of this data are the features, but there is some extra\ndata needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable,\nso we have made it accessible in the  C -api, which we document here. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1      ( otype ,   averageSlots ,   minNode ,   maxNode )    where  otype  is the name of the node type,  averageSlots  the average size of\nobjects in this type, measured in slots (usually words).  minNode  is the first\nnode of this type,  maxNode  the last, and the nodes of this node type are\nexactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of\nthat type usually occupy. The bigger the average object, the lower the levels.\nBooks have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type  phrase  and above\nit a node type  cluster , i.e. phrases are contained in clusters, but not vice\nversa. If all phrases are contained in clusters, and some clusters have more\nthan one phrase, the automatic level ranking of node types works out well in\nthis case. But if clusters only have very small phrases, and the big phrases do\nnot occur in clusters, then the algorithm may assign a lower rank to clusters\nthan to phrases. In general, it is too expensive to try to compute the levels in a sophisticated\nway. In order to remedy cases where the algorithm assigns wrong levels, you can\nadd a  @levels  key to the  otext  config feature. See text . C.order.data Description An  array  of all nodes in the correct order. This is the\norder in which  N()  alias  Node()  traverses all nodes. Rationale To order all nodes in the  canonical ordering  is quite a bit of\nwork, and we need this ordering all the time. C.rank.data Description An  array  of all indices of all nodes in the canonical order\narray. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know\nwhich position each node takes in the canonical order, in other words, at what\nindex we find it in the  C.order.data  array. C.levUp.data and C.levDown.data Description These tables feed the  L.d()  and  L.u()  functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized.\nIt is not advisable to use them directly, it is far better to use the  L  functions. Only when every bit of performance waste has to be squeezed out, this raw data\nmight be a deal. C.boundary.data Description These tables feed the  L.n()  and  L.p()  functions.\nIt is a tuple consisting of  firstSlots  and  lastSlots .\nThey are indexes for the first slot\nand last slot of nodes. Slot index For each slot,  firstSlot  gives all nodes (except\nslots) that start at that slot, and  lastSlot  gives all nodes (except slots)\nthat end at that slot.\nBoth  firstSlot  and  lastSlot  are tuples, and the\ninformation for node  n  can be found at position  n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is  chapter , and that of level 3 is  verse . And\nsuppose that we have features, named  bookHeading ,  chapterHeading , and verseHeading  that give the names or numbers of these. Custom names Note that the terms  book ,  chapter ,  verse  are not baked into Text-Fabric.\nIt is the corpus data, especially the  otext  config feature that\nspells out the names of the sections. Then  C.section.data  is a tuple of two mappings , let us call them  chapters \nand  verses . chapters  is a mapping, keyed by  book   nodes , and then by\nby chapter  headings , giving the corresponding\nchapter  node s as values. verses  is a mapping, keyed by  book   nodes , and then\nby chapter  headings , and then by verse  headings ,\ngiving the corresponding verse  node s as values. Supporting the  T -Api The  T -api is good in mapping nodes unto sections, such as books, chapters,\nverses and back. It knows how many chapters each book has, and how many verses\neach chapter. The  T  api is meant to make your life easier when you have to find passage\nlabels by nodes or vice versa. That is why you probably never need to consult\nthe underlying data. But you can! That data is stored in",
            "title": "Computed data"
        },
        {
            "location": "/Api/General/#miscellaneous",
            "text": "TF.version Description Contains the version number of the Text-Fabric\nlibrary. TF.banner Description Contains the name and the version of the Text-Fabric\nlibrary.",
            "title": "Miscellaneous"
        },
        {
            "location": "/Api/Bhsa/",
            "text": "BHSA\n\u00b6\n\n\nAbout\n\u00b6\n\n\nThe module \nbhsa.py\n\ncontains a number of handy functions on top of Text-Fabric and especially its \n\nSearch\n part.\n\n\nSet up\n\u00b6\n\n\nfrom tf.extra.bhsa import Bhsa\nimport Bhsa\nThe \nBhsa\n API is distributed with Text-Fabric.\nYou have to import it into your program.\nInitialisation\n\u00b6\n\n\nBhsa()\n1\nB\n \n=\n \nBhsa\n(\napi\n,\n \n'notebook'\n,\n \nversion\n=\nVERSION\n)\n\n\n\n\n\n\nDescription\nSilently loads some additional features, and \nB\n\nwill give access to some extra functions.\napi\nThe API resulting from an earlier call \nTF.load()\nSet up\nThis module comes in action after you have set up TF and loaded some features, e.g.\n1\n2\n3\n4\n5\n6\nVERSION\n \n=\n \n'2017'\n\n\nTF\n \n=\n \nFabric\n(\nlocations\n=\nf\n'~/github/etcbc/bhsa/tf/{VERSION}'\n)\n\n\napi\n \n=\n \nTF\n.\nload\n(\n'''\n\n\n  function sp gn nu\n\n\n'''\n)\n\n\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nThen we add the functionality of the \nbhsa\n module by a call to \nBhsa()\n.\nnotebook\nThis should be the name\nof your current notebook (without the \n.ipynb\n extension).\nThe Bhsa API will use this to generate a link to your notebook\non GitHub and NBViewer.\nLinking\n\u00b6\n\n\nB.shbLink()\n1\nB\n.\nshbLink\n(\nnode\n,\n \ntext\n=\nNone\n)\n\n\n\n\n\n\nDescription\nProduces a link to SHEBANQ\nnode\nnode\n can be an arbitrary node. The link targets the verse that\ncontains the first word contained by the node.\ntext\nYou may provide the text to be displayed as the link.\nThen the\npassage indicator (book chapter:verse) will be put\nin the tooltip (title) of the link.\nIf you do not provide a link text,\nthe passage indicator (book chapter:verse) will be chosen.\nWord 100000 on SHEBANQ\n1\nB\n.\nshbLink\n(\n100000\n)\n\n\n\n\n\n\nPlain display\n\u00b6\n\n\nStraightforward display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table.\nB.plain()\n1\nB\n.\nplain\n(\nnode\n,\n \nlinked\n=\nTrue\n,\n \nwithNodes\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a simple way.\nnode\nnode\n is a node of arbitrary type.\nlinked\nlinked\n indicates whether the result should be a link to SHEBANQ\nto the appropriate book/chapter/verse.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say \nasString=True\n.\nB.plainTuple()\n1\nB\n.\nplainTuple\n(\nnodes\n,\n \nseqNumber\n,\n \nlinked\n=\n1\n,\n \nwithNodes\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells.\nnodes\nnodes\n is an iterable (list, tuple, set, etc) of arbitrary nodes.\nseqNumber\nseqNumber\n is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes.\nlinked\nlinked=1\n the column number where the cell contents is\nlinked to\nthe relevant passage in to SHEBANQ;\n(the first data column is column 1)\nwithNodes, asString\nSame as in \nB.plain()\n.\nB.table()\n1\n2\n3\n4\n5\n6\n7\nB\n.\ntable\n(\n\n  \nresults\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nlinked, withNodes, asString\nSame as in \nB.plainTuple()\n.\nPretty display\n\u00b6\n\n\nGraphical display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way.\nB.prettySetup()\nDescription\nIn pretty displays, nodes are shown together with the values of a selected\nset of features. \nWith this function you can add features to the display.\nfeatures\nA string or iterable of feature names.\nThese features will be loaded automatically.\nIn pretty displays these features will show up as \nfeature=value\n,\nprovided the value is not \nNone\n, or something like None.\nAutomatic loading\nThese features will load automatically, no explicit loading is\nnecessary.\nnoneValues\nA set of values for which no display should be generated.\nThe default set is \nNone\n and the strings \nNA\n, \nnone\n, \nunknown\n.\nNone is useful\nKeep \nNone\n in the set. If not, all custom features will be displayed\nfor all kinds of nodes. So you will see clause types on words,\n  and part of speech on clause atoms, al with value \nNone\n.\nSuppress common values\nYou can use \nnoneValues\n also to suppress the normal values of a feature,\nin order to attrect attention to the more special values, e.g.\n1\nnoneValues\n=\n{\nNone\n,\n \n'NA'\n,\n \n'unknown'\n,\n \n'm'\n,\n \n'sg'\n,\n \n'p3'\n}\n\n\n\n\n\n\nNone values affect all features\nBeware of putting to much in \nnoneValues\n.\nThe contents of \nnoneValues\n affect the display of\nall features, not only the custom features.\nB.pretty()\n1\nB\n.\npretty\n(\nnode\n,\n \nwithNodes\n=\nFalse\n,\n \nsuppress\n=\nset\n(),\n \nhighlights\n=\n{})\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a graphical way.\nnode\nnode\n is a node of arbitrary type.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nsuppress\nsuppress=set()\n is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively.\nHighlighting\nWhen nodes such as verses and sentences are displayed by \npretty()\n,\ntheir contents is also displayed. You can selectively highlight\nthose parts.\nhighlights\nhighlights={}\n is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted.\nIf \nhighlights\n is a set, its nodes will be highlighted with a default color (yellow).\nIf it is a dictionary, it should map nodes to colors.\nAny color that is a valid \n\nCSS color\n\nqualifies.\nIf you map a node to the empty string, it will get the default highlight color.\ncolor names\nThe link above points to a series of handy color names and their previews.\nB.prettyTuple()\n1\n2\n3\n4\n5\n6\n7\nB\n.\nprettyTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes.\nBy verse\nWe examine all nodes in the tuple.\nWe collect and show all verses in which they\noccur and highlight the material corresponding to all the nodes in the tuple.\nThe highlighting can be tweaked by the optional \ncolorMap\n parameter.\nnodes, seqNumber, withNodes\nSame as in \nB.plainTuple()\n.\nsuppress\nSame as in \nB.pretty()\n.\ncolorMap\nThe nodes of the tuple will be highlighted.\nIf \ncolorMap\n is \nNone\n or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow.\nBut you can assign different colors to the members of the tuple:\n\ncolorMap\n must be a dictionary that maps the positions in a tuple \nto a color.\nIf a position is not mapped, it will not be highlighted.\nIf it is mapped to the empty string, it gets the default highlight color.\nOtherwise, it should be mapped to a string that is a valid\n    \nCSS color\n.\ncolor names\nThe link above points to a series of handy color names and their previews.\nhighlights\nSame as in \nB.pretty()\n.\nhighlights takes precedence over colorMap\nIf both \nhighlights\n and \ncolorMap\n are given, \ncolorMap\n is ignored.\nIf you need to micro-manage, \nhighlights\n is your thing.\nWhenever possible, use \ncolorMap\n.  \none big highlights dictionary\nIt is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run \nprettyTuple()\n for many different tuples with the same \nhighlights\n.\nIt does not harm performance if \nhighlights\n maps lots of nodes outside the tuple as well.\nB.show()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\n.\nshow\n(\n\n  \nresults\n,\n\n  \ncondensed\n=\nTrue\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe elements of the list are displayed by \nB.prettyTuple()\n.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\ncondensed\ncondensed\n indicates one of two modes of displaying the result list:\nTrue\n: instead of showing all results one by one,\n  we show all verses with all results in it highlighted.\n  That way, we blur the distinction between the individual results,\n  but it is easier to oversee where the results are.\n  This is how SHEBANQ displays its query results.\nFalse\n: make a separate display for each result tuple.\n  This gives the best account of the exact result set.\nmixing up highlights\nCondensing may mix-up the highlight coloring.\nIf a node occurs in two results, at different positions\nin the tuple, the \ncolorMap\n wants to assign it two colors!\nYet one color will be chosen, and it is unpredictable which one.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nwithNodes, suppress, colorMap, highlights\nSame as in \nB.prettyTuple()\n.\nSearch\n\u00b6\n\n\nB.search()\n1\nB\n.\nsearch\n(\nquery\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nSearches in the same way as the generic Text-Fabric \nS.search()\n.\nBut whereas the \nS\n version returns a generator which yields the results\none by one, the \nB\n version collects all results and sorts them.\nIt then reports the number of results.\nquery\nquery\n is the search template that has to be searched for.\nsilent\nsilent\n: if \nTrue\n it will suppress the reporting of the number of results.\nsearch template reference\nSee the \nsearch template reference",
            "title": "Hebrew Bible"
        },
        {
            "location": "/Api/Bhsa/#bhsa",
            "text": "",
            "title": "BHSA"
        },
        {
            "location": "/Api/Bhsa/#about",
            "text": "The module  bhsa.py \ncontains a number of handy functions on top of Text-Fabric and especially its  Search  part.",
            "title": "About"
        },
        {
            "location": "/Api/Bhsa/#set-up",
            "text": "from tf.extra.bhsa import Bhsa import Bhsa The  Bhsa  API is distributed with Text-Fabric.\nYou have to import it into your program.",
            "title": "Set up"
        },
        {
            "location": "/Api/Bhsa/#initialisation",
            "text": "Bhsa() 1 B   =   Bhsa ( api ,   'notebook' ,   version = VERSION )    Description Silently loads some additional features, and  B \nwill give access to some extra functions. api The API resulting from an earlier call  TF.load() Set up This module comes in action after you have set up TF and loaded some features, e.g. 1\n2\n3\n4\n5\n6 VERSION   =   '2017'  TF   =   Fabric ( locations = f '~/github/etcbc/bhsa/tf/{VERSION}' )  api   =   TF . load ( '''    function sp gn nu  ''' )  api . makeAvailableIn ( globals ())    Then we add the functionality of the  bhsa  module by a call to  Bhsa() . notebook This should be the name\nof your current notebook (without the  .ipynb  extension).\nThe Bhsa API will use this to generate a link to your notebook\non GitHub and NBViewer.",
            "title": "Initialisation"
        },
        {
            "location": "/Api/Bhsa/#linking",
            "text": "B.shbLink() 1 B . shbLink ( node ,   text = None )    Description Produces a link to SHEBANQ node node  can be an arbitrary node. The link targets the verse that\ncontains the first word contained by the node. text You may provide the text to be displayed as the link.\nThen the\npassage indicator (book chapter:verse) will be put\nin the tooltip (title) of the link.\nIf you do not provide a link text,\nthe passage indicator (book chapter:verse) will be chosen. Word 100000 on SHEBANQ 1 B . shbLink ( 100000 )",
            "title": "Linking"
        },
        {
            "location": "/Api/Bhsa/#plain-display",
            "text": "Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table. B.plain() 1 B . plain ( node ,   linked = True ,   withNodes = False ,   asString = False )    Description Displays the material that corresponds to a node in a simple way. node node  is a node of arbitrary type. linked linked  indicates whether the result should be a link to SHEBANQ\nto the appropriate book/chapter/verse. withNodes withNodes  indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say  asString=True . B.plainTuple() 1 B . plainTuple ( nodes ,   seqNumber ,   linked = 1 ,   withNodes = False ,   asString = False )    Description Displays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells. nodes nodes  is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber  is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes. linked linked=1  the column number where the cell contents is\nlinked to\nthe relevant passage in to SHEBANQ;\n(the first data column is column 1) withNodes, asString Same as in  B.plain() . B.table() 1\n2\n3\n4\n5\n6\n7 B . table ( \n   results , \n   start = 1 ,   end = len ( results ), \n   linked = 1 , \n   withNodes = False , \n   asString = False ,  )    Description Displays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable. results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. linked, withNodes, asString Same as in  B.plainTuple() .",
            "title": "Plain display"
        },
        {
            "location": "/Api/Bhsa/#pretty-display",
            "text": "Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way. B.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected\nset of features. \nWith this function you can add features to the display. features A string or iterable of feature names.\nThese features will be loaded automatically.\nIn pretty displays these features will show up as  feature=value ,\nprovided the value is not  None , or something like None. Automatic loading These features will load automatically, no explicit loading is\nnecessary. noneValues A set of values for which no display should be generated.\nThe default set is  None  and the strings  NA ,  none ,  unknown . None is useful Keep  None  in the set. If not, all custom features will be displayed\nfor all kinds of nodes. So you will see clause types on words,\n  and part of speech on clause atoms, al with value  None . Suppress common values You can use  noneValues  also to suppress the normal values of a feature,\nin order to attrect attention to the more special values, e.g. 1 noneValues = { None ,   'NA' ,   'unknown' ,   'm' ,   'sg' ,   'p3' }    None values affect all features Beware of putting to much in  noneValues .\nThe contents of  noneValues  affect the display of\nall features, not only the custom features. B.pretty() 1 B . pretty ( node ,   withNodes = False ,   suppress = set (),   highlights = {})    Description Displays the material that corresponds to a node in a graphical way. node node  is a node of arbitrary type. withNodes withNodes  indicates whether node numbers should be displayed. suppress suppress=set()  is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively. Highlighting When nodes such as verses and sentences are displayed by  pretty() ,\ntheir contents is also displayed. You can selectively highlight\nthose parts. highlights highlights={}  is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted. If  highlights  is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors.\nAny color that is a valid  CSS color \nqualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. B.prettyTuple() 1\n2\n3\n4\n5\n6\n7 B . prettyTuple ( \n   nodes ,   seqNumber , \n   withNodes = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes. By verse We examine all nodes in the tuple.\nWe collect and show all verses in which they\noccur and highlight the material corresponding to all the nodes in the tuple.\nThe highlighting can be tweaked by the optional  colorMap  parameter. nodes, seqNumber, withNodes Same as in  B.plainTuple() . suppress Same as in  B.pretty() . colorMap The nodes of the tuple will be highlighted.\nIf  colorMap  is  None  or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap  must be a dictionary that maps the positions in a tuple \nto a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid\n     CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in  B.pretty() . highlights takes precedence over colorMap If both  highlights  and  colorMap  are given,  colorMap  is ignored. If you need to micro-manage,  highlights  is your thing.\nWhenever possible, use  colorMap .   one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run  prettyTuple()  for many different tuples with the same  highlights .\nIt does not harm performance if  highlights  maps lots of nodes outside the tuple as well. B.show() 1\n2\n3\n4\n5\n6\n7\n8\n9 B . show ( \n   results , \n   condensed = True , \n   start = 1 ,   end = len ( results ), \n   withNodes = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays an iterable of tuples of nodes.\nThe elements of the list are displayed by  B.prettyTuple() . results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. condensed condensed  indicates one of two modes of displaying the result list: True : instead of showing all results one by one,\n  we show all verses with all results in it highlighted.\n  That way, we blur the distinction between the individual results,\n  but it is easier to oversee where the results are.\n  This is how SHEBANQ displays its query results. False : make a separate display for each result tuple.\n  This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring.\nIf a node occurs in two results, at different positions\nin the tuple, the  colorMap  wants to assign it two colors!\nYet one color will be chosen, and it is unpredictable which one. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. withNodes, suppress, colorMap, highlights Same as in  B.prettyTuple() .",
            "title": "Pretty display"
        },
        {
            "location": "/Api/Bhsa/#search",
            "text": "B.search() 1 B . search ( query ,   silent = False )    Description Searches in the same way as the generic Text-Fabric  S.search() .\nBut whereas the  S  version returns a generator which yields the results\none by one, the  B  version collects all results and sorts them.\nIt then reports the number of results. query query  is the search template that has to be searched for. silent silent : if  True  it will suppress the reporting of the number of results. search template reference See the  search template reference",
            "title": "Search"
        },
        {
            "location": "/Api/Cunei/",
            "text": "Cunei\n\u00b6\n\n\nAbout\n\u00b6\n\n\nThe module\n\ncunei.py\n\ncontains a number of handy functions to deal with TF nodes for cuneiform tablets\nand\n\nATF\n\ntranscriptions of them and \nCDLI\n photos and lineart.\n\n\nSee also\n\nabout\n,\n\nimages\n,\n\ntranscription\n.\n\n\nSet up\n\u00b6\n\n\nfrom tf.extra.cunei import Cunei\nimport Cunei\nThe \nCunei\n API is distributed with Text-Fabric.\nYou have to import it into your program.\nInitialisation\n\u00b6\n\n\nCunei()\n1\n2\n    \nCN\n \n=\n \nCunei\n(\n'~/github'\n,\n \n'Nino-cunei/uruk'\n,\n \n'notebook'\n)\n\n    \nCN\n.\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nDescription\nText-Fabric will be started for you and load all features.\nWhen \nCunei\n is\ninitializing, it scans the image directory of the repo and reports how many\nphotos and lineart images it sees.\nlocal GitHub\nThe argument \n~/github\n\nshould point to the directory where your local\ngithub repositories reside.\nUruk location\nThe argument \nNino-cunei/uruk\n\nshould point to the local GitHub repository\nwhere the Uruk corpus resides.\nnotebook\nThe third argument of \nCunei()\n should be the name\nof your current notebook (without the \n.ipynb\n extension).\nThe Cunei API will use this to generate a link to your notebook\non GitHub and NBViewer.\nNote\nYour current notebook can be anywhere on your system.\n\nCunei()\n can find its\nlocation, but not its name, hence you have to pass its name.\nLinking\n\u00b6\n\n\nCN.cdli()\n1\nCN\n.\ncdli\n(\ntablet\n,\n \nlinkText\n=\nNone\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nProduces a link to a tablet page on CDLI,\nto be placed in an output cell.\ntablet\ntablet\n is either a node of type \ntablet\n\nor a P-number of a tablet.\nlinkText\nYou may provide the text to be displayed as the link.\nIf you do not provide any,\nthe P-number of the tablet will be used.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say \nasString=True\n.\nCN.tabletLink()\n1\nCN\n.\ntabletLink\n(\nnode\n,\n \ntext\n=\nNone\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nProduces a link to CDLI\nnode\nnode\n can be an arbitrary node. The link targets the tablet that\ncontains the material contained by the node.\ntext\nYou may provide the text to be displayed as the link.\nIf you do not provide a link text,\nthe P-number of the tablet will be chosen.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say \nasString=True\n.\nSign 10000 on CDLI\n1\nCN\n.\ntabletLink\n(\n100000\n)\n\n\n\n\n\n\nPlain display\n\u00b6\n\n\nStraightforward display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table.\nCN.plain()\n1\nCN\n.\nplain\n(\nnode\n,\n \nlinked\n=\nTrue\n,\n \nwithNodes\n=\nFalse\n,\n \nlineart\n=\nTrue\n,\n \nlineNumbers\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a simple way.\nnode\nnode\n is a node of arbitrary type.\nlinked\nlinked\n indicates whether the result should be a link to CDLI\nto the tablet on which the node occurs.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nlineart\nlineart\n indicates whether to display a lineart image in addition\n(only relevant for signs and quads)\nlineNumbers\nlineNumbers\n indicates whether corresponding line numbers in the\nATF source should be displayed.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say \nasString=True\n.\nCN.plainTuple()\n1\n2\n3\n4\n5\n6\nCN\n.\nplainTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n \nlineart\n=\nTrue\n,\n \nlineNumbers\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells.\nnodes\nnodes\n is an iterable (list, tuple, set, etc) of arbitrary nodes.\nseqNumber\nseqNumber\n is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes.\nlinked\nlinked=1\n the column number where the cell contents is\nlinked to\nthe CDLI page of the containing tablet;\n(the first data column is column 1)\nwithNodes, lineart, lineNumbers, asString\nSame as in \nCN.plain()\n.\nCN.table()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCN\n.\ntable\n(\n\n  \nresults\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nlinked, withNodes, lineart, lineNumbers, asString\nSame as in \nCN.plainTuple()\n.\nPretty display\n\u00b6\n\n\nGraphical display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way.\nCN.pretty()\n1\nCN\n.\npretty\n(\nnode\n,\n \nwithNodes\n=\nFalse\n,\n \nsuppress\n=\nset\n(),\n \nhighlights\n=\n{})\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a graphical way.\nnode\nnode\n is a node of arbitrary type.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nlineart, lineNumbers\nSame as in \nCN.plain()\n.\nsuppress\nsuppress=set()\n is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively.\nHighlighting\nWhen nodes such as tablets and cases are displayed by \npretty()\n,\ntheir contents is also displayed. You can selectively highlight\nthose parts.\nhighlights\nhighlights={}\n is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted.\nIf \nhighlights\n is a set, its nodes will be highlighted with a default color (yellow).\nIf it is a dictionary, it should map nodes to colors.\nAny color that is a valid \n\nCSS color\n\nqualifies.\nIf you map a node to the empty string, it will get the default highlight color.\ncolor names\nThe link above points to a series of handy color names and their previews.\nCN.prettyTuple()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCN\n.\nprettyTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes.\nBy tablet\nWe examine all nodes in the tuple.\nWe collect and show all tablets in which they\noccur and highlight the material corresponding to the all nodes in the tuple.\nThe highlighting can be tweaked by the optional \ncolorMap\n parameter.\nnodes, seqNumber, withNodes, lineart, lineNumbers\nSame as in \nCN.plainTuple()\n.\nsuppress\nSame as in \nCN.pretty()\n.\ncolorMap\nThe nodes of the tuple will be highlighted.\nIf \ncolorMap\n is \nNone\n or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow.\nBut you can assign different colors to the members of the tuple:\n\ncolorMap\n must be a dictionary that maps the positions in a tuple \nto a color:\nIf a position is not mapped, it will not be highlighted.\nIf it is mapped to the empty string, it gets the default highlight color.\nOtherwise, it should be mapped to a string that is a valid\n    \nCSS color\n.\ncolor names\nThe link above points to a series of handy color names and their previews.\nhighlights\nSame as in \nB.pretty()\n.\nhighlights takes precedence over colorMap\nIf both \nhighlights\n and \ncolorMap\n are given, \ncolorMap\n is ignored.\nIf you need to micro-manage, \nhighlights\n is your thing.\nWhenever possible, use \ncolorMap\n.  \none big highlights dictionary\nIt is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run \nprettyTuple()\n for many different tuples with the same \nhighlights\n.\nIt does not harm performance if \nhighlights\n maps lots of nodes outside the tuple as well.\nCN.show()\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nCN\n.\nshow\n(\n\n  \nresults\n,\n\n  \ncondensed\n=\nTrue\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe elements of the list are displayed by \nCN.prettyTuple()\n.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\ncondensed\ncondensed\n indicates one of two modes of displaying the result list:\nTrue\n: instead of showing all results one by one,\n  we show all tablets with all results in it highlighted.\n  That way, we blur the distinction between the individual results,\n  but it is easier to oversee where the results are.\nFalse\n: make a separate display for each result tuple.\n  This gives the best account of the exact result set.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nwithNodes, lineart, lineNumbers, suppress, colorMap, highlights\nSame as in \nB.prettyTuple()\n.\nSearch\n\u00b6\n\n\nCN.search()\n1\nCN\n.\nsearch\n(\nquery\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nSearches in the same way as the generic Text-Fabric \nS.search()\n.\nBut whereas the \nS\n version returns a generator which yields the results\none by one, the \nCN\n version collects all results and sorts them.\nIt then reports the number of results.\nquery\nquery\n is the search template that has to be searched for.\nsilent\nsilent\n: if \nTrue\n it will suppress the reporting of the number of results.\nsearch template reference\nSee the \nsearch template reference\nATF representation\n\u00b6\n\n\nGenerate ATF\nSigns and quads and clusters can be represented by an ascii string,\nin the so-called Ascii Text Format,\n\nATF\n.\nWe provide a bunch of function that, given a node, generate the appropriate ATF\nrepresentation.\nCN.atfFromSign()\n1\nCN\n.\natfFromSign\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a sign.\nnode\nnode\n must have node type \nsign\n.\nflags\nflags\n whether the \nflags\n associated with the sign\nwill be included in the ATF.\nCN.atfFromQuad()\n1\nCN\n.\natfFromQuad\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a quad.\nnode\nnode\n must have node type \nquad\n.\nflags\nflags\n whether the \nflags\n associated with the quad\nwill be included in the ATF.\nCN.atfFromOuterQuad()\n1\nCN\n.\natfFromOuterQuad\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a quad or sign.\nnode\nnode\n must have node type \nquad\n or \nsign\n.\nflags\nflags\n whether the \nflags\n associated with the quad\nwill be included in the ATF.\nouter quads\nIf you take an ATF transcription line with linguistic material on it, and you\nsplit it on white space, and you forget the brackets that cluster quads and\nsigns, then you get a sequence of outer quads and signs.\nIf you need to get the ATF representation for these items, this function does\nconveniently produce them. You do not have to worry yourself about the sign/quad\ndistinction here.\nCN.atfFromCluster()\n1\nCN\n.\natfFromCluster\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a cluster.\nnode\nnode\n must have node type \nquad\n.\nclusters\nClusters are bracketings of\nquads that indicate proper names, uncertainty, or supplied material. In ATF they\nlook like \n( )a\n or \n[ ]\n or \n< >\nSub-clusters\nSub-clusters will also be\nrepresented. Signs belonging to multiple nested clusters will only be\nrepresented once.\nCN.getSource()\n1\nCN\n.\ngetSource\n(\nnode\n,\n \nnodeType\n=\nNone\n,\n \nlineNumbers\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDelivers the transcription source of nodes that correspond to the\nATF source line level.\nThis in contrast with the \nCN.atfFromXxx()\n functions that\nwork for nodes that correspond to parts of the ATF source lines.\nnode\nnode\n must have a type in \ntablet\n, \nface\n, \ncolumn\n,\n\ncomment\n, \nline\n, \ncase\n.\nnodeType\nIf \nnodeType\n is passed, only source lines of this type are returned.\nlineNumbers\nlineNumbers\n: if \nTrue\n, add line numbers to the result,\nthese numbers say where the source line occurs in the source file.\nTF from ATF conversion\nThe conversion of ATF to Text-Fabric has saved the original source lines and\ntheir line numbers in the features \nsrcLn\n and \nsrcLnNum\n respectively. This\nfunction makes use of those features.\nSections\n\u00b6\n\n\nSections in tablets\nText-Fabric supports 3 section levels in general.\nThe Uruk corpus uses them for \ntablets\n, \ncolumns\n and \nlines\n.\nBut lines may be divided in cases and subcases, which are also numbered.\nWe need to mimick some functions of the Text-Fabric \nT\n Api for sections,\nso that we can retrieve cases more easily.\nConsider search\nText-Fabric Search is a generic and powerful mechanism for information retrieval.\nIn most cases it is easier to extract nodes by search than by hand-written\ncode using the functions here.\nCN.nodeFromCase()\n1\nCN\n.\nnodeFromCase\n((\nP\n-\nnumber\n,\n \nface\n:\ncolumnNumber\n,\n \nhLineNumber\n))\n\n\n\n\n\n\nDescription\nGives you a node, if you specify a terminal case, i.e. a\nnumbered transcription line.\nCompare \nT.nodeFromSection()\nThis function is analogous to\n\nT.nodeFromSection()\n of Text-Fabric.\ncase specification\nThis function takes a single argument which must be\na tuple\n(\ntabletNumber\n, \nface\n:\ncolumnNumber\n, \nhierarchical-line-number\n).\ndots\nThe hierarchical number may contain the original \n.\n that they\noften have in the transcriptions, but you may also leave them out.\nNot found\nIf no such node exists, you get \nNone\n back.\nCN.caseFromNode()\n1\nCN\n.\ncaseFromNode\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGives you a terminal case specification,\nif you give a node of a case or something inside a case or line.\nCompare \nT.sectionFromNode()\nThis function is analogous to\n\nT.sectionFromNode()\n of Text-Fabric.\ncase specification\nA case specification is a tuple\n(\ntabletNumber\n, \nface\n:\ncolumnNumber\n, \nhierarchical-line-number\n).\nThe hierarchical line number will not contain dots.\nnode\nnode\n must be of a terminal case\n(these are the cases that have a full hierarchical\nnumber; these cases correspond to the individual numbered lines in the\ntranscription sources).\nother nodes\nIf \nnode\n corresponds to something inside a transcription line,\nthe node of the terminal case or line in which it is contained will be used.\nCN.lineFromNode()\n1\nCN\n.\nlineFromNode\n(\nnode\n)\n\n\n\n\n\n\nDescription\nIf called on a node corresponding to something inside a transcription line, it\nwill navigate to up to the terminal case or line in which it is contained, and\nreturn that node.\nnode\nnode\n must correspond to something inside a transcription line:\n\nsign\n, \nquad\n, \ncluster\n.\nCN.casesByLevel()\n1\nCN\n.\ncasesByLevel\n(\nk\n,\n \nterminal\n=\nTrue\n)\n\n\n\n\n\n\nDescription\nGrabs all (sub)cases of a specified level. You can choose to filter the result\nto those (sub)cases that are \nterminal\n, i.e. those which do not contain\nsubcases anymore. Such cases correspond to individual lines in the ATF.\nk\nk\n is an integer, indicating the level of (sub)cases you want.\n\n0\n is lines,\n\n1\n is top-level cases,\n\n2\n is subcases,\n\n3\n is subsubcases, and so on.\nterminal\nterminal\n: if \nTrue\n, only lines and cases that have the feature \nterminal\n\nare delivered.\nOtherwise, all lines/cases of that level will be delivered.\nCN.getOuterQuads()\n1\nCN\n.\ngetOuterQuads\n(\nnode\n)\n\n\n\n\n\n\nDescription\nCollects the outer quads and isolated signs under a node.\nnode\nnode\n is typically a tablet, face, column, line, or case.\nThis is the container of the outer quads.\nOuter quads\nOuter quads and isolated signs is what you get\nif you split line material by white space and\nremove cluster brackets.\nImages\n\u00b6\n\n\nCN.photo() and CN.lineart()\n1\n2\nCN\n.\nphoto\n(\nnodes\n,\n \nkey\n=\nNone\n,\n \nasLink\n=\nTrue\n,\n \nwithCaption\n=\n'bottom'\n,\n \n**\noptions\n)\n\n\nCN\n.\nlineart\n(\nnodes\n,\n \nkey\n=\nNone\n,\n \nasLink\n=\nTrue\n,\n \nwithCaption\n=\n'bottom'\n,\n \n**\noptions\n)\n\n\n\n\n\n\nDescription\nFetches photos or linearts for tablets, signs or quads, and returns it in a way\nthat it can be embedded in an output cell. The images that show up are clickable\nand link through to an online, higher resolution version on CDLI. Images will\nhave, by default, a caption that links to the relevant page on CDLI.\nPlacement\nThe result will be returned as a \nrow\n of images.\nSubsequent calls to \nphoto()\n and \nlineart()\n\nwill result in vertically stacked rows.\nnodes\nnodes\n is one or more \nnodes\n.\nAs far as they are of type \ntablet\n, \nquad\n or \nsign\n,\na photo or lineart will be looked up for them.\nby name\nInstead of a node you may also\nsupply the P-number of a tablet or the name of the sign or quad.\nkey\nkey\n is an optional string specifying which of the available images for\nthis node you want to use.\nlook up\nif you want to know which keys are available for a\nnode, supply \nkey='xxx'\n, or any non-existing key.\nasLink\nasLink=True\n: no image will be placed, only a link to the online\nimage at CDLI.\nIn this case the \ncaption\n will be suppressed, unless\nexplicitly given.\nwithCaption\nwithCaption='bottom'\n controls whether a CDLI link to the\ntablet page must be put under the image.\nYou can also specify \ntop\n, \nleft\n, \nright\n.\nIf left out, no caption will be placed.\noptions\noptions\n is a series of key=value arguments that\ncontrol the placement of the images,\nsuch as \nwidth=100\n, \nheight=200\n.\nCSS\nThe optional parameters \nheight\n and \nwidth\n control the height and width of the\nimages. The value should be a valid\n\nCSS\n length, such as\n\n100px\n, \n10em\n, \n32vw\n. If you pass an integer, or a decimal string without\nunit, your value will be converted to that many \npx\n.\nThese parameters are interpreted as setting a maximum value (in fact they will\nend up as \nmax-width\n and \nmax-height\n on the final \n<img/>\n element in the\nHTML.\nSo if you specify both \nwidth\n and \nheight\n, the image will be placed in tightly\nin a box of those dimensions without changing the aspect ratio.\nIf you want to force that the width of height you pass is completely consumed,\nyou can prefix your value with a \n!\n. In that case the aspect ratio maybe\nchanged. You can use the \n!\n also for both \nheight\n and \nwidth\n. In that case,\nthe rectangle will be completely filled, and the aspect ratio will be adjusted\nto that of the rectangle.\nThe way the effect of the \n!\n is achieved, is by adding \nmin-width\n and\n\nmin-height\n properties to the \n<img/>\n element.\nlocal images\nThe images will be called in by a little piece of generated HTML, using the\n\n<img/>\n tag. This only works if the image is within reach. To the images will\nbe copied to a sister directory of the notebook. The name of this directory is\n\ncdli-imagery\n. It will be created on-the-fly when needed. Copying will only be\ndone if needed. The names of the images will be changed, to prevent problems\nwith systems that cannot handle \n|\n and \n+\n characters in file names well.\nCN.imagery()\n1\nCN\n.\nimagery\n(\nobjectType\n,\n \nkind\n)\n\n\n\n\n\n\nDescription\nProvides the sets of locally available images by object type.\nfor tablets, it lists the P-numbers; for sign/quads: the ATF representations.\nobjectType\nobjectType\n is the type of thing: \nideograph\n or \ntablet\n.\nkind\nkind\n is \nphoto\n or \nlineart\n.",
            "title": "Cuneiform Tablets"
        },
        {
            "location": "/Api/Cunei/#cunei",
            "text": "",
            "title": "Cunei"
        },
        {
            "location": "/Api/Cunei/#about",
            "text": "The module cunei.py \ncontains a number of handy functions to deal with TF nodes for cuneiform tablets\nand ATF \ntranscriptions of them and  CDLI  photos and lineart.  See also about , images , transcription .",
            "title": "About"
        },
        {
            "location": "/Api/Cunei/#set-up",
            "text": "from tf.extra.cunei import Cunei import Cunei The  Cunei  API is distributed with Text-Fabric.\nYou have to import it into your program.",
            "title": "Set up"
        },
        {
            "location": "/Api/Cunei/#initialisation",
            "text": "Cunei() 1\n2      CN   =   Cunei ( '~/github' ,   'Nino-cunei/uruk' ,   'notebook' ) \n     CN . api . makeAvailableIn ( globals ())    Description Text-Fabric will be started for you and load all features.\nWhen  Cunei  is\ninitializing, it scans the image directory of the repo and reports how many\nphotos and lineart images it sees. local GitHub The argument  ~/github \nshould point to the directory where your local\ngithub repositories reside. Uruk location The argument  Nino-cunei/uruk \nshould point to the local GitHub repository\nwhere the Uruk corpus resides. notebook The third argument of  Cunei()  should be the name\nof your current notebook (without the  .ipynb  extension).\nThe Cunei API will use this to generate a link to your notebook\non GitHub and NBViewer. Note Your current notebook can be anywhere on your system. Cunei()  can find its\nlocation, but not its name, hence you have to pass its name.",
            "title": "Initialisation"
        },
        {
            "location": "/Api/Cunei/#linking",
            "text": "CN.cdli() 1 CN . cdli ( tablet ,   linkText = None ,   asString = False )    Description Produces a link to a tablet page on CDLI,\nto be placed in an output cell. tablet tablet  is either a node of type  tablet \nor a P-number of a tablet. linkText You may provide the text to be displayed as the link.\nIf you do not provide any,\nthe P-number of the tablet will be used. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say  asString=True . CN.tabletLink() 1 CN . tabletLink ( node ,   text = None ,   asString = False )    Description Produces a link to CDLI node node  can be an arbitrary node. The link targets the tablet that\ncontains the material contained by the node. text You may provide the text to be displayed as the link.\nIf you do not provide a link text,\nthe P-number of the tablet will be chosen. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say  asString=True . Sign 10000 on CDLI 1 CN . tabletLink ( 100000 )",
            "title": "Linking"
        },
        {
            "location": "/Api/Cunei/#plain-display",
            "text": "Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table. CN.plain() 1 CN . plain ( node ,   linked = True ,   withNodes = False ,   lineart = True ,   lineNumbers = False ,   asString = False )    Description Displays the material that corresponds to a node in a simple way. node node  is a node of arbitrary type. linked linked  indicates whether the result should be a link to CDLI\nto the tablet on which the node occurs. withNodes withNodes  indicates whether node numbers should be displayed. lineart lineart  indicates whether to display a lineart image in addition\n(only relevant for signs and quads) lineNumbers lineNumbers  indicates whether corresponding line numbers in the\nATF source should be displayed. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say  asString=True . CN.plainTuple() 1\n2\n3\n4\n5\n6 CN . plainTuple ( \n   nodes ,   seqNumber , \n   linked = 1 , \n   withNodes = False ,   lineart = True ,   lineNumbers = False , \n   asString = False ,  )    Description Displays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells. nodes nodes  is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber  is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes. linked linked=1  the column number where the cell contents is\nlinked to\nthe CDLI page of the containing tablet;\n(the first data column is column 1) withNodes, lineart, lineNumbers, asString Same as in  CN.plain() . CN.table() 1\n2\n3\n4\n5\n6\n7\n8\n9 CN . table ( \n   results , \n   start = 1 ,   end = len ( results ), \n   linked = 1 , \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   asString = False ,  )    Description Displays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable. results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. linked, withNodes, lineart, lineNumbers, asString Same as in  CN.plainTuple() .",
            "title": "Plain display"
        },
        {
            "location": "/Api/Cunei/#pretty-display",
            "text": "Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way. CN.pretty() 1 CN . pretty ( node ,   withNodes = False ,   suppress = set (),   highlights = {})    Description Displays the material that corresponds to a node in a graphical way. node node  is a node of arbitrary type. withNodes withNodes  indicates whether node numbers should be displayed. lineart, lineNumbers Same as in  CN.plain() . suppress suppress=set()  is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively. Highlighting When nodes such as tablets and cases are displayed by  pretty() ,\ntheir contents is also displayed. You can selectively highlight\nthose parts. highlights highlights={}  is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted. If  highlights  is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors.\nAny color that is a valid  CSS color \nqualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. CN.prettyTuple() 1\n2\n3\n4\n5\n6\n7\n8\n9 CN . prettyTuple ( \n   nodes ,   seqNumber , \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes. By tablet We examine all nodes in the tuple.\nWe collect and show all tablets in which they\noccur and highlight the material corresponding to the all nodes in the tuple.\nThe highlighting can be tweaked by the optional  colorMap  parameter. nodes, seqNumber, withNodes, lineart, lineNumbers Same as in  CN.plainTuple() . suppress Same as in  CN.pretty() . colorMap The nodes of the tuple will be highlighted.\nIf  colorMap  is  None  or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap  must be a dictionary that maps the positions in a tuple \nto a color: If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid\n     CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in  B.pretty() . highlights takes precedence over colorMap If both  highlights  and  colorMap  are given,  colorMap  is ignored. If you need to micro-manage,  highlights  is your thing.\nWhenever possible, use  colorMap .   one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run  prettyTuple()  for many different tuples with the same  highlights .\nIt does not harm performance if  highlights  maps lots of nodes outside the tuple as well. CN.show()  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 CN . show ( \n   results , \n   condensed = True , \n   start = 1 ,   end = len ( results ), \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays an iterable of tuples of nodes.\nThe elements of the list are displayed by  CN.prettyTuple() . results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. condensed condensed  indicates one of two modes of displaying the result list: True : instead of showing all results one by one,\n  we show all tablets with all results in it highlighted.\n  That way, we blur the distinction between the individual results,\n  but it is easier to oversee where the results are. False : make a separate display for each result tuple.\n  This gives the best account of the exact result set. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. withNodes, lineart, lineNumbers, suppress, colorMap, highlights Same as in  B.prettyTuple() .",
            "title": "Pretty display"
        },
        {
            "location": "/Api/Cunei/#search",
            "text": "CN.search() 1 CN . search ( query ,   silent = False )    Description Searches in the same way as the generic Text-Fabric  S.search() .\nBut whereas the  S  version returns a generator which yields the results\none by one, the  CN  version collects all results and sorts them.\nIt then reports the number of results. query query  is the search template that has to be searched for. silent silent : if  True  it will suppress the reporting of the number of results. search template reference See the  search template reference",
            "title": "Search"
        },
        {
            "location": "/Api/Cunei/#atf-representation",
            "text": "Generate ATF Signs and quads and clusters can be represented by an ascii string,\nin the so-called Ascii Text Format, ATF . We provide a bunch of function that, given a node, generate the appropriate ATF\nrepresentation. CN.atfFromSign() 1 CN . atfFromSign ( node ,   flags = False )    Description Reproduces the ATF representation of a sign. node node  must have node type  sign . flags flags  whether the  flags  associated with the sign\nwill be included in the ATF. CN.atfFromQuad() 1 CN . atfFromQuad ( node ,   flags = False )    Description Reproduces the ATF representation of a quad. node node  must have node type  quad . flags flags  whether the  flags  associated with the quad\nwill be included in the ATF. CN.atfFromOuterQuad() 1 CN . atfFromOuterQuad ( node ,   flags = False )    Description Reproduces the ATF representation of a quad or sign. node node  must have node type  quad  or  sign . flags flags  whether the  flags  associated with the quad\nwill be included in the ATF. outer quads If you take an ATF transcription line with linguistic material on it, and you\nsplit it on white space, and you forget the brackets that cluster quads and\nsigns, then you get a sequence of outer quads and signs. If you need to get the ATF representation for these items, this function does\nconveniently produce them. You do not have to worry yourself about the sign/quad\ndistinction here. CN.atfFromCluster() 1 CN . atfFromCluster ( node ,   flags = False )    Description Reproduces the ATF representation of a cluster. node node  must have node type  quad . clusters Clusters are bracketings of\nquads that indicate proper names, uncertainty, or supplied material. In ATF they\nlook like  ( )a  or  [ ]  or  < > Sub-clusters Sub-clusters will also be\nrepresented. Signs belonging to multiple nested clusters will only be\nrepresented once. CN.getSource() 1 CN . getSource ( node ,   nodeType = None ,   lineNumbers = False )    Description Delivers the transcription source of nodes that correspond to the\nATF source line level. This in contrast with the  CN.atfFromXxx()  functions that\nwork for nodes that correspond to parts of the ATF source lines. node node  must have a type in  tablet ,  face ,  column , comment ,  line ,  case . nodeType If  nodeType  is passed, only source lines of this type are returned. lineNumbers lineNumbers : if  True , add line numbers to the result,\nthese numbers say where the source line occurs in the source file. TF from ATF conversion The conversion of ATF to Text-Fabric has saved the original source lines and\ntheir line numbers in the features  srcLn  and  srcLnNum  respectively. This\nfunction makes use of those features.",
            "title": "ATF representation"
        },
        {
            "location": "/Api/Cunei/#sections",
            "text": "Sections in tablets Text-Fabric supports 3 section levels in general.\nThe Uruk corpus uses them for  tablets ,  columns  and  lines . But lines may be divided in cases and subcases, which are also numbered.\nWe need to mimick some functions of the Text-Fabric  T  Api for sections,\nso that we can retrieve cases more easily. Consider search Text-Fabric Search is a generic and powerful mechanism for information retrieval.\nIn most cases it is easier to extract nodes by search than by hand-written\ncode using the functions here. CN.nodeFromCase() 1 CN . nodeFromCase (( P - number ,   face : columnNumber ,   hLineNumber ))    Description Gives you a node, if you specify a terminal case, i.e. a\nnumbered transcription line. Compare  T.nodeFromSection() This function is analogous to T.nodeFromSection()  of Text-Fabric. case specification This function takes a single argument which must be\na tuple\n( tabletNumber ,  face : columnNumber ,  hierarchical-line-number ). dots The hierarchical number may contain the original  .  that they\noften have in the transcriptions, but you may also leave them out. Not found If no such node exists, you get  None  back. CN.caseFromNode() 1 CN . caseFromNode ( node )    Description Gives you a terminal case specification,\nif you give a node of a case or something inside a case or line. Compare  T.sectionFromNode() This function is analogous to T.sectionFromNode()  of Text-Fabric. case specification A case specification is a tuple\n( tabletNumber ,  face : columnNumber ,  hierarchical-line-number ).\nThe hierarchical line number will not contain dots. node node  must be of a terminal case\n(these are the cases that have a full hierarchical\nnumber; these cases correspond to the individual numbered lines in the\ntranscription sources). other nodes If  node  corresponds to something inside a transcription line,\nthe node of the terminal case or line in which it is contained will be used. CN.lineFromNode() 1 CN . lineFromNode ( node )    Description If called on a node corresponding to something inside a transcription line, it\nwill navigate to up to the terminal case or line in which it is contained, and\nreturn that node. node node  must correspond to something inside a transcription line: sign ,  quad ,  cluster . CN.casesByLevel() 1 CN . casesByLevel ( k ,   terminal = True )    Description Grabs all (sub)cases of a specified level. You can choose to filter the result\nto those (sub)cases that are  terminal , i.e. those which do not contain\nsubcases anymore. Such cases correspond to individual lines in the ATF. k k  is an integer, indicating the level of (sub)cases you want. 0  is lines, 1  is top-level cases, 2  is subcases, 3  is subsubcases, and so on. terminal terminal : if  True , only lines and cases that have the feature  terminal \nare delivered.\nOtherwise, all lines/cases of that level will be delivered. CN.getOuterQuads() 1 CN . getOuterQuads ( node )    Description Collects the outer quads and isolated signs under a node. node node  is typically a tablet, face, column, line, or case.\nThis is the container of the outer quads. Outer quads Outer quads and isolated signs is what you get\nif you split line material by white space and\nremove cluster brackets.",
            "title": "Sections"
        },
        {
            "location": "/Api/Cunei/#images",
            "text": "CN.photo() and CN.lineart() 1\n2 CN . photo ( nodes ,   key = None ,   asLink = True ,   withCaption = 'bottom' ,   ** options )  CN . lineart ( nodes ,   key = None ,   asLink = True ,   withCaption = 'bottom' ,   ** options )    Description Fetches photos or linearts for tablets, signs or quads, and returns it in a way\nthat it can be embedded in an output cell. The images that show up are clickable\nand link through to an online, higher resolution version on CDLI. Images will\nhave, by default, a caption that links to the relevant page on CDLI. Placement The result will be returned as a  row  of images.\nSubsequent calls to  photo()  and  lineart() \nwill result in vertically stacked rows. nodes nodes  is one or more  nodes .\nAs far as they are of type  tablet ,  quad  or  sign ,\na photo or lineart will be looked up for them. by name Instead of a node you may also\nsupply the P-number of a tablet or the name of the sign or quad. key key  is an optional string specifying which of the available images for\nthis node you want to use. look up if you want to know which keys are available for a\nnode, supply  key='xxx' , or any non-existing key. asLink asLink=True : no image will be placed, only a link to the online\nimage at CDLI.\nIn this case the  caption  will be suppressed, unless\nexplicitly given. withCaption withCaption='bottom'  controls whether a CDLI link to the\ntablet page must be put under the image.\nYou can also specify  top ,  left ,  right .\nIf left out, no caption will be placed. options options  is a series of key=value arguments that\ncontrol the placement of the images,\nsuch as  width=100 ,  height=200 . CSS The optional parameters  height  and  width  control the height and width of the\nimages. The value should be a valid CSS  length, such as 100px ,  10em ,  32vw . If you pass an integer, or a decimal string without\nunit, your value will be converted to that many  px . These parameters are interpreted as setting a maximum value (in fact they will\nend up as  max-width  and  max-height  on the final  <img/>  element in the\nHTML. So if you specify both  width  and  height , the image will be placed in tightly\nin a box of those dimensions without changing the aspect ratio. If you want to force that the width of height you pass is completely consumed,\nyou can prefix your value with a  ! . In that case the aspect ratio maybe\nchanged. You can use the  !  also for both  height  and  width . In that case,\nthe rectangle will be completely filled, and the aspect ratio will be adjusted\nto that of the rectangle. The way the effect of the  !  is achieved, is by adding  min-width  and min-height  properties to the  <img/>  element. local images The images will be called in by a little piece of generated HTML, using the <img/>  tag. This only works if the image is within reach. To the images will\nbe copied to a sister directory of the notebook. The name of this directory is cdli-imagery . It will be created on-the-fly when needed. Copying will only be\ndone if needed. The names of the images will be changed, to prevent problems\nwith systems that cannot handle  |  and  +  characters in file names well. CN.imagery() 1 CN . imagery ( objectType ,   kind )    Description Provides the sets of locally available images by object type.\nfor tablets, it lists the P-numbers; for sign/quads: the ATF representations. objectType objectType  is the type of thing:  ideograph  or  tablet . kind kind  is  photo  or  lineart .",
            "title": "Images"
        },
        {
            "location": "/Api/Lib/",
            "text": "Lib\n\u00b6\n\n\nWhile Text-Fabric is a generic package to deal with text and annotations\nin a model of nodes, edges, and features, there is need for some additions.\n\n\nTranscription\n\u00b6\n\n\ntranscription.py\n contains transliteration tables for Hebrew and Syriac that\nare being used in the \nBHSA\n.",
            "title": "Auxiliary"
        },
        {
            "location": "/Api/Lib/#lib",
            "text": "While Text-Fabric is a generic package to deal with text and annotations\nin a model of nodes, edges, and features, there is need for some additions.",
            "title": "Lib"
        },
        {
            "location": "/Api/Lib/#transcription",
            "text": "transcription.py  contains transliteration tables for Hebrew and Syriac that\nare being used in the  BHSA .",
            "title": "Transcription"
        },
        {
            "location": "/Model/Data-Model/",
            "text": "Text-Fabric Data Model\n\u00b6\n\n\nAt a glance\n\u00b6\n\n\nTake a text, put a grid around the words, and then leave out the words. What is\nleft, are the textual positions, or \nslots\n.\n\n\n\n\nPieces of text correspond to phrases, clauses, sentences, verses, chapters,\nbooks. Draw circles around those pieces, and then leave out their contents. What\nis left, are the textual objects, or \nnodes\n.\n\n\n\n\nNodes can be connected to other nodes by \nedges\n. A basic function of edges is\nto indicate \ncontainment\n: \nthis\n node corresponds to a set of slots that is\ncontained in the slots of \nthat\n node. But edges can also denote more abstract,\nlinguistic relations between nodes.\n\n\nNodes have types. Types are just a label that we use to make distinctions\nbetween word nodes, phrase nodes, ..., book nodes. The type assignment is an\nexample of a \nfeature\n of nodes: a mapping that assigns a piece of information\nto each node. This type assignment has a name: \notype\n, and every Text-Fabric\ndataset has such a feature.\n\n\nNodes may be linked to textual positions or \nslots\n. Some nodes are linked to a\nsingle slot, others to a set of slots, and yet others to no slots at all.\n\n\nNodes of the first kind are identified with their slots, they have the same\nnumber as slot as they have as node.\n\n\nNodes of the second kind have an edge to every slot (which is also a node) that\nthey are linked to. The collection of these edges from nodes of the second kind\nto nodes of the first kind, is an example of a \nfeature\n of edges: a mapping\nthat assigns to each pair of nodes a boolean value: is this pair a link or not?\nThis particular edge feature is called \noslots\n, and every Text-Fabric dataset\nhas such a feature.\n\n\nNodes of the third kind represent information that is not part of the main body\nof text. We could represent the lexicon in this way. However, it is also\npossible to consider \nlexeme\n as a node type, where every lexeme node is linked\nto the set of slots that have an occurrence of that lexeme.\n\n\nFabric metaphor\n\u00b6\n\n\n\n\nAD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen\n\n\nBefore we go on, we invite you to look at a few basic terms in the craft of\n\nweaving\n.\n\n\nA weaver sets up a set of fixed, parallel threads, the \nwarp\n. He then picks a\nthread, usually a colourful one, and sends it in a perpendicular way through the\nwarp. This thread is called the \nweft\n.\n\n\n\n\nThe instrument that carries the wefts through the warp is called the \nloom\n. The\nweaver continues operating the loom, back and forth, occasionally selecting new\nwefts, until he has completed a rectangular piece of fabric, the \nweave\n.\n\n\n\n\nsource\n\n\nNow Text-Fabric, the tool, can be seen as the loom that sends features (the\nwefts) through a warp (the system of nodes and edges).\n\n\nThe features \notype\n and \noslots\n are the ones that set up the system of\nnodes and edges. That's why we call them \nwarp\n features. Every Text-Fabric\ndataset contains these two warp features. (Later on we'll see a third member of\nthe warp, \notext\n). They provide the structure of a text and its annotations,\nwithout any content. Even the text itself is left out!\n\n\n\n\nAll other information is added to the warp as \nfeatures\n (the wefts): node\nfeatures and edge features. A feature is a special aspect of the textual\ninformation, isolated as a kind of module. It is a collection of values which\ncan be woven as a weft into the warp.\n\n\nOne of the more basic things to add to the warp is the text itself. Ancient\ntexts often have several text representations, like original (Unicode)\ncharacters or transliterated characters, with or without the complete set of\ndiacritical marks. In Text-Fabric we do not have to choose between them: we can\npackage each representation into a feature, and add it to the fabric.\n\n\nA Text-Fabric data set is a warp (\notype\n, \noslots\n) plus a collection of wefts\n(all other features). We may add other features to the same warp. Data sets with\nonly wefts, but no warps, are called modules. When you use modules with a\ndataset, the modules must have been constructed around the warp of the dataset.\n\n\nWhenever you use Text-Fabric to generate new data, you are weaving a weave. The\nresulting dataset is a tight fabric of individual features (wefts), whose values\nare taken for a set of nodes (warp).\n\n\n\n\nSome features deserve a privileged place. After all, we are dealing with \ntext\n,\nso we need a bit of information about which features carry textual\nrepresentations and sectioning information (e.g. books, chapters, verses).\n\n\nThis information is not hard-wired into Text-Fabric, but it is given in the form\nof a \nconfig\n feature. A config feature has no data, only metadata. Every\nText-Fabric dataset may contain a config feature called \notext\n, which\nspecifies which node types and features correspond to sectional units such as\nbooks, chapters, and verses. It also contains templates for generating text\nrepresentations for the slots.\n\n\nThe \notext\n feature is optional, because not all Text-Fabric datasets are\nexpected to have extensive sectioning and text representation definitions.\nEspecially when you are in the process of converting a data source (such as a\ntreebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load\nthe data without bothering about these matters.\n\n\n\n\nModel\n\u00b6\n\n\nWe summarize in brief statements our data model, including ways to represent the\ndata, serialize it, and compute with it.\n\n\nText objects:\n\n\n\n\noccupy arbitrary compositions of slots;\n\n\ncarry a \ntype\n (just a label); all \nslots\n carry the same type, the \nslot\n    type\n; e.g. \nword\n or \ncharacter\n;\n\n\ncan be annotated by \nfeatures\n (key-value pairs)\n\n\ncan be connected by directed, labelled links to other text objects.\n\n\n\n\nThe model knows which feature assigned values to nodes and edges. If two\ndifferent features assign a value to an edge or node, both values can be read\noff later; one through the one feature, and one through the other.\n\n\nThe data in Text-Fabric is organized as an annotated directed graph with a bit\nof additional structure. The correspondence is\n\n\n\n\ntext positions => the first so many slot numbers\n\n\ntext objects => nodes\n\n\nlinks between text objects => edges\n\n\ninformation associated with text objects => node features\n\n\nlabels on links between text objects => edge features\n\n\nNB:\n since every link is specified by an edge feature, every link is\n    implicitly labelled by the name of the edge feature. If the edge feature\n    assigns values to edges, those values come on top of the implicit label.\n\n\ntypes of text objects => a special node feature called \notype\n (read: object\n    type)\n\n\nextent of text objects in terms of slots => a special edge feature called\n    \noslots\n (read: object slots)\n\n\noptional specifications for sectioning and representing text => a special\n    config feature called \notext\n (read: object text)\n\n\n\n\nTogether, the \notype\n, \noslots\n, and the optional \notext\n features are called\nthe \nwarp\n of a Text-Fabric dataset.\n\n\nRepresentation\n\u00b6\n\n\nWe represent the elements that make up such a graph as follows:\n\n\n\n\nnodes are integers, starting with 1, without gaps;\n\n\nthe first \nmaxSlot\n nodes correspond exactly with the slots, in the same\n    order, where \nmaxSlot\n is the number of slots;\n\n\nnodes greater than \nmaxSlot\n correspond to general text objects;\n\n\nnode features are mappings of integers to values;\n\n\nedge features are mappings of pairs of integers to values; i.e. edges are\n    ordered pairs of integers; labelled edges are ordered tuples of two nodes and\n    a value;\n\n\nvalues (for nodes and for edges) are strings (Unicode, utf8) or numbers;\n\n\nthe \notype\n feature maps\n\n\nthe integers \n1..maxSlot\n (including) to the \nslot type\n, where \nmaxSlot\n is\n    the last \nslot\n,\n\n\nthe integers \nmaxSlot+1..maxNode\n (including) to the relevant text object\n    types;\n\n\n\n\n\n\nthe \noslots\n feature is an valueless edge feature, mapping all non-slot nodes\n    to sets of slots; so there is an \noslots\n edge between each non-slot node and\n    each slot \ncontained\n by that node;\n\n\na Text-Fabric dataset is a collection of node features and edge features\n    containing at least the \nwarp\n features \notype\n, \noslots\n, and, optionally\n    \notext\n.\n\n\n\n\nMore about the warp\n\u00b6\n\n\nThe warp/weft distinction is a handy way of separating textual organisation from\ntextual content. Let us discuss the warp features a bit more.\n\n\notype: node feature\n\u00b6\n\n\nMaps each node to a label. The label typically is the kind of object that the\nnode represents, with values such as\n\n\n1\n2\n3\n4\n5\n6\n7\nbook\nchapter\nverse\nsentence\nclause\nphrase\nword\n\n\n\n\n\n\nThere is a special kind of object type, the \nslot type\n, which is the atomic\nbuilding block of the text objects. It is assumed that the complete text is\nbuilt from a sequence of \nslots\n, from slot \n1\n till slot \nmaxSlot\n (including),\nwhere the slots are numbered consecutively. There must be at least one slot.\n\n\nAll other objects are defined with respect to the \nslots\n they contain.\n\n\nThe \nslot type\n does not have to be called \nslot\n literally. If your basic\nentity is \nword\n, you may also call it \nword\n. Slots are then filled with\n\nwords\n. You can model text on the basis of another atomic entity, such as\n\ncharacter\n. In that case, slots are filled with \ncharacters\n. Other choices may\nbe equally viable.\n\n\nThe only requirement is that all slots correspond exactly with the first so many\nnodes.\n\n\nThe \notype\n feature will map node \n1\n to a node type, and this node type is the\ntype of all subsequent slots and also of the things that fill the slots.\n\n\nNote also the sectional features \nbook chapter verse\n here. They will play a\nrole in the third, optional, warp feature \notext\n.\n\n\noslots: edge feature\n\u00b6\n\n\nDefines which slots are occupied by which objects. It does so by specifying\nedges from nodes to the slots they contain.\n\n\nFrom the information in \noslots\n we can compute the embedding relationships\nbetween all nodes.\n\n\nIt gives also rise to a canonical \nordering\n of nodes.\n\n\notext: config feature (optional)\n\u00b6\n\n\nDeclares which node types correspond to the first three levels of sectioning,\nusually \nbook\n, \nchapter\n, \nverse\n. Also declares the corresponding features to\nget the names or numbers of the sections in those levels. Text-Fabric uses this\ninformation to construct the so-called Text-API, with functions to\n\n\n\n\nconvert nodes to section labels and vice versa,\n\n\nrepresent section names in multiple languages,\n\n\nprint formatted text for node sets.\n\n\n\n\nIf information about sections or text representations are missing, Text-Fabric\nwill build a reduced Text-API for you, but it will continue.\n\n\nSerializing and precomputing\n\u00b6\n\n\nWhen Text-Fabric works with a dataset, it reads feature data files, and offers\nan API to process that data. The main task of Text-Fabric is to make processing\nefficient, so that it can be done in interactive ways, such as in a Jupyter\nnotebook. To that end, Text-Fabric\n\n\n\n\noptimizes feature data after reading it for the first time and stores it in\n    binary form for fast loading in next invocations;\n\n\nprecomputes additional data from the warp features in order to provide\n    convenient API functions.\n\n\n\n\nIn Text-Fabric, we have various ways of encoding this model:\n\n\n\n\nas plain text in \n.tf\n feature files,\n\n\nas Python data structures in memory,\n\n\nas compressed serializations of the same data structures inside \n.tfx\n files\n    in \n.tf\n cache directories.",
            "title": "Data"
        },
        {
            "location": "/Model/Data-Model/#text-fabric-data-model",
            "text": "",
            "title": "Text-Fabric Data Model"
        },
        {
            "location": "/Model/Data-Model/#at-a-glance",
            "text": "Take a text, put a grid around the words, and then leave out the words. What is\nleft, are the textual positions, or  slots .   Pieces of text correspond to phrases, clauses, sentences, verses, chapters,\nbooks. Draw circles around those pieces, and then leave out their contents. What\nis left, are the textual objects, or  nodes .   Nodes can be connected to other nodes by  edges . A basic function of edges is\nto indicate  containment :  this  node corresponds to a set of slots that is\ncontained in the slots of  that  node. But edges can also denote more abstract,\nlinguistic relations between nodes.  Nodes have types. Types are just a label that we use to make distinctions\nbetween word nodes, phrase nodes, ..., book nodes. The type assignment is an\nexample of a  feature  of nodes: a mapping that assigns a piece of information\nto each node. This type assignment has a name:  otype , and every Text-Fabric\ndataset has such a feature.  Nodes may be linked to textual positions or  slots . Some nodes are linked to a\nsingle slot, others to a set of slots, and yet others to no slots at all.  Nodes of the first kind are identified with their slots, they have the same\nnumber as slot as they have as node.  Nodes of the second kind have an edge to every slot (which is also a node) that\nthey are linked to. The collection of these edges from nodes of the second kind\nto nodes of the first kind, is an example of a  feature  of edges: a mapping\nthat assigns to each pair of nodes a boolean value: is this pair a link or not?\nThis particular edge feature is called  oslots , and every Text-Fabric dataset\nhas such a feature.  Nodes of the third kind represent information that is not part of the main body\nof text. We could represent the lexicon in this way. However, it is also\npossible to consider  lexeme  as a node type, where every lexeme node is linked\nto the set of slots that have an occurrence of that lexeme.",
            "title": "At a glance"
        },
        {
            "location": "/Model/Data-Model/#fabric-metaphor",
            "text": "AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen  Before we go on, we invite you to look at a few basic terms in the craft of weaving .  A weaver sets up a set of fixed, parallel threads, the  warp . He then picks a\nthread, usually a colourful one, and sends it in a perpendicular way through the\nwarp. This thread is called the  weft .   The instrument that carries the wefts through the warp is called the  loom . The\nweaver continues operating the loom, back and forth, occasionally selecting new\nwefts, until he has completed a rectangular piece of fabric, the  weave .   source  Now Text-Fabric, the tool, can be seen as the loom that sends features (the\nwefts) through a warp (the system of nodes and edges).  The features  otype  and  oslots  are the ones that set up the system of\nnodes and edges. That's why we call them  warp  features. Every Text-Fabric\ndataset contains these two warp features. (Later on we'll see a third member of\nthe warp,  otext ). They provide the structure of a text and its annotations,\nwithout any content. Even the text itself is left out!   All other information is added to the warp as  features  (the wefts): node\nfeatures and edge features. A feature is a special aspect of the textual\ninformation, isolated as a kind of module. It is a collection of values which\ncan be woven as a weft into the warp.  One of the more basic things to add to the warp is the text itself. Ancient\ntexts often have several text representations, like original (Unicode)\ncharacters or transliterated characters, with or without the complete set of\ndiacritical marks. In Text-Fabric we do not have to choose between them: we can\npackage each representation into a feature, and add it to the fabric.  A Text-Fabric data set is a warp ( otype ,  oslots ) plus a collection of wefts\n(all other features). We may add other features to the same warp. Data sets with\nonly wefts, but no warps, are called modules. When you use modules with a\ndataset, the modules must have been constructed around the warp of the dataset.  Whenever you use Text-Fabric to generate new data, you are weaving a weave. The\nresulting dataset is a tight fabric of individual features (wefts), whose values\nare taken for a set of nodes (warp).   Some features deserve a privileged place. After all, we are dealing with  text ,\nso we need a bit of information about which features carry textual\nrepresentations and sectioning information (e.g. books, chapters, verses).  This information is not hard-wired into Text-Fabric, but it is given in the form\nof a  config  feature. A config feature has no data, only metadata. Every\nText-Fabric dataset may contain a config feature called  otext , which\nspecifies which node types and features correspond to sectional units such as\nbooks, chapters, and verses. It also contains templates for generating text\nrepresentations for the slots.  The  otext  feature is optional, because not all Text-Fabric datasets are\nexpected to have extensive sectioning and text representation definitions.\nEspecially when you are in the process of converting a data source (such as a\ntreebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load\nthe data without bothering about these matters.",
            "title": "Fabric metaphor"
        },
        {
            "location": "/Model/Data-Model/#model",
            "text": "We summarize in brief statements our data model, including ways to represent the\ndata, serialize it, and compute with it.  Text objects:   occupy arbitrary compositions of slots;  carry a  type  (just a label); all  slots  carry the same type, the  slot\n    type ; e.g.  word  or  character ;  can be annotated by  features  (key-value pairs)  can be connected by directed, labelled links to other text objects.   The model knows which feature assigned values to nodes and edges. If two\ndifferent features assign a value to an edge or node, both values can be read\noff later; one through the one feature, and one through the other.  The data in Text-Fabric is organized as an annotated directed graph with a bit\nof additional structure. The correspondence is   text positions => the first so many slot numbers  text objects => nodes  links between text objects => edges  information associated with text objects => node features  labels on links between text objects => edge features  NB:  since every link is specified by an edge feature, every link is\n    implicitly labelled by the name of the edge feature. If the edge feature\n    assigns values to edges, those values come on top of the implicit label.  types of text objects => a special node feature called  otype  (read: object\n    type)  extent of text objects in terms of slots => a special edge feature called\n     oslots  (read: object slots)  optional specifications for sectioning and representing text => a special\n    config feature called  otext  (read: object text)   Together, the  otype ,  oslots , and the optional  otext  features are called\nthe  warp  of a Text-Fabric dataset.",
            "title": "Model"
        },
        {
            "location": "/Model/Data-Model/#representation",
            "text": "We represent the elements that make up such a graph as follows:   nodes are integers, starting with 1, without gaps;  the first  maxSlot  nodes correspond exactly with the slots, in the same\n    order, where  maxSlot  is the number of slots;  nodes greater than  maxSlot  correspond to general text objects;  node features are mappings of integers to values;  edge features are mappings of pairs of integers to values; i.e. edges are\n    ordered pairs of integers; labelled edges are ordered tuples of two nodes and\n    a value;  values (for nodes and for edges) are strings (Unicode, utf8) or numbers;  the  otype  feature maps  the integers  1..maxSlot  (including) to the  slot type , where  maxSlot  is\n    the last  slot ,  the integers  maxSlot+1..maxNode  (including) to the relevant text object\n    types;    the  oslots  feature is an valueless edge feature, mapping all non-slot nodes\n    to sets of slots; so there is an  oslots  edge between each non-slot node and\n    each slot  contained  by that node;  a Text-Fabric dataset is a collection of node features and edge features\n    containing at least the  warp  features  otype ,  oslots , and, optionally\n     otext .",
            "title": "Representation"
        },
        {
            "location": "/Model/Data-Model/#more-about-the-warp",
            "text": "The warp/weft distinction is a handy way of separating textual organisation from\ntextual content. Let us discuss the warp features a bit more.",
            "title": "More about the warp"
        },
        {
            "location": "/Model/Data-Model/#otype-node-feature",
            "text": "Maps each node to a label. The label typically is the kind of object that the\nnode represents, with values such as  1\n2\n3\n4\n5\n6\n7 book\nchapter\nverse\nsentence\nclause\nphrase\nword   There is a special kind of object type, the  slot type , which is the atomic\nbuilding block of the text objects. It is assumed that the complete text is\nbuilt from a sequence of  slots , from slot  1  till slot  maxSlot  (including),\nwhere the slots are numbered consecutively. There must be at least one slot.  All other objects are defined with respect to the  slots  they contain.  The  slot type  does not have to be called  slot  literally. If your basic\nentity is  word , you may also call it  word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with  characters . Other choices may\nbe equally viable.  The only requirement is that all slots correspond exactly with the first so many\nnodes.  The  otype  feature will map node  1  to a node type, and this node type is the\ntype of all subsequent slots and also of the things that fill the slots.  Note also the sectional features  book chapter verse  here. They will play a\nrole in the third, optional, warp feature  otext .",
            "title": "otype: node feature"
        },
        {
            "location": "/Model/Data-Model/#oslots-edge-feature",
            "text": "Defines which slots are occupied by which objects. It does so by specifying\nedges from nodes to the slots they contain.  From the information in  oslots  we can compute the embedding relationships\nbetween all nodes.  It gives also rise to a canonical  ordering  of nodes.",
            "title": "oslots: edge feature"
        },
        {
            "location": "/Model/Data-Model/#otext-config-feature-optional",
            "text": "Declares which node types correspond to the first three levels of sectioning,\nusually  book ,  chapter ,  verse . Also declares the corresponding features to\nget the names or numbers of the sections in those levels. Text-Fabric uses this\ninformation to construct the so-called Text-API, with functions to   convert nodes to section labels and vice versa,  represent section names in multiple languages,  print formatted text for node sets.   If information about sections or text representations are missing, Text-Fabric\nwill build a reduced Text-API for you, but it will continue.",
            "title": "otext: config feature (optional)"
        },
        {
            "location": "/Model/Data-Model/#serializing-and-precomputing",
            "text": "When Text-Fabric works with a dataset, it reads feature data files, and offers\nan API to process that data. The main task of Text-Fabric is to make processing\nefficient, so that it can be done in interactive ways, such as in a Jupyter\nnotebook. To that end, Text-Fabric   optimizes feature data after reading it for the first time and stores it in\n    binary form for fast loading in next invocations;  precomputes additional data from the warp features in order to provide\n    convenient API functions.   In Text-Fabric, we have various ways of encoding this model:   as plain text in  .tf  feature files,  as Python data structures in memory,  as compressed serializations of the same data structures inside  .tfx  files\n    in  .tf  cache directories.",
            "title": "Serializing and precomputing"
        },
        {
            "location": "/Model/File-formats/",
            "text": "Text-Fabric File Format\n\u00b6\n\n\nOverview\n\u00b6\n\n\nA \n.tf\n feature file starts with a \nheader\n, and is followed by the actual data.\nThe whole file is a plain text in UNICODE-utf8.\n\n\nHeader\n\u00b6\n\n\nA \n.tf\n feature file always starts with one or more metadata lines of the form\n\n\n1\n@key\n\n\n\n\n\n\nor\n\n\n1\n@key=value\n\n\n\n\n\n\nThe first line must be either\n\n\n1\n@node\n\n\n\n\n\n\nor\n\n\n1\n@edge\n\n\n\n\n\n\nor\n\n\n1\n@config\n\n\n\n\n\n\nThis tells Text-Fabric whether the data in the feature file is a \nnode\n feature\nor an \nedge\n feature. The value \n@config\n means that the file will be used as\nconfiguration info. It will only have metadata.\n\n\nThere \nmust\n also be a type declaration:\n\n\n1\n@valueType=type\n\n\n\n\n\n\nwhere type is \nstr\n or \nint\n. \n@valueType\n declares the type of the values in\nthis feature file. If it is anything other than \nstr\n (=\nstring\n), Text-Fabric\nwill convert it to that type when it reads the data from the file. Currently,\nthe only other supported type is \nint\n for integers.\n\n\nIn edge features, there \nmay\n also be a declaration\n\n\n1\n@edgeValues\n\n\n\n\n\n\nindicating that the edge feature carries values. The default is that an edge\ndoes not carry values.\n\n\nThe rest of the metadata is optional for now, but it is recommended to put a\ndate stamp in it like this\n\n\n1\n@dateCreated=2016-11-20T13:26:59Z\n\n\n\n\n\n\nThe time format should be \nISO 8601\n.\n\n\nData\n\u00b6\n\n\nAfter the metadata, there must be exactly one blank line, and every line after\nthat is data.\n\n\nData lines\n\u00b6\n\n\nThe form of a data line is\n\n\n1\nnode_spec value\n\n\n\n\n\n\nfor node features, and\n\n\n1\nnode_spec node_spec value\n\n\n\n\n\n\nfor edge features.\n\n\nThese fields are separated by single tabs.\n\n\nNB\n: This is the default format. Under \nOptimizations\n below we shall\ndescribe the bits that can be left out, which will lead to significant\nimprovement in space demands and processing speed.\n\n\nNode Specification\n\u00b6\n\n\nEvery line contains a feature value that pertains to all nodes defined by its\n\nnode_spec\n, or to all edges defined by its pair of \nnode_spec\ns.\n\n\nA node spec denotes a \nset\n of nodes.\n\n\nThe simplest form of a node spec is just a single integer. Examples:\n\n\n1\n2\n3\n3\n45\n425000\n\n\n\n\n\n\nRanges are also allowed. Examples\n\n\n1\n2\n3\n1-10\n5-13\n28-57045\n\n\n\n\n\n\nThe nodes denoted by a range are all numbers between the endpoints of the range\n(including at both sides). So\n\n\n1\n2-4\n\n\n\n\n\n\ndenotes the nodes \n2\n, \n3\n, and \n4\n.\n\n\nYou can also combine numbers and ranges arbitrarily by separating them with\ncommas. Examples\n\n\n1\n1-3,5-10,15,23-37\n\n\n\n\n\n\nSuch a specification denotes the union of what is denoted by each\ncomma-separated part.\n\n\nNB\n As node specs denote \nsets\n of nodes, the following node specs are in\nfact equivalent\n\n\n1\n2\n3\n1,1 and 1\n2-3 and 3,2\n1-5,2-7 and 1-7\n\n\n\n\n\n\nWe will be tolerant in that you may specify the end points of ranges in\narbitrary order:\n\n\n1\n1-3 is the same as 3-1\n\n\n\n\n\n\nEdges\n\u00b6\n\n\nAn edge is specified by an \nordered\n pair of nodes. The edge is \nfrom\n the first\nnode in the pair \nto\n the second one. An edge spec consists of two node specs.\nIt denotes all edges that are \nfrom\n a node denoted by the first node spec \nto\n\na node denoted by the second node spec. An edge might be labelled, in that case\nthe label of the edge is specified by the \nvalue\n after the two node specs.\n\n\nValue\n\u00b6\n\n\nThe value is arbitrary text. The type of the value must conform to the\n\n@valueType\n declaration in the feature file. If it is missing, it is assumed to\nbe \nstr\n, which is the type of Unicode-utf8 strings. If it is \nint\n, it should\nbe a valid representation of an integer number,\n\n\nThere are a few escapes:\n\n\n\n\n\\\\\n backslash\n\n\n\\t\n tab\n\n\n\\n\n newline These characters MUST always be escaped in a value string,\n    otherwise the line as a whole might be ambiguous.\n\n\n\n\nNB:\n There is no representation for the absence of a value. The empty string\nas value means that there is a value and it is the empty string. If you want to\ndescribe the fact that node \nn\n does not have a value for the feature in\nquestion, the node must be left out of the feature. In order words, there should\nbe no data line in the feature that targets this node.\n\n\nIf the declared value type (\n@valueType\n) of a feature is \nint\n, then its empty\nvalues will be taken as absence of values, though.\n\n\nConsistency requirements\n\u00b6\n\n\nThere are a few additional requirements on feature data, having to do with the\nfact that features annotate nodes or edges of a graph.\n\n\nSingle values\n\u00b6\n\n\nIt is assumed that a node feature assigns only one value to the same node. If\nthe data contains multiple assignments to a node, only the last assignment will\nbe honoured, the previous ones will be discarded.\n\n\nLikewise, it is assumed that an edge feature assigns only one value to the same\nedge. If the data contains multiple assignments to an edge, only the last\nassignment will be honoured.\n\n\nViolations maybe or may not be reported, and processing may continue without\nwarnings.",
            "title": "Format"
        },
        {
            "location": "/Model/File-formats/#text-fabric-file-format",
            "text": "",
            "title": "Text-Fabric File Format"
        },
        {
            "location": "/Model/File-formats/#overview",
            "text": "A  .tf  feature file starts with a  header , and is followed by the actual data.\nThe whole file is a plain text in UNICODE-utf8.",
            "title": "Overview"
        },
        {
            "location": "/Model/File-formats/#header",
            "text": "A  .tf  feature file always starts with one or more metadata lines of the form  1 @key   or  1 @key=value   The first line must be either  1 @node   or  1 @edge   or  1 @config   This tells Text-Fabric whether the data in the feature file is a  node  feature\nor an  edge  feature. The value  @config  means that the file will be used as\nconfiguration info. It will only have metadata.  There  must  also be a type declaration:  1 @valueType=type   where type is  str  or  int .  @valueType  declares the type of the values in\nthis feature file. If it is anything other than  str  (= string ), Text-Fabric\nwill convert it to that type when it reads the data from the file. Currently,\nthe only other supported type is  int  for integers.  In edge features, there  may  also be a declaration  1 @edgeValues   indicating that the edge feature carries values. The default is that an edge\ndoes not carry values.  The rest of the metadata is optional for now, but it is recommended to put a\ndate stamp in it like this  1 @dateCreated=2016-11-20T13:26:59Z   The time format should be  ISO 8601 .",
            "title": "Header"
        },
        {
            "location": "/Model/File-formats/#data",
            "text": "After the metadata, there must be exactly one blank line, and every line after\nthat is data.",
            "title": "Data"
        },
        {
            "location": "/Model/File-formats/#data-lines",
            "text": "The form of a data line is  1 node_spec value   for node features, and  1 node_spec node_spec value   for edge features.  These fields are separated by single tabs.  NB : This is the default format. Under  Optimizations  below we shall\ndescribe the bits that can be left out, which will lead to significant\nimprovement in space demands and processing speed.",
            "title": "Data lines"
        },
        {
            "location": "/Model/File-formats/#node-specification",
            "text": "Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of  node_spec s.  A node spec denotes a  set  of nodes.  The simplest form of a node spec is just a single integer. Examples:  1\n2\n3 3\n45\n425000   Ranges are also allowed. Examples  1\n2\n3 1-10\n5-13\n28-57045   The nodes denoted by a range are all numbers between the endpoints of the range\n(including at both sides). So  1 2-4   denotes the nodes  2 ,  3 , and  4 .  You can also combine numbers and ranges arbitrarily by separating them with\ncommas. Examples  1 1-3,5-10,15,23-37   Such a specification denotes the union of what is denoted by each\ncomma-separated part.  NB  As node specs denote  sets  of nodes, the following node specs are in\nfact equivalent  1\n2\n3 1,1 and 1\n2-3 and 3,2\n1-5,2-7 and 1-7   We will be tolerant in that you may specify the end points of ranges in\narbitrary order:  1 1-3 is the same as 3-1",
            "title": "Node Specification"
        },
        {
            "location": "/Model/File-formats/#edges",
            "text": "An edge is specified by an  ordered  pair of nodes. The edge is  from  the first\nnode in the pair  to  the second one. An edge spec consists of two node specs.\nIt denotes all edges that are  from  a node denoted by the first node spec  to \na node denoted by the second node spec. An edge might be labelled, in that case\nthe label of the edge is specified by the  value  after the two node specs.",
            "title": "Edges"
        },
        {
            "location": "/Model/File-formats/#value",
            "text": "The value is arbitrary text. The type of the value must conform to the @valueType  declaration in the feature file. If it is missing, it is assumed to\nbe  str , which is the type of Unicode-utf8 strings. If it is  int , it should\nbe a valid representation of an integer number,  There are a few escapes:   \\\\  backslash  \\t  tab  \\n  newline These characters MUST always be escaped in a value string,\n    otherwise the line as a whole might be ambiguous.   NB:  There is no representation for the absence of a value. The empty string\nas value means that there is a value and it is the empty string. If you want to\ndescribe the fact that node  n  does not have a value for the feature in\nquestion, the node must be left out of the feature. In order words, there should\nbe no data line in the feature that targets this node.  If the declared value type ( @valueType ) of a feature is  int , then its empty\nvalues will be taken as absence of values, though.",
            "title": "Value"
        },
        {
            "location": "/Model/File-formats/#consistency-requirements",
            "text": "There are a few additional requirements on feature data, having to do with the\nfact that features annotate nodes or edges of a graph.",
            "title": "Consistency requirements"
        },
        {
            "location": "/Model/File-formats/#single-values",
            "text": "It is assumed that a node feature assigns only one value to the same node. If\nthe data contains multiple assignments to a node, only the last assignment will\nbe honoured, the previous ones will be discarded.  Likewise, it is assumed that an edge feature assigns only one value to the same\nedge. If the data contains multiple assignments to an edge, only the last\nassignment will be honoured.  Violations maybe or may not be reported, and processing may continue without\nwarnings.",
            "title": "Single values"
        },
        {
            "location": "/Model/Optimizations/",
            "text": "File format Optimizations\n\u00b6\n\n\nRationale\n\u00b6\n\n\nIt is important to avoid an explosion of redundant data in \n.tf\n files. We want\nthe \n.tf\n format to be suitable for archiving, transparent to the human eye, and\neasy (i.e. fast) to process.\n\n\nUsing the implicit node\n\u00b6\n\n\nYou may leave out the node spec for node features, and the first node spec for\nedge features. When leaving out a node spec, you must also leave out the tab\nfollowing the node spec.\n\n\nA line with the first node spec left out denotes the singleton node set\nconsisting of the \nimplicit node\n. Here are the rules for implicit nodes.\n\n\n\n\nOn a line where there is an explicit node spec, the implicit node is equal to\n    the highest node denoted by the explicit node spec;\n\n\nOn a line without an explicit node spec, the implicit node is determined from\n    the previous line as follows:\n\n\nif there is no previous line, take \n1\n;\n\n\nelse take the implicit node of the previous line and increment it by \n1\n.\n\n\n\n\n\n\n\n\nFor edges, this optimization only happens for the \nfirst\n node spec. The second\nnode spec must always be explicit.\n\n\nThis optimizes some feature files greatly, e.g. the feature that contains the\nactual text of each word.\n\n\nInstead of\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n1 be\n2 reshit\n3 bara\n4 elohim\n5 et\n6 ha\n7 shamajim\n8 we\n9 et\n10 ha\n11 arets\n\n\n\n\n\n\nyou can just say\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nbe\nreshit\nbara\nelohim\net\nha\nshamajim\nwe\net\nha\narets\n\n\n\n\n\n\nThis optimization is not obligatory. It is a device that may be used if you want\nto optimize the size of data files that you want to distribute.\n\n\nOmitting empty values\n\u00b6\n\n\nIf the value is the empty string, you may also leave out the preceding tab (if\nthere is one). This is especially good for edge features, because most edges\njust consist of a node pair without any value.\n\n\nThis optimization will cause a conceptual ambiguity if there is only one field\npresent in a node feature, or if there are only two fields in an edge feature.\nIt could mean that the (first) node spec has been left out, or that the value\nhas been left out.\n\n\nIn those cases we will assume that the node spec has been left out for node\nfeatures.\n\n\nFor edge features, it depends on whether the edge is declared to have values\n(with \n@edgeValues\n). If the edge has values, then, as in the case of node\nfeatures, we assume that the first node spec has been left out. But if the edge\nhas no values, then we assume that both fields are node specs.\n\n\nSo, in a node feature a line like this\n\n\n1\n42\n\n\n\n\n\n\nmeans that the implicit node gets value \n42\n, and not that node \n42\n gets the\nempty value.\n\n\nLikewise, a line in an edge feature (without values) like this\n\n\n1\n42 43\n\n\n\n\n\n\nmeans that there is an edge from \n42\n to \n43\n with empty value, and not that\nthere is an edge from the implicit node to \n42\n with value 43.\n\n\nAnd, in the same edge, a line like this\n\n\n1\n42\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to \n42\n with the empty value.\n\n\nBut, in an edge with values, the same lines are interpreted thus:\n\n\n1\n42 43\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to node \n42\n with value \n43\n.\n\n\nAnd\n\n\n1\n42\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to node \n42\n with empty\nvalue.\n\n\nThe reason for these conventions is practical: edge features usually have empty\nlabels, and there are many edges. In case of the Hebrew Text database, there are\n1.5 million edges, so every extra character that is needed on a data line means\nthat the file size increases with 1.5 MB.\n\n\nNodes on the other hand, usually do not have empty values, and they are often\nspecified in a consecutive way, especially slot (word) nodes. There are quite\nmany distinct word features, and it would be a waste to have a column of half a\nmillion incremental integers in those files.\n\n\nAbsence of values\n\u00b6\n\n\nSay you have a node feature assigning a value to only 2000 of 400,000 nodes.\n(The Hebrew \nqere\n would be an example). It is better to make sure that the\nabsent values are not coded as the empty string. So the feature data will look\nlike 2000 lines, each with a node spec, rather than a sequence of 400,000 lines,\nmost empty.\n\n\nIf you want to leave out just a few isolated cases in a feature where most nodes\nget a value, you can do it like this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n@node\n\nx0000\n...\nx1000\n1002 x1002\nx1003\n...\nx9999\n\n\n\n\n\n\nHere all 10,000 nodes get a value, except node \n1001\n.\n\n\nNote on redundancy\n\u00b6\n\n\nSome features assign the same value to many nodes. It is tempting to make a\nvalue definition facility, so that values are coded by short codes, the most\nfrequent values getting the shortest codes. After some experiments, it turned\nout that the overall gain was just 50%.\n\n\nI find this advantage too small to justify the increased code complexity, and\nabove all, the reduced transparency of the \n.tf\n files.\n\n\nExamples\n\u00b6\n\n\nHere are a few more and less contrived examples of legal feature data lines.\n\n\nNode features\n\u00b6\n\n\n\n\n\\t\\n\n\n\n2 2\\t3\n\n\nfoo\\nbar\n\n\n1 Escape \\t as \\\\t\n\n\n\n\nmeaning\n\n\n\n\nnode 1 has value: \ntab\n \nnewline\n\n\nnode 2 has value: 2 \ntab\n 3\n\n\nnode 3 has value: foo \nnewline\n bar\n\n\nnode 1 gets a new value: Escape \n as \\t\n\n\n\n\nEdge features\n\u00b6\n\n\n\n\n1\n\n\n1 2\n\n\n2 3 foo\n\n\n1-2 2-3 bar\n\n\n\n\nmeaning\n\n\n\n\nedge from 1 to 1 with no value\n\n\nedge from 1 to 2 with no value\n\n\nedge from 2 to 3 with value foo\n\n\nfour edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can\n    go from a node to itself. Note also that this line reassigns a value to two\n    edges: 1->2 and 2->3.",
            "title": "Tweaks"
        },
        {
            "location": "/Model/Optimizations/#file-format-optimizations",
            "text": "",
            "title": "File format Optimizations"
        },
        {
            "location": "/Model/Optimizations/#rationale",
            "text": "It is important to avoid an explosion of redundant data in  .tf  files. We want\nthe  .tf  format to be suitable for archiving, transparent to the human eye, and\neasy (i.e. fast) to process.",
            "title": "Rationale"
        },
        {
            "location": "/Model/Optimizations/#using-the-implicit-node",
            "text": "You may leave out the node spec for node features, and the first node spec for\nedge features. When leaving out a node spec, you must also leave out the tab\nfollowing the node spec.  A line with the first node spec left out denotes the singleton node set\nconsisting of the  implicit node . Here are the rules for implicit nodes.   On a line where there is an explicit node spec, the implicit node is equal to\n    the highest node denoted by the explicit node spec;  On a line without an explicit node spec, the implicit node is determined from\n    the previous line as follows:  if there is no previous line, take  1 ;  else take the implicit node of the previous line and increment it by  1 .     For edges, this optimization only happens for the  first  node spec. The second\nnode spec must always be explicit.  This optimizes some feature files greatly, e.g. the feature that contains the\nactual text of each word.  Instead of   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 1 be\n2 reshit\n3 bara\n4 elohim\n5 et\n6 ha\n7 shamajim\n8 we\n9 et\n10 ha\n11 arets   you can just say   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 be\nreshit\nbara\nelohim\net\nha\nshamajim\nwe\net\nha\narets   This optimization is not obligatory. It is a device that may be used if you want\nto optimize the size of data files that you want to distribute.",
            "title": "Using the implicit node"
        },
        {
            "location": "/Model/Optimizations/#omitting-empty-values",
            "text": "If the value is the empty string, you may also leave out the preceding tab (if\nthere is one). This is especially good for edge features, because most edges\njust consist of a node pair without any value.  This optimization will cause a conceptual ambiguity if there is only one field\npresent in a node feature, or if there are only two fields in an edge feature.\nIt could mean that the (first) node spec has been left out, or that the value\nhas been left out.  In those cases we will assume that the node spec has been left out for node\nfeatures.  For edge features, it depends on whether the edge is declared to have values\n(with  @edgeValues ). If the edge has values, then, as in the case of node\nfeatures, we assume that the first node spec has been left out. But if the edge\nhas no values, then we assume that both fields are node specs.  So, in a node feature a line like this  1 42   means that the implicit node gets value  42 , and not that node  42  gets the\nempty value.  Likewise, a line in an edge feature (without values) like this  1 42 43   means that there is an edge from  42  to  43  with empty value, and not that\nthere is an edge from the implicit node to  42  with value 43.  And, in the same edge, a line like this  1 42   means that there is an edge from the implicit node to  42  with the empty value.  But, in an edge with values, the same lines are interpreted thus:  1 42 43   means that there is an edge from the implicit node to node  42  with value  43 .  And  1 42   means that there is an edge from the implicit node to node  42  with empty\nvalue.  The reason for these conventions is practical: edge features usually have empty\nlabels, and there are many edges. In case of the Hebrew Text database, there are\n1.5 million edges, so every extra character that is needed on a data line means\nthat the file size increases with 1.5 MB.  Nodes on the other hand, usually do not have empty values, and they are often\nspecified in a consecutive way, especially slot (word) nodes. There are quite\nmany distinct word features, and it would be a waste to have a column of half a\nmillion incremental integers in those files.",
            "title": "Omitting empty values"
        },
        {
            "location": "/Model/Optimizations/#absence-of-values",
            "text": "Say you have a node feature assigning a value to only 2000 of 400,000 nodes.\n(The Hebrew  qere  would be an example). It is better to make sure that the\nabsent values are not coded as the empty string. So the feature data will look\nlike 2000 lines, each with a node spec, rather than a sequence of 400,000 lines,\nmost empty.  If you want to leave out just a few isolated cases in a feature where most nodes\nget a value, you can do it like this:  1\n2\n3\n4\n5\n6\n7\n8\n9 @node\n\nx0000\n...\nx1000\n1002 x1002\nx1003\n...\nx9999   Here all 10,000 nodes get a value, except node  1001 .",
            "title": "Absence of values"
        },
        {
            "location": "/Model/Optimizations/#note-on-redundancy",
            "text": "Some features assign the same value to many nodes. It is tempting to make a\nvalue definition facility, so that values are coded by short codes, the most\nfrequent values getting the shortest codes. After some experiments, it turned\nout that the overall gain was just 50%.  I find this advantage too small to justify the increased code complexity, and\nabove all, the reduced transparency of the  .tf  files.",
            "title": "Note on redundancy"
        },
        {
            "location": "/Model/Optimizations/#examples",
            "text": "Here are a few more and less contrived examples of legal feature data lines.",
            "title": "Examples"
        },
        {
            "location": "/Model/Optimizations/#node-features",
            "text": "\\t\\n  2 2\\t3  foo\\nbar  1 Escape \\t as \\\\t   meaning   node 1 has value:  tab   newline  node 2 has value: 2  tab  3  node 3 has value: foo  newline  bar  node 1 gets a new value: Escape   as \\t",
            "title": "Node features"
        },
        {
            "location": "/Model/Optimizations/#edge-features",
            "text": "1  1 2  2 3 foo  1-2 2-3 bar   meaning   edge from 1 to 1 with no value  edge from 1 to 2 with no value  edge from 2 to 3 with value foo  four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can\n    go from a node to itself. Note also that this line reassigns a value to two\n    edges: 1->2 and 2->3.",
            "title": "Edge features"
        },
        {
            "location": "/Create/CreateTF/",
            "text": "Create a TF dataset\n\u00b6\n\n\nWe describe a conversion of an \nexample text\n to Text-Fabric.\n\n\nThis is not meant as a recipe, but as a description of the pieces of information\nthat have to be assembled from the source text, and how to compose that into a\nText-Fabric resource, which is a set of features.\n\n\nHow you turn this insight into an executable program is dependent on how the\nsource text is encoded and organized. See some\n\nexamples\n.\n\n\nAnalysis\n\u00b6\n\n\nThe text is a string with a little bit of structure. Some of that structure\ngives us our node types, and other bits give us the features.\n\n\nNode types\n\u00b6\n\n\nThe text is divided into main sections, subsections, paragraphs, and sentences.\nThe sentences are divided into words by white-space and/or punctuation.\n\n\nStep 1: define slots\n\u00b6\n\n\nMake a copy of the text, strip out all headings and split the string on\nwhite-space. We get a sequence of \"words\". These words may contain punctuation or\nother non-alphabetical signs. We do not care for the moment.\n\n\nThe indexes in this sequence, from 1 till the number of \"words\", are our slots.\nLet's say we have \nS\n of them.\n\n\nWe start constructing a mapping from numbers to node types, called \notype\n.\n\n\nWe assign to numbers 1, ... ,\nS\n the string \nword\n.\n\n\nThat means, we have now \nS\n nodes, all of type \nword\n.\n\n\nStep 2: add higher level nodes\n\u00b6\n\n\nFor each level of \nsection\n, \nsubsection\n and \nparagraph\n, make new nodes. Nodes\nare numbers, and we start making new nodes directly after \nS\n.\n\n\nWe have 4 main sections, so we extend the \notype\n mapping as follows:\n\n\n\n\nS+1\n ~ \nsection\n\n\nS+2\n ~ \nsection\n\n\nS+3\n ~ \nsection\n\n\nS+4\n ~ \nsection\n\n\n\n\nLikewise, we have 11 subsections, so we continue extending:\n\n\n\n\nS+5\n ~ \nsubsection\n\n\nS+6\n ~ \nsubsection\n\n\n...\n\n\nS+16\n ~ \nsubsection\n\n\n\n\nWe do the same for \nparagraph\n.\n\n\nAnd after that, we break the paragraphs up into sentences (split on \n.\n), and we\nadd so many nodes of type \nsentence\n.\n\n\nThe mapping \notype\n is called a \nfeature\n of nodes. Any mapping that assigns\nvalues to nodes, is called a (node-)feature.\n\n\nContainment\n\u00b6\n\n\nWe also have to record which words belong to which nodes. This information takes\nthe shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we\nhave to lists pairs of nodes \n(n, w)\n such that the word \nw\n \"belongs\" to the\nnode \nn\n.\n\n\nThis is in fact a set of edges (pairs of nodes are edges), and a set of edges is\nan \nedge feature\n. In general it is possible to assign values to pairs of nodes,\nbut for our containment information we just assign the empty value to every pair\nwe have in our set.\n\n\nThe edge feature that records the containment of words in nodes, is called\n\noslots\n.\n\n\nStep 3: map nodes to sets of words\n\u00b6\n\n\nFor each of the higher level nodes \nn\n (the ones beyond \nS\n) we have to\nlookup/remember/compute which words \nw\n belong to it, and put that in the\n\noslots\n mapping:\n\n\n\n\nS+1\n ~ { 1, 2, 3, ... x, ..., y }\n\n\nS+2\n ~ { y+1, y+2, ... } ...\n\n\nS+5\n ~ { 1, 2, 3, ... x }\n\n\nS+6\n ~ { x+1, x+2, ...}\n\n\n...\n\n\n\n\nFeatures\n\u00b6\n\n\nNow we have two features, a node feature \notype\n and an edge feature \noslots\n.\nThis is merely the skeleton of our text, the \nwarp\n so to speak. It contains the\ntextual positions, and the information what the meaningful chunks are.\n\n\nNow it is time to weave the information in.\n\n\nStep 4: the actual text\n\u00b6\n\n\nRemember the words with punctuation attached? We can split every word into three\nparts:\n\n\n\n\ntext: the alphabetical characters in between\n\n\nprefix: the non-alphabetical leading characters\n\n\nsuffix: the non-alphabetical trailing characters\n\n\n\n\nWe can make three node features, \nprefix\n, \ntext\n, and \nsuffix\n. Remember that\nnode features are mappings from numbers to values.\n\n\nHere we go:\n\n\n\n\nprefix[1]\n is the prefix of word 1\n\n\nsuffix[1]\n is the suffix of word 1\n\n\ntext[1]\n is the text of word 1\n\n\n...\n\n\n\n\nAnd so for all words.\n\n\nStep 5: more features\n\u00b6\n\n\nFor the sections and subsections we can make a feature \nheading\n, in which we\nstore the headings of those sections.\n\n\n\n\nheading[S+1]\n is \nIntroduction\n\n\nheading[S+5]\n is \nBasic concepts\n\n\nheading[S+16]\n is \nIdentity\n\n\n...\n\n\n\n\nFor paragraphs we can figure out their sequence number within the subsection,\nand store that in a feature \nnumber\n:\n\n\n\n\nnumber[p]\n is 1 if \np\n is the node corresponding to the first paragraph in a\n    subsection.\n\n\n\n\nIf you want absolute paragraph numbers, you can just add a feature for that:\n\n\n\n\nabs_number[p]\n is 23 if \np\n is the node corresponding to the 23th paragraph\n    in the corpus.\n\n\n\n\nMetadata\n\u00b6\n\n\nYou can supply metadata to all node features and edge features. Metadata must be\ngiven as a dictionary, where the keys are the names of the features in your\ndataset, and the values are themselves key-value pairs, where the values are\njust strings.\n\n\nYou can mention where the source data comes from, who did the conversion, and\nyou can give a description of the intention of this feature and the shape of its\nvalues.\n\n\nLater, when you save the whole dataset as TF, Text-Fabric will insert a\n\ndatecreated\n key-value.\n\n\nYou can also supply metadata for \n''\n (the empty key). These key-values will be\nadded to all other features. Here you can put stuff that pertains to the dataset\nas a whole, such as information about decisions that have been taken.\n\n\nYou should also provide some special metadata to the key \notext\n. This feature\nhas no data, only metadata. It is not a node feature, not an edge feature, but a\n\nconfig\n feature. \notext\n is responsible for sectioning and text representation.\n\n\nIf you specify \notext\n well, the \nT-API\n can make use of it, so that\nyou have convenient, generic functions to get at your sections and to serialize\nyour text in different formats.\n\n\nStep 6: sectioning metadata\n\u00b6\n\n\n\n\nsectionTypes: 'section,subsection,paragraph'\n\n\nsectionFeatures: 'title,title,number'\n\n\n\n\nThis tells Text-Fabric that node type \nsection\n corresponds to section level 1,\n\nsubsection\n to level 2, and \nparagraph\n to level 3. Moreover, Text-Fabric knows\nthat the heading of sections at level 1 and 2 are in the feature \ntitle\n, and\nthat the heading at level 3 is in the feature \nnumber\n.\n\n\nStep 7: text formats\n\u00b6\n\n\n\n\nfmt:text-orig-plain: '{prefix}{text}{suffix}'\n\n\nfmt:text-orig-bare: '{text} '\n\n\nfmt:text-orig-angle: ' <{text}> '\n\n\n\n\nHere you have provided a bunch of text representation formats to Text-Fabric.\nThe names of those formats are up to you, and the values as well.\n\n\nIf you have a list of word nodes, say \nws\n, then a user of your corpus can ask\nText-Fabric:\n\n\n1\nT\n.\ntext\n(\nws\n,\n \nfmt\n=\n'text-orig-plain'\n)\n\n\n\n\n\n\n\nThis will spit out the full textual representation of those words, including\nthe non-alphabetical stuff in their prefixes and suffixes.\n\n\nThe second format, \ntext-orig-bare\n, will leave prefix and suffix out.\n\n\nAnd if for whatever reason you need to wrap each word in angle brackets, you can\nachieve that with \ntext-orig-angle\n.\n\n\nAs an example of how text formats come in handy, have a look at the text formats\nthat have been designed for Hebrew:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nfmt:lex-orig-full: '{g_lex_utf8} '\nfmt:lex-orig-plain: '{lex_utf8} '\nfmt:lex-trans-full: '{g_lex} '\nfmt:lex-trans-plain: '{lex0} '\nfmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}'\nfmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}'\nfmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}'\nfmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}'\nfmt:text-trans-full-ketiv: '{g_word}{trailer}'\nfmt:text-trans-plain: '{g_cons}{trailer}'\n\n\n\n\n\n\nNote that the actual text-formats are not baked in into TF, but are supplied by\nyou, the corpus designer.\n\n\nWriting out TF\n\u00b6\n\n\nOnce you have assembled your features and metadata as data structures in memory,\nyou can use \nTF.save()\n to write out your data as a bunch\nof Text-Fabric files.\n\n\nStep 8: invoke TF.save()\n\u00b6\n\n\nThe call to make is\n\n\n1\nTF\n.\nsave\n(\nnodeFeatures\n=\n{},\n \nedgeFeatures\n=\n{},\n \nmetaData\n=\n{},\n \nmodule\n=\nNone\n)\n\n\n\n\n\n\n\nHere you supply for \nnodeFeatures\n a dictionary keyed by your node feature\nnames and valued by the feature data of those features.\n\n\nLikewise for the edge features.\n\n\nAnd the metadata you have composed goes into the \nmetaData\n parameter.\n\n\nFinally, the \nmodule\n parameter dictates where on your system the TF-files will\nbe written.\n\n\nFirst time usage\n\u00b6\n\n\nWhen you start using your new dataset in Text-Fabric, you'll notice that there\nis some upfront computation going on. Text-Fabric computes derived data,\nespecially about the relationships between nodes based on the slots they occupy.\nAll that information comes from \noslots\n. The \noslots\n information is very\nterse, and using it directly would result in a hefty performance penalty.\nLikewise, all feature data will be read from the textual \n.tf\n files,\nrepresented in memory as a dictionary, and then that dictionary will be\nserialized and gzipped into a \n.tfx\n file in a hidden directory \n.tf\n. These\n\n.tfx\n files load an order of magnitude faster than the original \n.tf\n files.\nText-Fabric uses the timestamps of the files to determine whether the \n.tfx\n\nfiles are outdated and need to be regenerated again.\n\n\nThis whole machinery is invisible to you, the user, except for the delay at\nfirst time use.\n\n\nEnriching your corpus\n\u00b6\n\n\nMaybe a linguistic friend of yours has a tool to determine the part of speech of\neach word in the text.\n\n\nUsing TF itself it is not that hard to create a new feature \npos\n, that maps\neach word node to the part of speech of that word.\n\n\nSee for example how Cody Kingham\n\nadds\n\nthe notion of linguistic head to the BHSA\ndatasource of the Hebrew Bible.\n\n\nStep 9: add the new feature\n\u00b6\n\n\nOnce you have the feature \npos\n, provide a bit of metadata, and call\n\n\n1\n2\n3\n4\n5\nTF\n.\nsave\n(\n\n  \nnodeFeatures\n=\n{\n'pos'\n:\n \nposData\n},\n\n  \nmetaData\n=\n{\n'pos'\n:\n \nposMetaData\n},\n\n  \nmodule\n=\n'linguistics'\n,\n\n\n)\n\n\n\n\n\n\n\nYou get a TF module consisting of one feature \npos.tf\n in the \nlinguistics\n\ndirectory.\n\n\nMaybe you have more linguistic features to add. You do not have to create those\nfeatures alongside the original corpus. It is perfectly possible to leave the\ncorpus alone in its own GitHub repo, and write your new features in another\nrepo.\n\n\nUsers can just obtain the corpus and your linguistic module separately. When\nthey call their Text-Fabric, they can point it to both locations, and\nText-Fabric treats it as one dataset.\n\n\nStep 10: use the new feature\n\u00b6\n\n\nThe call to \nTF=Fabric()\n looks like this\n\n\n1\nTF\n \n=\n \nFabric\n(\nlocations\n=\n[\ncorpusLocation\n,\n \nmoduleLocation\n])\n\n\n\n\n\n\n\nAll feature files found at these locations are loadable in your session.",
            "title": "Make TF"
        },
        {
            "location": "/Create/CreateTF/#create-a-tf-dataset",
            "text": "We describe a conversion of an  example text  to Text-Fabric.  This is not meant as a recipe, but as a description of the pieces of information\nthat have to be assembled from the source text, and how to compose that into a\nText-Fabric resource, which is a set of features.  How you turn this insight into an executable program is dependent on how the\nsource text is encoded and organized. See some examples .",
            "title": "Create a TF dataset"
        },
        {
            "location": "/Create/CreateTF/#analysis",
            "text": "The text is a string with a little bit of structure. Some of that structure\ngives us our node types, and other bits give us the features.",
            "title": "Analysis"
        },
        {
            "location": "/Create/CreateTF/#node-types",
            "text": "The text is divided into main sections, subsections, paragraphs, and sentences.\nThe sentences are divided into words by white-space and/or punctuation.",
            "title": "Node types"
        },
        {
            "location": "/Create/CreateTF/#step-1-define-slots",
            "text": "Make a copy of the text, strip out all headings and split the string on\nwhite-space. We get a sequence of \"words\". These words may contain punctuation or\nother non-alphabetical signs. We do not care for the moment.  The indexes in this sequence, from 1 till the number of \"words\", are our slots.\nLet's say we have  S  of them.  We start constructing a mapping from numbers to node types, called  otype .  We assign to numbers 1, ... , S  the string  word .  That means, we have now  S  nodes, all of type  word .",
            "title": "Step 1: define slots"
        },
        {
            "location": "/Create/CreateTF/#step-2-add-higher-level-nodes",
            "text": "For each level of  section ,  subsection  and  paragraph , make new nodes. Nodes\nare numbers, and we start making new nodes directly after  S .  We have 4 main sections, so we extend the  otype  mapping as follows:   S+1  ~  section  S+2  ~  section  S+3  ~  section  S+4  ~  section   Likewise, we have 11 subsections, so we continue extending:   S+5  ~  subsection  S+6  ~  subsection  ...  S+16  ~  subsection   We do the same for  paragraph .  And after that, we break the paragraphs up into sentences (split on  . ), and we\nadd so many nodes of type  sentence .  The mapping  otype  is called a  feature  of nodes. Any mapping that assigns\nvalues to nodes, is called a (node-)feature.",
            "title": "Step 2: add higher level nodes"
        },
        {
            "location": "/Create/CreateTF/#containment",
            "text": "We also have to record which words belong to which nodes. This information takes\nthe shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we\nhave to lists pairs of nodes  (n, w)  such that the word  w  \"belongs\" to the\nnode  n .  This is in fact a set of edges (pairs of nodes are edges), and a set of edges is\nan  edge feature . In general it is possible to assign values to pairs of nodes,\nbut for our containment information we just assign the empty value to every pair\nwe have in our set.  The edge feature that records the containment of words in nodes, is called oslots .",
            "title": "Containment"
        },
        {
            "location": "/Create/CreateTF/#step-3-map-nodes-to-sets-of-words",
            "text": "For each of the higher level nodes  n  (the ones beyond  S ) we have to\nlookup/remember/compute which words  w  belong to it, and put that in the oslots  mapping:   S+1  ~ { 1, 2, 3, ... x, ..., y }  S+2  ~ { y+1, y+2, ... } ...  S+5  ~ { 1, 2, 3, ... x }  S+6  ~ { x+1, x+2, ...}  ...",
            "title": "Step 3: map nodes to sets of words"
        },
        {
            "location": "/Create/CreateTF/#features",
            "text": "Now we have two features, a node feature  otype  and an edge feature  oslots .\nThis is merely the skeleton of our text, the  warp  so to speak. It contains the\ntextual positions, and the information what the meaningful chunks are.  Now it is time to weave the information in.",
            "title": "Features"
        },
        {
            "location": "/Create/CreateTF/#step-4-the-actual-text",
            "text": "Remember the words with punctuation attached? We can split every word into three\nparts:   text: the alphabetical characters in between  prefix: the non-alphabetical leading characters  suffix: the non-alphabetical trailing characters   We can make three node features,  prefix ,  text , and  suffix . Remember that\nnode features are mappings from numbers to values.  Here we go:   prefix[1]  is the prefix of word 1  suffix[1]  is the suffix of word 1  text[1]  is the text of word 1  ...   And so for all words.",
            "title": "Step 4: the actual text"
        },
        {
            "location": "/Create/CreateTF/#step-5-more-features",
            "text": "For the sections and subsections we can make a feature  heading , in which we\nstore the headings of those sections.   heading[S+1]  is  Introduction  heading[S+5]  is  Basic concepts  heading[S+16]  is  Identity  ...   For paragraphs we can figure out their sequence number within the subsection,\nand store that in a feature  number :   number[p]  is 1 if  p  is the node corresponding to the first paragraph in a\n    subsection.   If you want absolute paragraph numbers, you can just add a feature for that:   abs_number[p]  is 23 if  p  is the node corresponding to the 23th paragraph\n    in the corpus.",
            "title": "Step 5: more features"
        },
        {
            "location": "/Create/CreateTF/#metadata",
            "text": "You can supply metadata to all node features and edge features. Metadata must be\ngiven as a dictionary, where the keys are the names of the features in your\ndataset, and the values are themselves key-value pairs, where the values are\njust strings.  You can mention where the source data comes from, who did the conversion, and\nyou can give a description of the intention of this feature and the shape of its\nvalues.  Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated  key-value.  You can also supply metadata for  ''  (the empty key). These key-values will be\nadded to all other features. Here you can put stuff that pertains to the dataset\nas a whole, such as information about decisions that have been taken.  You should also provide some special metadata to the key  otext . This feature\nhas no data, only metadata. It is not a node feature, not an edge feature, but a config  feature.  otext  is responsible for sectioning and text representation.  If you specify  otext  well, the  T-API  can make use of it, so that\nyou have convenient, generic functions to get at your sections and to serialize\nyour text in different formats.",
            "title": "Metadata"
        },
        {
            "location": "/Create/CreateTF/#step-6-sectioning-metadata",
            "text": "sectionTypes: 'section,subsection,paragraph'  sectionFeatures: 'title,title,number'   This tells Text-Fabric that node type  section  corresponds to section level 1, subsection  to level 2, and  paragraph  to level 3. Moreover, Text-Fabric knows\nthat the heading of sections at level 1 and 2 are in the feature  title , and\nthat the heading at level 3 is in the feature  number .",
            "title": "Step 6: sectioning metadata"
        },
        {
            "location": "/Create/CreateTF/#step-7-text-formats",
            "text": "fmt:text-orig-plain: '{prefix}{text}{suffix}'  fmt:text-orig-bare: '{text} '  fmt:text-orig-angle: ' <{text}> '   Here you have provided a bunch of text representation formats to Text-Fabric.\nThe names of those formats are up to you, and the values as well.  If you have a list of word nodes, say  ws , then a user of your corpus can ask\nText-Fabric:  1 T . text ( ws ,   fmt = 'text-orig-plain' )    This will spit out the full textual representation of those words, including\nthe non-alphabetical stuff in their prefixes and suffixes.  The second format,  text-orig-bare , will leave prefix and suffix out.  And if for whatever reason you need to wrap each word in angle brackets, you can\nachieve that with  text-orig-angle .  As an example of how text formats come in handy, have a look at the text formats\nthat have been designed for Hebrew:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 fmt:lex-orig-full: '{g_lex_utf8} '\nfmt:lex-orig-plain: '{lex_utf8} '\nfmt:lex-trans-full: '{g_lex} '\nfmt:lex-trans-plain: '{lex0} '\nfmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}'\nfmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}'\nfmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}'\nfmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}'\nfmt:text-trans-full-ketiv: '{g_word}{trailer}'\nfmt:text-trans-plain: '{g_cons}{trailer}'   Note that the actual text-formats are not baked in into TF, but are supplied by\nyou, the corpus designer.",
            "title": "Step 7: text formats"
        },
        {
            "location": "/Create/CreateTF/#writing-out-tf",
            "text": "Once you have assembled your features and metadata as data structures in memory,\nyou can use  TF.save()  to write out your data as a bunch\nof Text-Fabric files.",
            "title": "Writing out TF"
        },
        {
            "location": "/Create/CreateTF/#step-8-invoke-tfsave",
            "text": "The call to make is  1 TF . save ( nodeFeatures = {},   edgeFeatures = {},   metaData = {},   module = None )    Here you supply for  nodeFeatures  a dictionary keyed by your node feature\nnames and valued by the feature data of those features.  Likewise for the edge features.  And the metadata you have composed goes into the  metaData  parameter.  Finally, the  module  parameter dictates where on your system the TF-files will\nbe written.",
            "title": "Step 8: invoke TF.save()"
        },
        {
            "location": "/Create/CreateTF/#first-time-usage",
            "text": "When you start using your new dataset in Text-Fabric, you'll notice that there\nis some upfront computation going on. Text-Fabric computes derived data,\nespecially about the relationships between nodes based on the slots they occupy.\nAll that information comes from  oslots . The  oslots  information is very\nterse, and using it directly would result in a hefty performance penalty.\nLikewise, all feature data will be read from the textual  .tf  files,\nrepresented in memory as a dictionary, and then that dictionary will be\nserialized and gzipped into a  .tfx  file in a hidden directory  .tf . These .tfx  files load an order of magnitude faster than the original  .tf  files.\nText-Fabric uses the timestamps of the files to determine whether the  .tfx \nfiles are outdated and need to be regenerated again.  This whole machinery is invisible to you, the user, except for the delay at\nfirst time use.",
            "title": "First time usage"
        },
        {
            "location": "/Create/CreateTF/#enriching-your-corpus",
            "text": "Maybe a linguistic friend of yours has a tool to determine the part of speech of\neach word in the text.  Using TF itself it is not that hard to create a new feature  pos , that maps\neach word node to the part of speech of that word.  See for example how Cody Kingham adds \nthe notion of linguistic head to the BHSA\ndatasource of the Hebrew Bible.",
            "title": "Enriching your corpus"
        },
        {
            "location": "/Create/CreateTF/#step-9-add-the-new-feature",
            "text": "Once you have the feature  pos , provide a bit of metadata, and call  1\n2\n3\n4\n5 TF . save ( \n   nodeFeatures = { 'pos' :   posData }, \n   metaData = { 'pos' :   posMetaData }, \n   module = 'linguistics' ,  )    You get a TF module consisting of one feature  pos.tf  in the  linguistics \ndirectory.  Maybe you have more linguistic features to add. You do not have to create those\nfeatures alongside the original corpus. It is perfectly possible to leave the\ncorpus alone in its own GitHub repo, and write your new features in another\nrepo.  Users can just obtain the corpus and your linguistic module separately. When\nthey call their Text-Fabric, they can point it to both locations, and\nText-Fabric treats it as one dataset.",
            "title": "Step 9: add the new feature"
        },
        {
            "location": "/Create/CreateTF/#step-10-use-the-new-feature",
            "text": "The call to  TF=Fabric()  looks like this  1 TF   =   Fabric ( locations = [ corpusLocation ,   moduleLocation ])    All feature files found at these locations are loadable in your session.",
            "title": "Step 10: use the new feature"
        },
        {
            "location": "/Create/ExampleText/",
            "text": "Introduction\n\u00b6\n\n\nThe Social Construction of Reality is a 1966 book about the sociology of\nknowledge by the sociologists Peter L. Berger and Thomas Luckmann.\n\n\nBerger and Luckmann introduced the term \"social construction\" into the social\nsciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their\ncentral concept is that people and groups interacting in a social system create,\nover time, concepts or mental representations of each other's actions, and that\nthese concepts eventually become habituated into reciprocal roles played by the\nactors in relation to each other. When these roles are made available to other\nmembers of society to enter into and play out, the reciprocal interactions are\nsaid to be institutionalized. In the process, meaning is embedded in society.\nKnowledge and people's conceptions (and beliefs) of what reality is become\nembedded in the institutional fabric of society. Reality is therefore said to be\nsocially constructed.\n\n\nIn 1998 the International Sociological Association listed The Social\nConstruction of Reality as the fifth-most important sociological book of the\n20th century\n\n\nBasic concepts\n\u00b6\n\n\nSocial stock of knowledge\n\u00b6\n\n\nEarlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max\nWeber, etc.) often focused too much on scientific and theoretical knowledge, but\nthis is only a small part of social knowledge, concerning a very limited group.\nCustoms, common interpretations, institutions, shared routines,\nhabitualizations, the who-is-who and who-does-what in social processes and the\ndivision of labor, constitute a much larger part of knowledge in society.\n\n\n\u201c\u2026theoretical knowledge is only a small and by no means the most important part\nof what passed for knowledge in a society\u2026 the primary knowledge about the\ninstitutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019\nabout a social world, an assemblage of maxims, morals, proverbial nuggets of\nwisdom, values and beliefs, myths, and so forth\u201d (p.65)\n\n\nSemantic fields\n\u00b6\n\n\nThe general body of knowledge is socially distributed, and classified in\nsemantic fields. The dynamic distribution and inter dependencies of these\nknowledge sectors provide structure to the social stock of knowledge:\n\n\n\u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026\nmy knowledge of my own occupation and its world is very rich and specific, while\nI have only very sketchy knowledge of the occupational worlds of others\u201d (p.43)\n\u201cThe social distribution of knowledge thus begins with the simple fact that I do\nnot know everything known to my fellowmen, and vice versa, and culminates in\nexceedingly complex and esoteric systems of expertise. Knowledge of how the\nsocially available stock of knowledge is distributed, at least in outline, is an\nimportant element of that same stock of knowledge.\u201d (p.46)\n\n\nLanguage and signs\n\u00b6\n\n\nLanguage also plays an important role in the analysis of integration of everyday\nreality. Language links up commonsense knowledge with finite provinces of\nmeaning, thus enabling people, for example, to interpret dreams through\nunderstandings relevant in the daytime. \"Language is capable of transcending the\nreality of everyday life altogether. It can refer to experiences pertaining to\nfinite provinces of meaning, it can span discrete spheres of reality...Language\nsoars into regions that are not only de facto but also a priori unavailable to\neveryday experience.\"p. 40. Regarding the function of language and signs, Berger\nand Luckmann are indebted to George Herbert Mead and other figures in the field\nknown as symbolic interactionism, as acknowledged in their Introduction,\nespecially regarding the possibility of constructing objectivity.\n\n\nSigns and language provide interoperability for the construction of everyday\nreality:\n\n\n\u201cA sign [has the] explicit intention to serve as an index of subjective meanings\n\u2026 Language is capable of becoming the objective repository of vast accumulations\nof meaning and experience, which it can then preserve in time and transmit to\nfollowing generations\u2026 Language also typifies experiences, allowing me to\nsubsume them under broad categories in terms of which they have meaning not only\nto myself but also to my fellowmen\u201d (p.35-39)\n\n\nSocial everyday reality\n\u00b6\n\n\nSocial everyday reality is characterized by Intersubjectivity (which refers to\nthe coexistence of multiple realities in this context)(p. 23-25):\n\n\n\u201cCompared to the reality of everyday life, other realities appear as finite\nprovinces of meaning, enclaves within the paramount reality marked by\ncircumscribed meanings and modes of experience\u201d (p.25)\n\n\nThis is in contrast to other realities, such as dreams, theoretical constructs,\nreligious or mystic beliefs, artistic and imaginary worlds, etc. While\nindividuals may visit other realities (such as watching a film), they are always\nbrought back to everyday reality (once the film ends)(p. 25).\n\n\nSociety as objective reality\n\u00b6\n\n\n\u201c Social order is a human product, or more precisely, an ongoing human\nproduction \u201d Institutionalization[edit] Institutionalization of social processes\ngrows out of the habitualization and customs, gained through mutual observation\nwith subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces\nuncertainty and danger and allows our limited attention span to focus on more\nthings at the same time, while institutionalized routines can be expected to\ncontinue \u201cas previously agreed\u201d:\n\n\n\u201cHabitualization carries with it the important psychological gain that choices\nare narrowed\u2026 the background of habitualized activity opens up a foreground for\ndeliberation and innovation [which demand a higher level of attention]\u2026 The most\nimportant gain is that each [member of society] will be able to predict the\nother\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026\nMany actions are possible on a low level of attention. Each action of one is no\nlonger a source of astonishment and potential danger to the other\u201c (p.53-57).\n\n\nSocial objective worlds\n\u00b6\n\n\nSocial (or institutional) objective worlds are one consequence of\ninstitutionalization, and are created when institutions are passed on to a new\ngeneration. This creates a reality that is vulnerable to the ideas of a minority\nwhich will then form the basis of social expectations in the future. The\nunderlying reasoning is fully transparent to the creators of an institution, as\nthey can reconstruct the circumstances under which they made agreements; while\nthe second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and\n\u201cself-evident\u201d and they might not understand the underlying logic.\n\n\n\u201c\u2026a social world [is] a comprehensive and given reality confronting the\nindividual in a manner analogous to the reality of the natural world\u2026 In early\nphases of socialization the child is quite incapable of distinguishing between\nthe objectivity of natural phenomena and the objectivity of the social\nformations\u2026 The objective reality of institutions is not diminished if the\nindividual does not understand their purpose or their mode of operation\u2026 He must\n\u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)\n\n\nDivision of labor\n\u00b6\n\n\nDivision of labor is another consequence of institutionalization. Institutions\nassign \u201croles\u201d to be performed by various actors, through typification of\nperformances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As\nspecialization increases in number as well as in size and sophistication, a\ncivilization's culture contains more and more sections of knowledge specific to\ngiven roles or tasks, sections which become more and more esoteric to\nnon-specialists. These areas of knowledge do not belong anymore to the common\nsocial world and culture.\n\n\n\u201cA society\u2019s stock of knowledge is structured in terms of what is generally\nrelevant and what is relevant only to specific roles\u2026 the social distribution of\nknowledge entails a dichotomization in terms of general and role-specific\nrelevance\u2026 because of the division of labor, role-specific knowledge will grow\nat a faster rate than generally relevant and accessible knowledge\u2026 The\nincreasing number and complexity of [the resulting] sub universes [of\nspecialized knowledge] make them increasingly inaccessible to outsiders\n(p.77-87)\n\n\nSymbolic universes\n\u00b6\n\n\nSymbolic universes are created to provide legitimation to the created\ninstitutional structure. Symbolic universes are a set of beliefs \u201ceverybody\nknows\u201d that aim at making the institutionalized structure plausible and\nacceptable for the individual\u2014who might otherwise not understand or agree with\nthe underlying logic of the institution. As an ideological system, the symbolic\nuniverse \u201cputs everything in its right place\u201d. It provides explanations for why\nwe do things the way we do. Proverbs, moral maxims, wise sayings, mythology,\nreligions and other theological thought, metaphysical traditions and other value\nsystems are part of the symbolic universe. They are all (more or less\nsophisticated) ways to legitimize established institutions.\n\n\n\u201cThe function of legitimation is to make objectively available and subjectively\nplausible the \u2018first-order\u2019 objections that have been institutionalized\u2026\nProverbs, moral maxims and wise sayings are common on this level\u2026 [as well as]\nexplicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a\ngeneral theory of man\u2026 The symbolic universe also orders history. It locates all\ncollective events in a cohesive unity that includes past, present and future.\u201d\n(p. 92-104)\n\n\nUniverse-maintenance\n\u00b6\n\n\nUniverse-maintenance refers to specific procedures undertaken, often by an elite\ngroup, when the symbolic universe does not fulfill its purpose anymore, which is\nto legitimize the institutional structure in place. This happens, for example,\nin generational shifts, or when deviants create an internal movement against\nestablished institutions (e.g. against revolutions), or when a society is\nconfronted with another society with a greatly different history and\ninstitutional structures. In primitive societies this happened through\nmythological systems, later on through theological thought. Today, an extremely\ncomplex set of science has secularized universe-maintenance.\n\n\n\u201cSpecific procedures of universe-maintenance become necessary when the symbolic\nuniverse has become a problem. As long as this is not the case, the symbolic\nuniverse is self-maintaining, that is self-legitimating. An intrinsic problem\npresents itself with the process of transmission of the symbolic universe from\none generation to another\u2026 [additionally] two societies confronting each other\nwith conflicting universes will both develop conceptual machinery designed to\nmaintain their respective universes\u2026 mythology represents the most archaic form\nof universe-maintenance\u2026 theological thought may be distinguished from its\nmythological predecessor simply in terms of its greater degree of theoretical\nsystematization\u2026 Modern science is an extreme step in this development.\n(p.104-116)\n\n\nSociety as subjective reality\n\u00b6\n\n\nSocialization\n\u00b6\n\n\nSocialization is a two-step induction of the individual to participate in the\nsocial institutional structure, meaning in its objective reality.\n\n\n\"The individual\u2026 is not born a member of society. He\u2026 becomes a member of\nsociety. In the life of every individual\u2026 there is a temporal sequence, in the\ncourse of which he is inducted into participation in the social dialectic\"\n(p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high\ndegree of symmetry between objective and subjective reality\u201d (p. 163)\n\n\nPrimary Socialization takes place as a child. It is highly charged emotionally\nand is not questioned. Secondary Socialization includes the acquisition of\nrole-specific knowledge, thus taking one\u2019s place in the social division of\nlabor. It is learned through training and specific rituals, and is not\nemotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s\nteacher\u201d. Training for secondary socialization can be very complex and depends\non the complexity of division of labor in a society. Primary socialization is\nmuch less flexible than secondary socialization. E.g. shame for nudity comes\nfrom primary socialization, adequate dress code depends on secondary: A\nrelatively minor shift in the subjective definition of reality would suffice for\nan individual to take for granted that one may go to the office without a tie. A\nmuch more drastic shift would be necessary to have him go, as a matter of\ncourse, without any clothes at all.\n\n\n\u201cThe child does not internalize the world of his significant others as one of\nmany possible worlds\u2026 It is for this reason that the world internalized in\nprimary socialization is so much more firmly entrenched in consciousness than\nworlds internalized in secondary socialization\u2026. Secondary socialization is the\ninternalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of\nsecondary socialization carry a high degree of anonymity\u2026 The same knowledge\ntaught by one teacher could also be taught by another\u2026 The institutional\ndistribution of tasks between primary and secondary socialization varies with\nthe complexity of the social distribution of knowledge\u201d (p. 129-147)\n\n\nConversation\n\u00b6\n\n\nConversation or verbal communication aims at reality-maintenance of the\nsubjective reality. What seems to be a useless and unnecessary communication of\nredundant banalities is actually a constant mutual reconfirmation of each\nother's internal thoughts, in that it maintains subjective reality.\n\n\n\u201cOne may view the individual\u2019s everyday life in terms of the working away of a\nconversational apparatus that ongoingly maintains, modifies and reconstructs his\nsubjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the\nstation,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire\nworld within which these apparently simple propositions make sense\u2026 the exchange\nconfirms the subjective reality of this world\u2026 the great part, if not all, of\neveryday conversation maintains subjective reality\u2026 imagine the effect\u2026of an\nexchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine,\ndarling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)\n\n\nIdentity\n\u00b6\n\n\nIdentity of an individual is subject to a struggle of affiliation to sometimes\nconflicting realities. For example, the reality from primary socialization\n(mother tells child not to steal) can be in contrast with second socialization\n(gang members teach teenager that stealing is cool). Our final social location\nin the institutional structure of society will ultimately also influence our\nbody and organism.\n\n\n\u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines\nhow long and in what manner the individual organism shall live\u2026 Society also\ndirectly penetrates the organism in its functioning, most importantly in respect\nto sexuality and nutrition. While both sexuality and nutrition are grounded in\nbiological drives\u2026 biological constitution does not tell him where he should\nseek sexual release and what he should eat.\u201d (p. 163-183)",
            "title": "Example data"
        },
        {
            "location": "/Create/ExampleText/#introduction",
            "text": "The Social Construction of Reality is a 1966 book about the sociology of\nknowledge by the sociologists Peter L. Berger and Thomas Luckmann.  Berger and Luckmann introduced the term \"social construction\" into the social\nsciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their\ncentral concept is that people and groups interacting in a social system create,\nover time, concepts or mental representations of each other's actions, and that\nthese concepts eventually become habituated into reciprocal roles played by the\nactors in relation to each other. When these roles are made available to other\nmembers of society to enter into and play out, the reciprocal interactions are\nsaid to be institutionalized. In the process, meaning is embedded in society.\nKnowledge and people's conceptions (and beliefs) of what reality is become\nembedded in the institutional fabric of society. Reality is therefore said to be\nsocially constructed.  In 1998 the International Sociological Association listed The Social\nConstruction of Reality as the fifth-most important sociological book of the\n20th century",
            "title": "Introduction"
        },
        {
            "location": "/Create/ExampleText/#basic-concepts",
            "text": "",
            "title": "Basic concepts"
        },
        {
            "location": "/Create/ExampleText/#social-stock-of-knowledge",
            "text": "Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max\nWeber, etc.) often focused too much on scientific and theoretical knowledge, but\nthis is only a small part of social knowledge, concerning a very limited group.\nCustoms, common interpretations, institutions, shared routines,\nhabitualizations, the who-is-who and who-does-what in social processes and the\ndivision of labor, constitute a much larger part of knowledge in society.  \u201c\u2026theoretical knowledge is only a small and by no means the most important part\nof what passed for knowledge in a society\u2026 the primary knowledge about the\ninstitutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019\nabout a social world, an assemblage of maxims, morals, proverbial nuggets of\nwisdom, values and beliefs, myths, and so forth\u201d (p.65)",
            "title": "Social stock of knowledge"
        },
        {
            "location": "/Create/ExampleText/#semantic-fields",
            "text": "The general body of knowledge is socially distributed, and classified in\nsemantic fields. The dynamic distribution and inter dependencies of these\nknowledge sectors provide structure to the social stock of knowledge:  \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026\nmy knowledge of my own occupation and its world is very rich and specific, while\nI have only very sketchy knowledge of the occupational worlds of others\u201d (p.43)\n\u201cThe social distribution of knowledge thus begins with the simple fact that I do\nnot know everything known to my fellowmen, and vice versa, and culminates in\nexceedingly complex and esoteric systems of expertise. Knowledge of how the\nsocially available stock of knowledge is distributed, at least in outline, is an\nimportant element of that same stock of knowledge.\u201d (p.46)",
            "title": "Semantic fields"
        },
        {
            "location": "/Create/ExampleText/#language-and-signs",
            "text": "Language also plays an important role in the analysis of integration of everyday\nreality. Language links up commonsense knowledge with finite provinces of\nmeaning, thus enabling people, for example, to interpret dreams through\nunderstandings relevant in the daytime. \"Language is capable of transcending the\nreality of everyday life altogether. It can refer to experiences pertaining to\nfinite provinces of meaning, it can span discrete spheres of reality...Language\nsoars into regions that are not only de facto but also a priori unavailable to\neveryday experience.\"p. 40. Regarding the function of language and signs, Berger\nand Luckmann are indebted to George Herbert Mead and other figures in the field\nknown as symbolic interactionism, as acknowledged in their Introduction,\nespecially regarding the possibility of constructing objectivity.  Signs and language provide interoperability for the construction of everyday\nreality:  \u201cA sign [has the] explicit intention to serve as an index of subjective meanings\n\u2026 Language is capable of becoming the objective repository of vast accumulations\nof meaning and experience, which it can then preserve in time and transmit to\nfollowing generations\u2026 Language also typifies experiences, allowing me to\nsubsume them under broad categories in terms of which they have meaning not only\nto myself but also to my fellowmen\u201d (p.35-39)",
            "title": "Language and signs"
        },
        {
            "location": "/Create/ExampleText/#social-everyday-reality",
            "text": "Social everyday reality is characterized by Intersubjectivity (which refers to\nthe coexistence of multiple realities in this context)(p. 23-25):  \u201cCompared to the reality of everyday life, other realities appear as finite\nprovinces of meaning, enclaves within the paramount reality marked by\ncircumscribed meanings and modes of experience\u201d (p.25)  This is in contrast to other realities, such as dreams, theoretical constructs,\nreligious or mystic beliefs, artistic and imaginary worlds, etc. While\nindividuals may visit other realities (such as watching a film), they are always\nbrought back to everyday reality (once the film ends)(p. 25).",
            "title": "Social everyday reality"
        },
        {
            "location": "/Create/ExampleText/#society-as-objective-reality",
            "text": "\u201c Social order is a human product, or more precisely, an ongoing human\nproduction \u201d Institutionalization[edit] Institutionalization of social processes\ngrows out of the habitualization and customs, gained through mutual observation\nwith subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces\nuncertainty and danger and allows our limited attention span to focus on more\nthings at the same time, while institutionalized routines can be expected to\ncontinue \u201cas previously agreed\u201d:  \u201cHabitualization carries with it the important psychological gain that choices\nare narrowed\u2026 the background of habitualized activity opens up a foreground for\ndeliberation and innovation [which demand a higher level of attention]\u2026 The most\nimportant gain is that each [member of society] will be able to predict the\nother\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026\nMany actions are possible on a low level of attention. Each action of one is no\nlonger a source of astonishment and potential danger to the other\u201c (p.53-57).",
            "title": "Society as objective reality"
        },
        {
            "location": "/Create/ExampleText/#social-objective-worlds",
            "text": "Social (or institutional) objective worlds are one consequence of\ninstitutionalization, and are created when institutions are passed on to a new\ngeneration. This creates a reality that is vulnerable to the ideas of a minority\nwhich will then form the basis of social expectations in the future. The\nunderlying reasoning is fully transparent to the creators of an institution, as\nthey can reconstruct the circumstances under which they made agreements; while\nthe second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and\n\u201cself-evident\u201d and they might not understand the underlying logic.  \u201c\u2026a social world [is] a comprehensive and given reality confronting the\nindividual in a manner analogous to the reality of the natural world\u2026 In early\nphases of socialization the child is quite incapable of distinguishing between\nthe objectivity of natural phenomena and the objectivity of the social\nformations\u2026 The objective reality of institutions is not diminished if the\nindividual does not understand their purpose or their mode of operation\u2026 He must\n\u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)",
            "title": "Social objective worlds"
        },
        {
            "location": "/Create/ExampleText/#division-of-labor",
            "text": "Division of labor is another consequence of institutionalization. Institutions\nassign \u201croles\u201d to be performed by various actors, through typification of\nperformances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As\nspecialization increases in number as well as in size and sophistication, a\ncivilization's culture contains more and more sections of knowledge specific to\ngiven roles or tasks, sections which become more and more esoteric to\nnon-specialists. These areas of knowledge do not belong anymore to the common\nsocial world and culture.  \u201cA society\u2019s stock of knowledge is structured in terms of what is generally\nrelevant and what is relevant only to specific roles\u2026 the social distribution of\nknowledge entails a dichotomization in terms of general and role-specific\nrelevance\u2026 because of the division of labor, role-specific knowledge will grow\nat a faster rate than generally relevant and accessible knowledge\u2026 The\nincreasing number and complexity of [the resulting] sub universes [of\nspecialized knowledge] make them increasingly inaccessible to outsiders\n(p.77-87)",
            "title": "Division of labor"
        },
        {
            "location": "/Create/ExampleText/#symbolic-universes",
            "text": "Symbolic universes are created to provide legitimation to the created\ninstitutional structure. Symbolic universes are a set of beliefs \u201ceverybody\nknows\u201d that aim at making the institutionalized structure plausible and\nacceptable for the individual\u2014who might otherwise not understand or agree with\nthe underlying logic of the institution. As an ideological system, the symbolic\nuniverse \u201cputs everything in its right place\u201d. It provides explanations for why\nwe do things the way we do. Proverbs, moral maxims, wise sayings, mythology,\nreligions and other theological thought, metaphysical traditions and other value\nsystems are part of the symbolic universe. They are all (more or less\nsophisticated) ways to legitimize established institutions.  \u201cThe function of legitimation is to make objectively available and subjectively\nplausible the \u2018first-order\u2019 objections that have been institutionalized\u2026\nProverbs, moral maxims and wise sayings are common on this level\u2026 [as well as]\nexplicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a\ngeneral theory of man\u2026 The symbolic universe also orders history. It locates all\ncollective events in a cohesive unity that includes past, present and future.\u201d\n(p. 92-104)",
            "title": "Symbolic universes"
        },
        {
            "location": "/Create/ExampleText/#universe-maintenance",
            "text": "Universe-maintenance refers to specific procedures undertaken, often by an elite\ngroup, when the symbolic universe does not fulfill its purpose anymore, which is\nto legitimize the institutional structure in place. This happens, for example,\nin generational shifts, or when deviants create an internal movement against\nestablished institutions (e.g. against revolutions), or when a society is\nconfronted with another society with a greatly different history and\ninstitutional structures. In primitive societies this happened through\nmythological systems, later on through theological thought. Today, an extremely\ncomplex set of science has secularized universe-maintenance.  \u201cSpecific procedures of universe-maintenance become necessary when the symbolic\nuniverse has become a problem. As long as this is not the case, the symbolic\nuniverse is self-maintaining, that is self-legitimating. An intrinsic problem\npresents itself with the process of transmission of the symbolic universe from\none generation to another\u2026 [additionally] two societies confronting each other\nwith conflicting universes will both develop conceptual machinery designed to\nmaintain their respective universes\u2026 mythology represents the most archaic form\nof universe-maintenance\u2026 theological thought may be distinguished from its\nmythological predecessor simply in terms of its greater degree of theoretical\nsystematization\u2026 Modern science is an extreme step in this development.\n(p.104-116)",
            "title": "Universe-maintenance"
        },
        {
            "location": "/Create/ExampleText/#society-as-subjective-reality",
            "text": "",
            "title": "Society as subjective reality"
        },
        {
            "location": "/Create/ExampleText/#socialization",
            "text": "Socialization is a two-step induction of the individual to participate in the\nsocial institutional structure, meaning in its objective reality.  \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of\nsociety. In the life of every individual\u2026 there is a temporal sequence, in the\ncourse of which he is inducted into participation in the social dialectic\"\n(p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high\ndegree of symmetry between objective and subjective reality\u201d (p. 163)  Primary Socialization takes place as a child. It is highly charged emotionally\nand is not questioned. Secondary Socialization includes the acquisition of\nrole-specific knowledge, thus taking one\u2019s place in the social division of\nlabor. It is learned through training and specific rituals, and is not\nemotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s\nteacher\u201d. Training for secondary socialization can be very complex and depends\non the complexity of division of labor in a society. Primary socialization is\nmuch less flexible than secondary socialization. E.g. shame for nudity comes\nfrom primary socialization, adequate dress code depends on secondary: A\nrelatively minor shift in the subjective definition of reality would suffice for\nan individual to take for granted that one may go to the office without a tie. A\nmuch more drastic shift would be necessary to have him go, as a matter of\ncourse, without any clothes at all.  \u201cThe child does not internalize the world of his significant others as one of\nmany possible worlds\u2026 It is for this reason that the world internalized in\nprimary socialization is so much more firmly entrenched in consciousness than\nworlds internalized in secondary socialization\u2026. Secondary socialization is the\ninternalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of\nsecondary socialization carry a high degree of anonymity\u2026 The same knowledge\ntaught by one teacher could also be taught by another\u2026 The institutional\ndistribution of tasks between primary and secondary socialization varies with\nthe complexity of the social distribution of knowledge\u201d (p. 129-147)",
            "title": "Socialization"
        },
        {
            "location": "/Create/ExampleText/#conversation",
            "text": "Conversation or verbal communication aims at reality-maintenance of the\nsubjective reality. What seems to be a useless and unnecessary communication of\nredundant banalities is actually a constant mutual reconfirmation of each\nother's internal thoughts, in that it maintains subjective reality.  \u201cOne may view the individual\u2019s everyday life in terms of the working away of a\nconversational apparatus that ongoingly maintains, modifies and reconstructs his\nsubjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the\nstation,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire\nworld within which these apparently simple propositions make sense\u2026 the exchange\nconfirms the subjective reality of this world\u2026 the great part, if not all, of\neveryday conversation maintains subjective reality\u2026 imagine the effect\u2026of an\nexchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine,\ndarling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)",
            "title": "Conversation"
        },
        {
            "location": "/Create/ExampleText/#identity",
            "text": "Identity of an individual is subject to a struggle of affiliation to sometimes\nconflicting realities. For example, the reality from primary socialization\n(mother tells child not to steal) can be in contrast with second socialization\n(gang members teach teenager that stealing is cool). Our final social location\nin the institutional structure of society will ultimately also influence our\nbody and organism.  \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines\nhow long and in what manner the individual organism shall live\u2026 Society also\ndirectly penetrates the organism in its functioning, most importantly in respect\nto sexuality and nutrition. While both sexuality and nutrition are grounded in\nbiological drives\u2026 biological constitution does not tell him where he should\nseek sexual release and what he should eat.\u201d (p. 163-183)",
            "title": "Identity"
        },
        {
            "location": "/Search/",
            "text": "Search Design\n\u00b6\n\n\nFabric metaphor\n\u00b6\n\n\n\n\nThe search space is a massive fabric of interconnected material. In it we\ndiscern the structures we are interested in: little pieces of fabric, also with\ninterconnected material.\n\n\nWhen we search, we have a fabric in mind, woven from specific material, stitched\ntogether in a specific manner.\n\n\nSearch in Text-Fabric works exactly like this: you give a sample patch, and\nText-Fabric fetches all pieces of the big fabric that match your patch.\n\n\n\n\nThe textile metaphor is particularly suited for grasping the search part of\nText-Fabric, so I'm going to stick to it for a while. I have used it in the\nactual code as well, and even in the proofs that certain parts of the algorithm\nterminate and are correct. Yet it remains a metaphor, and the fit is not exact.\n\n\nThe basic pattern of search is this:\n\n\n\n\n\n\n\n\ntextile\n\n\ntext\n\n\nexample\n\n\n\n\n\n\n\n\n\n\ntake several fleeces\n\n\npick the nodes corresponding to a node type\n\n\nword\ns, \nphrase\ns, \nclause\ns, \nverse\ns\n\n\n\n\n\n\nspin thick yarns from them\n\n\nfilter by feature conditions\n\n\npart-of-speech=verb\n gender=\nf\n \nbook=Genesis\n \nvt\n\n\n\n\n\n\nspin the yarns further into thin yarns\n\n\nthrow away nodes that do not have the right connections\n\n\nfeature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa\n\n\n\n\n\n\nstitch the yarns together with thread\n\n\nbuild results by selecting a member for every filtered node set\n\n\nword\n node \n123456\n in \nphrase\n node \n657890\n in \nclause\n node \n490567\n in \nverse\n node \n1403456\n\n\n\n\n\n\n\n\nWe will explain the stages of the fabrication process in detail.\n\n\nFleece\n\u00b6\n\n\n\n\nA fleece corresponds with a very simple search template that asks for all\nobjects of a given type:\n\n\n1\nword\n\n\n\n\n\n\nor\n\n\n1\nclause\n\n\n\n\n\n\nor, asking for multiple types:\n\n\n1\n2\n3\n4\n5\nverse\nclause\nphrase\nlex\nword\n\n\n\n\n\n\nFleeces are the raw material from which we fabricate our search results.\n\n\nEvery node type, such as \nword\n, \nsentence\n, \nbook\n corresponds to a fleece. In\nText-Fabric, every node has exactly one node type, so the whole space is neatly\ndivided into a small set of fleeces.\n\n\nThe most important characteristic of a fleece is it size: the number of nodes in\na node type.\n\n\nSpinning thick yarn\n\u00b6\n\n\n\n\nConsider search templates where we ask for specific members of a node type, by\ngiving feature constraints:\n\n\n1\n2\n3\n4\n5\n6\nverse book=Genesis\nclause type=rela\nphrase determined=yes\nlex id=jc/\nword number=pl vt\nvt\n\n\n\n\n\n\nEvery line in this search templates we call an \natom\n: a node type plus a\nfeature specification. The result of an atom is the set of all nodes in that\nnode type that satisfy those feature conditions. Finding the results of an atom\ncorresponds with first thing that we do with a fleece: \nspin\n a thick \nyarn\n\nfrom it. Yarns in general are obtained by spinning fleeces, i.e. by filtering\nnode sets that correspond to a node type.\n\n\nA search template may contain multiple atoms. Text-Fabric collects all atoms of\na template, grabs the corresponding fleeces, and spins thick yarns from them.\nFor each atom it will spin a yarn, and if there are several atoms referring to\nthe same node type, there will be several yarns spun from that fleece. This\nspinning of thick yarns out of fleeces happens in just one go. All fleeces\ntogether contain exactly all nodes, so Text-Fabric walks in one pass over all\nnodes, applies the feature conditions, and puts the nodes into the yarns\ndepending on which conditions apply.\n\n\nBy the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical\npass requires a fraction of a second.\n\n\nThe most important characteristic of a yarn is its \nthickness\n, which we define\nas the number of nodes in the yarn divided by the number of nodes in the fleece.\n\n\nFor example, a yarn consisting of the \nbook\n node of Genesis only, has a\nthickness of 1/39, because there are 39 books in the fleece. A much thicker yarn\nis that of the verbs, which has a thickness of roughly 1/6. A very thin thread\nis that of the word \n<CQH\n (which occurs only once) with a thickness of only\n1/400,000.\n\n\nSpinning thin yarns\n\u00b6\n\n\nIn order to find results, we have to further narrow down the search space. In\nother words, we are going to spin our thick yarns into thinner and thinner\nyarns. Before we can do that, we should make one thing clear.\n\n\nConnected by constraints\n\u00b6\n\n\nIf the template above were complete, it would lead to a monstrous number of\nresults. Because a result of a template like this is any combination of verse-,\nclause-, phrase-, lex-, word nodes that individually satisfy their own atom\ncondition. So the number of results is the product of the number of results of\nthe individual atoms, which is pretty enormous. It is hard to imagine a\nsituation where these results could be consumed.\n\n\nUsually, there are \nconstraints\n active between the atoms. For example in a\ntemplate like this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n1 verse book=Genesis\n2    clause type=rela\n3        phrase determined=yes\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l\n\n\n\n\n\n\nThe meaning of this template is that we look for a \nverse\n that\n\n\n\n\n(1) claims to be in Genesis\n\n\n(2) has a clause whose type is \nrela\n\n\n(3) which in turn has a phrase of a determined character\n\n\n(4) which contains a word in the plural and that has a verbal tense (vt)\n\n\n\n\nThere should also be a\n\n\n\n\n(6) lex object, identified by \njc/\n\n\n\n\nwhich is connected to the rest by the constraint that\n\n\n\n\n(8) the word of line 4 is contained in it.\n\n\n\n\nNote that all atoms are linked by constraints into one network. In graph\ntheoretical terms: this template consists of exactly one\n\nconnected component\n.\n\n\nIf this were not so, we would have in fact two independent search tasks, where\nthe result set would be the (cartesian) product of the result sets of the\nseparate components.\n\n\nFor example, if line 8 were missing, we would effectively search for things that\nmatch lines 1-4, and, independently, for things that match line 6. And every\nresult of the first part, combined with any result of the second part, would be\na valid result of the whole.\n\n\nWell, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the\nsame time, let alone that it wants to waste time to generate all the results in\nthe product. You will have to fire those search tasks one by one, and it is up\nto you how you combine the results.\n\n\nThe upshot it: \nthe atoms in the search template should form a network,\nconnected by constraints\n.\n\n\nText-Fabric will check this, and will only work with search templates that have\nonly one connected component.\n\n\nTerminology\n\u00b6\n\n\nBy now we have arrived at the idea that our search template is a graph\nunderneath: what we have called \natoms\n are in fact the nodes, and what we have\ncalled \nconstraints\n, are the edges.\n\n\nFrom now on, we will call the \natoms\n \nqnodes\n and the constraints \nqedges\n.\nThe \nq\n is to distinguish the nodes and the edges from the nodes and the edges\nof your dataset, the \ntext\n nodes and \ntext\n edges. When we use the term \nnodes\n\nand \nedges\n we will always refer to \ntext\n nodes and edges.\n\n\nWhen we are searching, we maintain a \nyarn\n for every \nqnode\n. This yarn starts\nout to be the thick yarn as described above, but we are going to thin them.\n\n\nWe can also see how our query templates are really \ntopographic\n: a query\ntemplate is a piece of local geography that we want to match against the data.\nFinding results is nothing else than instantiating \nqnodes\n of the search\ntemplate by text nodes in such a way that the \nqedges\n hold between the text\nedges.\n\n\nSpinning a qedge\n\u00b6\n\n\n\n\nSo, where were we? We have spun thick threads based on the \nqnodes\n\nindividually, but we have not done anything with the \nqedges\n. That is going to\nchange now.\n\n\nConsider this piece of search template:\n\n\n1\n2\n3\n4\n5\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l\n\n\n\n\n\n\nSo our \nqnodes\n are \nw\n and \nl\n, and our \nqedge\n is \nw ]] l\n. Note that a \nlex\n\nobject is the set of all occurrences of a lexeme. So \nw ]] l\n says that \nw\n is\nembedded in \nl\n, in other words, \nw\n is a slot contained in the slots of \nl\n. It\nis nothing else than the word \nw\n is an instance of the lexeme \njc/\n.\n\n\nWe will now check the pairs of (lex, word)-nodes in the text, where the lex node\nis taken from the yarn of the qnode \nl\n, and the word from the yarn of the qnode\n\nw\n.\n\n\nWe can throw away some words from the yarn of \nw\n, namely those words that do\nnot lie in a lexeme that is in the yarn of \nl\n. In other words: the words that\nare not instances of lexeme \njc/\n are out!\n\n\nConversely, if the lexeme \njc/\n does not have occurrences in the plural and with\na verbal tense, we must kick it out of the yarn of \nl\n, leaving no members in\nit. If a yarn gets empty, we have an early detection that the search yields no\nresults, and the whole process stops.\n\n\nIn our case, however, this is not so, and we continue.\n\n\nThis pass over the yarns at both sides of a qedge is a \nspin\n action. We spin\nthis qedge, and the result is that the two yarns become spun more thinly,\nhopefully.\n\n\n\n\nWith the yarn of words severely spun out, we are going to the next qedge, the\none between words and phrases.\n\n\n1\n2\n3        phrase determined=yes\n4            w:word number=pl vt\n\n\n\n\n\n\nThe indent is an implicit way of saying that the \"embeds\" relation \n[[\n holds\nbetween the \nphrase\n and the \nword\n. An equivalent formulation of the template\nis\n\n\n1\n2\n3\n4\np:phrase determined=yes\nw:word number=pl vt\n\np [[ w\n\n\n\n\n\n\nWe race along the yarn \nw\n of remaining words and check for each word if it is\ncontained in a phrase in the yarn of \np\n, the determined phrases. If it is not,\nwe throw the word out of the yarn of \nw\n. Similarly, we can throw out some\nphrases from the yarn of \np\n, namely those phrases that do not contain words in\nthe yarn of \nw\n. In other words: the phrases without plural words and verbal\ntense are also out.\n\n\n\n\nWe continue spinning, now between phrases and clauses.\n\n\n1\n2\n2    clause type=rela\n3        phrase determined=yes\n\n\n\n\n\n\nHere we loose the phrases that are not contained in a clause of \ntype=rela\n, and\nwe loose all clauses that do not embed one of the few phrases left.\n\n\n\n\nThe last spin action corresponds with\n\n\n1\n2\n1 verse book=Genesis\n2    clause type=rela\n\n\n\n\n\n\nSo we throw away all our results if they are outside Genesis.\n\n\nWe end up with a set of thin yarns, severely thinned out, even. This will be a\ngood starting point for the last stage: picking members from each yarn to form\nresults. We call this \nstitching\n and we'll get there in a moment.\n\n\nThe spread of a qedge\n\u00b6\n\n\nA very important property of a qedge is its \nspread\n. A qedge links a every node\n\nn\n in its \nfrom\n-yarn to zero, one, or more nodes in its \nto\n-yarn. The number\nof nodes in the \nto\n-yarn is a key property. The average number of nodes \nm\n in\nthe \nto\n-yarn per linked node \nn\n in the \nfrom\n-yarn is the \nspread\n of the\nedge.\n\n\nA few examples:\n\n\n\n\nAn edge that corresponds to \n]]\n, \nn\n embeds \nm\n. If this edge goes from books\n    to words, then every book node \nn\n is linked to every one of its words. So\n    very \nn\n has hundreds or thousands \nm\ns. The spread will roughly be 425,000 /\n    39 =~ 10,000\n\n\nThe opposite edge has a spread of exactly 1, because every word belongs to\n    exactly one book. Edges with spread 1 are very pleasant for our stitching\n    algorithm later on.\n\n\nAn edge corresponding to \n=\n. These qedges are super efficient, because their\n    relation \n=\n is a breeze to compute, and they have always a spread 1 in both\n    directions.\n\n\nAn edge corresponding to \n#\n, the node inequality relation. The relation \n#\n\n    is still a breeze to compute, but the result is heavy: the set of all nodes\n    not equal to a given node. The spread is nearly 100% of the yarn length, in\n    both directions. These edges are not worth to spin, because if you have two\n    yarns, no node will be excluded: if you have an \nn\n in the \nfrom\n-yarn, you\n    will always be able to find a different \nn\n in the \nto\n-yarn (except when bot\n    yarns are equal, and contain just one node).\n\n\nAn edge corresponding to \n==\n, the relation between nodes that are linked to\n    the same set of slots. The spread of this relation is not too big, but the\n    cost of computing it adds up quickly when applied to many cases.\n\n\n\n\nSpinning all qedges\n\u00b6\n\n\nLet us describe the spinning of yarns along edges in a bit more general way, and\nreflect on what it does for us.\n\n\nWe spin all qedges of a template. But after spinning a qedge, the yarns involved\nmay have changed. If that is the case, it makes sense to re-spin other qedges\nthat are involved in the changed yarns.\n\n\nThat is exactly what we do. We keep spinning, until the yarns have stabilized.\n\n\nA few key questions need to be addressed:\n\n\n\n\nDo the yarns stabilize?\n\n\nIf they stabilize, what have we got?\n\n\nIs this an efficient process?\n\n\n\n\nTermination of spinning\n\u00b6\n\n\nYes, spinning qedges until nothing changes any more, terminates, provided you do\nnot try to spin qedges that are up-to-date. If the yarns around an edge have not\nchanged, it does not make sense to spin that qedge. See\n\nhere\n for proof.\n\n\nWhat have we got?\n\u00b6\n\n\nAfter spinning, it is guaranteed that we have not thrown away results. All nodes\nthat are parts of valid results, are still in the yarns.\n\n\nBut, conversely, can it be that there are still nodes in the yarns that are not\npart of a result? Yes, that is possible.\n\n\nOnly when the graph of qnodes and qedges does not have a cycle, we know that all\nmembers of all yarns occur at least once in a result. See\n\nhere\n for proof.\n\n\nQuite a few interesting queries, however, have cycles in in their graphs. So, in\nthose cases, spinning qedges will not cause the maximal narrowing down of the\nsearch space.\n\n\nEfficiency\n\u00b6\n\n\nAnd that raises the question: how effective is the process of spinning qedges?\n\n\nThe answer is: it depends. If your qnodes have strong conditions on them, so\nthat the first yarn is already very thin, then every yarn that is connected to\nthis one by a qedge has also the chance to get very thin after spinning. In this\ncase, the combined filtering effect of all edges can produce a rapid narrowing\nof the search space.\n\n\nEspecially if we can implement edge spinning in an optimized way, this works\nlike a charm. When we come to stitching results (which is potentially very\nexpensive), we have already achieved a massive reduction of work.\n\n\nBut if none of the yarns is thin at the outset, spinning qedges will not result\nin appreciable thinning of the yarns, while it might be an enormous amount of\nwork, depending on the actual relations involved.\n\n\nThe good news is that it is possible to detect those situations. Text-Fabric\nestimates whether it makes sense to spin a qedge, and if not, it will just skip\nspinning that edge. Which will make the final result gathering (stitching) more\nexpensive.\n\n\nThere is more to efficiency than this. It turns out that the strategy by which\nyou select the next qedge to be spun, influences the efficiency. In general, it\nis best to always start with the thinnest yarns, and select edges that affect\nthem. Also here there is complication: not every qedge is equally expensive when\ncomputed over a yarn. It might be better to compute a cheaper edge over a\nthicker yarn.\n\n\nStitching\n\u00b6\n\n\n\n\nThe last step is actually getting results. A result is a bunch of nodes, one\nfrom each yarn, in such a way that result nodes on yarns fulfil the\nrelationships that the qedges of the search template dictate. If we can find\nsuch a set of nodes, we have stitched the yarns together. We call such a result\na \nstitch\n. A stitch is a tuple of text nodes, each corresponding to exactly one\nqnode.\n\n\nIt is not completely trivial to find stitches, let alone to collect them\nefficiently. The general procedure is as follows:\n\n\n\n\nchoose a yarn to start with;\n\n\ntry a node in that yarn as starting point\n\n\npick a qedge from the qnode associated with the yarn (the source yarn), to\n    another qnode and consider that yarn (the target yarn),\n\n\nfind a node in the target yarn that is in the right relationship with the node\n    selected in the source yarn,\n\n\nand so on, until all qedges have been used,\n\n\nif all has gone well, deliver the nodes found as a result.\n\n\n\n\nLet us look to these steps in a bit more detail. There is an element of choice,\nand it is very important to study how big this element of choice is in the\nvarious stages.\n\n\nFirst we select a yarn, and in that yarn a node. Usually we have many choices\nand at least one, because result seeking only makes sense if all yarns are\nnon-empty. The third choice is the related node in the target yarn. Here we may\nencounter anything from zero, one or many choices.\n\n\nIf there are zero choices, then we know that our provisional stitching of yarns\nso far cannot be completed into a full stitching of all yarns. If we have made\nchoices to get this far, then some of these choices have not been lucky. We have\nto back-track and try other alternatives.\n\n\nIf there is just one choice, it is easy: we pick the one and only possible node\nin the target yarn, without introducing new points of choice.\n\n\nIf there are many choices, we have to try them all, one by one. Some might lead\nto a full stitch, others not.\n\n\nAn important situation to be aware of, is when a qedge leads the stitching\nprocess to a yarn, out of which a node has already been chosen by an earlier\nstep. This is very well possible, since the search template might have cycles in\nthe qedges, or multiple qedges arrive at the same qnode.\n\n\nWhen this happens, we do not have to select a target node, we only have to check\nwhether the target node that has been selected before, stands in the right\nrelationship to the current source node. The relationship, that is, which is\ndictated by the current qedge that we are at. If so, we can stitch on with other\nedges, without introducing choice points (very much like the one-choice above).\nIf the relation fails to hold, this stitch is doomed, and we have to back-track\n(very much like the zero-choice above).\n\n\nStrategy of stitching\n\u00b6\n\n\nThe steps involved in stitching as described above are clear, but less clear is\nwhat yarn we shall select to start with, and in which order we shall follow the\nedges. We need a strategy, and multiple strategies might lead to the same\nresults, albeit with varying efficiency.\n\n\nIn Text-Fabric we employ a strategy, that makes the \nnarrowest\n choices first.\nWe call a choice narrow if there are few alternatives to choose from, and broad\nif there are many alternatives.\n\n\nBy giving precedence to narrow choices, we prune larger parts of the search tree\nwhen we fail. If we are stitching, the more nodes we have gathered in our\nstitch, the greater the chance that a blocking relationship is encountered, i.e.\na relationship that should hold between the nodes gathered so far, but which in\nfact does not hold.\n\n\nSo we want to get as many nodes in our stitch as quickly as possible.\n\n\nIf our search tree is narrowly branching near the root, and broadly branching\nnear the leaves, the top \nn\n levels of the tree contain relatively few nodes. So\nwe have relatively few possibilities to stitch n nodes together, and most\nreasons to fail will happen while visiting these \nn\n levels.\n\n\nIf on the other hand our search tree is broadly branching near the root, and\nnarrowly branching near the leaves, the top \nn\n levels of the tree contain many\nnodes. We will visit many nodes and try many stitchings of length \nn\n, of which\na lot will fail.\n\n\nI have also tried a different, more complicated strategy, which is still\nimplemented, and which can be used by means of an optional argument to\n\nS.study()\n,\nbut results of this strategy were not particularly good.\n\n\nSmall-first strategy\n\u00b6\n\n\nHere is the small-first strategy in a bit more detail.\n\n\n\n\nwe choose the smallest yarn to start with;\n\n\nfor every qedge we estimate its current \nspread\n, i.e. how many targets it has\n    per source on average, relative to the current source and target yarns;\n\n\nat every step there are three kinds of qedges:\n\n\nqedges that go between qnodes of which we have already stitched the yarns\n\n\nqedges that go from a yarn that is already part of the stitch to a yarn\n    outside the stitch\n\n\nqedges that do not start at a yarn in the current stitch\n\n\n\n\n\n\nat every step,\n\n\nwe first process all qedges of type (i), in arbitrary order;\n\n\nwe select one edge with minimal spread out of type (ii) and process it;\n\n\nwe postpone all edges of type (iii);\n\n\nwe redetermine which edges are in all types.\n\n\n\n\n\n\n\n\nIt cannot happen that at the end we have not visited all qnodes and yarns,\nbecause we have assumed that our search template consists of one connected\ncomponent. Every qnode can be reached from every other through a series of\nqedges. So, as we perform step after step, as long as there are qnodes in type\n(iii), we can be sure that there are also qnodes in a path from the qnodes we\nhave visited to the type (iii) qnodes. At least one of the qnodes in that path\nwill be a type (ii) node. In the end there will no type (iii) nodes be left.\n\n\nWe have added a few more things to optimize the process.\n\n\nA relationship between a source yarn and a target yarn can also be considered in\nthe opposite direction. If its spread in the opposite direction is less than its\nspread in the normal direction, we use the opposite direction.\n\n\nSecondly, before we start stitching, we can compute the order of qedges that we\nwill use for every stitch. We then sort the qnodes according to the order by\nwhich they will be encountered when we work through the qedges. When we are\nstitching, in the midst of a partial stitch, it is always the case that we have\nstitched qnodes 1 .. \nn\n for some \nn\n, and we still have to stitch all qnodes\nabove \nn\n. That means that when we try to finish partial stitches of which an\ninitial part has been fixed, the search process will not change that initial\npart of the stitch. Only when the algorithm has exhausted all possibilities\nbased on that initial part, it will change the last node of the initial part,\nreplace it by other options, and start searching further.\n\n\nThis means that we just can maintain our partial stitch in a single list. We do\nnot have to assemble many partial stitches as separate immutable tuples.\n\n\nWe have implemented our deliver function as a generator, that walks over all\nstitch possibilities while maintaining just one current stitch. When the stitch\nhas been completely filled in, a copy of it will be yielded, after which\nback-tracking occurs, by which the current stitch will get partly undefined,\nonly to be filled up again by further searching.\n\n\nRead it all in the source code:\n\ndef stitchOn(e)\n.",
            "title": "Search"
        },
        {
            "location": "/Search/#search-design",
            "text": "",
            "title": "Search Design"
        },
        {
            "location": "/Search/#fabric-metaphor",
            "text": "The search space is a massive fabric of interconnected material. In it we\ndiscern the structures we are interested in: little pieces of fabric, also with\ninterconnected material.  When we search, we have a fabric in mind, woven from specific material, stitched\ntogether in a specific manner.  Search in Text-Fabric works exactly like this: you give a sample patch, and\nText-Fabric fetches all pieces of the big fabric that match your patch.   The textile metaphor is particularly suited for grasping the search part of\nText-Fabric, so I'm going to stick to it for a while. I have used it in the\nactual code as well, and even in the proofs that certain parts of the algorithm\nterminate and are correct. Yet it remains a metaphor, and the fit is not exact.  The basic pattern of search is this:     textile  text  example      take several fleeces  pick the nodes corresponding to a node type  word s,  phrase s,  clause s,  verse s    spin thick yarns from them  filter by feature conditions  part-of-speech=verb  gender= f   book=Genesis   vt    spin the yarns further into thin yarns  throw away nodes that do not have the right connections  feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa    stitch the yarns together with thread  build results by selecting a member for every filtered node set  word  node  123456  in  phrase  node  657890  in  clause  node  490567  in  verse  node  1403456     We will explain the stages of the fabrication process in detail.",
            "title": "Fabric metaphor"
        },
        {
            "location": "/Search/#fleece",
            "text": "A fleece corresponds with a very simple search template that asks for all\nobjects of a given type:  1 word   or  1 clause   or, asking for multiple types:  1\n2\n3\n4\n5 verse\nclause\nphrase\nlex\nword   Fleeces are the raw material from which we fabricate our search results.  Every node type, such as  word ,  sentence ,  book  corresponds to a fleece. In\nText-Fabric, every node has exactly one node type, so the whole space is neatly\ndivided into a small set of fleeces.  The most important characteristic of a fleece is it size: the number of nodes in\na node type.",
            "title": "Fleece"
        },
        {
            "location": "/Search/#spinning-thick-yarn",
            "text": "Consider search templates where we ask for specific members of a node type, by\ngiving feature constraints:  1\n2\n3\n4\n5\n6 verse book=Genesis\nclause type=rela\nphrase determined=yes\nlex id=jc/\nword number=pl vt\nvt   Every line in this search templates we call an  atom : a node type plus a\nfeature specification. The result of an atom is the set of all nodes in that\nnode type that satisfy those feature conditions. Finding the results of an atom\ncorresponds with first thing that we do with a fleece:  spin  a thick  yarn \nfrom it. Yarns in general are obtained by spinning fleeces, i.e. by filtering\nnode sets that correspond to a node type.  A search template may contain multiple atoms. Text-Fabric collects all atoms of\na template, grabs the corresponding fleeces, and spins thick yarns from them.\nFor each atom it will spin a yarn, and if there are several atoms referring to\nthe same node type, there will be several yarns spun from that fleece. This\nspinning of thick yarns out of fleeces happens in just one go. All fleeces\ntogether contain exactly all nodes, so Text-Fabric walks in one pass over all\nnodes, applies the feature conditions, and puts the nodes into the yarns\ndepending on which conditions apply.  By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical\npass requires a fraction of a second.  The most important characteristic of a yarn is its  thickness , which we define\nas the number of nodes in the yarn divided by the number of nodes in the fleece.  For example, a yarn consisting of the  book  node of Genesis only, has a\nthickness of 1/39, because there are 39 books in the fleece. A much thicker yarn\nis that of the verbs, which has a thickness of roughly 1/6. A very thin thread\nis that of the word  <CQH  (which occurs only once) with a thickness of only\n1/400,000.",
            "title": "Spinning thick yarn"
        },
        {
            "location": "/Search/#spinning-thin-yarns",
            "text": "In order to find results, we have to further narrow down the search space. In\nother words, we are going to spin our thick yarns into thinner and thinner\nyarns. Before we can do that, we should make one thing clear.",
            "title": "Spinning thin yarns"
        },
        {
            "location": "/Search/#connected-by-constraints",
            "text": "If the template above were complete, it would lead to a monstrous number of\nresults. Because a result of a template like this is any combination of verse-,\nclause-, phrase-, lex-, word nodes that individually satisfy their own atom\ncondition. So the number of results is the product of the number of results of\nthe individual atoms, which is pretty enormous. It is hard to imagine a\nsituation where these results could be consumed.  Usually, there are  constraints  active between the atoms. For example in a\ntemplate like this:  1\n2\n3\n4\n5\n6\n7\n8 1 verse book=Genesis\n2    clause type=rela\n3        phrase determined=yes\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l   The meaning of this template is that we look for a  verse  that   (1) claims to be in Genesis  (2) has a clause whose type is  rela  (3) which in turn has a phrase of a determined character  (4) which contains a word in the plural and that has a verbal tense (vt)   There should also be a   (6) lex object, identified by  jc/   which is connected to the rest by the constraint that   (8) the word of line 4 is contained in it.   Note that all atoms are linked by constraints into one network. In graph\ntheoretical terms: this template consists of exactly one connected component .  If this were not so, we would have in fact two independent search tasks, where\nthe result set would be the (cartesian) product of the result sets of the\nseparate components.  For example, if line 8 were missing, we would effectively search for things that\nmatch lines 1-4, and, independently, for things that match line 6. And every\nresult of the first part, combined with any result of the second part, would be\na valid result of the whole.  Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the\nsame time, let alone that it wants to waste time to generate all the results in\nthe product. You will have to fire those search tasks one by one, and it is up\nto you how you combine the results.  The upshot it:  the atoms in the search template should form a network,\nconnected by constraints .  Text-Fabric will check this, and will only work with search templates that have\nonly one connected component.",
            "title": "Connected by constraints"
        },
        {
            "location": "/Search/#terminology",
            "text": "By now we have arrived at the idea that our search template is a graph\nunderneath: what we have called  atoms  are in fact the nodes, and what we have\ncalled  constraints , are the edges.  From now on, we will call the  atoms   qnodes  and the constraints  qedges .\nThe  q  is to distinguish the nodes and the edges from the nodes and the edges\nof your dataset, the  text  nodes and  text  edges. When we use the term  nodes \nand  edges  we will always refer to  text  nodes and edges.  When we are searching, we maintain a  yarn  for every  qnode . This yarn starts\nout to be the thick yarn as described above, but we are going to thin them.  We can also see how our query templates are really  topographic : a query\ntemplate is a piece of local geography that we want to match against the data.\nFinding results is nothing else than instantiating  qnodes  of the search\ntemplate by text nodes in such a way that the  qedges  hold between the text\nedges.",
            "title": "Terminology"
        },
        {
            "location": "/Search/#spinning-a-qedge",
            "text": "So, where were we? We have spun thick threads based on the  qnodes \nindividually, but we have not done anything with the  qedges . That is going to\nchange now.  Consider this piece of search template:  1\n2\n3\n4\n5 4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l   So our  qnodes  are  w  and  l , and our  qedge  is  w ]] l . Note that a  lex \nobject is the set of all occurrences of a lexeme. So  w ]] l  says that  w  is\nembedded in  l , in other words,  w  is a slot contained in the slots of  l . It\nis nothing else than the word  w  is an instance of the lexeme  jc/ .  We will now check the pairs of (lex, word)-nodes in the text, where the lex node\nis taken from the yarn of the qnode  l , and the word from the yarn of the qnode w .  We can throw away some words from the yarn of  w , namely those words that do\nnot lie in a lexeme that is in the yarn of  l . In other words: the words that\nare not instances of lexeme  jc/  are out!  Conversely, if the lexeme  jc/  does not have occurrences in the plural and with\na verbal tense, we must kick it out of the yarn of  l , leaving no members in\nit. If a yarn gets empty, we have an early detection that the search yields no\nresults, and the whole process stops.  In our case, however, this is not so, and we continue.  This pass over the yarns at both sides of a qedge is a  spin  action. We spin\nthis qedge, and the result is that the two yarns become spun more thinly,\nhopefully.   With the yarn of words severely spun out, we are going to the next qedge, the\none between words and phrases.  1\n2 3        phrase determined=yes\n4            w:word number=pl vt   The indent is an implicit way of saying that the \"embeds\" relation  [[  holds\nbetween the  phrase  and the  word . An equivalent formulation of the template\nis  1\n2\n3\n4 p:phrase determined=yes\nw:word number=pl vt\n\np [[ w   We race along the yarn  w  of remaining words and check for each word if it is\ncontained in a phrase in the yarn of  p , the determined phrases. If it is not,\nwe throw the word out of the yarn of  w . Similarly, we can throw out some\nphrases from the yarn of  p , namely those phrases that do not contain words in\nthe yarn of  w . In other words: the phrases without plural words and verbal\ntense are also out.   We continue spinning, now between phrases and clauses.  1\n2 2    clause type=rela\n3        phrase determined=yes   Here we loose the phrases that are not contained in a clause of  type=rela , and\nwe loose all clauses that do not embed one of the few phrases left.   The last spin action corresponds with  1\n2 1 verse book=Genesis\n2    clause type=rela   So we throw away all our results if they are outside Genesis.  We end up with a set of thin yarns, severely thinned out, even. This will be a\ngood starting point for the last stage: picking members from each yarn to form\nresults. We call this  stitching  and we'll get there in a moment.",
            "title": "Spinning a qedge"
        },
        {
            "location": "/Search/#the-spread-of-a-qedge",
            "text": "A very important property of a qedge is its  spread . A qedge links a every node n  in its  from -yarn to zero, one, or more nodes in its  to -yarn. The number\nof nodes in the  to -yarn is a key property. The average number of nodes  m  in\nthe  to -yarn per linked node  n  in the  from -yarn is the  spread  of the\nedge.  A few examples:   An edge that corresponds to  ]] ,  n  embeds  m . If this edge goes from books\n    to words, then every book node  n  is linked to every one of its words. So\n    very  n  has hundreds or thousands  m s. The spread will roughly be 425,000 /\n    39 =~ 10,000  The opposite edge has a spread of exactly 1, because every word belongs to\n    exactly one book. Edges with spread 1 are very pleasant for our stitching\n    algorithm later on.  An edge corresponding to  = . These qedges are super efficient, because their\n    relation  =  is a breeze to compute, and they have always a spread 1 in both\n    directions.  An edge corresponding to  # , the node inequality relation. The relation  # \n    is still a breeze to compute, but the result is heavy: the set of all nodes\n    not equal to a given node. The spread is nearly 100% of the yarn length, in\n    both directions. These edges are not worth to spin, because if you have two\n    yarns, no node will be excluded: if you have an  n  in the  from -yarn, you\n    will always be able to find a different  n  in the  to -yarn (except when bot\n    yarns are equal, and contain just one node).  An edge corresponding to  == , the relation between nodes that are linked to\n    the same set of slots. The spread of this relation is not too big, but the\n    cost of computing it adds up quickly when applied to many cases.",
            "title": "The spread of a qedge"
        },
        {
            "location": "/Search/#spinning-all-qedges",
            "text": "Let us describe the spinning of yarns along edges in a bit more general way, and\nreflect on what it does for us.  We spin all qedges of a template. But after spinning a qedge, the yarns involved\nmay have changed. If that is the case, it makes sense to re-spin other qedges\nthat are involved in the changed yarns.  That is exactly what we do. We keep spinning, until the yarns have stabilized.  A few key questions need to be addressed:   Do the yarns stabilize?  If they stabilize, what have we got?  Is this an efficient process?",
            "title": "Spinning all qedges"
        },
        {
            "location": "/Search/#termination-of-spinning",
            "text": "Yes, spinning qedges until nothing changes any more, terminates, provided you do\nnot try to spin qedges that are up-to-date. If the yarns around an edge have not\nchanged, it does not make sense to spin that qedge. See here  for proof.",
            "title": "Termination of spinning"
        },
        {
            "location": "/Search/#what-have-we-got",
            "text": "After spinning, it is guaranteed that we have not thrown away results. All nodes\nthat are parts of valid results, are still in the yarns.  But, conversely, can it be that there are still nodes in the yarns that are not\npart of a result? Yes, that is possible.  Only when the graph of qnodes and qedges does not have a cycle, we know that all\nmembers of all yarns occur at least once in a result. See here  for proof.  Quite a few interesting queries, however, have cycles in in their graphs. So, in\nthose cases, spinning qedges will not cause the maximal narrowing down of the\nsearch space.",
            "title": "What have we got?"
        },
        {
            "location": "/Search/#efficiency",
            "text": "And that raises the question: how effective is the process of spinning qedges?  The answer is: it depends. If your qnodes have strong conditions on them, so\nthat the first yarn is already very thin, then every yarn that is connected to\nthis one by a qedge has also the chance to get very thin after spinning. In this\ncase, the combined filtering effect of all edges can produce a rapid narrowing\nof the search space.  Especially if we can implement edge spinning in an optimized way, this works\nlike a charm. When we come to stitching results (which is potentially very\nexpensive), we have already achieved a massive reduction of work.  But if none of the yarns is thin at the outset, spinning qedges will not result\nin appreciable thinning of the yarns, while it might be an enormous amount of\nwork, depending on the actual relations involved.  The good news is that it is possible to detect those situations. Text-Fabric\nestimates whether it makes sense to spin a qedge, and if not, it will just skip\nspinning that edge. Which will make the final result gathering (stitching) more\nexpensive.  There is more to efficiency than this. It turns out that the strategy by which\nyou select the next qedge to be spun, influences the efficiency. In general, it\nis best to always start with the thinnest yarns, and select edges that affect\nthem. Also here there is complication: not every qedge is equally expensive when\ncomputed over a yarn. It might be better to compute a cheaper edge over a\nthicker yarn.",
            "title": "Efficiency"
        },
        {
            "location": "/Search/#stitching",
            "text": "The last step is actually getting results. A result is a bunch of nodes, one\nfrom each yarn, in such a way that result nodes on yarns fulfil the\nrelationships that the qedges of the search template dictate. If we can find\nsuch a set of nodes, we have stitched the yarns together. We call such a result\na  stitch . A stitch is a tuple of text nodes, each corresponding to exactly one\nqnode.  It is not completely trivial to find stitches, let alone to collect them\nefficiently. The general procedure is as follows:   choose a yarn to start with;  try a node in that yarn as starting point  pick a qedge from the qnode associated with the yarn (the source yarn), to\n    another qnode and consider that yarn (the target yarn),  find a node in the target yarn that is in the right relationship with the node\n    selected in the source yarn,  and so on, until all qedges have been used,  if all has gone well, deliver the nodes found as a result.   Let us look to these steps in a bit more detail. There is an element of choice,\nand it is very important to study how big this element of choice is in the\nvarious stages.  First we select a yarn, and in that yarn a node. Usually we have many choices\nand at least one, because result seeking only makes sense if all yarns are\nnon-empty. The third choice is the related node in the target yarn. Here we may\nencounter anything from zero, one or many choices.  If there are zero choices, then we know that our provisional stitching of yarns\nso far cannot be completed into a full stitching of all yarns. If we have made\nchoices to get this far, then some of these choices have not been lucky. We have\nto back-track and try other alternatives.  If there is just one choice, it is easy: we pick the one and only possible node\nin the target yarn, without introducing new points of choice.  If there are many choices, we have to try them all, one by one. Some might lead\nto a full stitch, others not.  An important situation to be aware of, is when a qedge leads the stitching\nprocess to a yarn, out of which a node has already been chosen by an earlier\nstep. This is very well possible, since the search template might have cycles in\nthe qedges, or multiple qedges arrive at the same qnode.  When this happens, we do not have to select a target node, we only have to check\nwhether the target node that has been selected before, stands in the right\nrelationship to the current source node. The relationship, that is, which is\ndictated by the current qedge that we are at. If so, we can stitch on with other\nedges, without introducing choice points (very much like the one-choice above).\nIf the relation fails to hold, this stitch is doomed, and we have to back-track\n(very much like the zero-choice above).",
            "title": "Stitching"
        },
        {
            "location": "/Search/#strategy-of-stitching",
            "text": "The steps involved in stitching as described above are clear, but less clear is\nwhat yarn we shall select to start with, and in which order we shall follow the\nedges. We need a strategy, and multiple strategies might lead to the same\nresults, albeit with varying efficiency.  In Text-Fabric we employ a strategy, that makes the  narrowest  choices first.\nWe call a choice narrow if there are few alternatives to choose from, and broad\nif there are many alternatives.  By giving precedence to narrow choices, we prune larger parts of the search tree\nwhen we fail. If we are stitching, the more nodes we have gathered in our\nstitch, the greater the chance that a blocking relationship is encountered, i.e.\na relationship that should hold between the nodes gathered so far, but which in\nfact does not hold.  So we want to get as many nodes in our stitch as quickly as possible.  If our search tree is narrowly branching near the root, and broadly branching\nnear the leaves, the top  n  levels of the tree contain relatively few nodes. So\nwe have relatively few possibilities to stitch n nodes together, and most\nreasons to fail will happen while visiting these  n  levels.  If on the other hand our search tree is broadly branching near the root, and\nnarrowly branching near the leaves, the top  n  levels of the tree contain many\nnodes. We will visit many nodes and try many stitchings of length  n , of which\na lot will fail.  I have also tried a different, more complicated strategy, which is still\nimplemented, and which can be used by means of an optional argument to S.study() ,\nbut results of this strategy were not particularly good.",
            "title": "Strategy of stitching"
        },
        {
            "location": "/Search/#small-first-strategy",
            "text": "Here is the small-first strategy in a bit more detail.   we choose the smallest yarn to start with;  for every qedge we estimate its current  spread , i.e. how many targets it has\n    per source on average, relative to the current source and target yarns;  at every step there are three kinds of qedges:  qedges that go between qnodes of which we have already stitched the yarns  qedges that go from a yarn that is already part of the stitch to a yarn\n    outside the stitch  qedges that do not start at a yarn in the current stitch    at every step,  we first process all qedges of type (i), in arbitrary order;  we select one edge with minimal spread out of type (ii) and process it;  we postpone all edges of type (iii);  we redetermine which edges are in all types.     It cannot happen that at the end we have not visited all qnodes and yarns,\nbecause we have assumed that our search template consists of one connected\ncomponent. Every qnode can be reached from every other through a series of\nqedges. So, as we perform step after step, as long as there are qnodes in type\n(iii), we can be sure that there are also qnodes in a path from the qnodes we\nhave visited to the type (iii) qnodes. At least one of the qnodes in that path\nwill be a type (ii) node. In the end there will no type (iii) nodes be left.  We have added a few more things to optimize the process.  A relationship between a source yarn and a target yarn can also be considered in\nthe opposite direction. If its spread in the opposite direction is less than its\nspread in the normal direction, we use the opposite direction.  Secondly, before we start stitching, we can compute the order of qedges that we\nwill use for every stitch. We then sort the qnodes according to the order by\nwhich they will be encountered when we work through the qedges. When we are\nstitching, in the midst of a partial stitch, it is always the case that we have\nstitched qnodes 1 ..  n  for some  n , and we still have to stitch all qnodes\nabove  n . That means that when we try to finish partial stitches of which an\ninitial part has been fixed, the search process will not change that initial\npart of the stitch. Only when the algorithm has exhausted all possibilities\nbased on that initial part, it will change the last node of the initial part,\nreplace it by other options, and start searching further.  This means that we just can maintain our partial stitch in a single list. We do\nnot have to assemble many partial stitches as separate immutable tuples.  We have implemented our deliver function as a generator, that walks over all\nstitch possibilities while maintaining just one current stitch. When the stitch\nhas been completely filled in, a copy of it will be yielded, after which\nback-tracking occurs, by which the current stitch will get partly undefined,\nonly to be filled up again by further searching.  Read it all in the source code: def stitchOn(e) .",
            "title": "Small-first strategy"
        }
    ]
}