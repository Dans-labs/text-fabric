{
    "docs": [
        {
            "location": "/",
            "text": "Text-Fabric\n\u00b6\n\n\nAbout\nText-Fabric is several things:\na \nbrowser\n for ancient text corpora\na Python3 package for processing ancient corpora\nA corpus of ancient texts and linguistic annotations represents a large body of knowledge.\nText-Fabric makes that knowledge accessible to non-programmers by means of \nbuilt-in a search interface that runs in your browser.\nFrom there the step to program your own analytics is not so big anymore.\nBecause you can call the Text-Fabric API from your Python programs, and\nit works really well in Jupyter notebooks.\nInstall\n\u00b6\n\n\nText Fabric is a Python(3) package on the Python Package Index,\nso you can install it easily with \npip3\n or \npip\n from\nthe command line.\n\n\nPrerequisites\nComputer\nYour computer should be a 64-bit machine and it needs at least 3 GB RAM memory.\nIt should run Linux, Macos, or Windows.\nclose other programs\nWhen you run the Text-Fabric browser for the first time, make sure that most of that minimum\nof 3GB RAM is actually available, and not in use by other programs.\nPython\nInstall or upgrade Python on your system to at least version 3.6.3.\nGo for the 64-bit version. Otherwise Python may not be able to address all the memory it needs.\nThe leanest install is provided by \npython.org\n.\nYou can also install it from \nanaconda.com\n.\non Windows?\nChoose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix,\n  you need to have additional developers' tools installed.\n  The Anaconda distribution has Jupyter out of the box.\nWhen installing Python, make sure that the installer adds the path to Python to \n  your environment variables.\nInstall Chrome of Firefox and set it as your default browser.\n\n  The Text-Fabric browser does not display well in Microsoft Edge,\n  for Edge does not support the\n  \ndetails\n\n  element.\nText-Fabric\nInstall Text-Fabric:\n1\npip3 install text-fabric\n\n\n\n\n\nOn Windows:\n1\npip install text-fabric\n\n\n\n\n\nto 3 or not to 3?\nFrom now on we always use \npython3\n and \npip3\n in instructions.\nOn Windows you have to say \npython\n and \npip\n.\nThere are more differences, when you go further with programming.\nAdvice: if you are serious on programming, consider switching to a \nUnix\n-like\nplatform, such as Linux or the Mac (macos).\nJupyter\nOptionally install \nJupyter\n as well:\n1\npip3 install jupyter\n\n\n\n\n\nJupyter Lab\nJupyter lab\n is a nice context to work with Jupyter notebooks.\nRecommended for working with the\nthe tutorials of Text-Fabric.\nAlso when you want to copy and paste cells from one notebook\nto another.\n1\n2\npip3 install jupyterlab\njupyter labextension install jupyterlab-toc\n\n\n\n\n\nThe toc-extension is handy to get an overview\nwhen working with the lengthy tutorial. It will create an extra\ntab in the Jupyter Lab interface with a table of contents of the\ncurrent notebook.\nUse the Text-Fabric browser\n\u00b6\n\n\nExplore your corpus without programming.\n\n\nStart up\nYou can open a terminal (such as \nbash\n on Unix and \ncmd.exe\n on Windows), and just say\n1\ntext-fabric bhsa\n\n\n\n\n\nor \n1\ntext-fabric cunei\n\n\n\n\n\nAfter (down)-loading the data your browser will open and load the search interface.\nThere you'll find links to further help.\ncaution \"trouble?\"\n    See the section Trouble below.\n\n\n\n\n\n\n\nAbove: Querying the BHSA data\n\n\nBelow: Querying the Cunei data\n\n\n\n\n\n\n\n\n\nFetching corpora\nThe Text-Fabric browser fetches the corpora it needs from GitHub automatically.\nThe TF data is fairly compact (25 MB for the Hebrew Bible, 1.6 MB for the Cunei corpus).\nSize of data\nThere might be sizable additional data (550 MB images for the Cunei corpus).\nIn that case, take care to have a good internet connection when you use the\nText-Fabric browser for the first time.\nSaving your session\nYour session will be saved in a file with extension \n.tfjob\n in the directory\nfrom where you have issued the \ntext-fabric\n command.\nFrom within the browser you can rename and duplicate sessions and move to\nother directories. You can also load other sessions in other tabs.\nMultiple windows\nAfter you have issued the \ntext-fabric\n command, you can open many \nbrowsers and windows and tabs with the same url.\nThey all use the same data, which only gets loaded when the command \ntext-fabric\n is run.\nIf you leave it on all day, you have instant access to the data.\nClose\nYou can close the data server by pressing Ctrl-C in the terminal where you have\nstarted \ntext-fabric\n up.\nGet corpora\n\u00b6\n\n\nMore about the data that Text-Fabric works with.\n\n\nAbout\nCorpora are usually stored in an online repository, such as GitHub or a research data archive\nsuch as \nDANS\n.\nAutomatically\nThere are a few corpora in Text-Fabric that are being supported\nwith extra modules.\nText-Fabric will fetch them for you if you use the Text-Fabric browser.\nAnd if you work with them from within a Python program (e.g. in a Jupyter Notebook),\nText-Fabric either fetches data automatically, or there is an easy function\nthat you can call to fetch data.\nManually\nYou can also download the data you need up-front.\nThere are basically two ways:\nfrom a release binary\nGo to the relevant GitHub repository, click on releases, and choose the relevant binary\nthat has been attached to the release.\nDownload it to your system.\nThis is the most economical way to get data.\nclone the complete repository\nClone the relevant GitHub repository to your system.\nThis will get you lots of additional data that you might not directly need.\nOn Windows?\nYou have to install \ngit\n in\n\nsome way\n\nfor this step.\nHebrew Bible\nIf you are in a Jupyter notebook or Python script,\nthis will fetch the data (25MB)\n1\n2\nfrom\n \ntf.extra.bhsa\n \nimport\n \ngetTf\n\n\ngetTf\n()\n\n\n\n\n\n\nOr if you want to fetch related modules, such as the \nphono\n transcriptions, you can say\n1\ngetTf\n(\nsource\n=\n'phono'\n,\n \nrelease\n=\n'1.0.1'\n)\n\n\n\n\n\n\nETCBC data\nInspect the repositories of the \netcbc organization on GitHub\n\nto see what is available. Per repository, click the \nReleases\n button so see\nthe release version that holds the relevant binaries with TF-data.\nFor example, try \ncrossrefs\n.\nETCBC versions\nThe data of the ETCBC comes in major versions, ranging from \n3\n (2011) via\n\n4\n, \n4b\n, \n2016\n, \n2017\n to \nc\n (2018).\nThe latter is a \ncontinuous\n version, which will change over time.\nIf you want to be in complete control, you can get the complete data repository\nfrom GitHub (5GB):\n1\n2\ncd\n ~/github/etcbc\ngit clone https://github.com/etcbc/bhsa\n\n\n\n\n\nand likewise you can get other ETCBC data modules such as \nphono\n.\nCuneiform tablets from Uruk\nIf you are in a Jupyter notebook or Python script,\nthe Cunei API will fetch the data for you automatically:\nThe TF-part is 1.6 MB\nthe photos and lineart are 550MB!\nCaution\nSo do this only when you have a good internet connection.\nIf you want to be in complete control, you can get the complete data repository\nfrom GitHub (1.5GB):\n1\n2\ncd\n ~/github/Nino-cunei\ngit clone https://github.com/Nino-cunei/uruk\n\n\n\n\n\nMore\nThe\n\nGreek\n and\n\nSyriac\n\nNew Testament have been converted to TF.\nWe have example corpora in Sanskrit, and Babylonian.\n1\n2\ncd\n ~/github\ngit clone https://github.com/etcbc/linksyr\n\n\n\n\n\n1\n2\ncd\n ~/github\ngit clone https://github.com/Dans-labs/text-fabric-data\n\n\n\n\n\nAll these are not supported by extra interfaces.\nDocumentation\n\u00b6\n\n\nAbout\nThere is extensive documentation.\nIf you start using the Text-Fabric API in your programs, you'll definitely need it.\nIf you are just starting with the Text-Fabric browser, it might help to\nlook at the online tutorials for the BHSA and the Cunei corpus to see what\nText-Fabric can reveal about the data.\nTutorials\nThere are tutorials and exercises to guide you into increasingly involved tasks\non specific corpora (outside this repo):\nBiblia Hebraica Stuttgartensia Amstelodamensis\nProto-Cuneiform tablets from Uruk IV/III\nThese links point to the \nstatic\n online versions.\nIf you want to get these Jupyter notebooks on your system in order to execute them yourself, \nyou can download them from a release:\nBHSA tutorial download\nCunei tutorial download\nThese are zip files, you can unpack them where you want.\nYou have to have Jupyter installed.\nReference\nThe pages you are reading now are the reference docs.\nIt explains the \ndata model\nIt specifies the \nfile format\nIt holds the \napi docs\nBackground\nFor more background information (earlier work, institutes, people, datasets), consult the\n\nwiki\n pages of SHEBANQ.\nPapers\nPapers (preprints on \narxiv\n), most of them published:\nParallel Texts in the Hebrew Bible, New Methods and Visualizations \nThe Hebrew Bible as Data: Laboratory - Sharing - Experiences\n\n   (preprint: \narxiv\n)\nLAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible\nAnnotation as a New Paradigm in Research Archiving\nPresentation\nHere is a motivational\n\npresentation\n,\ngiven just before\n\nSBL 2016\n\nin the Lutheran Church of San Antonio.\nGetting started with the API\n\u00b6\n\n\nInto the notebook\nStart programming: write a python script or code in the Jupyter notebook\n1\n2\ncd\n somewhere-else\njupyter notebook\n\n\n\n\n\nEnter the following text in a code cell\n1\n2\n3\n4\nfrom\n \ntf.fabric\n \nimport\n \nFabric\n\n\nTF\n \n=\n \nFabric\n(\nmodules\n=\n[\n'my/dataset'\n])\n\n\napi\n \n=\n \nTF\n.\nload\n(\n'sp lex'\n)\n\n\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nlocations\nMaybe you have to tell Text-Fabric exactly where your data is.\nIf you have the data in a directory \ntext-fabric-data\n\nunder your home directory  or under \n~/github\n, Text-Fabric can find it.\nIn your \nmodules\n argument you then specify one or more subdirectories of\n\ntext-fabric-data\n.\nUsing Hebrew data\nTo get started with the Hebrew corpus, use its tutorial in the BHSA repo:\n\nstart\n.\nOr go straight to the\n\nbhsa-api-docs\n.\nUsing Cuneiform data\nTo get started with the Uruk corpus, use its tutorial in the Nino-cunei repo:\n\nstart\n.\nOr go straight to the\n\ncunei-api-docs\n.\nDesign principles\n\u00b6\n\n\nMinimalistic model\nText-Fabric is based on a minimalistic data model for text plus annotations.\nA defining characteristic is that Text-Fabric does not make use of XML or JSON,\nbut stores text as a bunch of features in plain text files.\nThese features are interpreted against a \ngraph\n of nodes and edges, which make up the\nabstract fabric of the text.\nEfficient data processing\nBased on this model, Text-Fabric offers a \nprocessing API\n\nto search, navigate and process text and its annotations.\nSearch for patterns\nThe \nsearch API\n\nworks with search templates that define relational patterns\nwhich will be instantiated by nodes and edges of the fabric.\nSharing data\neasy sharing of sharing data\n\nStudents can pick and choose the feature data they need.\nWhen the time comes to share the fruits of their thought,\nthey can do so in various ways:\nwhen using the TF browser, results can be exported as PDF and stored\n  in a repository[\nwhen programming in a notebook, these notebooks can easily be shared online\n  by using GitHub of NBViewer.\nContributing data\nResearchers can easily\n\nproduce new data modules\n\nof text-fabric data out of their findings.\nFactory\nText-Fabric can be and has been used to construct websites,\nfor example \nSHEBANQ\n.\nIn the case of SHEBANQ, data has been converted to mysql databases.\nHowever, with the built-in data server, it is also possible to\nhave one Text-Fabric process serve multiple connections and requests.\nHistory\nThe foundational ideas derive from work done in and around the\n\nETCBC\n\nby Eep Talstra, Crist-Jan Doedens, Henk Harmsen, Ulrik Sandborg-Petersen\nand many others.\nThe author entered in that world in 2007 as a \n\nDANS\n employee, doing a joint small data project,\nand a bigger project SHEBANQ in 2013/2014.\nIn 2013 I developed\n\nLAF-Fabric\n\nin order to be able to construct the website\n\nSHEBANQ\n.\nI have taken out everything that makes LAF-Fabric complicated and\nall things that are not essential for the sake of raw data processing.\nFrequently Occurring Trouble\n\u00b6\n\n\nOlder versions\nOlder versions of Python and Text-Fabric may be in the way.\nThe following hygenic measures are known to be beneficial:\nPython related\nWhen you have upgraded Python, remove PATH statements for older versions from your system startup files.\nFor the Macos: look at \n.bashrc\n, \n.bash_profile\n in your home directory.\nFor Windows: on the command prompt, say \necho %path%\n to see what the content of your PATH\n  variable is. If you see references to older versions of python than you actually work with,\n  they need to be removed. \nHere is how\nOnly for Python3\nDo not remove references to Python 2.\n, but only outdated Python 3.\n versions. \nText-Fabric related\nSometimes \npip3 uninstall text-fabric\n fails to remove all traces of Text-Fabric.\nHere is how you can remove them manually:\nlocate the \nbin\n directory of the current Python, it is something like\n(Macos regular Python) \n/Library/Frameworks/Python.framework/Versions/3.7/bin\n(Windows Anaconda) \nC:\\Users\\You\\Anaconda3\\Scripts\nRemove the file \ntext-fabric\n from this directory if it exists.\nlocate the \nsite-packages\n directory of the current Python, it is something like\n(Macos regular Python)\n    \n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\nRemove the subdirectory \ntf\n from this location, plus all files with \ntext-fabric\n in the name.\nAfter this, you can make a fresh install of \ntext-fabric\n:\n1\npip3 install text-fabric\n\n\n\n\n\nInternal Server Error\nWhen the TF browser opens with an Internal Server error, the most likely reason is that\nthe TF data server has not started up without errors.\nLook back at the command prompt where you started \ntext-fabric\n.\nProbably somewhere down the road you see \nMemory Error\n.\nIt means that during loading TF did not have access too enough RAM memory.\nMaybe you had too many programs (or browser tabs) open at that time.\nSimply closing text-fabric and trying again is not enough, due to the memory error some\ncompiled TF data may have become corrupt.\nI am working toward a graceful solution to automatically recover from this.\nFor now, you have to manually remove the compiled data.\nYou find that data in directories named \n.tf\n (hidden in Macos and Linux, visible in Windows).\nLook into \ntext-fabric-data/etcbc/*/tf/c\n for \n*\n one of \nbhsa\n, \nphono\n, \nparallels\n.\nJust delete these \n.tf\n directories.\nThe free up some RAM and try again.\nNewest version of Text-Fabric does not show up\nWhen you get errors doing \npip3 install text-fabric\n, there is probably an older version around.\nYou have to say\n1\npip3 install --upgrade text-fabric\n\n\n\n\n\nIf this still does not download the most recent version of \ntext-fabric\n, it may have been cauched by caching.\nThen say:\n1\npip3 install --upgrade --no-cache-dir text-fabric\n\n\n\n\n\nYou can check what the newest distributed version of Text-Fabric is on\n\nPyPi\n.\nAuthor\n\u00b6\n\n\nDirk Roorda\n\n\nCo-creation\nWhile I wrote most of the code, a product like Text-Fabric is unthinkable without\nthe contributions of avid users that take the trouble to give feedback and file issues,\nand have the zeal and stamina to hold on when things are frustrating and bugs overwhelming.\nAcknowledgements\nIn particular I owe a lot to\nMartijn Naaijer\nCody Kingham\nChristiaan Erwich\nCale Johnson\nChristian H\u00f8ygaard-Jensen\nCamil Staps\nStephen Ku\nJames Cu\u00e9nod\nJohan de Joode\nCode statistics\nFor a feel of the size of this project, in terms of lines of code,\nsee \nCode lines",
            "title": "Home"
        },
        {
            "location": "/#text-fabric",
            "text": "About Text-Fabric is several things: a  browser  for ancient text corpora a Python3 package for processing ancient corpora A corpus of ancient texts and linguistic annotations represents a large body of knowledge.\nText-Fabric makes that knowledge accessible to non-programmers by means of \nbuilt-in a search interface that runs in your browser. From there the step to program your own analytics is not so big anymore.\nBecause you can call the Text-Fabric API from your Python programs, and\nit works really well in Jupyter notebooks.",
            "title": "Text-Fabric"
        },
        {
            "location": "/#install",
            "text": "Text Fabric is a Python(3) package on the Python Package Index,\nso you can install it easily with  pip3  or  pip  from\nthe command line.  Prerequisites Computer Your computer should be a 64-bit machine and it needs at least 3 GB RAM memory.\nIt should run Linux, Macos, or Windows. close other programs When you run the Text-Fabric browser for the first time, make sure that most of that minimum\nof 3GB RAM is actually available, and not in use by other programs. Python Install or upgrade Python on your system to at least version 3.6.3.\nGo for the 64-bit version. Otherwise Python may not be able to address all the memory it needs. The leanest install is provided by  python.org .\nYou can also install it from  anaconda.com . on Windows? Choose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix,\n  you need to have additional developers' tools installed.\n  The Anaconda distribution has Jupyter out of the box. When installing Python, make sure that the installer adds the path to Python to \n  your environment variables. Install Chrome of Firefox and set it as your default browser. \n  The Text-Fabric browser does not display well in Microsoft Edge,\n  for Edge does not support the\n   details \n  element. Text-Fabric Install Text-Fabric: 1 pip3 install text-fabric   On Windows: 1 pip install text-fabric   to 3 or not to 3? From now on we always use  python3  and  pip3  in instructions.\nOn Windows you have to say  python  and  pip .\nThere are more differences, when you go further with programming. Advice: if you are serious on programming, consider switching to a  Unix -like\nplatform, such as Linux or the Mac (macos). Jupyter Optionally install  Jupyter  as well: 1 pip3 install jupyter   Jupyter Lab Jupyter lab  is a nice context to work with Jupyter notebooks.\nRecommended for working with the\nthe tutorials of Text-Fabric.\nAlso when you want to copy and paste cells from one notebook\nto another. 1\n2 pip3 install jupyterlab\njupyter labextension install jupyterlab-toc   The toc-extension is handy to get an overview\nwhen working with the lengthy tutorial. It will create an extra\ntab in the Jupyter Lab interface with a table of contents of the\ncurrent notebook.",
            "title": "Install"
        },
        {
            "location": "/#use-the-text-fabric-browser",
            "text": "Explore your corpus without programming.  Start up You can open a terminal (such as  bash  on Unix and  cmd.exe  on Windows), and just say 1 text-fabric bhsa   or  1 text-fabric cunei   After (down)-loading the data your browser will open and load the search interface.\nThere you'll find links to further help. caution \"trouble?\"\n    See the section Trouble below.    Above: Querying the BHSA data  Below: Querying the Cunei data     Fetching corpora The Text-Fabric browser fetches the corpora it needs from GitHub automatically.\nThe TF data is fairly compact (25 MB for the Hebrew Bible, 1.6 MB for the Cunei corpus). Size of data There might be sizable additional data (550 MB images for the Cunei corpus).\nIn that case, take care to have a good internet connection when you use the\nText-Fabric browser for the first time. Saving your session Your session will be saved in a file with extension  .tfjob  in the directory\nfrom where you have issued the  text-fabric  command.\nFrom within the browser you can rename and duplicate sessions and move to\nother directories. You can also load other sessions in other tabs. Multiple windows After you have issued the  text-fabric  command, you can open many \nbrowsers and windows and tabs with the same url.\nThey all use the same data, which only gets loaded when the command  text-fabric  is run.\nIf you leave it on all day, you have instant access to the data. Close You can close the data server by pressing Ctrl-C in the terminal where you have\nstarted  text-fabric  up.",
            "title": "Use the Text-Fabric browser"
        },
        {
            "location": "/#get-corpora",
            "text": "More about the data that Text-Fabric works with.  About Corpora are usually stored in an online repository, such as GitHub or a research data archive\nsuch as  DANS . Automatically There are a few corpora in Text-Fabric that are being supported\nwith extra modules.\nText-Fabric will fetch them for you if you use the Text-Fabric browser.\nAnd if you work with them from within a Python program (e.g. in a Jupyter Notebook),\nText-Fabric either fetches data automatically, or there is an easy function\nthat you can call to fetch data. Manually You can also download the data you need up-front.\nThere are basically two ways: from a release binary Go to the relevant GitHub repository, click on releases, and choose the relevant binary\nthat has been attached to the release.\nDownload it to your system.\nThis is the most economical way to get data. clone the complete repository Clone the relevant GitHub repository to your system.\nThis will get you lots of additional data that you might not directly need. On Windows? You have to install  git  in some way \nfor this step. Hebrew Bible If you are in a Jupyter notebook or Python script,\nthis will fetch the data (25MB) 1\n2 from   tf.extra.bhsa   import   getTf  getTf ()    Or if you want to fetch related modules, such as the  phono  transcriptions, you can say 1 getTf ( source = 'phono' ,   release = '1.0.1' )    ETCBC data Inspect the repositories of the  etcbc organization on GitHub \nto see what is available. Per repository, click the  Releases  button so see\nthe release version that holds the relevant binaries with TF-data.\nFor example, try  crossrefs . ETCBC versions The data of the ETCBC comes in major versions, ranging from  3  (2011) via 4 ,  4b ,  2016 ,  2017  to  c  (2018).\nThe latter is a  continuous  version, which will change over time. If you want to be in complete control, you can get the complete data repository\nfrom GitHub (5GB): 1\n2 cd  ~/github/etcbc\ngit clone https://github.com/etcbc/bhsa   and likewise you can get other ETCBC data modules such as  phono . Cuneiform tablets from Uruk If you are in a Jupyter notebook or Python script,\nthe Cunei API will fetch the data for you automatically: The TF-part is 1.6 MB the photos and lineart are 550MB! Caution So do this only when you have a good internet connection. If you want to be in complete control, you can get the complete data repository\nfrom GitHub (1.5GB): 1\n2 cd  ~/github/Nino-cunei\ngit clone https://github.com/Nino-cunei/uruk   More The Greek  and Syriac \nNew Testament have been converted to TF. We have example corpora in Sanskrit, and Babylonian. 1\n2 cd  ~/github\ngit clone https://github.com/etcbc/linksyr   1\n2 cd  ~/github\ngit clone https://github.com/Dans-labs/text-fabric-data   All these are not supported by extra interfaces.",
            "title": "Get corpora"
        },
        {
            "location": "/#documentation",
            "text": "About There is extensive documentation. If you start using the Text-Fabric API in your programs, you'll definitely need it. If you are just starting with the Text-Fabric browser, it might help to\nlook at the online tutorials for the BHSA and the Cunei corpus to see what\nText-Fabric can reveal about the data. Tutorials There are tutorials and exercises to guide you into increasingly involved tasks\non specific corpora (outside this repo): Biblia Hebraica Stuttgartensia Amstelodamensis Proto-Cuneiform tablets from Uruk IV/III These links point to the  static  online versions.\nIf you want to get these Jupyter notebooks on your system in order to execute them yourself, \nyou can download them from a release: BHSA tutorial download Cunei tutorial download These are zip files, you can unpack them where you want.\nYou have to have Jupyter installed. Reference The pages you are reading now are the reference docs. It explains the  data model It specifies the  file format It holds the  api docs Background For more background information (earlier work, institutes, people, datasets), consult the wiki  pages of SHEBANQ. Papers Papers (preprints on  arxiv ), most of them published: Parallel Texts in the Hebrew Bible, New Methods and Visualizations  The Hebrew Bible as Data: Laboratory - Sharing - Experiences \n   (preprint:  arxiv ) LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible Annotation as a New Paradigm in Research Archiving Presentation Here is a motivational presentation ,\ngiven just before SBL 2016 \nin the Lutheran Church of San Antonio.",
            "title": "Documentation"
        },
        {
            "location": "/#getting-started-with-the-api",
            "text": "Into the notebook Start programming: write a python script or code in the Jupyter notebook 1\n2 cd  somewhere-else\njupyter notebook   Enter the following text in a code cell 1\n2\n3\n4 from   tf.fabric   import   Fabric  TF   =   Fabric ( modules = [ 'my/dataset' ])  api   =   TF . load ( 'sp lex' )  api . makeAvailableIn ( globals ())    locations Maybe you have to tell Text-Fabric exactly where your data is.\nIf you have the data in a directory  text-fabric-data \nunder your home directory  or under  ~/github , Text-Fabric can find it.\nIn your  modules  argument you then specify one or more subdirectories of text-fabric-data . Using Hebrew data To get started with the Hebrew corpus, use its tutorial in the BHSA repo: start . Or go straight to the bhsa-api-docs . Using Cuneiform data To get started with the Uruk corpus, use its tutorial in the Nino-cunei repo: start . Or go straight to the cunei-api-docs .",
            "title": "Getting started with the API"
        },
        {
            "location": "/#design-principles",
            "text": "Minimalistic model Text-Fabric is based on a minimalistic data model for text plus annotations. A defining characteristic is that Text-Fabric does not make use of XML or JSON,\nbut stores text as a bunch of features in plain text files. These features are interpreted against a  graph  of nodes and edges, which make up the\nabstract fabric of the text. Efficient data processing Based on this model, Text-Fabric offers a  processing API \nto search, navigate and process text and its annotations. Search for patterns The  search API \nworks with search templates that define relational patterns\nwhich will be instantiated by nodes and edges of the fabric. Sharing data easy sharing of sharing data \nStudents can pick and choose the feature data they need.\nWhen the time comes to share the fruits of their thought,\nthey can do so in various ways: when using the TF browser, results can be exported as PDF and stored\n  in a repository[ when programming in a notebook, these notebooks can easily be shared online\n  by using GitHub of NBViewer. Contributing data Researchers can easily produce new data modules \nof text-fabric data out of their findings. Factory Text-Fabric can be and has been used to construct websites,\nfor example  SHEBANQ .\nIn the case of SHEBANQ, data has been converted to mysql databases.\nHowever, with the built-in data server, it is also possible to\nhave one Text-Fabric process serve multiple connections and requests. History The foundational ideas derive from work done in and around the ETCBC \nby Eep Talstra, Crist-Jan Doedens, Henk Harmsen, Ulrik Sandborg-Petersen\nand many others. The author entered in that world in 2007 as a  DANS  employee, doing a joint small data project,\nand a bigger project SHEBANQ in 2013/2014.\nIn 2013 I developed LAF-Fabric \nin order to be able to construct the website SHEBANQ . I have taken out everything that makes LAF-Fabric complicated and\nall things that are not essential for the sake of raw data processing.",
            "title": "Design principles"
        },
        {
            "location": "/#frequently-occurring-trouble",
            "text": "Older versions Older versions of Python and Text-Fabric may be in the way.\nThe following hygenic measures are known to be beneficial: Python related When you have upgraded Python, remove PATH statements for older versions from your system startup files. For the Macos: look at  .bashrc ,  .bash_profile  in your home directory. For Windows: on the command prompt, say  echo %path%  to see what the content of your PATH\n  variable is. If you see references to older versions of python than you actually work with,\n  they need to be removed.  Here is how Only for Python3 Do not remove references to Python 2. , but only outdated Python 3.  versions.  Text-Fabric related Sometimes  pip3 uninstall text-fabric  fails to remove all traces of Text-Fabric.\nHere is how you can remove them manually: locate the  bin  directory of the current Python, it is something like (Macos regular Python)  /Library/Frameworks/Python.framework/Versions/3.7/bin (Windows Anaconda)  C:\\Users\\You\\Anaconda3\\Scripts Remove the file  text-fabric  from this directory if it exists. locate the  site-packages  directory of the current Python, it is something like (Macos regular Python)\n     /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages Remove the subdirectory  tf  from this location, plus all files with  text-fabric  in the name. After this, you can make a fresh install of  text-fabric : 1 pip3 install text-fabric   Internal Server Error When the TF browser opens with an Internal Server error, the most likely reason is that\nthe TF data server has not started up without errors. Look back at the command prompt where you started  text-fabric .\nProbably somewhere down the road you see  Memory Error . It means that during loading TF did not have access too enough RAM memory.\nMaybe you had too many programs (or browser tabs) open at that time. Simply closing text-fabric and trying again is not enough, due to the memory error some\ncompiled TF data may have become corrupt. I am working toward a graceful solution to automatically recover from this.\nFor now, you have to manually remove the compiled data. You find that data in directories named  .tf  (hidden in Macos and Linux, visible in Windows).\nLook into  text-fabric-data/etcbc/*/tf/c  for  *  one of  bhsa ,  phono ,  parallels .\nJust delete these  .tf  directories. The free up some RAM and try again. Newest version of Text-Fabric does not show up When you get errors doing  pip3 install text-fabric , there is probably an older version around.\nYou have to say 1 pip3 install --upgrade text-fabric   If this still does not download the most recent version of  text-fabric , it may have been cauched by caching.\nThen say: 1 pip3 install --upgrade --no-cache-dir text-fabric   You can check what the newest distributed version of Text-Fabric is on PyPi .",
            "title": "Frequently Occurring Trouble"
        },
        {
            "location": "/#author",
            "text": "Dirk Roorda  Co-creation While I wrote most of the code, a product like Text-Fabric is unthinkable without\nthe contributions of avid users that take the trouble to give feedback and file issues,\nand have the zeal and stamina to hold on when things are frustrating and bugs overwhelming. Acknowledgements In particular I owe a lot to Martijn Naaijer Cody Kingham Christiaan Erwich Cale Johnson Christian H\u00f8ygaard-Jensen Camil Staps Stephen Ku James Cu\u00e9nod Johan de Joode Code statistics For a feel of the size of this project, in terms of lines of code,\nsee  Code lines",
            "title": "Author"
        },
        {
            "location": "/News/",
            "text": "Changes\n\u00b6\n\n\nConsult the tutorials after changes\nWhen we change the API, we make sure that the tutorials shows off\nall possibilities:\n\nbhsa\n\n\ncunei\n5.5.8\n\u00b6\n\n\n2018-07-10\n\n\nBetter in catching out-of-memory errors.\nPrevents creation of corrupt compiled binary TF data.\n\n\n5.5.7\n\u00b6\n\n\n2018-07-09\n\n\nOptimization is export from TF browser.\n\n\n5.5.6\n\u00b6\n\n\n2018-07-09\n\n\nBetter help display.\n\n\n\n\nThe opened-state of help sections is remembered.\n\n\nYou can open help next to an other open section in the sidebar.\n\n\n\n\n5.5.5\n\u00b6\n\n\n2018-07-08\n\n\nCrisper icon.\n\n\n5.5.4\n\u00b6\n\n\n2018-07-6\n\n\nDocs updated. Little bit of refactoring.\n\n\n5.5.1-3\n\u00b6\n\n\n2018-07-4\n\n\nIn the TF browser, use a selection of all the features when working with the BHSA.\nOtherwise in Windows you might run out of memory, even if you have 8GB RAM.\n\n\n5.5\n\u00b6\n\n\n2018-07-4\n\n\nText-Fabric can download data for BHSA and Cunei. You do not have to clone github repositories for that.\nThe data downloaded by Text-Fabric ends up in \ntext-fabric-data\n under your home directory.\n\n\n5.4.5-7\n\u00b6\n\n\n2018-07-03\n\n\nExperimenting with setuptools to get the text-fabric script working\non Windows.\n\n\n5.4.4\n\u00b6\n\n\n2018-07-02\n\n\nAdded renaming/duplicating of jobs and change of directory.\n\n\n5.4.3\n\u00b6\n\n\n2018-06-29\n\n\nSmall fixes in error reporting in search.\n\n\n5.4.1-2\n\u00b6\n\n\n2018-06-28\n\n\nText-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata.\n\n\nSmall fixes.\n\n\n5.4\n\u00b6\n\n\n2018-06-26\n\n\nImproved interface and functionality of the text-fabric browser:\n\n\n\n\nyou can save your work\n\n\nyou can enter verse references and tablet P numbers\n\n\nthere is help\n\n\nthere is a side bar\n\n\n\n\nDocs not up to date\nThe API docs are not up-to-date: there are new functions in the Bhsa and Cunei APIs.\nThe server/service/client apis are not completely spelled out.\nHowever, the help for the text-fabric browser is included in the interface itself.\n5.3.3\n\u00b6\n\n\n2018-06-23\n\n\nSmall fix: command line args for text-fabric.\n\n\n5.3.0-2\n\u00b6\n\n\n2018-06-22\n\n\nBetter process management\nWhen the TF server is started, it cleans up remnant process that might get in the way otherwise.\nYou can also say\n1\ntext-fabric -k\n\n\n\n\n\nto kill all remnant processes,\nor\n1\ntext-fabric -k datasource\n\n\n\n\n\nto kill the processes for a specific datasource only.\nManual node entry\nYou can enter nodes manually in the TF browser.\nHandy for quick inspection.\nYou can click on the sequence number to\nappend the node tuple of that row to the tuple input field.\nThat way you can collect interesting results.\nName and Description\nYou can enter a name which will be used as title and file name during export.\nYou can enter a description in Markdown.\nWhen you export your query, the description appears\nformatted on top.\nProvenance\nIf you export a query, provenance is added, using DOIs.\nSmall fixes\nNo more blank pages due to double page breaks.\n5.2.1\n\u00b6\n\n\n2018-06-21\n\n\n\n\nAdded an \nexpand all\n checkbox in the text-fabric browser,\n  to expand all shown rows or to collapse them.\n\n\nExport function for search results in the text-fabric browser.\n  What you see is what you get, 1 pretty display per page if you have\n  the browser save it to pdf.\n\n\nSmall tweaks\n\n\n\n\n5.1\n\u00b6\n\n\n2018-06-21\n\n\nWhen displaying results in condensed mode, you\ncan now choose the level of the container in which results are highlighted.\nSo far it was fixed to \nverse\n for the bhsa and \ntablet\n for cunei.\n\n\nThe docs are lagging behind!\nBut it is shown in the tutorials and you can observer it in the text-fabric browser.\n\n\n5.0.1,2,3,4\n\u00b6\n\n\n2018-06-19\n\n\nAddressed start-up problems.\n\n\n5.0\n\u00b6\n\n\n2018-06-18\n\n\nBuilt in web server and client for local query running.\nIt is implemented for Bhsa and Cunei.\n\n\n4.4.2,3\n\u00b6\n\n\n2018-06-13\n\n\nNew distribution method with setuptools.\nText-Fabric has now dependencies on modules rpyc and bottle,\nbecause it contains a built-in data service and webserver.\n\n\nThis website is still barely functional, though.\n\n\n4.4.1\n\u00b6\n\n\n2018-06-10\n\n\nSearch API:\n\n\nEscapes in regular expression search was buggy and convoluted.\nIf a feature value contains a \n|\n then in an RE you have to enter \n\\|\n to match it.\nBut to have that work in a TF search, you needed to say \n\\\\\\|\n. \n\n\nOn the other hand, in the same case for \n.\n instead of \n|\n, you could just sat \n\\.\n\n\nIn the new situation you do not have to double escape in REs anymore.\nYou can just say \n\\|\n and \n\\.\n.\n\n\n4.4\n\u00b6\n\n\n2018-06-06\n\n\nSearch API:\n\n\nS.search() accepts a new optional parameter: \nwithContext\n.\nIt triggers the output of context information for nodes in the result tuples.\n\n\n4.3.4, 4.3.5\n\u00b6\n\n\n2018-06-05\n\n\nSearch API:\n\n\nThe \n/with/ /or/ /or/ /-/' quantifier is also allowed with zero\n/or/` s.\n\n\nSmall fix in the \n/with/\n quantifier if there are quantifiers between this one and its parent atom.\n\n\n4.3.3\n\u00b6\n\n\n2018-06-01\n\n\nSearch API:\n\n\nImproved quantifiers in search: \n\n\n\n\n/where/\n \n/have/\n \n/without/\n \n/with/\n \n/or/\n \n/-/\n;\n\n\nmuch clearer indentation rules (no caret anymore);\n\n\nbetter reporting by \nS.study()\n.\n\n\n\n\n4.3.2\n\u00b6\n\n\n2018-05-31\n\n\nSearch API: \n\n\n\n\nquantifiers may use the name \n..\n to refer to their parents\n\n\nyou may use names in the place of atoms, which lessens the need for constructs with \np = q\n\n\nstricter checks on the syntax and position of quantifiers\n\n\n\n\n4.3.1\n\u00b6\n\n\n2018-05-30\n\n\nDocs and metadata update\n\n\n4.3.0\n\u00b6\n\n\n2018-05-30\n\n\n\n\n\n\nAPI Change in Search.\n\n\nIn search templates I recently added things like\n\n\n1\n  word vt!\n\n\n\n\n\n\nwhich checks for words that do not have a value for feature \nvt\n.\n\n\nThe syntax for this has now changed to\n\n\n1\n  word vt#\n\n\n\n\n\n\n\n\n\n\nUnequal (#) in feature value conditions.\n\n\nNow you can say things like\n\n\n1\n  word vt#infa|infc\n\n\n\n\n\n\nmeaning that the value of feature is not one of \ninfa\n, \ninfc\n.\n\n\nSo, in addition to \n=\n we have \n#\n for \"not equal\".\n*   Quantifiers.\n\n\nYou can now use quantifiers in search. One of them is like \nNOTEXIST\n in MQL.\nSee the \ndocs\n\n\n\n\n\n\nA number of minor fixes.\n\n\n\n\n\n\n4.2.1\n\u00b6\n\n\n2018-05-25\n\n\n\n\nSeveral improvements in the pretty display in Bhsa and Cunei APIs\n\n\n\n\nUnder the hood changes in \nS.search()\n to prepare for \nquantifiers\n in search templates.\n\n\n\n\nTokenisation of quantifiers already works\n\n\nSearches can now spawn auxiliary searches without polluting intermediate data\n\n\nThis has been done by promoting the \nS\n API to a factory of search engines.\n    By deafault, \nS\n creates and maintains a single factory, so to the user\n    it is the same \nS\n. But when it needs to run a query in the middle of processing another query\n    it can just spawn another search engine to do that, without interfering with the\n    original search.\n\n\n\n\n\n\n\n\nNB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten.\n\n\n\n\n\n\n4.2\n\u00b6\n\n\n2018-05-23\n\n\nThe Search API has been extended:\n\n\n\n\nyou can use custom sets in your query templates\n\n\nyou can search in shallow mode: instead of full result tuples, you just get a set\n    of the top-level thing you mention in your template.\n\n\n\n\nThis functionality is a precursor for\n\nquantifiers in search templates\n\nbut is also a powerful addition to search in its own right.\n\n\n4.1.2\n\u00b6\n\n\n2018-05-17\n\n\nBhsa and Cunei APIs:\n\n\n\n\ncustom highlight colours also work for condensed results.\n\n\nyou can pass the \nhighlights\n parameter also to \nshow\n and \nprettyTuple\n\n\n\n\n4.1.1\n\u00b6\n\n\n2018-05-16\n\n\nBhsa API: you can customize the features that are shown in pretty displays.\n\n\n4.1\n\u00b6\n\n\n2018-05-16\n\n\nBhsa and Cunei APIs: you can customize the highlighting of search results:\n\n\n\n\ndifferent colours for different parts of the results\n\n\nyou can choose your colours freely from all that CSS has to offer.\n\n\n\n\nSee the updated search tutorials.\n\n\n4.0.3\n\u00b6\n\n\n2018-05-11\n\n\nNo changes, just quirks in the update process to get a new version of TF out.\n\n\n4.0.1\n\u00b6\n\n\n2018-05-11\n\n\nDocumentation updates.\n\n\n4.0.0\n\u00b6\n\n\n2018-05-11\n\n\n\n\nAdditions to Search.\n    You can now include the values of edges in your search templates.\n\n\nF.\nfeature\n.freqList()\n accepts a new parameter: \nnodeTypes\n. It will restrict its results to nodes in\n    one of the types in \nnodeTypes\n. \n\n\nYou can now also do \nE.\nfeature\n.freqList()\n.\n    It will count the number of edges if the edge is declared to be without values, \n    or it will give a frequency list of the edges by value if the edge has values.\n    Like \nF.freqList\n, you can pass parameters to constrain the frequency list to certain node types.\n    You can constrain the node types from which the edges start (\nnodeTypesFrom\n) and where they arrive\n    (\nnodeTypesTo\n).\n\n\nNew documentation system based on \nMkDocs\n.\n\n\n\n\n3.4.12\n\u00b6\n\n\n2018-05-02\n\n\nThe Cunei and Bhsa APIs show the version of Text-Fabric that is being called.\n\n\n3.4.11\n\u00b6\n\n\n2018-05-01\n\n\nCunei\n\n\n\n\ncases are divided horizontally and vertically, alternating with their\n    nesting level;\n\n\ncases have a feature \ndepth\n now, indicating at which level of nesting they\n    are.\n\n\n\n\n3.4.8-9-10\n\u00b6\n\n\n2018-04-30\n\n\nVarious small fixes, such as:\n\n\n\n\n\n\nBhsa: Lexeme links in pretty displays.\n\n\n\n\n\n\nCunei: Prevented spurious \n</div>\n in NbViewer.\n\n\n\n\n\n\n3.4.7\n\u00b6\n\n\nCunei: Modified local image names\n\n\n3.4.6\n\u00b6\n\n\nSmall tweaks in search.\n\n\n3.4.5\n\u00b6\n\n\n2018-04-28\n\n\nBhsa API:\n\n\n\n\nnew functions \nplain()\n and \ntable()\n for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by \npretty()\n and\n    \nshow()\n.\n\n\n\n\n3.4.4\n\u00b6\n\n\n2018-04-27\n\n\nCunei API:\n\n\n\n\nnew functions \nplain()\n and \ntable()\n for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by \npretty()\n and\n    \nshow()\n.\n\n\n\n\n3.4.2\n\u00b6\n\n\n2018-04-26\n\n\nBetter search documentation.\n\n\nCunei API: small fixes.\n\n\n3.4.1\n\u00b6\n\n\n2018-04-25\n\n\nBhsa API:\n\n\n\n\nSearch/show: you can now show results condensed: i.e. a list of passages with\n    highlighted results is returned. This is how SHEBANQ represents the results of\n    a query. If you have two results in the same verse, with \ncondensed=True\n you\n    get one verse display with two highlights, with \ncondensed=False\n you get two\n    verse displays with one highlight each.\n\n\n\n\nCunei API:\n\n\n\n\nSearch/show: the \npretty\n, \nprettyTuple\n, \nshow\n functions of the Bhsa API\n    have bee translated to the Cunei API. You can now get \nvery\n pretty displays\n    of search results.\n\n\n\n\n3.4\n\u00b6\n\n\n2018-04-23\n\n\nSearch\n:\n\n\n\n\nYou can use regular expressions to specify feature values in queries.\n\n\nYou could already search for nodes which have a non-None value for a certain\n    feature. Now you can also search for the complement: nodes that do not have a\n    certain feature.\n\n\n\n\nBhsa API:\n\n\nThe display of query results also works with lexeme nodes.\n\n\n3.3.4\n\u00b6\n\n\n2018-04-20\n\n\nCunei API: Better height and width control for images. Leaner captions.\n\n\n3.3.3\n\u00b6\n\n\n2018-04-19\n\n\nCunei API: \ncasesByLevel()\n returns case nodes in corpus order.\n\n\n3.3.2\n\u00b6\n\n\n2018-04-18\n\n\nChange in the Cunei api reflecting that undivided lines have no cases now (was:\nthey had a single case with the same material as the line). Also: the feature\n\nfullNumber\n on cases is now called \nnumber\n, and contains the full hierarchical\npart leading to a case. There is an extra feature \nterminal\n on lines and cases\nif they are not subdivided.\n\n\nChanges in Cunei and Bhsa api:\n\n\n\n\nfixed a bug that occurred when working outside a GitHub repository.\n\n\n\n\n3.3.1\n\u00b6\n\n\n2018-04-18\n\n\nChange in the Cunei api. \ncasesByLevel()\n now takes an optional argument\n\nterminal\n instead of \nwithChildren\n, with opposite values.\n\n\nwithChildren=False\n is ambiguous: will it deliver only cases that have no\nchildren (intended), or will it deliver cases and their children (understood,\nbut not intended).\n\n\nterminal=True\n: delivers only cases that are terminal.\n\n\nterminal=False\n: delivers all cases at that level.\n\n\n3.3\n\u00b6\n\n\n2018-04-14\n\n\nSmall fix in the bhsa api.\n\n\nBumped the version number because of the inclusion of corpus specific APIs.\n\n\n3.2.6\n\u00b6\n\n\n2018-04-14\n\n\n\n\nText-Fabric now contains corpus specific extras:\n\n\nbhsa.py\n for the Hebrew Bible (BHSA)\n\n\ncunei.py\n for the Proto-Cuneiform corpus Uruk\n\n\n\n\n\n\nThe \nFabric(locations=locations, modules=modules)\n constructor now uses \n['']\n\n    as default value for modules. Now you can use the \nlocations\n parameter on its\n    own to specify the search paths for TF features, leaving the \nmodules\n\n    parameter undefined, if you wish.\n\n\n\n\n3.2.5\n\u00b6\n\n\n2018-03-23\n\n\nEnhancement in search templates: you can now test for the presence of features.\nTill now, you could only test for one or more concrete values of features. So,\nin addition to things like\n\n\n1\nword number=plural tense=yiqtol\n\n\n\n\n\n\nyou can also say things like\n\n\n1\nword number=plural tense\n\n\n\n\n\n\nand it will give you words in the plural that have a tense.\n\n\n3.2.4\n\u00b6\n\n\n2018-03-20\n\n\nThe short API names \nF\n, \nT\n, \nL\n etc. have been aliased to longer names:\n\nFeature\n, \nText\n, \nLocality\n, etc.\n\n\n3.2.2\n\u00b6\n\n\n2018-02-27\n\n\nRemoved the sub module \ncunei.py\n. It is better to keep corpus dependent modules\nin outside the TF package.\n\n\n3.2.1\n\u00b6\n\n\n2018-02-26\n\n\nAdded a sub module \ncunei.py\n, which contains methods to produce ATF\ntranscriptions for nodes of certain types.\n\n\n3.2\n\u00b6\n\n\n2018-02-19\n\n\nAPI change\n Previously, the functions \nL.d()\n and \nL.u()\n took rank into\naccount. In the Hebrew Bible, that means that \nL.d(sentence)\n will not return a\nverse, even if the verse is contained in the sentence.\n\n\nThis might be handy for sentences and verses, but in general this behaviour\ncauses problems. It also disturbs the expectation that with these functions you\nget \nall\n embedders and embeddees.\n\n\nSo we have lifted this restriction. Still, the results of the \nL\n functions have\nan ordering that takes into account the levels of the returned nodes.\n\n\nEnhancement\n Previously, Text-Fabric determined the levels of node types\nautomatically, based on the average slot-size of nodes within the node types. So\nbooks get a lower level than chapters than verses than phrases, etc.\n\n\nHowever, working with cuneiform tablets revealed that containing node types may\nhave a smaller average size than contained node types. This happens when a\ncontainer type only contains small instances of the contained type and not the\nbigger ones.\n\n\nNow you can override the computation by text-fabric by means of a key-value in\nthe \notext\n feature. See the \napi\n.\n\n\n3.1.5\n\u00b6\n\n\n2018-02-15\n\n\nFixed a small problem in \nsectionFromNode(n)\n when \nn\n is a node within a\nprimary section but outside secondary/tertiary sections.\n\n\n3.1.4\n\u00b6\n\n\n2018-02-15\n\n\nSmall fix in the Text API. If your data set does not have language dependent\nfeatures, for section level 1 headings, such as \nbook@en\n, \nbook@sw\n, the Text\nAPI will not break, and the plain \nbook\n feature will be taken always.\n\n\nWe also reformatted all code with a PEP8 code formatter.\n\n\n3.1.3\n\u00b6\n\n\n2018-01-29\n\n\nSmall adaptions in conversion from MQL to TF, it can now also convert the MQL\ncoming from CALAP dataset (Syriac).\n\n\n3.1.2\n\u00b6\n\n\n2018-01-27\n\n\nNothing changed, only the names of some variables and the text of some messages.\nThe terminology has been made more consistent with the fabric metaphor, in\nparticular, \ngrid\n has been replaced by \nwarp\n.\n\n\n3.1.1\n\u00b6\n\n\n2017-10-21\n\n\nThe \nexportMQL()\n function now generates one single enumeration type that serves\nfor all enumeration features. That makes it possible to compare values of different\nenumeration features with each other, such as \nps\n and \nprs_ps\n.\n\n\n3.1\n\u00b6\n\n\n2017-10-20\n\n\nThe \nexportMQL()\n function now generates enumeration types for features, if\ncertain conditions are fulfilled. That makes it possible to query those features\nwith the \nIN\n relationship of MQL, like \n[chapter book IN (Genesis, Exodus)]\n.\n\n\n3.0.8\n\u00b6\n\n\n2017-10-07\n\n\nWhen reading edges with values, also the edges without a value are taken in.\n\n\n3.0.7\n\u00b6\n\n\n2017-10-07\n\n\nEdges with edge values did not allow for the absence of values. Now they do.\n\n\n3.0.6\n\u00b6\n\n\n2017-10-05\n\n\nA major tweak in the \nimportMQL()\n function so that it can\nhandle gaps in the monad sequence. The issue arose when converting MQL for\nversion 3 of the \nBHSA\n. In that version there\nare somewhat arbitrary gaps in the monad sequence between the books of the\nHebrew Bible. I transform a gapped sequence of monads into a continuous sequence\nof slots.\n\n\n3.0.5\n\u00b6\n\n\n2017-10-05\n\n\nAnother little tweak in the \nimportMQL()\n function so that it\ncan handle more patterns in the MQL dump file. The issue arose when converting\nMQL for version 3 of the \nBHSA\n.\n\n\n3.0.4\n\u00b6\n\n\n2017-10-04\n\n\nLittle tweak in the \nimportMQL()\n function so that it can handle\nmore patterns in the MQL dump file. The issue arose when converting MQL for\n\nextrabiblical\n material.\n\n\n3.0.2, 3.0.3\n\u00b6\n\n\n2017-10-03\n\n\nNo changes, only an update of the package metadata, to reflect that Text-Fabric\nhas moved from \nETCBC\n to\n\nDans-labs\n.\n\n\n3.0.1\n\u00b6\n\n\n2017-10-02\n\n\nBug fix in reading edge features with values.\n\n\n3.0.0\n\u00b6\n\n\n2017-10-02\n\n\nMQL! You can now convert MQL data into a TF dataset:\n\nimportMQL()\n. We had already \nexportMQL()\n.\n\n\nThe consequence is that we can operate with much agility between the worlds of\nMQL and TF.\n\n\nWe can start with source data in MQL, convert it to TF, combine it with other TF\ndata sources, compute additional stuff and add it, and then finally export it as\nenriched MQL, so that the enriched data can be queried by MQL.\n\n\n2.3.15\n\u00b6\n\n\n2017-09-29\n\n\nCompletion: TF defines the concept of\n\nedges\n that\ncarry a value. But so far we have not used them. It turned out that it was\nimpossible to let TF know that an edge carries values, when\n\nsaving\n data\nas a new feature. Now it is possible.\n\n\n2.3.14\n\u00b6\n\n\n2017-09-29\n\n\nBug fix: it was not possible to get\n\nT.nodeFromSection(('2_Chronicles', 36, 23))\n, the last verse in the Bible.\n\n\nThis is the consequence of a bug in precomputing the sections\n\nsections\n. The\npreparation step used\n\n\n1\nrange\n(\nfirstVerse\n,\n \nlastVerse\n)\n\n\n\n\n\n\n\nsomewhere, which should of course have been\n\n\n1\nrange\n(\nfirstVerse\n,\n \nlastVerse\n \n+\n \n1\n)\n\n\n\n\n\n\n\n2.3.13\n\u00b6\n\n\n2017-09-28\n\n\nLoading TF was not completely silent if \nsilent=True\n was passed. Better now.\n\n\n2.3.12\n\u00b6\n\n\n2017-09-18\n\n\n\n\n\n\nSmall fix in\n    \nTF.save()\n.\n    The spec says that the metadata under the empty key will be inserted into all\n    features, but in fact this did not happen. Instead it was used as a default\n    when some feature did not have metadata specified.\n\n\nFrom now on, that metadata will spread through all features.\n\n\n\n\n\n\nNew API function \nexplore\n, to get a list of all known\n    features in a dataset.\n\n\n\n\n\n\n2.3.11\n\u00b6\n\n\n2017-09-18\n\n\n\n\nSmall fix in Search: the implementation of the relation operator \n||\n\n    (disjoint slot sets) was faulty. Repaired.\n\n\nThe\n    \nsearch tutorial\n\n    got an extra example: how to look for gaps. Gaps are not a primitive in the TF\n    search language. Yet the language turns out to be powerful enough to look for\n    gaps. This answers a question by Cody Kingham.\n\n\n\n\n2.3.10\n\u00b6\n\n\n2017-08-24\n\n\nWhen defining text formats in the \notext.tf\n feature, you can now include\nnewlines and tabs in the formats. Enter them as \n\\n\n and \n\\t\n.\n\n\n2.3.9\n\u00b6\n\n\n2017-07-24\n\n\nTF has a list of default locations to look for data sources: \n~/Downloads\n,\n\n~/github\n, etc. Now \n~/Dropbox\n has been added to that list.\n\n\n2.3.8\n\u00b6\n\n\n2017-07-24\n\n\nThe section levels (book, chapter, verse) were supposed to be customizable\nthrough the \notext\n feature. But in\nfact, up till version 2.3.7 this did not work. From now on the names of the\nsection types and the features that name/number them, are given in the \notext\n\nfeature. It is still the case that exactly three levels must be specified,\notherwise it does not work.\n\n\n2.3.7\n\u00b6\n\n\n2017-05-12\n\n\nFixes. Added an extra default location for looking for text-fabric-data sources,\nfor the benefit of running text-fabric within a shared notebook service.\n\n\n2.3.5, 2.3.6\n\u00b6\n\n\n2017-03-01\n\n\nBug fix in Search. Spotted by Cody Kingham. Relational operators between atoms\nin the template got discarded after an outdent.\n\n\n2.3.4\n\u00b6\n\n\n2017-02-12\n\n\nAlso the \nFabric()\n call can be made silent now.\n\n\n2.3.3\n\u00b6\n\n\n2017-02-11\n\n\nImprovements:\n\n\n\n\nyou can load features more silently. See \nTF.load()\n;\n\n\nyou can search more silently. See \nS.study()\n;\n\n\nyou can search more concisely. See the new \nS.search()\n;\n\n\nwhen fetching results, the \namount\n parameter of\n    \nS.fetch()\n has been renamed to \nlimit\n;\n\n\nthe tutorial notebooks (see links on top) have been updated.\n\n\n\n\n2.3.2\n\u00b6\n\n\n2017-02-03\n\n\nBug fix: the results of \nF.feature.s()\n, \nE.feature.f()\n, and \nE.features.t()\n\nare now all tuples. They were a mixture of tuples and lists.\n\n\n2.3.1\n\u00b6\n\n\n2017-01-23\n\n\nBug fix: when searching simple queries with only one query node, the result\nnodes were delivered as integers, instead of 1-tuples of integers.\n\n\n2.3\n\u00b6\n\n\n2017-01-13\n\n\nWe start archiving releases of Text-Fabric at \nZenodo\n.\n\n\n2.2.1\n\u00b6\n\n\n2017-01-09\n\n\nSmall fixes.\n\n\n2.2.0\n\u00b6\n\n\n2017-01-06\n\n\nNew: sortKey\n\u00b6\n\n\nThe API has a new member: \nsortKey\n\n\nNew relationships in templates: \nnearness\n. See for\nexamples the end of the\n\nsearchTutorial\n.\nThanks to James Cu\u00e9nod for requesting nearness operators.\n\n\nFixes\n\u00b6\n\n\n\n\nin \nS.glean()\n word nodes were not printed;\n\n\nthe check whether the search graph consists of a single connected component\n    did not handle the case of one node without edges well;\n\n\n\n\n2.1.3\n\u00b6\n\n\n2017-01-04\n\n\nVarious fixes.\n\n\n2.1.0\n\u00b6\n\n\n2017-01-04\n\n\nNew: relations\n\u00b6\n\n\nSome relations have been added to search templates:\n\n\n\n\n=:\n and \n:=\n and \n::\n: \nstart at same slot\n, \nend at same slot\n, \nstart at\n    same slot and end at same slot\n\n\n<:\n and \n:>\n: \nadjacent before\n and \nadjacent next\n.\n\n\n\n\nThe latter two can also be used through the \nL\n-api: \nL.p()\n and \nL.n()\n.\n\n\nThe data that feeds them is precomputed and available as \nC.boundary\n.\n\n\nNew: enhanced search templates\n\u00b6\n\n\nYou can now easily make extra constraints in search templates without naming\natoms.\n\n\nSee the\n\nsearchTutorial\n\nfor an updated exposition on searching.\n\n\n2.0.0\n\u00b6\n\n\n2016-12-23\n\n\nNew: Search\n\u00b6\n\n\n\n\nWant to feel cosy with Christmas? Put your laptop on your lap, update\nText-Fabric, and start playing with search. Your laptop will spin itself warm\nwith your queries!\n\n\nText-Fabric just got a powerful search facility, based on (graph)-templates.\n\n\nIt is still very fresh, and more experimentation will be needed. Feedback is\nwelcome.\n\n\nStart with the\n\ntutorial\n.\n\n\nThe implementation of this search engine can be nicely explained with a textile\nmetaphor: spinning wool into yarn and then stitching the yarns together.\n\n\nThat will be explained further in a document that I'll love to write during\nXmas.\n\n\n1.2.7\n\u00b6\n\n\n2016-12-14\n\n\nNew\n\u00b6\n\n\nF.otype.sInterval()\n\n\n1.2.6\n\u00b6\n\n\n2016-12-14\n\n\nbug fix\n\u00b6\n\n\nThere was an error in computing the order of nodes. One of the consequences was\nthat objects that occupy the same slots were not ordered properly. And that had\nas consequence that you could not go up from words in one-word phrases to their\ncontaining phrase.\n\n\nIt has been remedied.\n\n\nNote\nYour computed data needs to be refreshed. This can be done by calling a new\nfunction \nTF.clearCache()\n. When you use TF after\nthis, you will see it working quite hard to recompute a bunch of data.\n1.2.5\n\u00b6\n\n\n2016-12-13\n\n\nDocumentation update\n\n\n1.2.0\n\u00b6\n\n\n2016-12-08\n\n\nNote\nData update needed\nNew\n\u00b6\n\n\nFrequency lists\n\u00b6\n\n\nF.feature.freqList()\n: get a sorted frequency list for any\nfeature. Handy as a first step in exploring a feature.\n\n\nExport to MQL\n\u00b6\n\n\nTF.exportMQL()\n: export a whole dataset as a MQL database.\nIncluding all modules that you have loaded with it.\n\n\nChanged\n\u00b6\n\n\nThe slot numbers start at 0, no longer at 1. Personally I prefer the zero\nstarting point, but Emdros insists on positive monads and objects ids. Most\nimportant is that users do not have to add/subtract one from the numbers they\nsee in TF if they want to use it in MQL and vice versa.\n\n\nBecause of this you need to update your data too:\n\n\n1\n2\n    \ncd\n ~/github/text-fabric-data\n    git pull origin master",
            "title": "News"
        },
        {
            "location": "/News/#changes",
            "text": "Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off\nall possibilities: bhsa  cunei",
            "title": "Changes"
        },
        {
            "location": "/News/#558",
            "text": "2018-07-10  Better in catching out-of-memory errors.\nPrevents creation of corrupt compiled binary TF data.",
            "title": "5.5.8"
        },
        {
            "location": "/News/#557",
            "text": "2018-07-09  Optimization is export from TF browser.",
            "title": "5.5.7"
        },
        {
            "location": "/News/#556",
            "text": "2018-07-09  Better help display.   The opened-state of help sections is remembered.  You can open help next to an other open section in the sidebar.",
            "title": "5.5.6"
        },
        {
            "location": "/News/#555",
            "text": "2018-07-08  Crisper icon.",
            "title": "5.5.5"
        },
        {
            "location": "/News/#554",
            "text": "2018-07-6  Docs updated. Little bit of refactoring.",
            "title": "5.5.4"
        },
        {
            "location": "/News/#551-3",
            "text": "2018-07-4  In the TF browser, use a selection of all the features when working with the BHSA.\nOtherwise in Windows you might run out of memory, even if you have 8GB RAM.",
            "title": "5.5.1-3"
        },
        {
            "location": "/News/#55",
            "text": "2018-07-4  Text-Fabric can download data for BHSA and Cunei. You do not have to clone github repositories for that.\nThe data downloaded by Text-Fabric ends up in  text-fabric-data  under your home directory.",
            "title": "5.5"
        },
        {
            "location": "/News/#545-7",
            "text": "2018-07-03  Experimenting with setuptools to get the text-fabric script working\non Windows.",
            "title": "5.4.5-7"
        },
        {
            "location": "/News/#544",
            "text": "2018-07-02  Added renaming/duplicating of jobs and change of directory.",
            "title": "5.4.4"
        },
        {
            "location": "/News/#543",
            "text": "2018-06-29  Small fixes in error reporting in search.",
            "title": "5.4.3"
        },
        {
            "location": "/News/#541-2",
            "text": "2018-06-28  Text-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata.  Small fixes.",
            "title": "5.4.1-2"
        },
        {
            "location": "/News/#54",
            "text": "2018-06-26  Improved interface and functionality of the text-fabric browser:   you can save your work  you can enter verse references and tablet P numbers  there is help  there is a side bar   Docs not up to date The API docs are not up-to-date: there are new functions in the Bhsa and Cunei APIs.\nThe server/service/client apis are not completely spelled out.\nHowever, the help for the text-fabric browser is included in the interface itself.",
            "title": "5.4"
        },
        {
            "location": "/News/#533",
            "text": "2018-06-23  Small fix: command line args for text-fabric.",
            "title": "5.3.3"
        },
        {
            "location": "/News/#530-2",
            "text": "2018-06-22  Better process management When the TF server is started, it cleans up remnant process that might get in the way otherwise.\nYou can also say 1 text-fabric -k   to kill all remnant processes,\nor 1 text-fabric -k datasource   to kill the processes for a specific datasource only. Manual node entry You can enter nodes manually in the TF browser.\nHandy for quick inspection.\nYou can click on the sequence number to\nappend the node tuple of that row to the tuple input field.\nThat way you can collect interesting results. Name and Description You can enter a name which will be used as title and file name during export. You can enter a description in Markdown.\nWhen you export your query, the description appears\nformatted on top. Provenance If you export a query, provenance is added, using DOIs. Small fixes No more blank pages due to double page breaks.",
            "title": "5.3.0-2"
        },
        {
            "location": "/News/#521",
            "text": "2018-06-21   Added an  expand all  checkbox in the text-fabric browser,\n  to expand all shown rows or to collapse them.  Export function for search results in the text-fabric browser.\n  What you see is what you get, 1 pretty display per page if you have\n  the browser save it to pdf.  Small tweaks",
            "title": "5.2.1"
        },
        {
            "location": "/News/#51",
            "text": "2018-06-21  When displaying results in condensed mode, you\ncan now choose the level of the container in which results are highlighted.\nSo far it was fixed to  verse  for the bhsa and  tablet  for cunei.  The docs are lagging behind!\nBut it is shown in the tutorials and you can observer it in the text-fabric browser.",
            "title": "5.1"
        },
        {
            "location": "/News/#501234",
            "text": "2018-06-19  Addressed start-up problems.",
            "title": "5.0.1,2,3,4"
        },
        {
            "location": "/News/#50",
            "text": "2018-06-18  Built in web server and client for local query running.\nIt is implemented for Bhsa and Cunei.",
            "title": "5.0"
        },
        {
            "location": "/News/#4423",
            "text": "2018-06-13  New distribution method with setuptools.\nText-Fabric has now dependencies on modules rpyc and bottle,\nbecause it contains a built-in data service and webserver.  This website is still barely functional, though.",
            "title": "4.4.2,3"
        },
        {
            "location": "/News/#441",
            "text": "2018-06-10  Search API:  Escapes in regular expression search was buggy and convoluted.\nIf a feature value contains a  |  then in an RE you have to enter  \\|  to match it.\nBut to have that work in a TF search, you needed to say  \\\\\\| .   On the other hand, in the same case for  .  instead of  | , you could just sat  \\.  In the new situation you do not have to double escape in REs anymore.\nYou can just say  \\|  and  \\. .",
            "title": "4.4.1"
        },
        {
            "location": "/News/#44",
            "text": "2018-06-06  Search API:  S.search() accepts a new optional parameter:  withContext .\nIt triggers the output of context information for nodes in the result tuples.",
            "title": "4.4"
        },
        {
            "location": "/News/#434-435",
            "text": "2018-06-05  Search API:  The  /with/ /or/ /or/ /-/' quantifier is also allowed with zero /or/` s.  Small fix in the  /with/  quantifier if there are quantifiers between this one and its parent atom.",
            "title": "4.3.4, 4.3.5"
        },
        {
            "location": "/News/#433",
            "text": "2018-06-01  Search API:  Improved quantifiers in search:    /where/   /have/   /without/   /with/   /or/   /-/ ;  much clearer indentation rules (no caret anymore);  better reporting by  S.study() .",
            "title": "4.3.3"
        },
        {
            "location": "/News/#432",
            "text": "2018-05-31  Search API:    quantifiers may use the name  ..  to refer to their parents  you may use names in the place of atoms, which lessens the need for constructs with  p = q  stricter checks on the syntax and position of quantifiers",
            "title": "4.3.2"
        },
        {
            "location": "/News/#431",
            "text": "2018-05-30  Docs and metadata update",
            "title": "4.3.1"
        },
        {
            "location": "/News/#430",
            "text": "2018-05-30    API Change in Search.  In search templates I recently added things like  1   word vt!   which checks for words that do not have a value for feature  vt .  The syntax for this has now changed to  1   word vt#     Unequal (#) in feature value conditions.  Now you can say things like  1   word vt#infa|infc   meaning that the value of feature is not one of  infa ,  infc .  So, in addition to  =  we have  #  for \"not equal\".\n*   Quantifiers.  You can now use quantifiers in search. One of them is like  NOTEXIST  in MQL.\nSee the  docs    A number of minor fixes.",
            "title": "4.3.0"
        },
        {
            "location": "/News/#421",
            "text": "2018-05-25   Several improvements in the pretty display in Bhsa and Cunei APIs   Under the hood changes in  S.search()  to prepare for  quantifiers  in search templates.   Tokenisation of quantifiers already works  Searches can now spawn auxiliary searches without polluting intermediate data  This has been done by promoting the  S  API to a factory of search engines.\n    By deafault,  S  creates and maintains a single factory, so to the user\n    it is the same  S . But when it needs to run a query in the middle of processing another query\n    it can just spawn another search engine to do that, without interfering with the\n    original search.     NB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten.",
            "title": "4.2.1"
        },
        {
            "location": "/News/#42",
            "text": "2018-05-23  The Search API has been extended:   you can use custom sets in your query templates  you can search in shallow mode: instead of full result tuples, you just get a set\n    of the top-level thing you mention in your template.   This functionality is a precursor for quantifiers in search templates \nbut is also a powerful addition to search in its own right.",
            "title": "4.2"
        },
        {
            "location": "/News/#412",
            "text": "2018-05-17  Bhsa and Cunei APIs:   custom highlight colours also work for condensed results.  you can pass the  highlights  parameter also to  show  and  prettyTuple",
            "title": "4.1.2"
        },
        {
            "location": "/News/#411",
            "text": "2018-05-16  Bhsa API: you can customize the features that are shown in pretty displays.",
            "title": "4.1.1"
        },
        {
            "location": "/News/#41",
            "text": "2018-05-16  Bhsa and Cunei APIs: you can customize the highlighting of search results:   different colours for different parts of the results  you can choose your colours freely from all that CSS has to offer.   See the updated search tutorials.",
            "title": "4.1"
        },
        {
            "location": "/News/#403",
            "text": "2018-05-11  No changes, just quirks in the update process to get a new version of TF out.",
            "title": "4.0.3"
        },
        {
            "location": "/News/#401",
            "text": "2018-05-11  Documentation updates.",
            "title": "4.0.1"
        },
        {
            "location": "/News/#400",
            "text": "2018-05-11   Additions to Search.\n    You can now include the values of edges in your search templates.  F. feature .freqList()  accepts a new parameter:  nodeTypes . It will restrict its results to nodes in\n    one of the types in  nodeTypes .   You can now also do  E. feature .freqList() .\n    It will count the number of edges if the edge is declared to be without values, \n    or it will give a frequency list of the edges by value if the edge has values.\n    Like  F.freqList , you can pass parameters to constrain the frequency list to certain node types.\n    You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive\n    ( nodeTypesTo ).  New documentation system based on  MkDocs .",
            "title": "4.0.0"
        },
        {
            "location": "/News/#3412",
            "text": "2018-05-02  The Cunei and Bhsa APIs show the version of Text-Fabric that is being called.",
            "title": "3.4.12"
        },
        {
            "location": "/News/#3411",
            "text": "2018-05-01  Cunei   cases are divided horizontally and vertically, alternating with their\n    nesting level;  cases have a feature  depth  now, indicating at which level of nesting they\n    are.",
            "title": "3.4.11"
        },
        {
            "location": "/News/#348-9-10",
            "text": "2018-04-30  Various small fixes, such as:    Bhsa: Lexeme links in pretty displays.    Cunei: Prevented spurious  </div>  in NbViewer.",
            "title": "3.4.8-9-10"
        },
        {
            "location": "/News/#347",
            "text": "Cunei: Modified local image names",
            "title": "3.4.7"
        },
        {
            "location": "/News/#346",
            "text": "Small tweaks in search.",
            "title": "3.4.6"
        },
        {
            "location": "/News/#345",
            "text": "2018-04-28  Bhsa API:   new functions  plain()  and  table()  for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by  pretty()  and\n     show() .",
            "title": "3.4.5"
        },
        {
            "location": "/News/#344",
            "text": "2018-04-27  Cunei API:   new functions  plain()  and  table()  for plainly representing nodes, tuples\n    and result lists, as opposed to the abundant representations by  pretty()  and\n     show() .",
            "title": "3.4.4"
        },
        {
            "location": "/News/#342",
            "text": "2018-04-26  Better search documentation.  Cunei API: small fixes.",
            "title": "3.4.2"
        },
        {
            "location": "/News/#341",
            "text": "2018-04-25  Bhsa API:   Search/show: you can now show results condensed: i.e. a list of passages with\n    highlighted results is returned. This is how SHEBANQ represents the results of\n    a query. If you have two results in the same verse, with  condensed=True  you\n    get one verse display with two highlights, with  condensed=False  you get two\n    verse displays with one highlight each.   Cunei API:   Search/show: the  pretty ,  prettyTuple ,  show  functions of the Bhsa API\n    have bee translated to the Cunei API. You can now get  very  pretty displays\n    of search results.",
            "title": "3.4.1"
        },
        {
            "location": "/News/#34",
            "text": "2018-04-23  Search :   You can use regular expressions to specify feature values in queries.  You could already search for nodes which have a non-None value for a certain\n    feature. Now you can also search for the complement: nodes that do not have a\n    certain feature.   Bhsa API:  The display of query results also works with lexeme nodes.",
            "title": "3.4"
        },
        {
            "location": "/News/#334",
            "text": "2018-04-20  Cunei API: Better height and width control for images. Leaner captions.",
            "title": "3.3.4"
        },
        {
            "location": "/News/#333",
            "text": "2018-04-19  Cunei API:  casesByLevel()  returns case nodes in corpus order.",
            "title": "3.3.3"
        },
        {
            "location": "/News/#332",
            "text": "2018-04-18  Change in the Cunei api reflecting that undivided lines have no cases now (was:\nthey had a single case with the same material as the line). Also: the feature fullNumber  on cases is now called  number , and contains the full hierarchical\npart leading to a case. There is an extra feature  terminal  on lines and cases\nif they are not subdivided.  Changes in Cunei and Bhsa api:   fixed a bug that occurred when working outside a GitHub repository.",
            "title": "3.3.2"
        },
        {
            "location": "/News/#331",
            "text": "2018-04-18  Change in the Cunei api.  casesByLevel()  now takes an optional argument terminal  instead of  withChildren , with opposite values.  withChildren=False  is ambiguous: will it deliver only cases that have no\nchildren (intended), or will it deliver cases and their children (understood,\nbut not intended).  terminal=True : delivers only cases that are terminal.  terminal=False : delivers all cases at that level.",
            "title": "3.3.1"
        },
        {
            "location": "/News/#33",
            "text": "2018-04-14  Small fix in the bhsa api.  Bumped the version number because of the inclusion of corpus specific APIs.",
            "title": "3.3"
        },
        {
            "location": "/News/#326",
            "text": "2018-04-14   Text-Fabric now contains corpus specific extras:  bhsa.py  for the Hebrew Bible (BHSA)  cunei.py  for the Proto-Cuneiform corpus Uruk    The  Fabric(locations=locations, modules=modules)  constructor now uses  [''] \n    as default value for modules. Now you can use the  locations  parameter on its\n    own to specify the search paths for TF features, leaving the  modules \n    parameter undefined, if you wish.",
            "title": "3.2.6"
        },
        {
            "location": "/News/#325",
            "text": "2018-03-23  Enhancement in search templates: you can now test for the presence of features.\nTill now, you could only test for one or more concrete values of features. So,\nin addition to things like  1 word number=plural tense=yiqtol   you can also say things like  1 word number=plural tense   and it will give you words in the plural that have a tense.",
            "title": "3.2.5"
        },
        {
            "location": "/News/#324",
            "text": "2018-03-20  The short API names  F ,  T ,  L  etc. have been aliased to longer names: Feature ,  Text ,  Locality , etc.",
            "title": "3.2.4"
        },
        {
            "location": "/News/#322",
            "text": "2018-02-27  Removed the sub module  cunei.py . It is better to keep corpus dependent modules\nin outside the TF package.",
            "title": "3.2.2"
        },
        {
            "location": "/News/#321",
            "text": "2018-02-26  Added a sub module  cunei.py , which contains methods to produce ATF\ntranscriptions for nodes of certain types.",
            "title": "3.2.1"
        },
        {
            "location": "/News/#32",
            "text": "2018-02-19  API change  Previously, the functions  L.d()  and  L.u()  took rank into\naccount. In the Hebrew Bible, that means that  L.d(sentence)  will not return a\nverse, even if the verse is contained in the sentence.  This might be handy for sentences and verses, but in general this behaviour\ncauses problems. It also disturbs the expectation that with these functions you\nget  all  embedders and embeddees.  So we have lifted this restriction. Still, the results of the  L  functions have\nan ordering that takes into account the levels of the returned nodes.  Enhancement  Previously, Text-Fabric determined the levels of node types\nautomatically, based on the average slot-size of nodes within the node types. So\nbooks get a lower level than chapters than verses than phrases, etc.  However, working with cuneiform tablets revealed that containing node types may\nhave a smaller average size than contained node types. This happens when a\ncontainer type only contains small instances of the contained type and not the\nbigger ones.  Now you can override the computation by text-fabric by means of a key-value in\nthe  otext  feature. See the  api .",
            "title": "3.2"
        },
        {
            "location": "/News/#315",
            "text": "2018-02-15  Fixed a small problem in  sectionFromNode(n)  when  n  is a node within a\nprimary section but outside secondary/tertiary sections.",
            "title": "3.1.5"
        },
        {
            "location": "/News/#314",
            "text": "2018-02-15  Small fix in the Text API. If your data set does not have language dependent\nfeatures, for section level 1 headings, such as  book@en ,  book@sw , the Text\nAPI will not break, and the plain  book  feature will be taken always.  We also reformatted all code with a PEP8 code formatter.",
            "title": "3.1.4"
        },
        {
            "location": "/News/#313",
            "text": "2018-01-29  Small adaptions in conversion from MQL to TF, it can now also convert the MQL\ncoming from CALAP dataset (Syriac).",
            "title": "3.1.3"
        },
        {
            "location": "/News/#312",
            "text": "2018-01-27  Nothing changed, only the names of some variables and the text of some messages.\nThe terminology has been made more consistent with the fabric metaphor, in\nparticular,  grid  has been replaced by  warp .",
            "title": "3.1.2"
        },
        {
            "location": "/News/#311",
            "text": "2017-10-21  The  exportMQL()  function now generates one single enumeration type that serves\nfor all enumeration features. That makes it possible to compare values of different\nenumeration features with each other, such as  ps  and  prs_ps .",
            "title": "3.1.1"
        },
        {
            "location": "/News/#31",
            "text": "2017-10-20  The  exportMQL()  function now generates enumeration types for features, if\ncertain conditions are fulfilled. That makes it possible to query those features\nwith the  IN  relationship of MQL, like  [chapter book IN (Genesis, Exodus)] .",
            "title": "3.1"
        },
        {
            "location": "/News/#308",
            "text": "2017-10-07  When reading edges with values, also the edges without a value are taken in.",
            "title": "3.0.8"
        },
        {
            "location": "/News/#307",
            "text": "2017-10-07  Edges with edge values did not allow for the absence of values. Now they do.",
            "title": "3.0.7"
        },
        {
            "location": "/News/#306",
            "text": "2017-10-05  A major tweak in the  importMQL()  function so that it can\nhandle gaps in the monad sequence. The issue arose when converting MQL for\nversion 3 of the  BHSA . In that version there\nare somewhat arbitrary gaps in the monad sequence between the books of the\nHebrew Bible. I transform a gapped sequence of monads into a continuous sequence\nof slots.",
            "title": "3.0.6"
        },
        {
            "location": "/News/#305",
            "text": "2017-10-05  Another little tweak in the  importMQL()  function so that it\ncan handle more patterns in the MQL dump file. The issue arose when converting\nMQL for version 3 of the  BHSA .",
            "title": "3.0.5"
        },
        {
            "location": "/News/#304",
            "text": "2017-10-04  Little tweak in the  importMQL()  function so that it can handle\nmore patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical  material.",
            "title": "3.0.4"
        },
        {
            "location": "/News/#302-303",
            "text": "2017-10-03  No changes, only an update of the package metadata, to reflect that Text-Fabric\nhas moved from  ETCBC  to Dans-labs .",
            "title": "3.0.2, 3.0.3"
        },
        {
            "location": "/News/#301",
            "text": "2017-10-02  Bug fix in reading edge features with values.",
            "title": "3.0.1"
        },
        {
            "location": "/News/#300",
            "text": "2017-10-02  MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already  exportMQL() .  The consequence is that we can operate with much agility between the worlds of\nMQL and TF.  We can start with source data in MQL, convert it to TF, combine it with other TF\ndata sources, compute additional stuff and add it, and then finally export it as\nenriched MQL, so that the enriched data can be queried by MQL.",
            "title": "3.0.0"
        },
        {
            "location": "/News/#2315",
            "text": "2017-09-29  Completion: TF defines the concept of edges  that\ncarry a value. But so far we have not used them. It turned out that it was\nimpossible to let TF know that an edge carries values, when saving  data\nas a new feature. Now it is possible.",
            "title": "2.3.15"
        },
        {
            "location": "/News/#2314",
            "text": "2017-09-29  Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible.  This is the consequence of a bug in precomputing the sections sections . The\npreparation step used  1 range ( firstVerse ,   lastVerse )    somewhere, which should of course have been  1 range ( firstVerse ,   lastVerse   +   1 )",
            "title": "2.3.14"
        },
        {
            "location": "/News/#2313",
            "text": "2017-09-28  Loading TF was not completely silent if  silent=True  was passed. Better now.",
            "title": "2.3.13"
        },
        {
            "location": "/News/#2312",
            "text": "2017-09-18    Small fix in\n     TF.save() .\n    The spec says that the metadata under the empty key will be inserted into all\n    features, but in fact this did not happen. Instead it was used as a default\n    when some feature did not have metadata specified.  From now on, that metadata will spread through all features.    New API function  explore , to get a list of all known\n    features in a dataset.",
            "title": "2.3.12"
        },
        {
            "location": "/News/#2311",
            "text": "2017-09-18   Small fix in Search: the implementation of the relation operator  || \n    (disjoint slot sets) was faulty. Repaired.  The\n     search tutorial \n    got an extra example: how to look for gaps. Gaps are not a primitive in the TF\n    search language. Yet the language turns out to be powerful enough to look for\n    gaps. This answers a question by Cody Kingham.",
            "title": "2.3.11"
        },
        {
            "location": "/News/#2310",
            "text": "2017-08-24  When defining text formats in the  otext.tf  feature, you can now include\nnewlines and tabs in the formats. Enter them as  \\n  and  \\t .",
            "title": "2.3.10"
        },
        {
            "location": "/News/#239",
            "text": "2017-07-24  TF has a list of default locations to look for data sources:  ~/Downloads , ~/github , etc. Now  ~/Dropbox  has been added to that list.",
            "title": "2.3.9"
        },
        {
            "location": "/News/#238",
            "text": "2017-07-24  The section levels (book, chapter, verse) were supposed to be customizable\nthrough the  otext  feature. But in\nfact, up till version 2.3.7 this did not work. From now on the names of the\nsection types and the features that name/number them, are given in the  otext \nfeature. It is still the case that exactly three levels must be specified,\notherwise it does not work.",
            "title": "2.3.8"
        },
        {
            "location": "/News/#237",
            "text": "2017-05-12  Fixes. Added an extra default location for looking for text-fabric-data sources,\nfor the benefit of running text-fabric within a shared notebook service.",
            "title": "2.3.7"
        },
        {
            "location": "/News/#235-236",
            "text": "2017-03-01  Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms\nin the template got discarded after an outdent.",
            "title": "2.3.5, 2.3.6"
        },
        {
            "location": "/News/#234",
            "text": "2017-02-12  Also the  Fabric()  call can be made silent now.",
            "title": "2.3.4"
        },
        {
            "location": "/News/#233",
            "text": "2017-02-11  Improvements:   you can load features more silently. See  TF.load() ;  you can search more silently. See  S.study() ;  you can search more concisely. See the new  S.search() ;  when fetching results, the  amount  parameter of\n     S.fetch()  has been renamed to  limit ;  the tutorial notebooks (see links on top) have been updated.",
            "title": "2.3.3"
        },
        {
            "location": "/News/#232",
            "text": "2017-02-03  Bug fix: the results of  F.feature.s() ,  E.feature.f() , and  E.features.t() \nare now all tuples. They were a mixture of tuples and lists.",
            "title": "2.3.2"
        },
        {
            "location": "/News/#231",
            "text": "2017-01-23  Bug fix: when searching simple queries with only one query node, the result\nnodes were delivered as integers, instead of 1-tuples of integers.",
            "title": "2.3.1"
        },
        {
            "location": "/News/#23",
            "text": "2017-01-13  We start archiving releases of Text-Fabric at  Zenodo .",
            "title": "2.3"
        },
        {
            "location": "/News/#221",
            "text": "2017-01-09  Small fixes.",
            "title": "2.2.1"
        },
        {
            "location": "/News/#220",
            "text": "2017-01-06",
            "title": "2.2.0"
        },
        {
            "location": "/News/#new-sortkey",
            "text": "The API has a new member:  sortKey  New relationships in templates:  nearness . See for\nexamples the end of the searchTutorial .\nThanks to James Cu\u00e9nod for requesting nearness operators.",
            "title": "New: sortKey"
        },
        {
            "location": "/News/#fixes",
            "text": "in  S.glean()  word nodes were not printed;  the check whether the search graph consists of a single connected component\n    did not handle the case of one node without edges well;",
            "title": "Fixes"
        },
        {
            "location": "/News/#213",
            "text": "2017-01-04  Various fixes.",
            "title": "2.1.3"
        },
        {
            "location": "/News/#210",
            "text": "2017-01-04",
            "title": "2.1.0"
        },
        {
            "location": "/News/#new-relations",
            "text": "Some relations have been added to search templates:   =:  and  :=  and  :: :  start at same slot ,  end at same slot ,  start at\n    same slot and end at same slot  <:  and  :> :  adjacent before  and  adjacent next .   The latter two can also be used through the  L -api:  L.p()  and  L.n() .  The data that feeds them is precomputed and available as  C.boundary .",
            "title": "New: relations"
        },
        {
            "location": "/News/#new-enhanced-search-templates",
            "text": "You can now easily make extra constraints in search templates without naming\natoms.  See the searchTutorial \nfor an updated exposition on searching.",
            "title": "New: enhanced search templates"
        },
        {
            "location": "/News/#200",
            "text": "2016-12-23",
            "title": "2.0.0"
        },
        {
            "location": "/News/#new-search",
            "text": "Want to feel cosy with Christmas? Put your laptop on your lap, update\nText-Fabric, and start playing with search. Your laptop will spin itself warm\nwith your queries!  Text-Fabric just got a powerful search facility, based on (graph)-templates.  It is still very fresh, and more experimentation will be needed. Feedback is\nwelcome.  Start with the tutorial .  The implementation of this search engine can be nicely explained with a textile\nmetaphor: spinning wool into yarn and then stitching the yarns together.  That will be explained further in a document that I'll love to write during\nXmas.",
            "title": "New: Search"
        },
        {
            "location": "/News/#127",
            "text": "2016-12-14",
            "title": "1.2.7"
        },
        {
            "location": "/News/#new",
            "text": "F.otype.sInterval()",
            "title": "New"
        },
        {
            "location": "/News/#126",
            "text": "2016-12-14",
            "title": "1.2.6"
        },
        {
            "location": "/News/#bug-fix",
            "text": "There was an error in computing the order of nodes. One of the consequences was\nthat objects that occupy the same slots were not ordered properly. And that had\nas consequence that you could not go up from words in one-word phrases to their\ncontaining phrase.  It has been remedied.  Note Your computed data needs to be refreshed. This can be done by calling a new\nfunction  TF.clearCache() . When you use TF after\nthis, you will see it working quite hard to recompute a bunch of data.",
            "title": "bug fix"
        },
        {
            "location": "/News/#125",
            "text": "2016-12-13  Documentation update",
            "title": "1.2.5"
        },
        {
            "location": "/News/#120",
            "text": "2016-12-08  Note Data update needed",
            "title": "1.2.0"
        },
        {
            "location": "/News/#new_1",
            "text": "",
            "title": "New"
        },
        {
            "location": "/News/#frequency-lists",
            "text": "F.feature.freqList() : get a sorted frequency list for any\nfeature. Handy as a first step in exploring a feature.",
            "title": "Frequency lists"
        },
        {
            "location": "/News/#export-to-mql",
            "text": "TF.exportMQL() : export a whole dataset as a MQL database.\nIncluding all modules that you have loaded with it.",
            "title": "Export to MQL"
        },
        {
            "location": "/News/#changed",
            "text": "The slot numbers start at 0, no longer at 1. Personally I prefer the zero\nstarting point, but Emdros insists on positive monads and objects ids. Most\nimportant is that users do not have to add/subtract one from the numbers they\nsee in TF if they want to use it in MQL and vice versa.  Because of this you need to update your data too:  1\n2      cd  ~/github/text-fabric-data\n    git pull origin master",
            "title": "Changed"
        },
        {
            "location": "/Code/Overview/",
            "text": "Code organisation\n\u00b6\n\n\nAbout\nThe code base of Text-Fabric is evolving to a considerable\n\nsize\n.\nHowever, the code can be divided into a few major parts,\neach with their own, identifiable task.\nThe base\nThe\n\nbase\n\nof Text-Fabric is responsible for:\nData management\nText-Fabric data consists of \nfeature files\n.\nTF must be able to load them, save them, import/export from MQL.\nProvide an API\nTF must offer an API for handling its data in applications.\nThat means: feature lookup, containment lookup, text serialization.\nPrecomputation\nIn order to make its API work efficiently, TF has to precompute certain\ncompiled forms of the data.\nTF search\nTF contains a\n\nsearch engine\n\nbased on templates, which are little graphs\nof nodes and edges that must be instantiated against the corpus.\nThe template language is inspired by MQL, but has a different syntax.\nIt is both weaker and stronger than MQL.\nSearch templates are the most accessible way to get at the data,\neasier than hand-coding your own little programs.\nThe underlying engine is quite complicated.\nSometimes it is faster than hand coding,\nsometimes (much) slower.\nTF apps\nTF contains corpus-dependent\n\napps\n\nmainly for pretty displaying the contents of your corpus and automatic data loading from the internet.\nTF web interface\nTF contains a \n\nlocal web interface\n\nfor interacting with your corpus without programming.\nThe local web interface lets you fire queries (search templates) to TF and interact\nwith the results:\nexpanding rows to pretty displays;\ncondensing results to verious container types;\nexporting results as PDF and CSV.",
            "title": "Overview"
        },
        {
            "location": "/Code/Overview/#code-organisation",
            "text": "About The code base of Text-Fabric is evolving to a considerable size . However, the code can be divided into a few major parts,\neach with their own, identifiable task. The base The base \nof Text-Fabric is responsible for: Data management Text-Fabric data consists of  feature files .\nTF must be able to load them, save them, import/export from MQL. Provide an API TF must offer an API for handling its data in applications.\nThat means: feature lookup, containment lookup, text serialization. Precomputation In order to make its API work efficiently, TF has to precompute certain\ncompiled forms of the data. TF search TF contains a search engine \nbased on templates, which are little graphs\nof nodes and edges that must be instantiated against the corpus.\nThe template language is inspired by MQL, but has a different syntax.\nIt is both weaker and stronger than MQL. Search templates are the most accessible way to get at the data,\neasier than hand-coding your own little programs. The underlying engine is quite complicated.\nSometimes it is faster than hand coding,\nsometimes (much) slower. TF apps TF contains corpus-dependent apps \nmainly for pretty displaying the contents of your corpus and automatic data loading from the internet. TF web interface TF contains a  local web interface \nfor interacting with your corpus without programming. The local web interface lets you fire queries (search templates) to TF and interact\nwith the results: expanding rows to pretty displays; condensing results to verious container types; exporting results as PDF and CSV.",
            "title": "Code organisation"
        },
        {
            "location": "/Code/Stats/",
            "text": "cloc\n\n\ngithub.com/AlDanial/cloc v 1.74  T=1.96 s (30.6 files/s, 9580.8 lines/s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nfiles\n\n\nblank\n\n\ncomment\n\n\ncode\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n32\n\n\n1125\n\n\n1223\n\n\n7884\n\n\n\n\n\n\nMarkdown\n\n\n23\n\n\n2108\n\n\n0\n\n\n5797\n\n\n\n\n\n\nCSS\n\n\n2\n\n\n17\n\n\n4\n\n\n374\n\n\n\n\n\n\nJavaScript\n\n\n2\n\n\n11\n\n\n1\n\n\n213\n\n\n\n\n\n\nYAML\n\n\n1\n\n\n4\n\n\n0\n\n\n54\n\n\n\n\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n\n\n\n\nSUM:\n\n\n60\n\n\n3265\n\n\n1228\n\n\n14322",
            "title": "Stats (overall)"
        },
        {
            "location": "/Code/StatsBase/",
            "text": "cloc\n\n\ngithub.com/AlDanial/cloc v 1.74  T=0.12 s (101.9 files/s, 30809.6 lines/s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nfiles\n\n\nblank\n\n\ncomment\n\n\ncode\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n12\n\n\n396\n\n\n229\n\n\n3004\n\n\n\n\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n\n\n\n\nSUM:\n\n\n12\n\n\n396\n\n\n229\n\n\n3004",
            "title": "Stats (TF-base)"
        },
        {
            "location": "/Code/StatsSearch/",
            "text": "cloc\n\n\ngithub.com/AlDanial/cloc v 1.74  T=0.09 s (101.8 files/s, 35468.4 lines/s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nfiles\n\n\nblank\n\n\ncomment\n\n\ncode\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n9\n\n\n359\n\n\n305\n\n\n2473\n\n\n\n\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n\n\n\n\nSUM:\n\n\n9\n\n\n359\n\n\n305\n\n\n2473",
            "title": "Stats (TF-search)"
        },
        {
            "location": "/Code/StatsApps/",
            "text": "cloc\n\n\ngithub.com/AlDanial/cloc v 1.74  T=0.11 s (55.9 files/s, 20065.8 lines/s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nfiles\n\n\nblank\n\n\ncomment\n\n\ncode\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n5\n\n\n164\n\n\n487\n\n\n1486\n\n\n\n\n\n\nCSS\n\n\n1\n\n\n2\n\n\n0\n\n\n14\n\n\n\n\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n\n\n\n\nSUM:\n\n\n6\n\n\n166\n\n\n487\n\n\n1500",
            "title": "Stats (TF-apps)"
        },
        {
            "location": "/Code/StatsServer/",
            "text": "cloc\n\n\ngithub.com/AlDanial/cloc v 1.74  T=0.10 s (87.7 files/s, 15960.4 lines/s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nfiles\n\n\nblank\n\n\ncomment\n\n\ncode\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n6\n\n\n170\n\n\n183\n\n\n681\n\n\n\n\n\n\nCSS\n\n\n1\n\n\n15\n\n\n4\n\n\n360\n\n\n\n\n\n\nJavaScript\n\n\n2\n\n\n11\n\n\n1\n\n\n213\n\n\n\n\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n--------\n\n\n\n\n\n\nSUM:\n\n\n9\n\n\n196\n\n\n188\n\n\n1254",
            "title": "Stats (TF-server)"
        },
        {
            "location": "/Api/General/",
            "text": "Text-Fabric API\n\u00b6\n\n\nTutorial\nThe tutorials for the\n\nHebrew Bible\n\nand the\n\nUruk Cuneiform Tablets\n\nput the Text-Fabric API on show for two distinguished (and vastly different) corpora.\nGeneric API\nThis is the API of Text-Fabric in general.\nText-Fabric has no baked in knowledge of particular corpora.\nHowever, Text-Fabric comes with several additions that make working\nwith specific corpora easier.\nHebrew Bible: \nBHSA\nProto-cuneiform tablets from Uruk: \nCunei\nLoading\n\u00b6\n\n\nTF=Fabric()\n1\n2\nfrom\n \ntf.fabric\n \nimport\n \nFabric\n\n\nTF\n \n=\n \nFabric\n(\nlocations\n=\ndirectories\n,\n \nmodules\n=\nsubdirectories\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nText-Fabric is initialized for a corpus. It will search a set of directories\nand catalog all \n.tf\n files it finds there.\nThese are the features you can subsequently load.\nHere \ndirectories\n and \nsubdirectories\n are strings with directory names\nseparated by newlines, or iterables of directories.\nlocations, modules\nThe directories specified in \nlocations\n will be searched for \nmodules\n, which\nare paths that will be appended to the paths in \nlocations\n.\nAll \n.tf\n files (non-recursively) in any \nmodule\n will be added to the feature\nset to be loaded in this session. The order in \nmodules\n is important, because\nif a feature occurs in multiple modules, the last one will be chosen. In this\nway you can easily override certain features in one module by features in an\nother module of your choice.\notext@ in modules\nIf modules contain features with a name starting with \notext@\n, then the format\ndefinitions in these features will be added to the format definitions in the\nregular \notext\n feature (which is a WARP feature). In this way, modules that\ndefine new features for text representation, also can add new formats to the\nText-API.\nDefaults\nThe \nlocations\n list has a few defaults:\n1\n2\n3\n~/Downloads/text-fabric-data\n~/text-fabric-data\n~/github/text-fabric-data\n\n\n\n\n\nSo if you have stored your main Text-Fabric dataset in\n\ntext-fabric-data\n in one of these directories\nyou do not have to pass a location to Fabric.\nThe \nmodules\n list defaults to \n['']\n. So if you leave it out, Text-Fabric will\njust search the paths specified in \nlocations\n.\nsilent\nIf \nsilent=True\n is passed, banners and normal progress messages are suppressed.\nTF.explore()\n1\n2\nfeatures\n \n=\n \nTF\n.\nexplore\n(\nsilent\n=\nFalse\n,\n \nshow\n=\nTrue\n)\n\n\nfeatures\n\n\n\n\n\n\nor\n1\n2\nTF\n.\nexplore\n(\nsilent\n=\nFalse\n,\n \nshow\n=\nFalse\n)\n\n\nTF\n.\nfeatureSets\n\n\n\n\n\n\nDescription\nThis will give you a dictionary of all available features by kind. The kinds\nare: \nnodes\n, \nedges\n, \nconfigs\n, \ncomputeds\n.\nsilent\nWith \nsilent=False\n a message containing the total numbers of features is issued.\nshow\nThe resulting dictionary is delivered in \nTF.featureSets\n, but if you say\n\nshow=True\n, the dictionary is returned as function result.\napi=TF.load()\n1\napi\n \n=\n \nTF\n.\nload\n(\nfeatures\n,\n \nadd\n=\nFalse\n,\n \nsilent\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReads the features indicated by \nfeatures\n and loads them in memory\nready to be used in the rest of the program.\nfeatures\nfeatures\n is a string containing space separated feature names, or an\niterable of feature names. The feature names are just the names of \n.tf\n files\nwithout directory information and without extension.\nadd\nIf later on you want load more features, you can either:\nadd the features to the original \nload()\n statement and just run it again\nmake a new statement: \nTF.load(newfeatures, add=True)\n. The new features will\n    be added to the same api, so you do not have to to call\n    \napi.makeAvailableIn(globals())\n again after this!\nsilent\nThe features will be loaded rather silently, most messages will be suppressed.\nTime consuming operations will always be announced, so that you know what\nText-Fabric is doing. If \nsilent=True\n is passed, all informational messages\nwill be suppressed. This is handy I you want to load data as part of other\nmethods, on-the-fly.\napi.makeAvailableIn(globals())\n1\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nDescription\nThis method will export every member of the API (such as \nN\n, \nF\n, \nE\n, \nL\n, \nT\n,\n\ninfo\n) to the global namespace. From now on, we will omit the \napi.\n in our\ndocumentation.\nContents of the API\nAfter having loaded the features by \napi = TF.load(...)\n, the \napi\n harbours\nyour Text-Fabric API. You can access node feature \nmydata\n by \napi.F.mydata.v(node)\n, edge\nfeature \nmylink\n by \napi.E.mylink.f(node)\n, and so on.\nIf you are working with a single data source in your program, it is a bit\ntedious to write the initial \napi.\n all the time.\nBy this methodd you can avoid that.\nLonger names\nThere are also longer names which can be used as aliases to the single capital\nletters. This might or might not improve the readability of your program.\nshort name\nlong name\nN\nNodes\nF\nFeature\nFs\nFeatureString\nFall\nAllFeatures\nE\nEdge\nEs\nEdgeString\nEall\nAllEdges\nC\nComputed\nCs\nComputedString\nCall\nAllComputeds\nL\nLocality\nT\nText\nS\nSearch\nignored\n1\napi\n.\nignored\n\n\n\n\n\n\nDescription\nIf you want to know which features were found but ignored (because the feature\nis also present in another, later, location), you can use this attribute\nto inspect the ignored features and their locations.\nloadLog()\n1\napi\n.\nloadlog\n()\n\n\n\n\n\n\nDescription\nAfter loading you can view all messages using this method.\nIt also shows the messages that have been suppressed due to \nsilent=True\n.\nNavigating nodes\n\u00b6\n\n\nN()\n1\n2\nfor\n \nn\n \nin\n \nN\n():\n\n    \naction\n\n\n\n\n\n\nDescription\nThe result of \nN()\n is a generator that walks through all nodes in the\n\ncanonical order\n (see below).\nIterating over \nN()\n delivers you all words and structural elements of\nyour corpus in a very natural order.\nWalking nodes\nMost processing boils down to walking through the nodes by visiting node sets in\na suitable order. Occasionally, during the walk you might want to visit\nembedding or embedded nodes to glean some feature information from them.\nMore ways of walking\nLater, under \nFeatures\n there is another convenient way to walk through\nnodes.\ncanonical order\nThe canonical order is a way to sort the nodes in your corpus in such a way\nthat you can enumerate all nodes in the order you encounter them if you\nwalk through your corpus.\nBriefly this means:\nembedder nodes come before the nodes that lie embedded in them;\nearlier stuff comes before later stuff,\nif a verse coincides with a sentence, the verse comes before the sentence,\n    because verses generally contain sentences and not the other way round;\nif two objects are intersecting, but none embeds the other, the one with the\n    smallest slot that does not occur in the other, comes first.\nfirst things first, big things first\nThat means, roughly, that you start with a\nbook node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis\n1:1, then a sentence node, then a clause node, a phrase node, and the first word\nnode. Then follow all word nodes in the first phrase, then the phrase node of\nthe second phrase, followed by the word nodes in that phrase. When ever you\nenter a higher structure, you will first get the node corresponding to that\nstructure, and after that the nodes corresponding to the building blocks of that\nstructure.\nThis concept follows the intuition that slot sets with smaller elements come\nbefore slot set with bigger elements, and embedding slot sets come before\nembedded slot sets. Hence, if you enumerate a set of nodes that happens to\nconstitute a tree hierarchy based on slot set embedding, and you enumerate those\nnodes in the slot set order, you will walk the tree in pre-order.\nThis order is a modification of the one as described in (Doedens 1994, 3.6.3).\n\nDoedens, Crist-Jan (1994), \nText Databases. One Database Model and Several\nRetrieval Languages\n, number 14 in Language and Computers, Editions Rodopi,\nAmsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1,\n\nhttp://books.google.nl/books?id=9ggOBRz1dO4C\n. The order as defined by\nDoedens corresponds to walking trees in post-order.\nFor a lot of processing, it is handy to have a the stack of embedding elements\navailable when working with an element. That is the advantage of pre-order over\npost-order. It is very much like SAX parsing in the XML world.\nsortNodes()\n1\nsortNodes\n(\nnodeSet\n)\n\n\n\n\n\n\nDescription\ndelivers an iterable of nodes as a tuple sorted by the \ncanonical ordering\n.\nnodeSet\nAn iterable of nodes to be sorted.\nsortKey\n1\nnodeList\n \n=\n \nsorted\n(\nnodes\n,\n \nkey\n=\nsortKey\n)\n\n\n\n\n\n\nDescription\nA function that provides for each node the key to be used to sort nodes in the\ncanonical ordering. That means that the following two pieces of code do the same\nthing:\nsortNodes(nodeSet)\n and \nsorted(nodeSet, key=sortKey)\n.\nSorting tuples of nodes\nHandy to sort things that are not nodes themselves, but data structures with\nnodes in it, e.g. search results: if \nresults\n is a list of tuples of nodes, we\ncould sort them in canonical order like this:\n1\nsorted\n(\nnodeSet\n,\n \nkey\n=\nlambda\n \nr\n:\n \nsortKey\n(\nr\n[\n0\n]))\n\n\n\n\n\n\nLocality\n\u00b6\n\n\nLocal navigation\nHere are the methods by which you can navigate easily from a node to its\nneighbours: parents and children, previous and next siblings.\nL\nThe Locality API is exposed as \nL\n or \nLocality\n.\notype parameter\nIn all of the following \nL\n-functions, if the \notype\n parameter is passed, the result is filtered and\nonly nodes with \notype=nodeType\n are retained.\nResults of the \nL.\n functions are tuples, not single nodes\nEven if an \nL\n-function returns a single node, it is packed in a \ntuple\n.\n  So to get the node itself, you have to dereference the tuple:\n1\nL\n.\nu\n(\nnode\n)[\n0\n]\n\n\n\n\n\n\nL.u()\n1\nL\n.\nu\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of nodes \nupward\n, i.e. embedder nodes.\nnode\nThe node whose embedder nodes will be delivered.\nThe result never includes \nnode\n itself.\nL.d()\n1\nL\n.\nd\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of nodes \ndownward\n, i.e. embedded nodes.\nnode\nThe node whose embedded nodes will be delivered.\nThe result never includes \nnode\n itself.\nL.n()\n1\nL\n.\nn\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of adjacent \nnext\n nodes.\nnode\nThe node whose right adjacent nodes will be delivered;\ni.e. the nodes whose first slot immediately follow the last slot \nof \nnode\n.\nThe result never includes \nnode\n itself.\nL.p()\n1\nL\n.\np\n(\nnode\n,\n \notype\n=\nnodeType\n)\n\n\n\n\n\n\nDescription\nProduces an ordered tuple of adjacent \nprevious\n nodes from \nnode\n, i.e. nodes\nwhose last slot just precedes the first slot of \nnode\n.\nDescription\nProduces an ordered tuple of adjacent \nprevious\n nodes.\nnode\nThe node whose lefy adjacent nodes will be delivered;\ni.e. the nodes whose last slot immediately precede the first slot \nof \nnode\n.\nLocality and levels\nHere is something that is very important to be aware of when using \nsortNodes\n\nand the \nL.d(n)\n and \nL.u(n)\n methods.\nWhen we order nodes and report on which nodes embed which other nodes, we do not\nonly take into account the sets of slots the nodes occupy, but also their\n\nlevel\n. See \nlevels\n and \ntext\n.\nBoth the \nL.d(n)\n and \nL.u(n)\n work as follows:\nL.d(n)\n returns nodes such that embedding nodes come before embedded nodes\n    words)\nL.u(n)\n returns nodes such that embedded nodes come before embedding nodes\n    books)\nN.B.:\n Suppose you have node types \nverse\n and \nsentence\n, and usually a\nverse has multiple sentences, but not vice versa. Then you expect that\nL.d(verseNode)\n will contain sentence nodes,\nL.d(sentenceNode)\n will \nnot\n contain verse nodes.\nBut if there is a verse with exactly one sentence, and both have exactly the\nsame words, then that is a case where:\nL.d(verseNode)\n will contain one sentence node,\nL.d(sentenceNode)\n will contain \none\n verse node.\nText\n\u00b6\n\n\nOverview\nHere are the functions that enable you to get the actual text in the dataset.\nThere are several things to accomplish here, such as\ngetting text given book, chapter, and verse;\ngiven a node, produce the book, chapter and verse indicators in which the node\n    is contained;\nhandle multilingual book names;\nswitch between various text representations.\nThe details of the Text API are dependent on the \nwarp\n feature \notext\n, which\nis a config feature.\nT\nThe Text API is exposed as \nT\n or \nText\n.\nSections\n\u00b6\n\n\nSection levels\nIn \notext\n the main section levels (usually \nbook\n, \nchapter\n, \nverse\n) can be\ndefined. It loads the features it needs (so you do not have to specify those\nfeatures, unless you want to use them via \nF\n). And finally, it makes some\nfunctions available by which you can make handy use of that information.\nSection levels are generic\nIn this documentation, we call the main section level \nbook\n, the second level\n\nchapter\n, and the third level \nverse\n. Text-Fabric, however, is completely\nagnostic about how these levels are called. It is prepared to distinguish three\nsection levels, but how they are called, must be configured in the dataset. The\ntask of the \notext\n feature is to declare which node type and feature correspond\nwith which section level. Text-Fabric assumes that the first section level may\nhave multilingual headings, but that section levels two and three have single\nlanguage headings (numbers of some kind).\nString versus number\nChapter and verse numbers will be considered to be strings or\nintegers, depending on whether your dataset has declared the corresponding\nfeature \nvalueType\n as \nstr\n or as \nint\n.\nConceivably, other works might have chapter and verse numbers\nlike \nXIV\n, '3A', '4.5', and in those cases these numbers are obviously not\n\nint\ns.\notext is optional\nIf \notext\n is missing, the Text API will not be build. If it exists, but\ndoes not specify sections, that part of the Text API will not be built. Likewise\nfor text representations.\nlevels of node types\nUsually, Text-Fabric computes the hierarchy of node types correctly, in the\nsense that node types that act as containers have a lower level than node types\nthat act as containees. So books have the lowest level, words the highest. See\n\nlevels\n. However, if this level assignment turns out to be wrong for\nyour dataset, you can configure the right order in the \notext\n feature, by means\nof a key \nlevels\n with value a comma separated list of levels. Example:\n1\n@levels=tablet,face,column,line,case,cluster,quad,comment,sign\n\n\n\n\n\nT.sectionFromNode()\n1\nT\n.\nsectionFromNode\n(\nnode\n,\n \nlastSlot\n=\nFalse\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\nReturns the book/chapter/verse indications that correspond to the reference\nnode, which is the first or last slot belonging \nn\n, dependent on \nlastSlot\n.\nThe result is a tuple, consisting of the book name (in language \nlang\n), the\nchapter number, and the verse number.\nnode\nThe node from which we obtain a section specification.\nlastSlot\nWhether the reference node will be the last slot contained by the \nnode\n argument\nor the first node.\nlang\nThe language to be used for the section parts, as far as they are language dependent.\ncrossing verse boundaries\nSometimes a sentence or clause in a verse continue into the next verse.\nIn those cases, this function will return a different results for\n\nlastSlot=False\n and \nlastSlot=True\n.\nnodes outside sections\nNodes that lie outside any book, chapter, verse will get a \nNone\n in the\ncorresponding members of the returned tuple.\nT.nodeFromSection()\n1\nT\n.\nnodeFromSection\n(\nsection\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\nGiven a \nsection\n tuple, return the node of it.\nsection\nsection\n consists of a book name (in language \nlang\n),\nand a chapter number and a verse\nnumber (both as strings or number depending on the value type of the\ncorresponding feature). The verse number may be left out, the result is then a\nchapter node. Both verse and chapter numbers may be left out, the result is then\na book node. If all three are present, de result is a verse node.\nlang\nThe language assumed for the section parts, as far as they are language dependent.\nBook names and languages\n\u00b6\n\n\nBook names and nodes\nThe names of the books may be available in multiple languages. The book names\nare stored in node features with names of the form \nbook@\nla\n, where \nla\n is\nthe \nISO 639\n two-letter code for that\nlanguage. Text-Fabric will always load these features.\nT.languages\n1\nT\n.\nlanguages\n\n\n\n\n\n\nDescription\nA dictionary of the languages that are available for book names.\nT.bookName()\n1\nT\n.\nbookName\n(\nnode\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\ngives the name of the book in which a node occurs.\nnode\nThe node in question.\nlang\nThe \nlang\n parameter is a two letter language code. The default is \nen\n\n(English).\nIf there is no feature data for the language chosen, the value of the ordinary\n\nbook\n feature of the dataset will be returned.\nWorks for all nodes\nn\n may or may not be a book node. If not, \nbookName()\n retrieves the\nembedding book node first.\nT.bookNode()\n1\nT\n.\nbookNode\n(\nname\n,\n \nlang\n=\n'en'\n)\n\n\n\n\n\n\nDescription\ngives the node of the book identified by its name\nname\nThe name of the book.\nlang\nThe language in which the book name is supplied by the \nname\n parameter.\nIf \nlang\n can not be found, the value of the ordinary \nbook\n feature of the\ndataset will be used.\nIf \nname\n cannot be found in the specified language, \nNone\n will be returned.\nFunction name follows configured section level\nIf your dataset has configured section level one under an other name,\nsay \ntablet\n, then these two methods follow that name. Instead of \nT.bookName()\n\nand \nT.bookNode()\n we have then \nT.tabletName()\n and \nT.tabletNode()\n.\nText representation\n\u00b6\n\n\nText formats\nText can be represented in multiple ways. We provide a number of formats with\nstructured names.\nA format name is a string of keywords separated by \n-\n:\nwhat\n-\nhow\n-\nfullness\n-\nmodifier\nFor Hebrew any combination of the follwoing could be useful formats:\nkeyword\nvalue\nmeaning\nwhat\ntext\nwords as they belong to the text\nwhat\nlex\nlexemes of the words\nhow\norig\nin the original script (Hebrew, Greek, Syriac) (all Unicode)\nhow\ntrans\nin (latin) transliteration\nhow\nphono\nin phonetic/phonological transcription\nfullness\nfull\ncomplete with accents and all diacritical marks\nfullness\nplain\nwith accents and diacritical marks stripped, in Hebrew only the consonants are left\nmodifier\nketiv\n(Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default);\nThe default format is \ntext-orig-full\n, we assume that every TF dataset defines\nthis format.\nRemember that the formats are defined in the \notext\n warp config feature of your\nset, not by Text-Fabric.\nFreedom of names for formats\nThere is complete freedom of choosing names for text formats.\nThey do not have to complied with the above-mentioned scheme.\nT.formats\n1\nT\n.\nformats\n\n\n\n\n\n\nDescription\nShow the text representation formats that have been defined in your dataset.\nT.text()\n1\nT\n.\ntext\n(\nnodes\n,\n \nfmt\n=\nNone\n)\n\n\n\n\n\n\nDescription\nGives the text that corresponds to a bunch of nodes.\nnodes\nnodes\n can be an arbitrary iterable of nodes.\nNo attempt will be made to sort the nodes.\nIf you need order, it is\nbetter to order the nodes before you feed them to \nT.text()\n.\nfmt\nThe format of text-representation is given with \nfmt\n, with default \ntext-orig-full\n.\nIf the \nfmt\n\ncannot be found, the default is taken.\nIf the default format is not defined in the\n\notext\n feature of the dataset,\nthe node numbers will be output instead.\nThis function does not give error messages, because that could easily overwhelm\nthe output stream, especially in a notebook.\nNon slot nodes allowed\nIn most cases, the nodes fed to \nT.text()\n are slots, and the formats are\ntemplates that use features that are defined for slots.\nBut nothing prevents you to define a format for non-slot nodes, and use features\ndefined for a non-slot node type.\nIf, for example, your slot type is \nglyph\n, and you want a format that renders\nlexemes, which are not defined for glyphs but for words, you can just define a\nformat in terms of word features.\nIt is your responsibility to take care to use the formats for node types for\nwhich they make sense.\nEscape whitespace in formats\nWhen defining formats in \notext.tf\n, if you need a newline or tab in the format,\nspecify it as \n\\n\n and \n\\t\n.\nSearching\n\u00b6\n\n\nWhat is Text-Fabric Search?\nYou can query for graph like structures in your data set. The structure you are\ninterested in has to be written as a \nsearch template\n, offered to \nS.search()\n\nwhich returns the matching results as tuples of nodes.\nA search template expresses a pattern of nodes and edges with additional conditions\nalso known as \nquantifiers\n.\nA result of a search template is a tuple of nodes that instantiate the nodes in\nthe pattern, in such a way that the edges of the pattern are also instantiated\nbetween the nodes of the result tuple.\nMoreover, the quantifiers will hold for each result tuple.\nS\nThe Search API is exposed as \nS\n or \nSearch\n.\nSearch templates\n\u00b6\n\n\nSearch primer\nA search template consists of a bunch of lines, possibly indented, that specify\nobjects to look for. Here is a simple example:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        word pos=noun gender=feminine number=singular\n\n\n\n\n\nThis template looks for word combinations within a sentence within chapter 2 of\neither Genesis or Exodus, where one of the words is a verb and the other is a\nnoun. Both have a feminine inflection, but the verb is plural and the noun is\nsingular.\nThe indentation signifies embedding, i.e. containment. The two words are\ncontained in the same sentence, the sentence is contained in the chapter, the\nchapter in the book.\nThe conditions you specify on book, chapter, word are all conditions in terms of\n\nnode features\n. You can use all features in the corpus for\nthis.\nThe order of the two words is immaterial. If there are cases where the verb\nfollows the noun, they will be included in the results.\nAlso, the words do not have to be adjacent. If there are cases with words\nintervening between the noun and the verb, they will be included in the results.\nSpeaking of results: the \nS.search()\n function returns its results as tuples of\nnodes:\n1\n(\nbook\n,\n \nchapter\n,\n \nsentence\n,\n \nword1\n,\n \nword2\n)\n\n\n\n\n\n\nWith these nodes in hand, you can programmatically gather all information about\nthe results that the corpus provides.\nIf the order between the verb and the noun is important, you can specify that as\nan additional constraint. You can give the words a name, and state a relational\ncondition. Here we state that the noun precedes the verb.\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        vb:word pos=verb gender=feminine number=plural\n        nn:word pos=noun gender=feminine number=singular\nnn < vb\n\n\n\n\n\nThis can be written a bit more economically as:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        > word pos=noun gender=feminine number=singular\n\n\n\n\n\nIf you insist that the noun immediately precedes the verb, you can use a\ndifferent relational operator:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular\n\n\n\n\n\nThere are more kinds of relational operators.\nIf the noun must be the first word in the sentence, you can specify it as\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        w:word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\ns =: w\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\nbook name=Genesis|Exodus\n   chapter number=2\n      sentence\n        =: word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n\n\n\n\n\nIf the verb must be the last word in the sentence, you can specify it as\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: w:word pos=verb gender=feminine number=plural\ns := w\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n        :=\n\n\n\n\n\nYou can also use the \nedge features\n in the corpus as\nrelational operators as well.\nSuppose we have an edge feature \nsub\n between clauses, such that if main clause\n\nm\n has subordinate clauses \ns1\n, \ns2\n and \ns3\n, then\n1\nE.sub.f(m) = (s1, s2, s3)\n\n\n\n\n\nYou can use this relation in search. Suppose we want to find the noun verb pair\nin subordinate clauses only. We can use this template:\n1\n2\n3\n4\n5\n6\n7\nbook name=Genesis|Exodus\n   chapter number=2\n      m:clause\n        s:clause\n          word pos=verb gender=feminine number=plural\n          :> word pos=noun gender=feminine number=singular\nm -sub> s\n\n\n\n\n\nor a bit more economically:\n1\n2\n3\n4\n5\n6\nbook name=Genesis|Exodus\n  chapter number=2\n    clause\n      -sub> clause\n        word    pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular\n\n\n\n\n\nRead \nm -sub> s\n as: there is a \nsub\n-arrow from \nm\n to \ns\n.\nEdge features may have values.\nFor example, the\n\ncrossref feature\n\nis a set of edges between parallel verses, with the levels of confidence\nas values. This number is an integer between 0 and 100.\nWe can ask for parallel verses in an unqualified way:\n1\n2\nverse\n-crossref> verse\n\n\n\n\n\nBut we can also ask for the cases with a specific confidence:\n1\n2\nverse\n-crossref=90> verse\n\n\n\n\n\nor cases with a high confidence:\n1\n2\nverse\n-crossref>95> verse\n\n\n\n\n\nor cases with a low confidence:\n1\n2\nverse\n-crossref<80> verse\n\n\n\n\n\nAll feature conditions that you can assert on node features, you can also\nassert for edge features. If an edge feature is integer valued, such as \ncrossref\n\nyou can use comparisons; if it is string valued, you can use regular expressions.\nIn both cases you can also use the other constructs, such as\n1\n2\nverse\n-crossref=66|77> verse\n\n\n\n\n\nTo get a more specific introduction to search, consult the search tutorials for\n\nHebrew\n and\n\nCuneiform\n.\nFinally an example with quantifiers. We want all clauses where Pred-phrases\nconsist of verbs only:\n1\n2\n3\n4\n5\n6\n7\n8\n  clause\n  /where/\n    phrase function=Pred\n  /have/\n    /without/\n      word sp#verb\n    /-/\n  /-/\n\n\n\n\n\nSearch template reference\nWe have these kinds of lines in a template:\ncomment\n lines\nif a line starts with \n#\n it is a comment line`;\nyou cannot comment out parts of lines, only whole lines;\nif a line is empty or has whitespace only, it is a comment line;\ncomment lines are allowed everywhere;\ncomment lines are ignored.\natom\n lines\n(simple): \nindent name:otype-or-set features\nExamples\nword pos=verb gender=feminine\nvb:word pos=verb gender=feminine\nvb pos=verb gender=feminine\nThe indent is significant. Indent is counted as the number of white space\n    characters, where tabs count for just 1. \nAvoid tabs!\n.\nThe \nname:\n part is optional.\n    If present, it defines a name for this atom that can be used\n    in relational comparisons and other atoms.\nThe \notype-or-set\n part is optional.\n    If it is absent, the \nname\n part must be present.\n    The meaning of\n1\n2\np:phrase sp=verb\np vs=qal\n\n\n\n\n\nis identical to the meaning of\n1\n2\n3\np:phrase sp=verb\npnew:phrase vs=qal\np = pnew\n\n\n\n\n\n(with relop): \nindent op name:otype-or-set features\n<: word pos=verb gender=feminine\nThe relation operator specifies an extra constraint between a preceding atom\n    and this atom.\nThe preceding atom may be the parent, provided we are at its first child, or\n    it may the preceding sibling.\nYou can leave out the \nname:otype-or-set features\n bit. In that case, the\n    relation holds between the preceding atom and its parent.\nThe \nname:\n part is optional. Exactly as in the case without relop.\nThe \notype-or-set\n part is optional. Exactly as in the case without relop.\nThe otype-or-set is either a node type that exists in your TF data set,\nor it is the name of a set that you have passed in the \nsets\n parameter alongside\nthe query itself when you call \nS.search()\n or \nS.study()\n.\nSee \nfeature specifications\n below for all\nfull variety of feature constraints on nodes and edges.\nfeature\n lines: \nfeatures\nIndent is not significant. Continuation of feature constraints after a\n    preceding atom line or other feature line. This way you can divide lengthy\n    feature constraints over multiple lines.\nSee \nfeature specifications\n below for the\nfull variety of feature constraints on nodes and edges.\nrelation\n lines: \nname operator name\ns := w\nm -sub> s\nm <sub- s\nIndents and spacing are ignored.\nThere must be white-space around the operator.\nOperators that come from edge features may be enriched with values.\nSee \nrelational operators\n below for the\nwhole spectrum of relational constraints on nodes.\nquantifier\n sub-templates:\n    Atom lines that contain an otype or set may be followed by\n    Quantifiers consist of search templates themselves, demarcated by some\n    special keywords:\n/without/\n/where/\n and \n/have/\n/with/\n and \n/or/\n/-/\nSee \nquantifiers\n below for all the syntax and semantics.\nFeature specifications\n\u00b6\n\n\nFeatures specs\nThe \nfeatures\n above is a specification of what features with which values to\nsearch for. This specification must be written as a white-space separated list\nof \nfeature specs\n.\nA \nfeature spec\n has the form \nname\n \nvalueSpec\n, with no space between the \nname\n\nand the \nvalueSpec\n.\nThe \nvalueSpec\n may have the following forms and meanings:\nform\nevaluates to \nTrue\n the feature \nname\n ...\nhas any value except \nNone\n#\nhas value \nNone\n=\nvalues\nhas one of the values specified\n#\nvalues\nhas none of the values specified\n>\nvalue\nis greater than \nvalue\n<\nvalue\nis less than \nvalue\n~\nregular expression\nhas a value and it matches \nregular expression\nAll these forms are also valid as \n-\nname\n \nform\n>\n and \n<\nname\n \nform\n-\n, in which case\nthey specify value constraints on edge features.\nThis is only meaningful if the edge feature is declared to have values (most edge features\ndo not have values).\nAdditional constraints\nThere may be no space around the \n=#<>~\n.\nname\n must be a feature name that exists in the dataset. If it references a\n    feature that is not yet loaded, the feature will be loaded automatically.\nvalues\n must be a \n|\n separated list of feature values, no quotes. No spaces\n    around the \n|\n. If you need a space or \n|\n or \n\\\n in a value, escape it by a\n    \n\\\n. Escape tabs and newlines as \n\\t\n and \n\\n\n.\nWhen comparing values with \n<\n and \n>\n:\nvalue\n must be an integer (negative values allowed);\nYou can do numeric comparisons only on number-valued features, not on\n    string-valued features.\nregular expression\n must be a string that conforms to the Python\n    \nregular axpression syntax\nIf you need a space in your regular expression, you have to escape it with a\n    \n\\\n.\nYou can do regular expressions only on string-valued features, not on\n    number-valued features.\nRelational operators\n\u00b6\n\n\nOperator lines\nNode comparison\n=\n: is equal (meaning the same node, a clause and a verse that occupy the\n    same slots are still unequal)\n    \n#\n: is unequal (meaning a different node, a clause and a verse that occupy\n    the same slots are still unequal)\n    \n<\n \n>\n: before and after (in the \ncanonical ordering\n)\n    \nSlot comparison\n==\n: occupy the same slots (identical slot sets)\n    \n&&\n: overlap (the intersection of both slot sets is not empty)\n    \n##\n: occupy different slots (but they may overlap, the set of slots of the\n    two are different as sets)\n    \n||\n: occupy disjoint slots (no slot occupied by the one is also occupied by\n    the other)\n    \n[[ ]]\n: embeds and contains (slot set inclusion, in both directions)\n    \n<<\n \n>>\n: before and after (with respect to the slots occupied: left ends\n    before right starts and vice versa)\n    \n<:\n \n:>\n: \nadjacent\n before and after (with respect to the slots occupied:\n    left ends immediately before right starts and vice versa)\n    \n=:\n left and right start at the same slot\n    \n:=\n left and right end at the same slot\n    \n::\n left and right start and end at the same slot\n    \nNearness comparison\nSome of the adjacency relations can actually be weakened. Instead of requiring\nthat one slot is equal to an other slot, you can require that they are \nk-near\n,\ni.e. they are at most \nk\n apart. Here are the relationships where you can do\nthat. Instead of typing the letter \nk\n, provide the actual number you want.\n<k:\n \n:k>\n: \nk\n-\nadjacent\n before and after (with respect to the slots\n    occupied: left ends \nk\n-near where right starts and vice versa)\n    \n=k:\n left and right start at \nk\n-near slots\n    \n:k=\n left and right end at \nk\n-near slots\n    \n:k:\n left and right start and end at \nk\n-near slots\n    \nBased on edge features\n-\nname\n>\n \n<\nname\n-\n: connected by the edge feature \nname\nin both directions;\nthese forms work for edges that do and do not have values;\n-\nname\n \nvalueSpec\n>\n \n<\nname\n \nvalueSpec\n-\n: connected by the edge feature \nname\nin both directions;\nthese forms work only for edges that do have values.\nQuantifiers\n\u00b6\n\n\nQuantifiers\nExperimental\nThis part of search templates is still experimental.\nbugs may be discovered\nthe syntax of quantifiers may change\nWhat is a quantifier?\nQuantifiers are powerful expressions in templates.\nThey state conditions on a given atom in your template.\nThe atom in question is called the \nparent\n atom.\nThe conditions may involve \nmany\n nodes that are related to the parent,\nas in:\nall embedded words are a verb\n;\nwithout a following predicate phrase\n;\nwith a mother clause or a mother phrase\n.\nThat is where the term \nquantifier\n comes from.\nA quantifier \nquantifies\n its parent atom.\n/without/\nSyntax:\n1\n2\n3\n4\natom\n/without/\ntemplateN\n/-/\n\n\n\n\n\nMeaning:\nnode \nr\n is a result of this template if and only if \nr\n is a result of \natom\n and\nthere is no tuple \nRN\n such that (\nr\n, \nRN\n) is a result of\n1\n2\natom\ntemplateN\n\n\n\n\n\n/where/\nSyntax:\n1\n2\n3\n4\n5\n6\natom\n/where/\ntemplateA\n/have/\ntemplateH\n/-/\n\n\n\n\n\nMeaning:\nnode \nr\n is a result of this template if and only if \nr\n is a result of \natom\n and\nfor all tuples (\nRA\n) such that (\nr\n, \nRA\n) is a result of\n1\n2\natom\ntemplateA\n\n\n\n\n\nthere is a tuple \nRH\n such that (\nr\n, \nRA\n, \nRH\n)  is a result of\n1\n2\n3\natom\ntemplateA\ntemplateH\n\n\n\n\n\n/with/\nSyntax:\n1\n2\n3\n4\n5\n6\n7\n8\natom\n/with/\ntemplateO1\n/or/\ntemplateO2\n/or/\ntemplateO3\n/-/\n\n\n\n\n\nMeaning:\nnode \nr\n is a result of this template if and only if:\nthere is a tuple \nR1\n such that (\nr\n, \nR1\n) is a result of\n1\n2\natom\ntemplateO1\n\n\n\n\n\nor there is a tuple \nR2\n such that (\nr\n, \nR2\n) is a result of\n1\n2\natom\ntemplateO2\n\n\n\n\n\nor there is a tuple \nR3\n such that (\nr\n, \nR3\n) is a result of\n1\n2\natom\ntemplateO3\n\n\n\n\n\n???+ note \"1 or more alternatives\n    This quantifier can be used with any number of \n/or/\n keywords, including\n    none at all. If there is no \n/or/\n, there is just one alternative.\n    The only difference between\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n```\natom\n/with/\ntemplate\n/-/\n```\n\nand\n\n```\natom\ntemplate\n```\n\nis that the results of the first query contain tuples with only one\nelement, corresponding to the `atom`.\nThe second query contains tuples of which the first element\ncorresponds to the `atom`, and the remaining members correspond to\nthe `template`.\n\n\n\n\n\nParent\nThe \natom\n bit is an atom line, it acts as the \nparent\n of the quantifier.\nMultiple quantifiers\nYou may have multiple quantifiers for one parent.\nNot in result tuples\nWhereas a the search for a normal template\nproceeds by finding a tuples that instantiates all it nodes\nin such a way that all relationships expressed in the template hold, a quantifier template\nis not instantiated.\nIt asserts a condition that has to be tested for all nodes relative\nits parent. None of the atoms in a template of a quantifier corresponds to\na node in a final result tuple.\nMay be nested\nTemplates within a quantifier may contain other quantifiers.\nThe idea is, that whenever a search template is evaluated, quantifiers at the outer level of \nget interpreted.\nThis interpretation gives rise to one or more templates to be constructed and run.\nThose new templates have been stripped of the outer layer of quantifiers,\nand when these templates are executed, the quantifiers at the next level have become outer.\nAnd so on.\nRestrictions\nDue to the implementation of quantifiers there are certain restrictions.\nQuantifiers must be put immediately below their parents or below\n    preceding quantifiers of the same parent.\nThe keywords of a quantifier must appear on lines with exactly the same indentation\n    as the atom they quantify.\nThe templates of a quantifier must have equal or greater indent than its keywords;\nThe names accessible to the templates inside a quantifier are:\nthe name \n..\n, which is the name of the atom that is quantified;\n    this name is automagically valid in quantifier templates;\nthe name of the atom that is quantified (if that atom has a given name);\nnames defined in the template itself;\nin \n/where/\n, \ntemplateH\n may use names defined in \ntemplateA\n;\n    but only if these names are defined outside any quantifier of\n    \ntemplateA\n.\nThe following situations block the visibility of names:\nin \n/with/\n, \ntemplateO\ni\n may not use names defined in \ntemplateO\nj\n for \nj\n other than \ni\n;\nnames defined outer quantifiers are not accessible in inner quantifiers;\nnames defined inner quantifiers are not accessible in outer quantifiers.\nWhen you nest quantifiers, think of the way they will be recomposed into\nordinary templates. This dictates whether your quantifier is syntactically valid or not.\nIndentation\nThe indentation in quantifiers relative to their parent atom will be preserved.\nNested quantifiers\nConsider\n1\n2\n3\n4\n5\n6\n7\n8\n9\nclause\n/where/\n  phrase function=Pred\n/have/\n  /without/\n    word sp#verb\n  /-/\n/-/\n  phrase function=Subj\n\n\n\n\n\nThe auxiliary templates that will be run are:\nFor the outer quantifier:\n1\n2\nclause\n  phrase function=Pred\n\n\n\n\n\nand\n1\n2\n3\n4\n5\nclause\n  phrase function=Pred\n  /without/\n    word sp#verb\n  /-/\n\n\n\n\n\nFor the inner quantifier:\n1\n2\nphrase function=Pred\n  word sp#verb\n\n\n\n\n\nNote that the auxiliary template for the inner quantifier\nis shifted in its entirety to the left, but that the \nrelative indentation is exactly as it shows in the original template.\nImplementation\nHere is a description of the implementation of the quantifiers.\nIt is not the real implementation, but it makes clear what is going on, and why\nthe quantifiers have certain limitations, and how indentation works.\nThe basic idea is:\na quantifier leads to the execution of one or more separate searche templates;\nthe results of these searches are combined by means of set operations:\n    \ndifference\n, \nintersection\n, \nunion\n, dependent on the nature of the quantifier;\nthe end result of this combination will fed as a custom set to the original template after\n    stripping the whole quantifier from that template.\n    So we replace a quantifier by a custom set.\nSuppose we have\n1\n2\n3\n4\n5\n6\nclause typ=Wx0\nQUANTIFIER1\nQUANTIFIER2\n...\nQUANTIFIERn\n  rest-of-template\n\n\n\n\n\nWe compute a set of clauses \nfilteredClauses1\n based on \n1\n2\nclause typ=Wx0\nQUANTIFIER1\n\n\n\n\n\nand then compute a new set \nfilteredClauses2\n based on\n1\n2\n3\n4\n5\nS.search('''\nfclause typ=Wx0\nQUANTIFIER2\n''',\n    customSets=dict(fclause=filteredClauses1)\n\n\n\n\n\nand so on until we have had QUANTIFIERn,\nleading to a set \nfilteredClausesN\n of clauses\nthat pass all filters set by the quantifiers.\nFinally, we deliver the results of\n1\n2\n3\n4\n5\nS.search('''\nfclause\n  rest-of-template\n''',\n    customSets=dict(fclause=filteredClausesN)\n\n\n\n\n\nSearch API\n\u00b6\n\n\nS.relationsLegend()\n1\nS\n.\nrelationsLegend\n()\n\n\n\n\n\n\nDescription\nGives dynamic help about the basic relations that you can use in your search\ntemplate. It includes the edge features that are available in your dataset.\nS.search()\n1\nS\n.\nsearch\n(\nquery\n,\n \nlimit\n=\nNone\n,\n \nshallow\n=\nFalse\n,\n \nsets\n=\nNone\n,\n \nwithContext\n=\nNone\n)\n\n\n\n\n\n\nDescription\nSearches for combinations of nodes that together match a search template.\nThis method returns a \ngenerator\n which yields the results one by one. One result\nis a tuple of nodes, where each node corresponds to an \natom\n-line in your\n\nsearch template\n.\nquery\nThe query is a search template, i.e. a string that conforms to the rules described above.\nshallow\nIf \nTrue\n or \n1\n, the result is a set of things that match the top-level element\nof the \nquery\n.\nIf \n2\n or a bigger number \nn\n, return the set of truncated result tuples: only\nthe first \nn\n members of each tuple is retained.\nIf \nFalse\n or \n0\n, a sorted list of all result tuples will be returned.\nsets\nIf not \nNone\n, it should be a dictionary of sets, keyed by a names.\nIn \nquery\n you can refer to those names to invoke those sets.\nlimit\nIf \nlimit\n is a number, it will fetch only that many results.\nwithContext\nSpecifies that for all nodes in the result-(tuple)s context information\nhas to be supplied.\nIf \nwithContext\n is \nTrue\n, all features in the current TF dataset will\nbe looked up for all nodes in the results of the query.\nIf it is a list, the list will be split on white-space into a list of\nfeature names. These are the features that will be looked up for all result nodes.\nYou can also pass an iterable of feature names.\nYou may ask for node features and for edge features. For edge features, only\nnode pairs within the result set will be delivered. If edge features carry values,\nthe values will also be delivered.\nIf you ask for any features at all, the warp features \notype\n and \noslots\n will\nalways be in the result.\nIf \nwithContext\n is not \nNone\n, the result of \nsearch()\n is a tuple\n\n(\n \nqueryResults\n \n,\n \ncontextInfo\n \n)\n,\nwhere \nqueryResults\n is a sorted list of results, never a generator,\neven if \nlimit\n is \nNone\n).\ncontextInfo\n is a dictionary of feature data, keyed by the name of the feature.\nThe values are dictionaries keyed by node (integers) and valued by the values of\nthat feature for those nodes. \nNone\n values will not be included in the dictionary. \nTF as Database\nBy means of \nS.search(query, withContext=True)\n you can use one \nTF\n instance as a\ndatabase that multiple clients can use without the need for each client to call the \ncostly \nload\n methods.\nYou have to come up with a process that runs TF, has all features loaded, and\nthat can respond to queries from other processes.\nWebservers can use such a daemonized TF to build efficient controllers.\nSupport for TF as daemon is upcoming, it will be included in the Text-Fabric code base.\nGenerator versus tuple\nIf \nlimit\n is specified, the result is not a generator but a tuple of results.\nMore info on the search plan\nSearching is complex. The search template must be parsed, interpreted, and\ntranslated into a search plan. The following methods expose parts of the search\nprocess, and may provide you with useful information in case the search does not\ndeliver what you expect.\nsee the plan\nthe method \nS.showPlan()\n below shows you at a glance the correspondence\nbetween the nodes in each result tuple and your search template.\nS.study()\n1\nS\n.\nstudy\n(\nsearchTemplate\n,\n \nstrategy\n=\nNone\n,\n \nsilent\n=\nFalse\n,\n \nshallow\n=\nFalse\n,\n \nsets\n=\nNone\n)\n\n\n\n\n\n\nDescription\nYour search template will be checked, studied, the search\nspace will be narrowed down, and a plan for retrieving the results will be set\nup.\nIf your query has quantifiers, the asscociated search templates will be constructed\nand executed. These searches will be reported clearly.\nsearchTemplate\nThe search template is a string that conforms to the rules described above.\nstrategy\nIn order to tame the performance of search, the strategy by which results are fetched\nmatters a lot.\nThe search strategy is an implementation detail, but we bring\nit to the surface nevertheless.\nTo see the names of the available strategies, just call\n\nS.study('', strategy='x')\n and you will get a list of options reported to\nchoose from.\nFeel free to experiment. To see what the strategies do,\nsee the \ncode\n.\nsilent\nIf you want to suppress most of the output, say \nsilent=True\n.\nshallow, sets\nAs in \nS.search()\n.\nS.showPlan()\n1\nS\n.\nshowPlan\n(\ndetails\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nSearch results are tuples of nodes and the plan shows which part of the tuple\ncorresponds to which part of the search template.\ndetails\nIf you say \ndetails=True\n, you also get an overview of the search space and a\ndescription of how the results will be retrieved.\nafter S.study()\nThis function is only meaningful after a call to \nS.study()\n.\nSearch results\n\u00b6\n\n\nPreparation versus result fetching\nThe method \nS.search()\n above combines the interpretation of a given\ntemplate, the setting up of a plan, the constraining of the search space\nand the fetching of results.\nHere are a few methods that do actual result fetching.\nThey must be called after a previous \nS.search()\n or \nS.study()\n.\nS.count()\n1\nS\n.\ncount\n(\nprogress\n=\nNone\n,\n \nlimit\n=\nNone\n)\n\n\n\n\n\n\nDescription\nCounts the results, with progress messages, optionally up to a limit.\nprogress\nEvery so often it shows a progress message.\nThe frequency is \nprogress\n results, default every 100.\nlimit\nFetch results up to a given \nlimit\n, default 1000.\nSetting \nlimit\n to 0 or a negative value means no limit: all results will be\ncounted.\nwhy needed\nYou typically need this in cases where result fetching turns out to\nbe (very) slow.\ngenerator versus list\nlen(S.results())\n does not work, because \nS.results()\n is a generator\nthat delivers its results as they come.\nS.fetch()\n1\nS\n.\nfetch\n(\nlimit\n=\nNone\n)\n\n\n\n\n\n\nDescription\nFinally, you can retrieve the results. The result of \nfetch()\n is not a list of\nall results, but a \ngenerator\n. It will retrieve results as long as they are\nrequested and their are still results.\nlimit\nTries to get that many results and collects them in a tuple.\nSo if limit is not \nNone\n, the result is a tuple with a known length.\nIterating over the \nfetch()\n generator\nYou typically fetch results by saying:\n1\n2\n3\n4\ni\n \n=\n \n0\n\n\nfor\n \nr\n \nin\n \nS\n.\nresults\n():\n\n    \ndo_something\n(\nr\n[\n0\n])\n\n    \ndo_something_else\n(\nr\n[\n1\n])\n\n\n\n\n\n\nAlternatively, you can set the \nlimit\n parameter, to ask for just so many\nresults. They will be fetched, and when they are all collected, returned as a\ntuple.\nFetching a limited amount of results\n1\nS\n.\nfetch\n(\nlimit\n=\n10\n)\n\n\n\n\n\n\ngives you the first bunch of results quickly.\nS.glean()\n1\nS\n.\nglean\n(\nr\n)\n\n\n\n\n\n\nDescription\nA search result is just a tuple of nodes that correspond to your template, as\nindicated by \nshowPlan()\n. Nodes give you access to all information that the\ncorpus has about it.\nThe \nglean()\n function is here to just give you a first impression quickly.  \nr\nPass a raw result tuple \nr\n, and you get a string indicating where it occurs,\nin terms of sections, \nand what text is associated with the results.\nInspecting results\n1\n2\nfor\n \nresult\n \nin\n \nS\n.\nfetch\n(\nlimit\n=\n10\n):\n\n    \nprint\n(\nS\n.\nglean\n(\nresult\n))\n\n\n\n\n\n\nis a handy way to get an impression of the first bunch of results.\nUniversal\nThis function works on all tuples of nodes, whether they have been\nobtained by search or not.\nMore ways of showing results\nIf you work in one of the corpora for which the TF-API has been extended,\nyou will be provided with more powerful methods \nshow()\n and \ntable()\n\nto display your results. See \nCunei\n and \nBhsa\n.\nFeatures\n\u00b6\n\n\nFeatures\nTF can give you information of all features it has encountered.\nTF.featureSets\n1\nTF\n.\nfeatureSets\n\n\n\n\n\n\nDescription\nReturns a dictionary with keys \nnodes\n, \nedges\n, \nconfigs\n, \ncomputeds\n.\nUnder each key there is the set of feature names in that category.\nSo you can easily test whether a node feature or edge feature is present in the\ndataset you are working with.\nconfigs\nThese are config features, with metadata only, no data. E.g. \notext\n.\ncomputeds\nThese are blocks of precomputed data, available under the \nC.\n API, see below.\nMay be unloaded\nThe sets do not indicate whether a feature is loaded or not.\nThere are other functions that give you the loaded node features (\nFall()\n)\nand the loaded edge features (\nEall()\n).\nNode features\n\u00b6\n\n\nNode Features\nF\nThe node features API is exposed as \nF\n (\nFs\n) or \nFeature\n (\nFeatureString\n).\nFall() aka AllFeatures()\n1\n2\nFall\n()\n\n\nAllFeatures\n()\n\n\n\n\n\n\nDescription\nReturns a sorted list of all usable, loaded node feature names.\nF.\nfeature\n aka Feature.\nfeature\n1\n2\nF\n.\npart_of_speech\n\n\nFeature\n.\npart_of_speech\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in node features.\nIn this example, we assume there is a feature called\n\npart_of_speech\n.\nTricky feature names\nIf the feature name is not\na valid python identifier, you can not use this function,\nyou should use \nFs\n instead.\nFs(feature) aka FeatureString(feature)\n1\n2\n3\n4\nFs\n(\nfeature\n)\n\n\nFeatureString\n(\nfeature\n)\n\n\nFs\n(\n'part-of-speech'\n)\n\n\nFeatureString\n(\n'part-of-speech'\n)\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in node features.\nfeature\nIn this example, in line 1 and 2, the feature name is contained in\nthe variable \nfeature\n.\nIn lines 3 and 4, \nwe assume there is a feature called\n\npart-of-speech\n.\nNote that this is not a valid name in Python, yet we\ncan work with features with such names.\nBoth methods have identical results\nSuppose we have just issued \nfeature = 'pos'.\nThen the result of\nFs(feature)\nand\nF.pos` is identical.\nIn most cases \nF\n works just fine, but \nFs\n is needed in two cases:\nif we need to work with a feature whose name is not a valid\n    Python name;\nif we determine the feature we work with dynamically, at run time.\nSimple forms\nIn the sequel we'll give examples based on the simple form only.\nF.\nfeature\n.v(node)\n1\nF\n.\npart_of_speech\n.\nv\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the value of a \nfeature\n, such as \npart_of_speech\n for a node.\nnode\nThe node whose value for the feature is being retrieved.\nF.\nfeature\n.s(value)\n1\n2\nF\n.\npart_of_speech\n.\ns\n(\nvalue\n)\n\n\nF\n.\npart_of_speech\n.\ns\n(\n'noun'\n)\n\n\n\n\n\n\nDescription\nReturns a generator of all nodes in the canonical order with a given value for a given feature.\nThis is an other way to walk through nodes than using \nN()\n.\nvalue\nThe test value: all nodes with this value are yielded, the others pass through.\nnouns\nThe second line gives you all nodes which are nouns according to the corpus.\nF.`\nfeature\n.freqList()\n1\nF\n.\npart_of_speech\n.\nfreqList\n(\nnodeTypes\n=\nNone\n)\n\n\n\n\n\n\nDescription\nInspect the values of \nfeature\n (in this example: \npart_of_speech\n)\nand see how often they occur. The result is a\nlist of pairs \n(value, frequency)\n, ordered by \nfrequency\n, highest frequencies\nfirst.\nnodeTypes\nIf you pass a set of nodeTypes, only the values for nodes within those\ntypes will be counted.\nF.otype\notype\n is a special node feature and has additional capabilities.\nDescription\nF.otype.slotType\n is the node type that can fill the slots (usually: \nword\n)\nF.otype.maxSlot\n is the largest slot number\nF.otype.maxNode\n is the largest node number\nF.otype.all\n is a list of all \notypes\n from big to small (from books through\n    clauses to words)\nF.otype.sInterval(otype)\n is like \nF.otype.s(otype)\n, but instead of\n    returning you a range to iterate over, it will give you the starting and\n    ending nodes of \notype\n. This makes use of the fact that the data is so\n    organized that all node types have single ranges of nodes as members.\nEdge features\n\u00b6\n\n\nEdge Features\nE\nThe edge features API is exposed as \nE\n (\nEs\n) or \nEdge\n (\nEdgeString\n).\nEall() aka AllEdges()\n1\n2\nEall\n()\n\n\nAllEdges\n()\n\n\n\n\n\n\nDescription\nReturns a sorted list of all usable, loaded edge feature names.\nE.\nfeature\n aka Edge.\nfeature\n1\n2\nE\n.\nhead\n\n\nFeature\n.\nhead\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in edge features.\nIn this example, we assume there is a feature called\n\nhead\n.\nTricky feature names\nIf the feature name is not\na valid python identifier, you can not use this function,\nyou should use \nEs\n instead.\nEs(feature) aka EdgeString(feature)\n1\n2\n3\n4\nEs\n(\nfeature\n)\n\n\nEdgeString\n(\nfeature\n)\n\n\nEs\n(\n'head'\n)\n\n\nEdgeString\n(\n'head'\n)\n\n\n\n\n\n\nDescription\nReturns a sub-api for retrieving data that is stored in edge features.\nfeature\nIn this example, in line 1 and 2, the feature name is contained in\nthe variable \nfeature\n.\nIn lines 3 and 4, \nwe assume there is a feature called\n\nhead\n.\nBoth methods have identical results\nSuppose we have just issued \nfeature = 'head'.\nThen the result of\nEs(feature)\nand\nE.pos` is identical.\nIn most cases \nE\n works just fine, but \nEs\n is needed in two cases:\nif we need to work with a feature whose name is not a valid\n    Python name;\nif we determine the feature we work with dynamically, at run time.\nSimple forms\nIn the sequel we'll give examples based on the simple form only.\nE.\nfeature\n.f(node)\n1\nE\n.\nhead\n.\nf\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the nodes reached by \nfeature\n-edges \nfrom\n a certain node.\nThese edges must be specified in \nfeature\n, in this case \nhead\n.\nThe result is an ordered tuple\n(again, in the \ncanonical order\n. The members of the\nresult are just nodes, if \nhead\n describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value.\nIf there are no edges from the node, the empty tuple is returned, rather than \nNone\n.\nnode\nThe node \nfrom\n which the edges in question start.\nE.\nfeature\n.t(node)\n1\nE\n.\nhead\n.\nt\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGet the nodes reached by \nfeature\n-edges \nto\n a certain node.\nThese edges must be specified in \nfeature\n, in this case \nhead\n.\nThe result is an ordered tuple\n(again, in the \ncanonical order\n. The members of the\nresult are just nodes, if \nfeature\n describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value.\nIf there are no edges to \nn\n, the empty tuple is returned, rather than \nNone\n.\nnode\nThe node \nto\n which the edges in question go.\nE.\nfeature\n.freqList()\n1\nE\n.\nop\n.\nfreqList\n(\nnodeTypesFrom\n=\nNone\n,\n \nnodeTypesTo\n=\nNone\n)\n\n\n\n\n\n\nDescription\nIf the edge feature has no values, simply return the number of node pairs\nbetween an edge of this kind exists.\nIf the edge feature does have values, we \ninspect them\nand see how often they occur. The result is a\nlist of pairs \n(value, frequency)\n, ordered by \nfrequency\n, highest frequencies\nfirst.\nnodeTypesFrom\nIf not \nNone\n,\nonly the values for edges that start from a node with type\nwithin \nnodeTypesFrom\n\nwill be counted.\nnodeTypesTo\nIf not \nNone\n,\nonly the values for edges that go to a node with type\nwithin \nnodeTypesTo\n\nwill be counted.\nE.oslots\noslots\n is a special edge feature and is mainly used to construct other parts\nof the API. It has less capabilities, and you will rarely need it. It does not\nhave \n.f\n and \n.t\n methods, but an \n.s\n method instead.\nDescription\nE.oslots.s(node)\n\nGives the sorted list of slot numbers linked to a node,\nor put otherwise: the slots that \nsupport\n that node.\nnode\nThe node whose slots are being delivered.\nMessaging\n\u00b6\n\n\nTimed messages\nError and informational messages can be issued, with a time indication.\ninfo(), error()\n1\ninfo\n(\nmsg\n,\n \ntm\n=\nTrue\n,\n \nnl\n=\nTrue\n)\n\n\n\n\n\n\nDescription\nSends a message to standard output, possibly with time and newline.\nif \ninfo()\n is being used, the message is sent to \nstdout\n;\nif \nerror()\n is being used, the message is sent to \nstderr\n;\nIn a Jupyter notebook, the standard error is displayed with\na reddish background colour.\ntm\nIf \nTrue\n, an indicator of the elapsed time will be prepended to the message.\nnl\nIf \nTrue\n a newline will be appended.\nindent()\n1\nindent\n(\nlevel\n=\nNone\n,\n \nreset\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nChanges the level of indentation of messages and possibly resets the time.\nlevel\nThe level of indentation, an integer.  Subsequent\n\ninfo()\n and \nerror()\n will display their messages with this indent.\nreset\nIf \nTrue\n, the elapsed time to will be reset to 0 at the given level.\nTimers at different levels are independent of each other.\nSaving features\n\u00b6\n\n\nTF.save()\n1\nTF\n.\nsave\n(\nnodeFeatures\n=\n{},\n \nedgeFeatures\n=\n{},\n \nmetaData\n=\n{},\n \nmodule\n=\nNone\n)\n\n\n\n\n\n\nDescription\nIf you have collected feature data in dictionaries, keyed by the\nnames of the features, and valued by their feature data,\nthen you can save that data to \n.tf\n feature files on disk.\nIt is this easy to export new data as features:\ncollect the data and metadata of\nthe features and \nfeed it in an orderly way to \nTF.save()\n and there you go.\nnodeFeatures\nThe data of a node feature is a dictionary with nodes as keys (integers!) and\nstrings or numbers as (feature) values.\nedgeFeatures\nThe data of an edge feature is a dictionary with nodes as keys, and sets or\ndictionaries as values. These sets should be sets of nodes (integers!), and\nthese dictionaries should have nodes as keys and strings or numbers as values.\nmetadata\nEvery feature will receive metadata from \nmetaData\n, which is a dictionary\nmapping a feature name to its metadata.\nvalue types\nThe type of the values should conform to \n@valueType\n (\nint\n or \nstr\n), which\nmust be stated in the metadata.\nedge values\nIf you save an edge feature, and there are values in that edge feature, you have\nto say so, by specifying \nedgeValues = True\n in the metadata for that feature.\ngeneric metadata\nmetaData\n may also contain fields under\n  the empty name. These fields will be added to all features in \nnodeFeatures\n and\n  \nedgeFeatures\n.\nconfig features\nIf you need to write the \nconfig\n feature \notext\n,\nwhich is a metadata-only feature, just\nadd the metadata under key \notext\n in \nmetaData\n and make sure\nthat \notext\n is not a key in \nnodeFeatures\n nor in\n\nedgeFeatures\n.\nThese fields will be written into the separate config feature \notext\n,\nwith no data associated.\nsave location\nThe (meta)data will be written to the very last module in the list of locations\nthat you specified when calling \nFabric()\n or to what you passed as \nmodule\n in\nthe same location. If that module does not exist, it will be created in the last\n\nlocation\n. If both \nlocations\n and \nmodules\n are empty, writing will take place\nin the current directory.\nClearing the cache\n\u00b6\n\n\nTF.clearCache()\n1\nTF\n.\nclearCache\n()\n\n\n\n\n\n\nDescription\nText-Fabric precomputes data for you, so that it can be loaded faster. If the\noriginal data is updated, Text-Fabric detects it, and will recompute that data.\nBut there are cases, when the algorithms of Text-Fabric have changed, without\nany changes in the data, where you might want to clear the cache of precomputed\nresults.\nCalling this function just does it, and it is equivalent with manually removing\nall \n.tfx\n files inside the hidden \n.tf\n directory inside your dataset.\nNo need to load\nIt is not needed to execute a \nTF.load()\n first.\nMQL\n\u00b6\n\n\nData interchange with MQL\nYou can interchange with MQL data. Text-Fabric can read and write MQL dumps. An\nMQL dump is a text file, like an SQL dump. It contains the instructions to\ncreate and fill a complete database.\nTF.exportMQL()\n1\nTF\n.\nexportMQL\n(\ndbName\n,\n \ndirName\n)\n\n\n\n\n\n\nDescription\nExports the complete TF dataset into single MQL database.\ndirName, dbName\nThe exported file will be written to \ndirName/dbName.mql\n. If \ndirName\n starts\nwith \n~\n, the \n~\n will be expanded to your home directory. Likewise, \n..\n will\nbe expanded to the parent of the current directory, and \n.\n to the current\ndirectory, both only at the start of \ndirName\n.\nCorrespondence TF and MQL\nThe resulting MQL database has the following properties with respect to the\nText-Fabric dataset it comes from:\nthe TF \nslots\n correspond exactly with the MQL \nmonads\n and have the same\n    numbers; provided the monad numbers in the MQL dump are consecutive. In MQL\n    this is not obligatory. Even if there gaps in the monads sequence, we will\n    fill the holes during conversion, so the slots are tightly consecutive;\nthe TF \nnodes\n correspond exactly with the MQL \nobjects\n and have the same\n    numbers\nNode features in MQL\nThe values of TF features are of two types, \nint\n and \nstr\n, and they translate\nto corresponding MQL types \ninteger\n and \nstring\n. The actual values do not\nundergo any transformation.\nThat means that in MQL queries, you use quotes if the feature is a string feature.\nOnly if the feature is a number feature, you may omit the quotes:\n1\n2\n[word sp='verb']\n[verse chapter=1 and verse=1]\n\n\n\n\n\nEnumeration types\nIt is attractive to use eumeration types for the values of a feature, whereever\npossible, because then you can query those features in MQL with \nIN\n and without\nquotes:\n1\n[chapter book IN (Genesis, Exodus)]\n\n\n\n\n\nWe will generate enumerations for eligible features.\nInteger values can already be queried like this, even if they are not part of an\nenumeration. So we restrict ourselves to node features with string values. We\nput the following extra restrictions:\nthe number of distinct values is less than 1000\nall values must be legal C names, in practice: starting with a letter,\n    followed by letters, digits, or \n_\n. The letters can only be plain ASCII\n    letters, uppercase and lowercase.\nFeatures that comply with these restrictions will get an enumeration type.\nCurrently, we provide no ways to configure this in more detail.\nMerged enumeration types\nInstead of creating separate enumeration types for individual features,\nwe collect all enumerated values for all those features into one\nbig enumeration type.\nThe reason is that MQL considers equal values in different types as\ndistinct values. If we had separate types, we could never compare\nvalues for different features.\nValues of edge features are ignored\nThere is no place for edge values in\nMQL. There is only one concept of feature in MQL: object features,\nwhich are node features.\nBut TF edges without values can be seen as node features: nodes are\nmapped onto sets of nodes to which the edges go. And that notion is supported by\nMQL:\nedge features are translated into MQL features of type \nLIST OF id_d\n,\ni.e. lists of object identifiers.\nLegal names in MQL\nMQL names for databases, object types and features must be valid C identifiers\n(yes, the computer language C). The requirements are:\nstart with a letter (ASCII, upper-case or lower-case)\nfollow by any sequence of ASCII upper/lower-case letters or digits or\n    underscores (\n_\n)\navoid being a reserved word in the C language\nSo, we have to change names coming from TF if they are invalid in MQL. We do\nthat by replacing illegal characters by \n_\n, and, if the result does not start\nwith a letter, we prepend an \nx\n. We do not check whether the name is a reserved\nC word.\nWith these provisos:\nthe given \ndbName\n correspond to the MQL \ndatabase name\nthe TF \notypes\n correspond to the MQL \nobjects\nthe TF \nfeatures\n correspond to the MQL \nfeatures\nFile size\nThe MQL export is usually quite massive (500 MB for the Hebrew Bible).\nIt can be compressed greatly, especially by the program \nbzip2\n.\nExisiting database\nIf you try to import an MQL file in Emdros, and there exists already a file or\ndirectory with the same name as the MQL database, your import will fail\nspectacularly. So do not do that. A good way to prevent it is:\nexport the MQL to outside your \ntext-fabric-data\n directory, e.g. to\n    \n~/Downloads\n;\nbefore importing the MQL file, delete the previous copy;\nDelete existing copy\n1\n2\ncd\n ~/Downloads\nrm dataset \n;\n mql -b \n3\n < dataset.mql\n\n\n\n\n\nTF.importMQL()\n1\nTF\n.\nimportMQL\n(\nmqlFile\n,\n \nslotType\n=\nNone\n,\n \notext\n=\nNone\n,\n \nmeta\n=\nNone\n)\n\n\n\n\n\n\nDescription\nConverts an MQL database dump to a Text-Fabric dataset.\nDestination directory\nIt is recommended to call this \nimportMQL\n on a TF instance called with\n1\nlocations\n=\ntargetDir\n,\n \nmodules\n=\n''\n\n\n\n\n\n\nThen the resulting features will be written in the targetDir.\nIn fact, the rules are exactly the same as for \nTF.save()\n.\nslotType\nYou have to tell which object type in the MQL file acts as the slot type,\nbecause TF cannot see that on its own.\notext\nYou can pass the information about sections and text formats as the parameter\n\notext\n. This info will end up in the \notext.tf\n feature. Pass it as a\ndictionary of keys and values, like so:\n1\n2\n3\n4\notext\n \n=\n \n{\n\n    \n'fmt:text-trans-plain'\n:\n \n'{glyphs}{trailer}'\n,\n\n    \n'sectionFeatures'\n:\n \n'book,chapter,verse'\n,\n\n\n}\n\n\n\n\n\n\nmeta\nLikewise, you can add a dictionary of keys and values that will added to the\nmetadata of all features. Handy to add provenance data here:\n1\n2\n3\n4\n5\nmeta\n \n=\n \ndict\n(\n\n    \ndataset\n=\n'DLC'\n,\n\n    \ndatasetName\n=\n'Digital Language Corpus'\n,\n\n    \nauthor\n=\n\"That 's me\"\n,\n\n\n)\n\n\n\n\n\n\nComputed data\n\u00b6\n\n\nPre-computing\nIn order to make the API work, Text-Fabric prepares some data and saves it in\nquick-load format. Most of this data are the features, but there is some extra\ndata needed for the special functions of the WARP features and the L-API.\nNormally, you do not use this data, but since it is there, it might be valuable,\nso we have made it accessible in the \nC\n-api, which we document here.\nC.levels.data\nDescription\nA sorted list of object types plus basic information about them.\nEach entry in the list has the shape\n1\n    \n(\notype\n,\n \naverageSlots\n,\n \nminNode\n,\n \nmaxNode\n)\n\n\n\n\n\n\nwhere \notype\n is the name of the node type, \naverageSlots\n the average size of\nobjects in this type, measured in slots (usually words). \nminNode\n is the first\nnode of this type, \nmaxNode\n the last, and the nodes of this node type are\nexactly the nodes between these two values (including).\nLevel computation and customization\nAll node types have a level, defined by the average amount of slots object of\nthat type usually occupy. The bigger the average object, the lower the levels.\nBooks have the lowest level, words the highest level.\nHowever, this can be overruled. Suppose you have a node type \nphrase\n and above\nit a node type \ncluster\n, i.e. phrases are contained in clusters, but not vice\nversa. If all phrases are contained in clusters, and some clusters have more\nthan one phrase, the automatic level ranking of node types works out well in\nthis case. But if clusters only have very small phrases, and the big phrases do\nnot occur in clusters, then the algorithm may assign a lower rank to clusters\nthan to phrases.\nIn general, it is too expensive to try to compute the levels in a sophisticated\nway. In order to remedy cases where the algorithm assigns wrong levels, you can\nadd a \n@levels\n key to the \notext\n config feature. See\n\ntext\n.\nC.order.data\nDescription\nAn \narray\n of all nodes in the correct order. This is the\norder in which \nN()\n alias \nNode()\n traverses all nodes.\nRationale\nTo order all nodes in the \ncanonical ordering\n is quite a bit of\nwork, and we need this ordering all the time.\nC.rank.data\nDescription\nAn \narray\n of all indices of all nodes in the canonical order\narray. It can be viewed as its inverse.\nOrder arbitrary node sets\nI we want to order a set of nodes in the canonical ordering, we need to know\nwhich position each node takes in the canonical order, in other words, at what\nindex we find it in the \nC.order.data\n array.\nC.levUp.data and C.levDown.data\nDescription\nThese tables feed the \nL.d()\n and \nL.u()\n functions.\nUse with care\nThey consist of a fair amount of megabytes, so they are heavily optimized.\nIt is not advisable to use them directly, it is far better to use the \nL\n functions.\nOnly when every bit of performance waste has to be squeezed out, this raw data\nmight be a deal.\nC.boundary.data\nDescription\nThese tables feed the \nL.n()\n and \nL.p()\n functions.\nIt is a tuple consisting of \nfirstSlots\n and \nlastSlots\n.\nThey are indexes for the first slot\nand last slot of nodes.\nSlot index\nFor each slot, \nfirstSlot\n gives all nodes (except\nslots) that start at that slot, and \nlastSlot\n gives all nodes (except slots)\nthat end at that slot.\nBoth \nfirstSlot\n and \nlastSlot\n are tuples, and the\ninformation for node \nn\n can be found at position \nn-MaxSlot-1\n.\nC.sections.data\nDescription\nLet us assume for the sake of clarity, that the node type of section level 1 is\n\nbook\n, that of level 2 is \nchapter\n, and that of level 3 is \nverse\n. And\nsuppose that we have features, named \nbookHeading\n, \nchapterHeading\n, and\n\nverseHeading\n that give the names or numbers of these.\nCustom names\nNote that the terms \nbook\n, \nchapter\n, \nverse\n are not baked into Text-Fabric.\nIt is the corpus data, especially the \notext\n config feature that\nspells out the names of the sections.\nThen \nC.section.data\n is a tuple of two mappings , let us call them \nchapters\n\nand \nverses\n.\nchapters\n is a mapping, keyed by \nbook\n \nnodes\n, and then by\nby chapter \nheadings\n, giving the corresponding\nchapter \nnode\ns as values.\nverses\n is a mapping, keyed by \nbook\n \nnodes\n, and then\nby chapter \nheadings\n, and then by verse \nheadings\n,\ngiving the corresponding verse \nnode\ns as values.\nSupporting the \nT\n-Api\nThe \nT\n-api is good in mapping nodes unto sections, such as books, chapters,\nverses and back. It knows how many chapters each book has, and how many verses\neach chapter.\nThe \nT\n api is meant to make your life easier when you have to find passage\nlabels by nodes or vice versa. That is why you probably never need to consult\nthe underlying data. But you can! That data is stored in\nMiscellaneous\n\u00b6\n\n\nTF.version\nDescription\nContains the version number of the Text-Fabric\nlibrary.\nTF.banner\nDescription\nContains the name and the version of the Text-Fabric\nlibrary.",
            "title": "General"
        },
        {
            "location": "/Api/General/#text-fabric-api",
            "text": "Tutorial The tutorials for the Hebrew Bible \nand the Uruk Cuneiform Tablets \nput the Text-Fabric API on show for two distinguished (and vastly different) corpora. Generic API This is the API of Text-Fabric in general.\nText-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several additions that make working\nwith specific corpora easier. Hebrew Bible:  BHSA Proto-cuneiform tablets from Uruk:  Cunei",
            "title": "Text-Fabric API"
        },
        {
            "location": "/Api/General/#loading",
            "text": "TF=Fabric() 1\n2 from   tf.fabric   import   Fabric  TF   =   Fabric ( locations = directories ,   modules = subdirectories ,   silent = False )    Description Text-Fabric is initialized for a corpus. It will search a set of directories\nand catalog all  .tf  files it finds there.\nThese are the features you can subsequently load. Here  directories  and  subdirectories  are strings with directory names\nseparated by newlines, or iterables of directories. locations, modules The directories specified in  locations  will be searched for  modules , which\nare paths that will be appended to the paths in  locations . All  .tf  files (non-recursively) in any  module  will be added to the feature\nset to be loaded in this session. The order in  modules  is important, because\nif a feature occurs in multiple modules, the last one will be chosen. In this\nway you can easily override certain features in one module by features in an\nother module of your choice. otext@ in modules If modules contain features with a name starting with  otext@ , then the format\ndefinitions in these features will be added to the format definitions in the\nregular  otext  feature (which is a WARP feature). In this way, modules that\ndefine new features for text representation, also can add new formats to the\nText-API. Defaults The  locations  list has a few defaults: 1\n2\n3 ~/Downloads/text-fabric-data\n~/text-fabric-data\n~/github/text-fabric-data   So if you have stored your main Text-Fabric dataset in text-fabric-data  in one of these directories\nyou do not have to pass a location to Fabric. The  modules  list defaults to  [''] . So if you leave it out, Text-Fabric will\njust search the paths specified in  locations . silent If  silent=True  is passed, banners and normal progress messages are suppressed. TF.explore() 1\n2 features   =   TF . explore ( silent = False ,   show = True )  features    or 1\n2 TF . explore ( silent = False ,   show = False )  TF . featureSets    Description This will give you a dictionary of all available features by kind. The kinds\nare:  nodes ,  edges ,  configs ,  computeds . silent With  silent=False  a message containing the total numbers of features is issued. show The resulting dictionary is delivered in  TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api   =   TF . load ( features ,   add = False ,   silent = False )    Description Reads the features indicated by  features  and loads them in memory\nready to be used in the rest of the program. features features  is a string containing space separated feature names, or an\niterable of feature names. The feature names are just the names of  .tf  files\nwithout directory information and without extension. add If later on you want load more features, you can either: add the features to the original  load()  statement and just run it again make a new statement:  TF.load(newfeatures, add=True) . The new features will\n    be added to the same api, so you do not have to to call\n     api.makeAvailableIn(globals())  again after this! silent The features will be loaded rather silently, most messages will be suppressed.\nTime consuming operations will always be announced, so that you know what\nText-Fabric is doing. If  silent=True  is passed, all informational messages\nwill be suppressed. This is handy I you want to load data as part of other\nmethods, on-the-fly. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ())    Description This method will export every member of the API (such as  N ,  F ,  E ,  L ,  T , info ) to the global namespace. From now on, we will omit the  api.  in our\ndocumentation. Contents of the API After having loaded the features by  api = TF.load(...) , the  api  harbours\nyour Text-Fabric API. You can access node feature  mydata  by  api.F.mydata.v(node) , edge\nfeature  mylink  by  api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit\ntedious to write the initial  api.  all the time.\nBy this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital\nletters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored    Description If you want to know which features were found but ignored (because the feature\nis also present in another, later, location), you can use this attribute\nto inspect the ignored features and their locations. loadLog() 1 api . loadlog ()    Description After loading you can view all messages using this method.\nIt also shows the messages that have been suppressed due to  silent=True .",
            "title": "Loading"
        },
        {
            "location": "/Api/General/#navigating-nodes",
            "text": "N() 1\n2 for   n   in   N (): \n     action    Description The result of  N()  is a generator that walks through all nodes in the canonical order  (see below).\nIterating over  N()  delivers you all words and structural elements of\nyour corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in\na suitable order. Occasionally, during the walk you might want to visit\nembedding or embedded nodes to glean some feature information from them. More ways of walking Later, under  Features  there is another convenient way to walk through\nnodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way\nthat you can enumerate all nodes in the order you encounter them if you\nwalk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence,\n    because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the\n    smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a\nbook node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis\n1:1, then a sentence node, then a clause node, a phrase node, and the first word\nnode. Then follow all word nodes in the first phrase, then the phrase node of\nthe second phrase, followed by the word nodes in that phrase. When ever you\nenter a higher structure, you will first get the node corresponding to that\nstructure, and after that the nodes corresponding to the building blocks of that\nstructure. This concept follows the intuition that slot sets with smaller elements come\nbefore slot set with bigger elements, and embedding slot sets come before\nembedded slot sets. Hence, if you enumerate a set of nodes that happens to\nconstitute a tree hierarchy based on slot set embedding, and you enumerate those\nnodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994),  Text Databases. One Database Model and Several\nRetrieval Languages , number 14 in Language and Computers, Editions Rodopi,\nAmsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, http://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by\nDoedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements\navailable when working with an element. That is the advantage of pre-order over\npost-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet )    Description delivers an iterable of nodes as a tuple sorted by the  canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList   =   sorted ( nodes ,   key = sortKey )    Description A function that provides for each node the key to be used to sort nodes in the\ncanonical ordering. That means that the following two pieces of code do the same\nthing: sortNodes(nodeSet)  and  sorted(nodeSet, key=sortKey) . Sorting tuples of nodes Handy to sort things that are not nodes themselves, but data structures with\nnodes in it, e.g. search results: if  results  is a list of tuples of nodes, we\ncould sort them in canonical order like this: 1 sorted ( nodeSet ,   key = lambda   r :   sortKey ( r [ 0 ]))",
            "title": "Navigating nodes"
        },
        {
            "location": "/Api/General/#locality",
            "text": "Local navigation Here are the methods by which you can navigate easily from a node to its\nneighbours: parents and children, previous and next siblings. L The Locality API is exposed as  L  or  Locality . otype parameter In all of the following  L -functions, if the  otype  parameter is passed, the result is filtered and\nonly nodes with  otype=nodeType  are retained. Results of the  L.  functions are tuples, not single nodes Even if an  L -function returns a single node, it is packed in a  tuple .\n  So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ]    L.u() 1 L . u ( node ,   otype = nodeType )    Description Produces an ordered tuple of nodes  upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered.\nThe result never includes  node  itself. L.d() 1 L . d ( node ,   otype = nodeType )    Description Produces an ordered tuple of nodes  downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered.\nThe result never includes  node  itself. L.n() 1 L . n ( node ,   otype = nodeType )    Description Produces an ordered tuple of adjacent  next  nodes. node The node whose right adjacent nodes will be delivered;\ni.e. the nodes whose first slot immediately follow the last slot \nof  node .\nThe result never includes  node  itself. L.p() 1 L . p ( node ,   otype = nodeType )    Description Produces an ordered tuple of adjacent  previous  nodes from  node , i.e. nodes\nwhose last slot just precedes the first slot of  node . Description Produces an ordered tuple of adjacent  previous  nodes. node The node whose lefy adjacent nodes will be delivered;\ni.e. the nodes whose last slot immediately precede the first slot \nof  node . Locality and levels Here is something that is very important to be aware of when using  sortNodes \nand the  L.d(n)  and  L.u(n)  methods. When we order nodes and report on which nodes embed which other nodes, we do not\nonly take into account the sets of slots the nodes occupy, but also their level . See  levels  and  text . Both the  L.d(n)  and  L.u(n)  work as follows: L.d(n)  returns nodes such that embedding nodes come before embedded nodes\n    words) L.u(n)  returns nodes such that embedded nodes come before embedding nodes\n    books) N.B.:  Suppose you have node types  verse  and  sentence , and usually a\nverse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode)  will contain sentence nodes, L.d(sentenceNode)  will  not  contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the\nsame words, then that is a case where: L.d(verseNode)  will contain one sentence node, L.d(sentenceNode)  will contain  one  verse node.",
            "title": "Locality"
        },
        {
            "location": "/Api/General/#text",
            "text": "Overview Here are the functions that enable you to get the actual text in the dataset.\nThere are several things to accomplish here, such as getting text given book, chapter, and verse; given a node, produce the book, chapter and verse indicators in which the node\n    is contained; handle multilingual book names; switch between various text representations. The details of the Text API are dependent on the  warp  feature  otext , which\nis a config feature. T The Text API is exposed as  T  or  Text .",
            "title": "Text"
        },
        {
            "location": "/Api/General/#sections",
            "text": "Section levels In  otext  the main section levels (usually  book ,  chapter ,  verse ) can be\ndefined. It loads the features it needs (so you do not have to specify those\nfeatures, unless you want to use them via  F ). And finally, it makes some\nfunctions available by which you can make handy use of that information. Section levels are generic In this documentation, we call the main section level  book , the second level chapter , and the third level  verse . Text-Fabric, however, is completely\nagnostic about how these levels are called. It is prepared to distinguish three\nsection levels, but how they are called, must be configured in the dataset. The\ntask of the  otext  feature is to declare which node type and feature correspond\nwith which section level. Text-Fabric assumes that the first section level may\nhave multilingual headings, but that section levels two and three have single\nlanguage headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or\nintegers, depending on whether your dataset has declared the corresponding\nfeature  valueType  as  str  or as  int . Conceivably, other works might have chapter and verse numbers\nlike  XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. otext is optional If  otext  is missing, the Text API will not be build. If it exists, but\ndoes not specify sections, that part of the Text API will not be built. Likewise\nfor text representations. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the\nsense that node types that act as containers have a lower level than node types\nthat act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for\nyour dataset, you can configure the right order in the  otext  feature, by means\nof a key  levels  with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign   T.sectionFromNode() 1 T . sectionFromNode ( node ,   lastSlot = False ,   lang = 'en' )    Description Returns the book/chapter/verse indications that correspond to the reference\nnode, which is the first or last slot belonging  n , dependent on  lastSlot .\nThe result is a tuple, consisting of the book name (in language  lang ), the\nchapter number, and the verse number. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the  node  argument\nor the first node. lang The language to be used for the section parts, as far as they are language dependent. crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse.\nIn those cases, this function will return a different results for lastSlot=False  and  lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a  None  in the\ncorresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section ,   lang = 'en' )    Description Given a  section  tuple, return the node of it. section section  consists of a book name (in language  lang ),\nand a chapter number and a verse\nnumber (both as strings or number depending on the value type of the\ncorresponding feature). The verse number may be left out, the result is then a\nchapter node. Both verse and chapter numbers may be left out, the result is then\na book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent.",
            "title": "Sections"
        },
        {
            "location": "/Api/General/#book-names-and-languages",
            "text": "Book names and nodes The names of the books may be available in multiple languages. The book names\nare stored in node features with names of the form  book@ la , where  la  is\nthe  ISO 639  two-letter code for that\nlanguage. Text-Fabric will always load these features. T.languages 1 T . languages    Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node ,   lang = 'en' )    Description gives the name of the book in which a node occurs. node The node in question. lang The  lang  parameter is a two letter language code. The default is  en \n(English). If there is no feature data for the language chosen, the value of the ordinary book  feature of the dataset will be returned. Works for all nodes n  may or may not be a book node. If not,  bookName()  retrieves the\nembedding book node first. T.bookNode() 1 T . bookNode ( name ,   lang = 'en' )    Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the  name  parameter. If  lang  can not be found, the value of the ordinary  book  feature of the\ndataset will be used. If  name  cannot be found in the specified language,  None  will be returned. Function name follows configured section level If your dataset has configured section level one under an other name,\nsay  tablet , then these two methods follow that name. Instead of  T.bookName() \nand  T.bookNode()  we have then  T.tabletName()  and  T.tabletNode() .",
            "title": "Book names and languages"
        },
        {
            "location": "/Api/General/#text-representation",
            "text": "Text formats Text can be represented in multiple ways. We provide a number of formats with\nstructured names. A format name is a string of keywords separated by  - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is  text-orig-full , we assume that every TF dataset defines\nthis format. Remember that the formats are defined in the  otext  warp config feature of your\nset, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats.\nThey do not have to complied with the above-mentioned scheme. T.formats 1 T . formats    Description Show the text representation formats that have been defined in your dataset. T.text() 1 T . text ( nodes ,   fmt = None )    Description Gives the text that corresponds to a bunch of nodes. nodes nodes  can be an arbitrary iterable of nodes.\nNo attempt will be made to sort the nodes.\nIf you need order, it is\nbetter to order the nodes before you feed them to  T.text() . fmt The format of text-representation is given with  fmt , with default  text-orig-full .\nIf the  fmt \ncannot be found, the default is taken.\nIf the default format is not defined in the otext  feature of the dataset,\nthe node numbers will be output instead. This function does not give error messages, because that could easily overwhelm\nthe output stream, especially in a notebook. Non slot nodes allowed In most cases, the nodes fed to  T.text()  are slots, and the formats are\ntemplates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features\ndefined for a non-slot node type. If, for example, your slot type is  glyph , and you want a format that renders\nlexemes, which are not defined for glyphs but for words, you can just define a\nformat in terms of word features. It is your responsibility to take care to use the formats for node types for\nwhich they make sense. Escape whitespace in formats When defining formats in  otext.tf , if you need a newline or tab in the format,\nspecify it as  \\n  and  \\t .",
            "title": "Text representation"
        },
        {
            "location": "/Api/General/#searching",
            "text": "What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are\ninterested in has to be written as a  search template , offered to  S.search() \nwhich returns the matching results as tuples of nodes. A search template expresses a pattern of nodes and edges with additional conditions\nalso known as  quantifiers . A result of a search template is a tuple of nodes that instantiate the nodes in\nthe pattern, in such a way that the edges of the pattern are also instantiated\nbetween the nodes of the result tuple. Moreover, the quantifiers will hold for each result tuple. S The Search API is exposed as  S  or  Search .",
            "title": "Searching"
        },
        {
            "location": "/Api/General/#search-templates",
            "text": "Search primer A search template consists of a bunch of lines, possibly indented, that specify\nobjects to look for. Here is a simple example: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        word pos=noun gender=feminine number=singular   This template looks for word combinations within a sentence within chapter 2 of\neither Genesis or Exodus, where one of the words is a verb and the other is a\nnoun. Both have a feminine inflection, but the verb is plural and the noun is\nsingular. The indentation signifies embedding, i.e. containment. The two words are\ncontained in the same sentence, the sentence is contained in the chapter, the\nchapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for\nthis. The order of the two words is immaterial. If there are cases where the verb\nfollows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words\nintervening between the noun and the verb, they will be included in the results. Speaking of results: the  S.search()  function returns its results as tuples of\nnodes: 1 ( book ,   chapter ,   sentence ,   word1 ,   word2 )    With these nodes in hand, you can programmatically gather all information about\nthe results that the corpus provides. If the order between the verb and the noun is important, you can specify that as\nan additional constraint. You can give the words a name, and state a relational\ncondition. Here we state that the noun precedes the verb. 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        vb:word pos=verb gender=feminine number=plural\n        nn:word pos=noun gender=feminine number=singular\nnn < vb   This can be written a bit more economically as: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        > word pos=noun gender=feminine number=singular   If you insist that the noun immediately precedes the verb, you can use a\ndifferent relational operator: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        word pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular   There are more kinds of relational operators. If the noun must be the first word in the sentence, you can specify it as 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        w:word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\ns =: w   or a bit more economically: 1\n2\n3\n4\n5 book name=Genesis|Exodus\n   chapter number=2\n      sentence\n        =: word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural   If the verb must be the last word in the sentence, you can specify it as 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: w:word pos=verb gender=feminine number=plural\ns := w   or a bit more economically: 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n   chapter number=2\n      s:sentence\n        word pos=noun gender=feminine number=singular\n        <: word pos=verb gender=feminine number=plural\n        :=   You can also use the  edge features  in the corpus as\nrelational operators as well. Suppose we have an edge feature  sub  between clauses, such that if main clause m  has subordinate clauses  s1 ,  s2  and  s3 , then 1 E.sub.f(m) = (s1, s2, s3)   You can use this relation in search. Suppose we want to find the noun verb pair\nin subordinate clauses only. We can use this template: 1\n2\n3\n4\n5\n6\n7 book name=Genesis|Exodus\n   chapter number=2\n      m:clause\n        s:clause\n          word pos=verb gender=feminine number=plural\n          :> word pos=noun gender=feminine number=singular\nm -sub> s   or a bit more economically: 1\n2\n3\n4\n5\n6 book name=Genesis|Exodus\n  chapter number=2\n    clause\n      -sub> clause\n        word    pos=verb gender=feminine number=plural\n        :> word pos=noun gender=feminine number=singular   Read  m -sub> s  as: there is a  sub -arrow from  m  to  s . Edge features may have values.\nFor example, the crossref feature \nis a set of edges between parallel verses, with the levels of confidence\nas values. This number is an integer between 0 and 100.\nWe can ask for parallel verses in an unqualified way: 1\n2 verse\n-crossref> verse   But we can also ask for the cases with a specific confidence: 1\n2 verse\n-crossref=90> verse   or cases with a high confidence: 1\n2 verse\n-crossref>95> verse   or cases with a low confidence: 1\n2 verse\n-crossref<80> verse   All feature conditions that you can assert on node features, you can also\nassert for edge features. If an edge feature is integer valued, such as  crossref \nyou can use comparisons; if it is string valued, you can use regular expressions.\nIn both cases you can also use the other constructs, such as 1\n2 verse\n-crossref=66|77> verse   To get a more specific introduction to search, consult the search tutorials for Hebrew  and Cuneiform . Finally an example with quantifiers. We want all clauses where Pred-phrases\nconsist of verbs only: 1\n2\n3\n4\n5\n6\n7\n8   clause\n  /where/\n    phrase function=Pred\n  /have/\n    /without/\n      word sp#verb\n    /-/\n  /-/   Search template reference We have these kinds of lines in a template: comment  lines if a line starts with  #  it is a comment line`; you cannot comment out parts of lines, only whole lines; if a line is empty or has whitespace only, it is a comment line; comment lines are allowed everywhere; comment lines are ignored. atom  lines (simple):  indent name:otype-or-set features Examples word pos=verb gender=feminine vb:word pos=verb gender=feminine vb pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space\n    characters, where tabs count for just 1.  Avoid tabs! . The  name:  part is optional.\n    If present, it defines a name for this atom that can be used\n    in relational comparisons and other atoms. The  otype-or-set  part is optional.\n    If it is absent, the  name  part must be present.\n    The meaning of 1\n2 p:phrase sp=verb\np vs=qal   is identical to the meaning of 1\n2\n3 p:phrase sp=verb\npnew:phrase vs=qal\np = pnew   (with relop):  indent op name:otype-or-set features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom\n    and this atom. The preceding atom may be the parent, provided we are at its first child, or\n    it may the preceding sibling. You can leave out the  name:otype-or-set features  bit. In that case, the\n    relation holds between the preceding atom and its parent. The  name:  part is optional. Exactly as in the case without relop. The  otype-or-set  part is optional. Exactly as in the case without relop. The otype-or-set is either a node type that exists in your TF data set,\nor it is the name of a set that you have passed in the  sets  parameter alongside\nthe query itself when you call  S.search()  or  S.study() . See  feature specifications  below for all\nfull variety of feature constraints on nodes and edges. feature  lines:  features Indent is not significant. Continuation of feature constraints after a\n    preceding atom line or other feature line. This way you can divide lengthy\n    feature constraints over multiple lines. See  feature specifications  below for the\nfull variety of feature constraints on nodes and edges. relation  lines:  name operator name s := w m -sub> s m <sub- s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values. See  relational operators  below for the\nwhole spectrum of relational constraints on nodes. quantifier  sub-templates:\n    Atom lines that contain an otype or set may be followed by\n    Quantifiers consist of search templates themselves, demarcated by some\n    special keywords: /without/ /where/  and  /have/ /with/  and  /or/ /-/ See  quantifiers  below for all the syntax and semantics.",
            "title": "Search templates"
        },
        {
            "location": "/Api/General/#feature-specifications",
            "text": "Features specs The  features  above is a specification of what features with which values to\nsearch for. This specification must be written as a white-space separated list\nof  feature specs . A  feature spec  has the form  name   valueSpec , with no space between the  name \nand the  valueSpec .\nThe  valueSpec  may have the following forms and meanings: form evaluates to  True  the feature  name  ... has any value except  None # has value  None = values has one of the values specified # values has none of the values specified > value is greater than  value < value is less than  value ~ regular expression has a value and it matches  regular expression All these forms are also valid as  - name   form >  and  < name   form - , in which case\nthey specify value constraints on edge features.\nThis is only meaningful if the edge feature is declared to have values (most edge features\ndo not have values). Additional constraints There may be no space around the  =#<>~ . name  must be a feature name that exists in the dataset. If it references a\n    feature that is not yet loaded, the feature will be loaded automatically. values  must be a  |  separated list of feature values, no quotes. No spaces\n    around the  | . If you need a space or  |  or  \\  in a value, escape it by a\n     \\ . Escape tabs and newlines as  \\t  and  \\n . When comparing values with  <  and  > : value  must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on\n    string-valued features. regular expression  must be a string that conforms to the Python\n     regular axpression syntax If you need a space in your regular expression, you have to escape it with a\n     \\ . You can do regular expressions only on string-valued features, not on\n    number-valued features.",
            "title": "Feature specifications"
        },
        {
            "location": "/Api/General/#relational-operators",
            "text": "Operator lines Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the\n    same slots are still unequal)\n     # : is unequal (meaning a different node, a clause and a verse that occupy\n    the same slots are still unequal)\n     <   > : before and after (in the  canonical ordering )\n     Slot comparison == : occupy the same slots (identical slot sets)\n     && : overlap (the intersection of both slot sets is not empty)\n     ## : occupy different slots (but they may overlap, the set of slots of the\n    two are different as sets)\n     || : occupy disjoint slots (no slot occupied by the one is also occupied by\n    the other)\n     [[ ]] : embeds and contains (slot set inclusion, in both directions)\n     <<   >> : before and after (with respect to the slots occupied: left ends\n    before right starts and vice versa)\n     <:   :> :  adjacent  before and after (with respect to the slots occupied:\n    left ends immediately before right starts and vice versa)\n     =:  left and right start at the same slot\n     :=  left and right end at the same slot\n     ::  left and right start and end at the same slot\n     Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring\nthat one slot is equal to an other slot, you can require that they are  k-near ,\ni.e. they are at most  k  apart. Here are the relationships where you can do\nthat. Instead of typing the letter  k , provide the actual number you want. <k:   :k> :  k - adjacent  before and after (with respect to the slots\n    occupied: left ends  k -near where right starts and vice versa)\n     =k:  left and right start at  k -near slots\n     :k=  left and right end at  k -near slots\n     :k:  left and right start and end at  k -near slots\n     Based on edge features - name >   < name - : connected by the edge feature  name in both directions; these forms work for edges that do and do not have values; - name   valueSpec >   < name   valueSpec - : connected by the edge feature  name in both directions; these forms work only for edges that do have values.",
            "title": "Relational operators"
        },
        {
            "location": "/Api/General/#quantifiers",
            "text": "Quantifiers Experimental This part of search templates is still experimental. bugs may be discovered the syntax of quantifiers may change What is a quantifier? Quantifiers are powerful expressions in templates. They state conditions on a given atom in your template.\nThe atom in question is called the  parent  atom.\nThe conditions may involve  many  nodes that are related to the parent,\nas in: all embedded words are a verb ; without a following predicate phrase ; with a mother clause or a mother phrase . That is where the term  quantifier  comes from. A quantifier  quantifies  its parent atom. /without/ Syntax: 1\n2\n3\n4 atom\n/without/\ntemplateN\n/-/   Meaning: node  r  is a result of this template if and only if  r  is a result of  atom  and\nthere is no tuple  RN  such that ( r ,  RN ) is a result of 1\n2 atom\ntemplateN   /where/ Syntax: 1\n2\n3\n4\n5\n6 atom\n/where/\ntemplateA\n/have/\ntemplateH\n/-/   Meaning: node  r  is a result of this template if and only if  r  is a result of  atom  and\nfor all tuples ( RA ) such that ( r ,  RA ) is a result of 1\n2 atom\ntemplateA   there is a tuple  RH  such that ( r ,  RA ,  RH )  is a result of 1\n2\n3 atom\ntemplateA\ntemplateH   /with/ Syntax: 1\n2\n3\n4\n5\n6\n7\n8 atom\n/with/\ntemplateO1\n/or/\ntemplateO2\n/or/\ntemplateO3\n/-/   Meaning: node  r  is a result of this template if and only if:\nthere is a tuple  R1  such that ( r ,  R1 ) is a result of 1\n2 atom\ntemplateO1   or there is a tuple  R2  such that ( r ,  R2 ) is a result of 1\n2 atom\ntemplateO2   or there is a tuple  R3  such that ( r ,  R3 ) is a result of 1\n2 atom\ntemplateO3   ???+ note \"1 or more alternatives\n    This quantifier can be used with any number of  /or/  keywords, including\n    none at all. If there is no  /or/ , there is just one alternative.\n    The only difference between  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ```\natom\n/with/\ntemplate\n/-/\n```\n\nand\n\n```\natom\ntemplate\n```\n\nis that the results of the first query contain tuples with only one\nelement, corresponding to the `atom`.\nThe second query contains tuples of which the first element\ncorresponds to the `atom`, and the remaining members correspond to\nthe `template`.   Parent The  atom  bit is an atom line, it acts as the  parent  of the quantifier. Multiple quantifiers You may have multiple quantifiers for one parent. Not in result tuples Whereas a the search for a normal template\nproceeds by finding a tuples that instantiates all it nodes\nin such a way that all relationships expressed in the template hold, a quantifier template\nis not instantiated.\nIt asserts a condition that has to be tested for all nodes relative\nits parent. None of the atoms in a template of a quantifier corresponds to\na node in a final result tuple. May be nested Templates within a quantifier may contain other quantifiers.\nThe idea is, that whenever a search template is evaluated, quantifiers at the outer level of \nget interpreted.\nThis interpretation gives rise to one or more templates to be constructed and run.\nThose new templates have been stripped of the outer layer of quantifiers,\nand when these templates are executed, the quantifiers at the next level have become outer.\nAnd so on. Restrictions Due to the implementation of quantifiers there are certain restrictions. Quantifiers must be put immediately below their parents or below\n    preceding quantifiers of the same parent. The keywords of a quantifier must appear on lines with exactly the same indentation\n    as the atom they quantify. The templates of a quantifier must have equal or greater indent than its keywords; The names accessible to the templates inside a quantifier are: the name  .. , which is the name of the atom that is quantified;\n    this name is automagically valid in quantifier templates; the name of the atom that is quantified (if that atom has a given name); names defined in the template itself; in  /where/ ,  templateH  may use names defined in  templateA ;\n    but only if these names are defined outside any quantifier of\n     templateA . The following situations block the visibility of names: in  /with/ ,  templateO i  may not use names defined in  templateO j  for  j  other than  i ; names defined outer quantifiers are not accessible in inner quantifiers; names defined inner quantifiers are not accessible in outer quantifiers. When you nest quantifiers, think of the way they will be recomposed into\nordinary templates. This dictates whether your quantifier is syntactically valid or not. Indentation The indentation in quantifiers relative to their parent atom will be preserved. Nested quantifiers Consider 1\n2\n3\n4\n5\n6\n7\n8\n9 clause\n/where/\n  phrase function=Pred\n/have/\n  /without/\n    word sp#verb\n  /-/\n/-/\n  phrase function=Subj   The auxiliary templates that will be run are: For the outer quantifier: 1\n2 clause\n  phrase function=Pred   and 1\n2\n3\n4\n5 clause\n  phrase function=Pred\n  /without/\n    word sp#verb\n  /-/   For the inner quantifier: 1\n2 phrase function=Pred\n  word sp#verb   Note that the auxiliary template for the inner quantifier\nis shifted in its entirety to the left, but that the \nrelative indentation is exactly as it shows in the original template. Implementation Here is a description of the implementation of the quantifiers.\nIt is not the real implementation, but it makes clear what is going on, and why\nthe quantifiers have certain limitations, and how indentation works. The basic idea is: a quantifier leads to the execution of one or more separate searche templates; the results of these searches are combined by means of set operations:\n     difference ,  intersection ,  union , dependent on the nature of the quantifier; the end result of this combination will fed as a custom set to the original template after\n    stripping the whole quantifier from that template.\n    So we replace a quantifier by a custom set. Suppose we have 1\n2\n3\n4\n5\n6 clause typ=Wx0\nQUANTIFIER1\nQUANTIFIER2\n...\nQUANTIFIERn\n  rest-of-template   We compute a set of clauses  filteredClauses1  based on  1\n2 clause typ=Wx0\nQUANTIFIER1   and then compute a new set  filteredClauses2  based on 1\n2\n3\n4\n5 S.search('''\nfclause typ=Wx0\nQUANTIFIER2\n''',\n    customSets=dict(fclause=filteredClauses1)   and so on until we have had QUANTIFIERn,\nleading to a set  filteredClausesN  of clauses\nthat pass all filters set by the quantifiers. Finally, we deliver the results of 1\n2\n3\n4\n5 S.search('''\nfclause\n  rest-of-template\n''',\n    customSets=dict(fclause=filteredClausesN)",
            "title": "Quantifiers"
        },
        {
            "location": "/Api/General/#search-api",
            "text": "S.relationsLegend() 1 S . relationsLegend ()    Description Gives dynamic help about the basic relations that you can use in your search\ntemplate. It includes the edge features that are available in your dataset. S.search() 1 S . search ( query ,   limit = None ,   shallow = False ,   sets = None ,   withContext = None )    Description Searches for combinations of nodes that together match a search template.\nThis method returns a  generator  which yields the results one by one. One result\nis a tuple of nodes, where each node corresponds to an  atom -line in your search template . query The query is a search template, i.e. a string that conforms to the rules described above. shallow If  True  or  1 , the result is a set of things that match the top-level element\nof the  query . If  2  or a bigger number  n , return the set of truncated result tuples: only\nthe first  n  members of each tuple is retained. If  False  or  0 , a sorted list of all result tuples will be returned. sets If not  None , it should be a dictionary of sets, keyed by a names.\nIn  query  you can refer to those names to invoke those sets. limit If  limit  is a number, it will fetch only that many results. withContext Specifies that for all nodes in the result-(tuple)s context information\nhas to be supplied. If  withContext  is  True , all features in the current TF dataset will\nbe looked up for all nodes in the results of the query. If it is a list, the list will be split on white-space into a list of\nfeature names. These are the features that will be looked up for all result nodes. You can also pass an iterable of feature names. You may ask for node features and for edge features. For edge features, only\nnode pairs within the result set will be delivered. If edge features carry values,\nthe values will also be delivered. If you ask for any features at all, the warp features  otype  and  oslots  will\nalways be in the result. If  withContext  is not  None , the result of  search()  is a tuple (   queryResults   ,   contextInfo   ) ,\nwhere  queryResults  is a sorted list of results, never a generator,\neven if  limit  is  None ). contextInfo  is a dictionary of feature data, keyed by the name of the feature.\nThe values are dictionaries keyed by node (integers) and valued by the values of\nthat feature for those nodes.  None  values will not be included in the dictionary.  TF as Database By means of  S.search(query, withContext=True)  you can use one  TF  instance as a\ndatabase that multiple clients can use without the need for each client to call the \ncostly  load  methods.\nYou have to come up with a process that runs TF, has all features loaded, and\nthat can respond to queries from other processes. Webservers can use such a daemonized TF to build efficient controllers. Support for TF as daemon is upcoming, it will be included in the Text-Fabric code base. Generator versus tuple If  limit  is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and\ntranslated into a search plan. The following methods expose parts of the search\nprocess, and may provide you with useful information in case the search does not\ndeliver what you expect. see the plan the method  S.showPlan()  below shows you at a glance the correspondence\nbetween the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate ,   strategy = None ,   silent = False ,   shallow = False ,   sets = None )    Description Your search template will be checked, studied, the search\nspace will be narrowed down, and a plan for retrieving the results will be set\nup. If your query has quantifiers, the asscociated search templates will be constructed\nand executed. These searches will be reported clearly. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched\nmatters a lot.\nThe search strategy is an implementation detail, but we bring\nit to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x')  and you will get a list of options reported to\nchoose from. Feel free to experiment. To see what the strategies do,\nsee the  code . silent If you want to suppress most of the output, say  silent=True . shallow, sets As in  S.search() . S.showPlan() 1 S . showPlan ( details = False )    Description Search results are tuples of nodes and the plan shows which part of the tuple\ncorresponds to which part of the search template. details If you say  details=True , you also get an overview of the search space and a\ndescription of how the results will be retrieved. after S.study() This function is only meaningful after a call to  S.study() .",
            "title": "Search API"
        },
        {
            "location": "/Api/General/#search-results",
            "text": "Preparation versus result fetching The method  S.search()  above combines the interpretation of a given\ntemplate, the setting up of a plan, the constraining of the search space\nand the fetching of results. Here are a few methods that do actual result fetching.\nThey must be called after a previous  S.search()  or  S.study() . S.count() 1 S . count ( progress = None ,   limit = None )    Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message.\nThe frequency is  progress  results, default every 100. limit Fetch results up to a given  limit , default 1000.\nSetting  limit  to 0 or a negative value means no limit: all results will be\ncounted. why needed You typically need this in cases where result fetching turns out to\nbe (very) slow. generator versus list len(S.results())  does not work, because  S.results()  is a generator\nthat delivers its results as they come. S.fetch() 1 S . fetch ( limit = None )    Description Finally, you can retrieve the results. The result of  fetch()  is not a list of\nall results, but a  generator . It will retrieve results as long as they are\nrequested and their are still results. limit Tries to get that many results and collects them in a tuple.\nSo if limit is not  None , the result is a tuple with a known length. Iterating over the  fetch()  generator You typically fetch results by saying: 1\n2\n3\n4 i   =   0  for   r   in   S . results (): \n     do_something ( r [ 0 ]) \n     do_something_else ( r [ 1 ])    Alternatively, you can set the  limit  parameter, to ask for just so many\nresults. They will be fetched, and when they are all collected, returned as a\ntuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 )    gives you the first bunch of results quickly. S.glean() 1 S . glean ( r )    Description A search result is just a tuple of nodes that correspond to your template, as\nindicated by  showPlan() . Nodes give you access to all information that the\ncorpus has about it. The  glean()  function is here to just give you a first impression quickly.   r Pass a raw result tuple  r , and you get a string indicating where it occurs,\nin terms of sections, \nand what text is associated with the results. Inspecting results 1\n2 for   result   in   S . fetch ( limit = 10 ): \n     print ( S . glean ( result ))    is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been\nobtained by search or not. More ways of showing results If you work in one of the corpora for which the TF-API has been extended,\nyou will be provided with more powerful methods  show()  and  table() \nto display your results. See  Cunei  and  Bhsa .",
            "title": "Search results"
        },
        {
            "location": "/Api/General/#features",
            "text": "Features TF can give you information of all features it has encountered. TF.featureSets 1 TF . featureSets    Description Returns a dictionary with keys  nodes ,  edges ,  configs ,  computeds . Under each key there is the set of feature names in that category. So you can easily test whether a node feature or edge feature is present in the\ndataset you are working with. configs These are config features, with metadata only, no data. E.g.  otext . computeds These are blocks of precomputed data, available under the  C.  API, see below. May be unloaded The sets do not indicate whether a feature is loaded or not.\nThere are other functions that give you the loaded node features ( Fall() )\nand the loaded edge features ( Eall() ).",
            "title": "Features"
        },
        {
            "location": "/Api/General/#node-features",
            "text": "Node Features F The node features API is exposed as  F  ( Fs ) or  Feature  ( FeatureString ). Fall() aka AllFeatures() 1\n2 Fall ()  AllFeatures ()    Description Returns a sorted list of all usable, loaded node feature names. F. feature  aka Feature. feature 1\n2 F . part_of_speech  Feature . part_of_speech    Description Returns a sub-api for retrieving data that is stored in node features.\nIn this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not\na valid python identifier, you can not use this function,\nyou should use  Fs  instead. Fs(feature) aka FeatureString(feature) 1\n2\n3\n4 Fs ( feature )  FeatureString ( feature )  Fs ( 'part-of-speech' )  FeatureString ( 'part-of-speech' )    Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in\nthe variable  feature . In lines 3 and 4, \nwe assume there is a feature called part-of-speech .\nNote that this is not a valid name in Python, yet we\ncan work with features with such names. Both methods have identical results Suppose we have just issued  feature = 'pos'.\nThen the result of Fs(feature) and F.pos` is identical. In most cases  F  works just fine, but  Fs  is needed in two cases: if we need to work with a feature whose name is not a valid\n    Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .v(node) 1 F . part_of_speech . v ( node )    Description Get the value of a  feature , such as  part_of_speech  for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1\n2 F . part_of_speech . s ( value )  F . part_of_speech . s ( 'noun' )    Description Returns a generator of all nodes in the canonical order with a given value for a given feature.\nThis is an other way to walk through nodes than using  N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F.` feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None )    Description Inspect the values of  feature  (in this example:  part_of_speech )\nand see how often they occur. The result is a\nlist of pairs  (value, frequency) , ordered by  frequency , highest frequencies\nfirst. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those\ntypes will be counted. F.otype otype  is a special node feature and has additional capabilities. Description F.otype.slotType  is the node type that can fill the slots (usually:  word ) F.otype.maxSlot  is the largest slot number F.otype.maxNode  is the largest node number F.otype.all  is a list of all  otypes  from big to small (from books through\n    clauses to words) F.otype.sInterval(otype)  is like  F.otype.s(otype) , but instead of\n    returning you a range to iterate over, it will give you the starting and\n    ending nodes of  otype . This makes use of the fact that the data is so\n    organized that all node types have single ranges of nodes as members.",
            "title": "Node features"
        },
        {
            "location": "/Api/General/#edge-features",
            "text": "Edge Features E The edge features API is exposed as  E  ( Es ) or  Edge  ( EdgeString ). Eall() aka AllEdges() 1\n2 Eall ()  AllEdges ()    Description Returns a sorted list of all usable, loaded edge feature names. E. feature  aka Edge. feature 1\n2 E . head  Feature . head    Description Returns a sub-api for retrieving data that is stored in edge features.\nIn this example, we assume there is a feature called head . Tricky feature names If the feature name is not\na valid python identifier, you can not use this function,\nyou should use  Es  instead. Es(feature) aka EdgeString(feature) 1\n2\n3\n4 Es ( feature )  EdgeString ( feature )  Es ( 'head' )  EdgeString ( 'head' )    Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in\nthe variable  feature . In lines 3 and 4, \nwe assume there is a feature called head . Both methods have identical results Suppose we have just issued  feature = 'head'.\nThen the result of Es(feature) and E.pos` is identical. In most cases  E  works just fine, but  Es  is needed in two cases: if we need to work with a feature whose name is not a valid\n    Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .f(node) 1 E . head . f ( node )    Description Get the nodes reached by  feature -edges  from  a certain node.\nThese edges must be specified in  feature , in this case  head .\nThe result is an ordered tuple\n(again, in the  canonical order . The members of the\nresult are just nodes, if  head  describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than  None . node The node  from  which the edges in question start. E. feature .t(node) 1 E . head . t ( node )    Description Get the nodes reached by  feature -edges  to  a certain node.\nThese edges must be specified in  feature , in this case  head .\nThe result is an ordered tuple\n(again, in the  canonical order . The members of the\nresult are just nodes, if  feature  describes edges without values. Otherwise\nthe members are pairs (tuples) of a node and a value. If there are no edges to  n , the empty tuple is returned, rather than  None . node The node  to  which the edges in question go. E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None ,   nodeTypesTo = None )    Description If the edge feature has no values, simply return the number of node pairs\nbetween an edge of this kind exists. If the edge feature does have values, we \ninspect them\nand see how often they occur. The result is a\nlist of pairs  (value, frequency) , ordered by  frequency , highest frequencies\nfirst. nodeTypesFrom If not  None ,\nonly the values for edges that start from a node with type\nwithin  nodeTypesFrom \nwill be counted. nodeTypesTo If not  None ,\nonly the values for edges that go to a node with type\nwithin  nodeTypesTo \nwill be counted. E.oslots oslots  is a special edge feature and is mainly used to construct other parts\nof the API. It has less capabilities, and you will rarely need it. It does not\nhave  .f  and  .t  methods, but an  .s  method instead. Description E.oslots.s(node) \nGives the sorted list of slot numbers linked to a node,\nor put otherwise: the slots that  support  that node. node The node whose slots are being delivered.",
            "title": "Edge features"
        },
        {
            "location": "/Api/General/#messaging",
            "text": "Timed messages Error and informational messages can be issued, with a time indication. info(), error() 1 info ( msg ,   tm = True ,   nl = True )    Description Sends a message to standard output, possibly with time and newline. if  info()  is being used, the message is sent to  stdout ; if  error()  is being used, the message is sent to  stderr ; In a Jupyter notebook, the standard error is displayed with\na reddish background colour. tm If  True , an indicator of the elapsed time will be prepended to the message. nl If  True  a newline will be appended. indent() 1 indent ( level = None ,   reset = False )    Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer.  Subsequent info()  and  error()  will display their messages with this indent. reset If  True , the elapsed time to will be reset to 0 at the given level.\nTimers at different levels are independent of each other.",
            "title": "Messaging"
        },
        {
            "location": "/Api/General/#saving-features",
            "text": "TF.save() 1 TF . save ( nodeFeatures = {},   edgeFeatures = {},   metaData = {},   module = None )    Description If you have collected feature data in dictionaries, keyed by the\nnames of the features, and valued by their feature data,\nthen you can save that data to  .tf  feature files on disk. It is this easy to export new data as features:\ncollect the data and metadata of\nthe features and \nfeed it in an orderly way to  TF.save()  and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and\nstrings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or\ndictionaries as values. These sets should be sets of nodes (integers!), and\nthese dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from  metaData , which is a dictionary\nmapping a feature name to its metadata. value types The type of the values should conform to  @valueType  ( int  or  str ), which\nmust be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have\nto say so, by specifying  edgeValues = True  in the metadata for that feature. generic metadata metaData  may also contain fields under\n  the empty name. These fields will be added to all features in  nodeFeatures  and\n   edgeFeatures . config features If you need to write the  config  feature  otext ,\nwhich is a metadata-only feature, just\nadd the metadata under key  otext  in  metaData  and make sure\nthat  otext  is not a key in  nodeFeatures  nor in edgeFeatures .\nThese fields will be written into the separate config feature  otext ,\nwith no data associated. save location The (meta)data will be written to the very last module in the list of locations\nthat you specified when calling  Fabric()  or to what you passed as  module  in\nthe same location. If that module does not exist, it will be created in the last location . If both  locations  and  modules  are empty, writing will take place\nin the current directory.",
            "title": "Saving features"
        },
        {
            "location": "/Api/General/#clearing-the-cache",
            "text": "TF.clearCache() 1 TF . clearCache ()    Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the\noriginal data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without\nany changes in the data, where you might want to clear the cache of precomputed\nresults. Calling this function just does it, and it is equivalent with manually removing\nall  .tfx  files inside the hidden  .tf  directory inside your dataset. No need to load It is not needed to execute a  TF.load()  first.",
            "title": "Clearing the cache"
        },
        {
            "location": "/Api/General/#mql",
            "text": "Data interchange with MQL You can interchange with MQL data. Text-Fabric can read and write MQL dumps. An\nMQL dump is a text file, like an SQL dump. It contains the instructions to\ncreate and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName ,   dirName )    Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to  dirName/dbName.mql . If  dirName  starts\nwith  ~ , the  ~  will be expanded to your home directory. Likewise,  ..  will\nbe expanded to the parent of the current directory, and  .  to the current\ndirectory, both only at the start of  dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the\nText-Fabric dataset it comes from: the TF  slots  correspond exactly with the MQL  monads  and have the same\n    numbers; provided the monad numbers in the MQL dump are consecutive. In MQL\n    this is not obligatory. Even if there gaps in the monads sequence, we will\n    fill the holes during conversion, so the slots are tightly consecutive; the TF  nodes  correspond exactly with the MQL  objects  and have the same\n    numbers Node features in MQL The values of TF features are of two types,  int  and  str , and they translate\nto corresponding MQL types  integer  and  string . The actual values do not\nundergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature.\nOnly if the feature is a number feature, you may omit the quotes: 1\n2 [word sp='verb']\n[verse chapter=1 and verse=1]   Enumeration types It is attractive to use eumeration types for the values of a feature, whereever\npossible, because then you can query those features in MQL with  IN  and without\nquotes: 1 [chapter book IN (Genesis, Exodus)]   We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an\nenumeration. So we restrict ourselves to node features with string values. We\nput the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter,\n    followed by letters, digits, or  _ . The letters can only be plain ASCII\n    letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type.\nCurrently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features,\nwe collect all enumerated values for all those features into one\nbig enumeration type. The reason is that MQL considers equal values in different types as\ndistinct values. If we had separate types, we could never compare\nvalues for different features. Values of edge features are ignored There is no place for edge values in\nMQL. There is only one concept of feature in MQL: object features,\nwhich are node features.\nBut TF edges without values can be seen as node features: nodes are\nmapped onto sets of nodes to which the edges go. And that notion is supported by\nMQL:\nedge features are translated into MQL features of type  LIST OF id_d ,\ni.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers\n(yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or\n    underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do\nthat by replacing illegal characters by  _ , and, if the result does not start\nwith a letter, we prepend an  x . We do not check whether the name is a reserved\nC word. With these provisos: the given  dbName  correspond to the MQL  database name the TF  otypes  correspond to the MQL  objects the TF  features  correspond to the MQL  features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible).\nIt can be compressed greatly, especially by the program  bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or\ndirectory with the same name as the MQL database, your import will fail\nspectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your  text-fabric-data  directory, e.g. to\n     ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1\n2 cd  ~/Downloads\nrm dataset  ;  mql -b  3  < dataset.mql   TF.importMQL() 1 TF . importMQL ( mqlFile ,   slotType = None ,   otext = None ,   meta = None )    Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this  importMQL  on a TF instance called with 1 locations = targetDir ,   modules = ''    Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for  TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type,\nbecause TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the  otext.tf  feature. Pass it as a\ndictionary of keys and values, like so: 1\n2\n3\n4 otext   =   { \n     'fmt:text-trans-plain' :   '{glyphs}{trailer}' , \n     'sectionFeatures' :   'book,chapter,verse' ,  }    meta Likewise, you can add a dictionary of keys and values that will added to the\nmetadata of all features. Handy to add provenance data here: 1\n2\n3\n4\n5 meta   =   dict ( \n     dataset = 'DLC' , \n     datasetName = 'Digital Language Corpus' , \n     author = \"That 's me\" ,  )",
            "title": "MQL"
        },
        {
            "location": "/Api/General/#computed-data",
            "text": "Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in\nquick-load format. Most of this data are the features, but there is some extra\ndata needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable,\nso we have made it accessible in the  C -api, which we document here. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1      ( otype ,   averageSlots ,   minNode ,   maxNode )    where  otype  is the name of the node type,  averageSlots  the average size of\nobjects in this type, measured in slots (usually words).  minNode  is the first\nnode of this type,  maxNode  the last, and the nodes of this node type are\nexactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of\nthat type usually occupy. The bigger the average object, the lower the levels.\nBooks have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type  phrase  and above\nit a node type  cluster , i.e. phrases are contained in clusters, but not vice\nversa. If all phrases are contained in clusters, and some clusters have more\nthan one phrase, the automatic level ranking of node types works out well in\nthis case. But if clusters only have very small phrases, and the big phrases do\nnot occur in clusters, then the algorithm may assign a lower rank to clusters\nthan to phrases. In general, it is too expensive to try to compute the levels in a sophisticated\nway. In order to remedy cases where the algorithm assigns wrong levels, you can\nadd a  @levels  key to the  otext  config feature. See text . C.order.data Description An  array  of all nodes in the correct order. This is the\norder in which  N()  alias  Node()  traverses all nodes. Rationale To order all nodes in the  canonical ordering  is quite a bit of\nwork, and we need this ordering all the time. C.rank.data Description An  array  of all indices of all nodes in the canonical order\narray. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know\nwhich position each node takes in the canonical order, in other words, at what\nindex we find it in the  C.order.data  array. C.levUp.data and C.levDown.data Description These tables feed the  L.d()  and  L.u()  functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized.\nIt is not advisable to use them directly, it is far better to use the  L  functions. Only when every bit of performance waste has to be squeezed out, this raw data\nmight be a deal. C.boundary.data Description These tables feed the  L.n()  and  L.p()  functions.\nIt is a tuple consisting of  firstSlots  and  lastSlots .\nThey are indexes for the first slot\nand last slot of nodes. Slot index For each slot,  firstSlot  gives all nodes (except\nslots) that start at that slot, and  lastSlot  gives all nodes (except slots)\nthat end at that slot.\nBoth  firstSlot  and  lastSlot  are tuples, and the\ninformation for node  n  can be found at position  n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is  chapter , and that of level 3 is  verse . And\nsuppose that we have features, named  bookHeading ,  chapterHeading , and verseHeading  that give the names or numbers of these. Custom names Note that the terms  book ,  chapter ,  verse  are not baked into Text-Fabric.\nIt is the corpus data, especially the  otext  config feature that\nspells out the names of the sections. Then  C.section.data  is a tuple of two mappings , let us call them  chapters \nand  verses . chapters  is a mapping, keyed by  book   nodes , and then by\nby chapter  headings , giving the corresponding\nchapter  node s as values. verses  is a mapping, keyed by  book   nodes , and then\nby chapter  headings , and then by verse  headings ,\ngiving the corresponding verse  node s as values. Supporting the  T -Api The  T -api is good in mapping nodes unto sections, such as books, chapters,\nverses and back. It knows how many chapters each book has, and how many verses\neach chapter. The  T  api is meant to make your life easier when you have to find passage\nlabels by nodes or vice versa. That is why you probably never need to consult\nthe underlying data. But you can! That data is stored in",
            "title": "Computed data"
        },
        {
            "location": "/Api/General/#miscellaneous",
            "text": "TF.version Description Contains the version number of the Text-Fabric\nlibrary. TF.banner Description Contains the name and the version of the Text-Fabric\nlibrary.",
            "title": "Miscellaneous"
        },
        {
            "location": "/Api/Apps/",
            "text": "Apps\n\u00b6\n\n\nAbout\nText-Fabric is a generic engine to process text and annotations.\nWhen working with specific corpora, we want to have more power at our fingertips.\nWe need extra power on top of the TF engine.\nThe way we have chosen to do it is via \napps\n.\nAn app is a bunch of extra functions that \nknow\n the structure of a specific corpus.\nCurrent apps\n\u00b6\n\n\nCurrent apps\nAt the moment we have these apps\nbhsa\ncunei\nThe structure of apps\n\u00b6\n\n\nApp components\nThe apps themselves are modules inside \n\ntf.extra\nFor each \napp\n, you find there:\nmodule\napp\n.py\n\ncontains all the functionality specific to the corpus in question, organized as an extended\nTF api. In the code this is referred to as the \nextraApi\n.\nwebapp\nthe package \napp\n-app\n\nis used by the text-fabric browser, and contain settings and assets\nto set up a browsing experience.\nconfig.py\n: settings\na \nstatic\n folder with fonts and logos.\nconfig.py\nContains values for parameters and an API calling function.\nextraApi(locations, modules)\nResponsible for calling the extra Api for the corpus\nwith the desired locations and modules.\nThis extraApi will be active as a TF data server,\ninteracting with a local webserver that serves local\nweb page in the browser.\nweb browsing settings\nThe Text-Fabric data server, webserver and browser need settings:\nsetting\nexample\ndescription\nprotocol\nhttp://\nprotocol of local website\nhost\nlocalhost\nserver address of local website\nwebport\n8001\nport for the local website\nport\n18981\nport through wich the data server and the web server communicate\ndata settings\nThe Text-Fabric data server needs context information:\nsetting\ntype\ndescription\nlocations\nlist\nwhere to look for tf features\nmodules\nlist\ncombines with locations to search paths for tf features\nlocalDir\ndirectory name\ntemporary directory for writing and reading\noptions\ntuple\nnames of extra options for seaerching and displaying query results\ncondenseType\nstring\nthe default container type to which query results may be condensed\nPROVENANCE\ndict\ncorpus specific provenance metadata: name and DOI\nThe generic part of apps\n\u00b6\n\n\nApp helpers\nApps turn out to have several things in common that we want to deal with generically.\nThese functions are collected in the\n\napphelpers\n\nmodule of TF.\nGeneric/specific\nSometimes there is an intricate mix of functionality that is shared by all apps and that\nis specific to some app.\nHere is our logistics of functionality.\nThere are a number of methods that are offered as a generic function and\njust added as a method to the \nextraApi\n of the app, e.g. \npretty()\n\nFor example, the Bhsa app imports \npretty\n first:\n1\nfrom\n \ntf.apphelpers\n \nimport\n \npretty\n\n\n\n\n\n\nand in the Bhsa \n__init__()\n function it says:\n1\nself\n.\npretty\n \n=\n \ntypes\n.\nMethodType\n(\npretty\n,\n \nself\n)\n\n\n\n\n\n\nwhich adds the function \npretty\n as an instance method to the class Bhsa.\nThe first argument \nextraApi\n of the function \npretty\n acts as the \nself\n when \n\npretty()\n is used as a method of Bhsa.\nSo although we define \npretty(extraApi, ...)\n as a generic function,\nthrough its argument \nextraApi\n we can call app specific functionality.\nWe follow this pattern for quite a bit of functions.\nThey all have \nextraApi\n as first argument.\nTwo contexts\nMost functions with the \nextraApi\n argument are meant to perform their duty in two contexts:\nwhen called in a Jupyter notebook they deliver output meant for a notebook output cell,\n  using methods provided by the \nipython\n package.\nwhen called by the web app they deliver output meant for the TF browser website,\n  generating raw HTML.\nThe \nextraApi\n is the rich app specific API, and when we construct this API, we pass the information\nwhether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app.\nWe pass this information by setting the attribute \nasApi\n on the \nextraApi\n. \nIf it is set, we use the \nextraApi\n in the web app context.\nMost of the code in such functions is independent of \nasApi\n.\nThe main difference is how to output the result: by a call to an IPython display method, or by\nreturning raw HTML.\nTF Data getters\n\u00b6\n\n\nAuto loading of TF data\nThe specific apps have functions to load and download data from github.\nThey check first whether there is local data in a github repository,\nand if not, they check a local text-fabric-data directory,\nand if not, they download data from a know online GitHub repo into the local\ntext-fabric-data directory.\nThe data functions provided take parameters with these meanings:\ndataUrl\nThe complete url from which to download data.\nghBase\nThe location of the local github directory, usually \n~/github\n.\nThis directory is expected to be subdivided by org and then by repo, just as the online\nGitHub.\nNote\nThe relative path within the local github/text-fabric-data directory to the directory\nthat holds versions of TF data.\nNote\nThe version of the TF data of interest.\nhasData(dataRel, ghBase, version)\nChecks whether there is TF data in standard locations.\nReturns the full path of the local github directory if the data is found in the expected place below it.\nReturns the full path of the local text-fabric-data directory if the data is found in the expected place below it.\nReturns \nFalse\n if there no offline copy of the data has been found in these locations.\ngetData(dataUrl, dataRel, ghBase, version)\nChecks whether there is TF data in standard locations.\nIf not, downloads data from \ndataUrl\n and places it in \n~/text-fabric-data/dataRel/version\ngetDataCustom(dataUrl, dest)\nRetrieves a zip file from \ndataUrl\n, and unpacks it at directory \ndest\n locally.\nTF search performers\n\u00b6\n\n\nsearch(extraApi, query, silent=False, sets=None, shallow=False)\nThis is a thin wrapper around the generic search interface of TF:\n\nS.search\nThe extra thing it does it collecting the results.\n\nS.search()\n may yield a generator, and this \nsearch()\n makes sure to iterate\nover that generator, collect the results, and return them as a sorted list.\nContext Jupyter\nThe intended context of this function is: an ordinary Python program (including\nthe Jupyter notebook).\nWeb apps can better use \nrunSearch\n below.\nrunSearch(api, query, cache)\nA wrapper around the generic search interface of TF.\nBefore running the TF search, the \nquery\n will be looked up in the \ncache\n.\nIf present, its cached results/error messages will be returned.\nIf not, the query will be run, results/error messages collected, put in the \ncache\n, and returned.\nContext web app\nThe intended context of this function is: web app.\nrunSearchCondensed(api, query, cache, condenseType)\nWhen query results need to be condensed into a container, this function takes care of that.\nIt first tries the \ncache\n for condensed query results.\nIf that fails,\nit collects the bare query results from the cache or by running the query.\nThen it condenses the results, puts them in the \ncache\n, and returns them.\nContext web app\nThe intended context of this function is: web app.\nTabular display\n\u00b6\n\n\ntable(extraApi, tuples, ...)\nTakes a list of \ntuples\n and\ncomposes it into a Markdown table.\nContext Jupyter\nThe intended context of this function is:\nthe Jupyter notebook.\ncompose(extraApi, tuples, start. position, opened, ...)\nTakes a list of \ntuples\n and\ncomposes it into an HTML table.\nSome of the rows will be expandable, namely the rows specified by \nopened\n,\nfor which extra data has been fetched.\nContext web app\nThe intended context of this function is: web app.\nplainTuple(extraApi, tuple)\nDisplays a \ntuple\n of nodes as a table row:\na markdown row in the context Jupyter;\nan HTML row in the context web app.\nPretty display\n\u00b6\n\n\nWhat is pretty?\nNodes are just numbers, but they stand for all the information that the corpus\nhas about a certain item.\n\npretty(node)\n makes a lot of that information visible in an app dependent way.\nFor the Bhsa it means showing nested and intruding sentences, clauses and phrases.\nFor the Cunei tablets it means showing alternating vertical and horizontal\nsubdivisions of faces into columns, lines and cases.\nWhen you show a pretty representation of a node,\nusually pretty representations of \"contained\" nodes will also be drawn.\nYou can selectively highlight those nodes with custom colors.\nWhen pretty-displaying a tuple of nodes, container nodes that contain those nodes\nwill be looked up and displayed, and the actual tuple nodes will be highlighted.\nYou can customize the highlight colors by selecting colors on the basis of the\npostions of nodes in their tuples, or you can explicitly pass a micro-managed\ncolormap of nodes to colors.\nIn pretty displays you can opt for showing/hiding the node numbers,\nfor suppressing certain standard features, and there are app dependent options.\nIn the case of Cunei tablets, you can opt to show the lineart of signs and quads,\nand to show the line numbers of the source transcriptions.\nshow(extraApi, tuples, ...)\nTakes a list of \ntuples\n and\ncomposes it into a sequence of pretty displays per tuple.\nContext Jupyter\nThe intended context of this function is:\nthe Jupyter notebook.\nprettyTuple(extraApi, tuple)\nDisplays a \ntuple\n of nodes as an expanded display, both\nin the context Jupyter and\nin the context web app.\npretty(extrApi, node, ...)\nDisplays a single \nnode\n as an expanded display, both\nin the context Jupyter and\nin the context web app.\nprettyPre(extraApi, node, ...)\nHelper for \npretty\n.\nPretty display is pretty complicated.\nThere are large portions of functionality that are generic,\nand large portions that are app specific.\nThis function computes a lot of generic things, based on which\na pretty display can be constructed.\nprettySetup(extraApi, features=None, noneValues=None)\nPretty displays show a chosen set of standard features for nodes.\nBy means of the parameter \nsuppress\n you can leave out certain features.\nBut what if you want to add features to the display?\nThat is what \nprettySetup()\n does.\nIt adds a list of \nfeatures\n to the display, provided they are loadable.\nIf they are not yet loaded, they will be loaded.\nFeatures with values that represent no information, will be suppressed.\nBut you can customise what counts as no information, by passing a set\nof such values as \nnoneValues\n. \nHTML and Markdown\n\u00b6\n\n\ngetBoundary(api, node)\nUtility function to ask from the TF API the first slot and the last slot contained in a node.\ngetFeatures(extraApi, node, ...)\nHelper for \npretty()\n: wrap the requested features and their values for \nnode\n in HTML for pretty display.\ngetContext(api, nodes)\nGet the features and values for a set of \nnodes\n. \nAll loaded features will be retrieved.\nheader(extraApi)\nGet the app-specific links to data and documentation and wrap it into HTML for display in the TF browser.\noutLink(text, href, title=None, ...)\nProduce a formatted HTML link.\nhtmlEsc(val)\nProduce a representation of \nval\n that is safe for usage in a HTML context.\nmdEsc(val)\nProduce a representation of \nval\n that is safe for usage in a Markdown context.\ndm(markdown)\nDisplay a \nmarkdown\n string in a Jupyter notebook.\nConstants\n\u00b6\n\n\nFixed values\nThe following values are used by other parts of the program:\nname\ndescription\nRESULT\nthe label of a query result: \nresult\nGH_BASE\nthe location of the local github directory\nURL_GH\nthe url to the GitHub site\nURL_NB\nthe url to the NBViewer site",
            "title": "Apps"
        },
        {
            "location": "/Api/Apps/#apps",
            "text": "About Text-Fabric is a generic engine to process text and annotations. When working with specific corpora, we want to have more power at our fingertips. We need extra power on top of the TF engine. The way we have chosen to do it is via  apps .\nAn app is a bunch of extra functions that  know  the structure of a specific corpus.",
            "title": "Apps"
        },
        {
            "location": "/Api/Apps/#current-apps",
            "text": "Current apps At the moment we have these apps bhsa cunei",
            "title": "Current apps"
        },
        {
            "location": "/Api/Apps/#the-structure-of-apps",
            "text": "App components The apps themselves are modules inside  tf.extra For each  app , you find there: module app .py \ncontains all the functionality specific to the corpus in question, organized as an extended\nTF api. In the code this is referred to as the  extraApi . webapp the package  app -app \nis used by the text-fabric browser, and contain settings and assets\nto set up a browsing experience. config.py : settings a  static  folder with fonts and logos. config.py Contains values for parameters and an API calling function. extraApi(locations, modules) Responsible for calling the extra Api for the corpus\nwith the desired locations and modules. This extraApi will be active as a TF data server,\ninteracting with a local webserver that serves local\nweb page in the browser. web browsing settings The Text-Fabric data server, webserver and browser need settings: setting example description protocol http:// protocol of local website host localhost server address of local website webport 8001 port for the local website port 18981 port through wich the data server and the web server communicate data settings The Text-Fabric data server needs context information: setting type description locations list where to look for tf features modules list combines with locations to search paths for tf features localDir directory name temporary directory for writing and reading options tuple names of extra options for seaerching and displaying query results condenseType string the default container type to which query results may be condensed PROVENANCE dict corpus specific provenance metadata: name and DOI",
            "title": "The structure of apps"
        },
        {
            "location": "/Api/Apps/#the-generic-part-of-apps",
            "text": "App helpers Apps turn out to have several things in common that we want to deal with generically.\nThese functions are collected in the apphelpers \nmodule of TF. Generic/specific Sometimes there is an intricate mix of functionality that is shared by all apps and that\nis specific to some app.\nHere is our logistics of functionality. There are a number of methods that are offered as a generic function and\njust added as a method to the  extraApi  of the app, e.g.  pretty() \nFor example, the Bhsa app imports  pretty  first: 1 from   tf.apphelpers   import   pretty    and in the Bhsa  __init__()  function it says: 1 self . pretty   =   types . MethodType ( pretty ,   self )    which adds the function  pretty  as an instance method to the class Bhsa.\nThe first argument  extraApi  of the function  pretty  acts as the  self  when  pretty()  is used as a method of Bhsa. So although we define  pretty(extraApi, ...)  as a generic function,\nthrough its argument  extraApi  we can call app specific functionality. We follow this pattern for quite a bit of functions.\nThey all have  extraApi  as first argument. Two contexts Most functions with the  extraApi  argument are meant to perform their duty in two contexts: when called in a Jupyter notebook they deliver output meant for a notebook output cell,\n  using methods provided by the  ipython  package. when called by the web app they deliver output meant for the TF browser website,\n  generating raw HTML. The  extraApi  is the rich app specific API, and when we construct this API, we pass the information\nwhether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app. We pass this information by setting the attribute  asApi  on the  extraApi . \nIf it is set, we use the  extraApi  in the web app context. Most of the code in such functions is independent of  asApi .\nThe main difference is how to output the result: by a call to an IPython display method, or by\nreturning raw HTML.",
            "title": "The generic part of apps"
        },
        {
            "location": "/Api/Apps/#tf-data-getters",
            "text": "Auto loading of TF data The specific apps have functions to load and download data from github.\nThey check first whether there is local data in a github repository,\nand if not, they check a local text-fabric-data directory,\nand if not, they download data from a know online GitHub repo into the local\ntext-fabric-data directory. The data functions provided take parameters with these meanings: dataUrl The complete url from which to download data. ghBase The location of the local github directory, usually  ~/github .\nThis directory is expected to be subdivided by org and then by repo, just as the online\nGitHub. Note The relative path within the local github/text-fabric-data directory to the directory\nthat holds versions of TF data. Note The version of the TF data of interest. hasData(dataRel, ghBase, version) Checks whether there is TF data in standard locations.\nReturns the full path of the local github directory if the data is found in the expected place below it.\nReturns the full path of the local text-fabric-data directory if the data is found in the expected place below it.\nReturns  False  if there no offline copy of the data has been found in these locations. getData(dataUrl, dataRel, ghBase, version) Checks whether there is TF data in standard locations.\nIf not, downloads data from  dataUrl  and places it in  ~/text-fabric-data/dataRel/version getDataCustom(dataUrl, dest) Retrieves a zip file from  dataUrl , and unpacks it at directory  dest  locally.",
            "title": "TF Data getters"
        },
        {
            "location": "/Api/Apps/#tf-search-performers",
            "text": "search(extraApi, query, silent=False, sets=None, shallow=False) This is a thin wrapper around the generic search interface of TF: S.search The extra thing it does it collecting the results. S.search()  may yield a generator, and this  search()  makes sure to iterate\nover that generator, collect the results, and return them as a sorted list. Context Jupyter The intended context of this function is: an ordinary Python program (including\nthe Jupyter notebook).\nWeb apps can better use  runSearch  below. runSearch(api, query, cache) A wrapper around the generic search interface of TF.\nBefore running the TF search, the  query  will be looked up in the  cache .\nIf present, its cached results/error messages will be returned.\nIf not, the query will be run, results/error messages collected, put in the  cache , and returned. Context web app The intended context of this function is: web app. runSearchCondensed(api, query, cache, condenseType) When query results need to be condensed into a container, this function takes care of that.\nIt first tries the  cache  for condensed query results.\nIf that fails,\nit collects the bare query results from the cache or by running the query.\nThen it condenses the results, puts them in the  cache , and returns them. Context web app The intended context of this function is: web app.",
            "title": "TF search performers"
        },
        {
            "location": "/Api/Apps/#tabular-display",
            "text": "table(extraApi, tuples, ...) Takes a list of  tuples  and\ncomposes it into a Markdown table. Context Jupyter The intended context of this function is:\nthe Jupyter notebook. compose(extraApi, tuples, start. position, opened, ...) Takes a list of  tuples  and\ncomposes it into an HTML table.\nSome of the rows will be expandable, namely the rows specified by  opened ,\nfor which extra data has been fetched. Context web app The intended context of this function is: web app. plainTuple(extraApi, tuple) Displays a  tuple  of nodes as a table row: a markdown row in the context Jupyter; an HTML row in the context web app.",
            "title": "Tabular display"
        },
        {
            "location": "/Api/Apps/#pretty-display",
            "text": "What is pretty? Nodes are just numbers, but they stand for all the information that the corpus\nhas about a certain item. pretty(node)  makes a lot of that information visible in an app dependent way. For the Bhsa it means showing nested and intruding sentences, clauses and phrases. For the Cunei tablets it means showing alternating vertical and horizontal\nsubdivisions of faces into columns, lines and cases. When you show a pretty representation of a node,\nusually pretty representations of \"contained\" nodes will also be drawn. You can selectively highlight those nodes with custom colors. When pretty-displaying a tuple of nodes, container nodes that contain those nodes\nwill be looked up and displayed, and the actual tuple nodes will be highlighted. You can customize the highlight colors by selecting colors on the basis of the\npostions of nodes in their tuples, or you can explicitly pass a micro-managed\ncolormap of nodes to colors. In pretty displays you can opt for showing/hiding the node numbers,\nfor suppressing certain standard features, and there are app dependent options. In the case of Cunei tablets, you can opt to show the lineart of signs and quads,\nand to show the line numbers of the source transcriptions. show(extraApi, tuples, ...) Takes a list of  tuples  and\ncomposes it into a sequence of pretty displays per tuple. Context Jupyter The intended context of this function is:\nthe Jupyter notebook. prettyTuple(extraApi, tuple) Displays a  tuple  of nodes as an expanded display, both in the context Jupyter and in the context web app. pretty(extrApi, node, ...) Displays a single  node  as an expanded display, both in the context Jupyter and in the context web app. prettyPre(extraApi, node, ...) Helper for  pretty .\nPretty display is pretty complicated.\nThere are large portions of functionality that are generic,\nand large portions that are app specific. This function computes a lot of generic things, based on which\na pretty display can be constructed. prettySetup(extraApi, features=None, noneValues=None) Pretty displays show a chosen set of standard features for nodes.\nBy means of the parameter  suppress  you can leave out certain features.\nBut what if you want to add features to the display? That is what  prettySetup()  does.\nIt adds a list of  features  to the display, provided they are loadable.\nIf they are not yet loaded, they will be loaded. Features with values that represent no information, will be suppressed.\nBut you can customise what counts as no information, by passing a set\nof such values as  noneValues .",
            "title": "Pretty display"
        },
        {
            "location": "/Api/Apps/#html-and-markdown",
            "text": "getBoundary(api, node) Utility function to ask from the TF API the first slot and the last slot contained in a node. getFeatures(extraApi, node, ...) Helper for  pretty() : wrap the requested features and their values for  node  in HTML for pretty display. getContext(api, nodes) Get the features and values for a set of  nodes . \nAll loaded features will be retrieved. header(extraApi) Get the app-specific links to data and documentation and wrap it into HTML for display in the TF browser. outLink(text, href, title=None, ...) Produce a formatted HTML link. htmlEsc(val) Produce a representation of  val  that is safe for usage in a HTML context. mdEsc(val) Produce a representation of  val  that is safe for usage in a Markdown context. dm(markdown) Display a  markdown  string in a Jupyter notebook.",
            "title": "HTML and Markdown"
        },
        {
            "location": "/Api/Apps/#constants",
            "text": "Fixed values The following values are used by other parts of the program: name description RESULT the label of a query result:  result GH_BASE the location of the local github directory URL_GH the url to the GitHub site URL_NB the url to the NBViewer site",
            "title": "Constants"
        },
        {
            "location": "/Api/Bhsa/",
            "text": "BHSA\n\u00b6\n\n\nAbout\n\u00b6\n\n\nThe module \nbhsa.py\n\ncontains a number of handy functions on top of Text-Fabric and especially its \n\nSearch\n part.\n\n\nSet up\n\u00b6\n\n\nimport Bhsa\nThe \nBhsa\n API is distributed with Text-Fabric.\nYou have to import it into your program:\n1\nfrom\n \ntf.extra.bhsa\n \nimport\n \nBhsa\n,\n \nhasTf\n,\n \ngetTf\n\n\n\n\n\n\nThe \nhasTf\n and \ngetTf\n functions are only needed if you want to\nautomatically download TF data from one of the\nETCBC repositories on GitHub.\nGet data\n\u00b6\n\n\nhasTf(source, version, relative)\nChecks whether the TF data for a source/version of a release of an\nETCBC repository is locally present.\nIf the data is already present under your local\n\n~/github\n directory,\nit will return the full path to \n~/github\n on your machine.\nIf not, but the data is already present under your local\n\n~/text-fabric-data\n directory,\nit will return the full path to \n~/text-fabric-data\n on your machine.\nIf no data is present locally, it will return \nFalse\n.\ngetTf(source, release, version, relative)\nGets the TF data for a source/version of a release of an\nETCBC repository.\nIf the data is already present under your local\n\n~/github\n directory or \n~/text-fabric-data\n directory,\nit will not be downloaded. \nIf not, it will be downloaded to a location within\n\n~/text-fabric-data\n.\nAll arguments are optional, the defaults are:\nargument\ndefault\ndescription\nsource\nbhsa\nrepo within ETCBC organization\nrelease\n1.3\nrelease version of repo\nversion\nc\nversion of the BHSA\nrelative\n{}/tf\nrelative path of TF data within repo. The \n{}\n will be substituted with the value of \nsource\n.\nMain data\nMost recent version of the main BHSA data\n1\ngetTf\n()\n\n\n\n\n\n\nPhono\nMost recent version of the \nphono\n features:\n\n1\ngetTf\n(\nsource\n=\n'phono'\n,\n \nrelease\n=\n'1.0.1'\n)\n\n\n\n\nget and load data\nHere is an incantation to auto-load data:\n1\n2\n3\n4\n5\nfrom\n \ntf.extra.bhsa\n \nimport\n \nhasTf\n,\n \ngetTf\n,\n \nBhsa\n\n\n\ngetTf\n(\nversion\n=\n'2017'\n)\n\n\nloc\n \n=\n \nhasTf\n(\nversion\n=\n'2017'\n)\n\n\nTF\n \n=\n \nFabric\n(\nlocations\n=\n[\nf\n'{loc}/etcbc/bhsa'\n],\n \nmodules\n=\n[\n'tf/2017'\n])\n\n\n\n\n\n\nAnd if you want to load phono data as well:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nfrom\n \ntf.extra.bhsa\n \nimport\n \nhasTf\n,\n \ngetTf\n,\n \nBhsa\n\n\n\ngetTf\n(\nsource\n=\n'bhsa'\n,\n \nversion\n=\n'2017'\n)\n\n\nlocMain\n \n=\n \nhasTf\n(\nsource\n=\n'bhsa'\n,\n \nversion\n=\n'2017'\n)\n\n\n\ngetTf\n(\nsource\n=\n'phono'\n,\n \nversion\n=\n'2017'\n)\n\n\nlocPhono\n \n=\n \nhasTf\n(\nsource\n=\n'phono'\n,\n \nversion\n=\n'2017'\n)\n\n\n\nTF\n \n=\n \nFabric\n(\n\n  \nlocations\n=\n[\nf\n'{locMain}/etcbc/bhsa'\n,\n \nf\n'{locPhono}/etcbc/phono'\n],\n\n  \nmodules\n=\n[\n'tf/2017'\n],\n\n\n)\n\n\n\n\n\n\nThis will work also in cases where the main BHSA data is in your\n\n~/github\n directory and the phono data is in your \n~/text-fabric-data\n\ndirectory or vice versa.\nInitialisation\n\u00b6\n\n\nBhsa()\n1\nB\n \n=\n \nBhsa\n(\napi\n,\n \n'notebook'\n,\n \nversion\n=\nVERSION\n)\n\n\n\n\n\n\nDescription\nSilently loads some additional features, and \nB\n\nwill give access to some extra functions.\napi\nThe API resulting from an earlier call \nTF.load()\nSet up\nThis module comes in action after you have set up TF and loaded some features, e.g.\n1\n2\n3\n4\n5\n6\nVERSION\n \n=\n \n'2017'\n\n\nTF\n \n=\n \nFabric\n(\nlocations\n=\nf\n'~/github/etcbc/bhsa/tf/{VERSION}'\n)\n\n\napi\n \n=\n \nTF\n.\nload\n(\n'''\n\n\n  function sp gn nu\n\n\n'''\n)\n\n\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nThen we add the functionality of the \nbhsa\n module by a call to \nBhsa()\n.\nnotebook\nThis should be the name\nof your current notebook (without the \n.ipynb\n extension).\nThe Bhsa API will use this to generate a link to your notebook\non GitHub and NBViewer.\nLinking\n\u00b6\n\n\nB.shbLink()\n1\nB\n.\nshbLink\n(\nnode\n,\n \ntext\n=\nNone\n)\n\n\n\n\n\n\nDescription\nProduces a link to SHEBANQ\nnode\nnode\n can be an arbitrary node. The link targets the verse that\ncontains the first word contained by the node.\ntext\nYou may provide the text to be displayed as the link.\nThen the\npassage indicator (book chapter:verse) will be put\nin the tooltip (title) of the link.\nIf you do not provide a link text,\nthe passage indicator (book chapter:verse) will be chosen.\nWord 100000 on SHEBANQ\n1\nB\n.\nshbLink\n(\n100000\n)\n\n\n\n\n\n\nPlain display\n\u00b6\n\n\nStraightforward display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table.\nB.plain()\n1\nB\n.\nplain\n(\nnode\n,\n \nlinked\n=\nTrue\n,\n \nwithNodes\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a simple way.\nnode\nnode\n is a node of arbitrary type.\nlinked\nlinked\n indicates whether the result should be a link to SHEBANQ\nto the appropriate book/chapter/verse.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say \nasString=True\n.\nB.plainTuple()\n1\nB\n.\nplainTuple\n(\nnodes\n,\n \nseqNumber\n,\n \nlinked\n=\n1\n,\n \nwithNodes\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells.\nnodes\nnodes\n is an iterable (list, tuple, set, etc) of arbitrary nodes.\nseqNumber\nseqNumber\n is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes.\nlinked\nlinked=1\n the column number where the cell contents is\nlinked to\nthe relevant passage in to SHEBANQ;\n(the first data column is column 1)\nwithNodes, asString\nSame as in \nB.plain()\n.\nB.table()\n1\n2\n3\n4\n5\n6\n7\nB\n.\ntable\n(\n\n  \nresults\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nlinked, withNodes, asString\nSame as in \nB.plainTuple()\n.\nPretty display\n\u00b6\n\n\nGraphical display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way.\nB.prettySetup()\nDescription\nIn pretty displays, nodes are shown together with the values of a selected\nset of features. \nWith this function you can add features to the display.\nfeatures\nA string or iterable of feature names.\nThese features will be loaded automatically.\nIn pretty displays these features will show up as \nfeature=value\n,\nprovided the value is not \nNone\n, or something like None.\nAutomatic loading\nThese features will load automatically, no explicit loading is\nnecessary.\nnoneValues\nA set of values for which no display should be generated.\nThe default set is \nNone\n and the strings \nNA\n, \nnone\n, \nunknown\n.\nNone is useful\nKeep \nNone\n in the set. If not, all custom features will be displayed\nfor all kinds of nodes. So you will see clause types on words,\n  and part of speech on clause atoms, al with value \nNone\n.\nSuppress common values\nYou can use \nnoneValues\n also to suppress the normal values of a feature,\nin order to attrect attention to the more special values, e.g.\n1\nnoneValues\n=\n{\nNone\n,\n \n'NA'\n,\n \n'unknown'\n,\n \n'm'\n,\n \n'sg'\n,\n \n'p3'\n}\n\n\n\n\n\n\nNone values affect all features\nBeware of putting to much in \nnoneValues\n.\nThe contents of \nnoneValues\n affect the display of\nall features, not only the custom features.\nB.pretty()\n1\nB\n.\npretty\n(\nnode\n,\n \nwithNodes\n=\nFalse\n,\n \nsuppress\n=\nset\n(),\n \nhighlights\n=\n{})\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a graphical way.\nnode\nnode\n is a node of arbitrary type.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nsuppress\nsuppress=set()\n is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively.\nHighlighting\nWhen nodes such as verses and sentences are displayed by \npretty()\n,\ntheir contents is also displayed. You can selectively highlight\nthose parts.\nhighlights\nhighlights={}\n is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted.\nIf \nhighlights\n is a set, its nodes will be highlighted with a default color (yellow).\nIf it is a dictionary, it should map nodes to colors.\nAny color that is a valid \n\nCSS color\n\nqualifies.\nIf you map a node to the empty string, it will get the default highlight color.\ncolor names\nThe link above points to a series of handy color names and their previews.\nB.prettyTuple()\n1\n2\n3\n4\n5\n6\n7\nB\n.\nprettyTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes.\nBy verse\nWe examine all nodes in the tuple.\nWe collect and show all verses in which they\noccur and highlight the material corresponding to all the nodes in the tuple.\nThe highlighting can be tweaked by the optional \ncolorMap\n parameter.\nnodes, seqNumber, withNodes\nSame as in \nB.plainTuple()\n.\nsuppress\nSame as in \nB.pretty()\n.\ncolorMap\nThe nodes of the tuple will be highlighted.\nIf \ncolorMap\n is \nNone\n or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow.\nBut you can assign different colors to the members of the tuple:\n\ncolorMap\n must be a dictionary that maps the positions in a tuple \nto a color.\nIf a position is not mapped, it will not be highlighted.\nIf it is mapped to the empty string, it gets the default highlight color.\nOtherwise, it should be mapped to a string that is a valid\n    \nCSS color\n.\ncolor names\nThe link above points to a series of handy color names and their previews.\nhighlights\nSame as in \nB.pretty()\n.\nhighlights takes precedence over colorMap\nIf both \nhighlights\n and \ncolorMap\n are given, \ncolorMap\n is ignored.\nIf you need to micro-manage, \nhighlights\n is your thing.\nWhenever possible, use \ncolorMap\n.  \none big highlights dictionary\nIt is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run \nprettyTuple()\n for many different tuples with the same \nhighlights\n.\nIt does not harm performance if \nhighlights\n maps lots of nodes outside the tuple as well.\nB.show()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\n.\nshow\n(\n\n  \nresults\n,\n\n  \ncondensed\n=\nTrue\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe elements of the list are displayed by \nB.prettyTuple()\n.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\ncondensed\ncondensed\n indicates one of two modes of displaying the result list:\nTrue\n: instead of showing all results one by one,\n    we show all verses with all results in it highlighted.\n    That way, we blur the distinction between the individual results,\n    but it is easier to oversee where the results are.\n    This is how SHEBANQ displays its query results.\nFalse\n: make a separate display for each result tuple.\n    This gives the best account of the exact result set.\nmixing up highlights\nCondensing may mix-up the highlight coloring.\nIf a node occurs in two results, at different positions\nin the tuple, the \ncolorMap\n wants to assign it two colors!\nYet one color will be chosen, and it is unpredictable which one.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nwithNodes, suppress, colorMap, highlights\nSame as in \nB.prettyTuple()\n.\nSearch\n\u00b6\n\n\nB.search()\n1\nB\n.\nsearch\n(\nquery\n,\n \nsilent\n=\nFalse\n,\n \nshallow\n=\nFalse\n,\n \nsets\n=\nNone\n)\n\n\n\n\n\n\nDescription\nSearches in the same way as the generic Text-Fabric \nS.search()\n.\nBut whereas the \nS\n version returns a generator which yields the results\none by one, the \nB\n version collects all results and sorts them.\nIt then reports the number of results.\nquery\nquery\n is the search template that has to be searched for.\nsilent\nsilent\n: if \nTrue\n it will suppress the reporting of the number of results.\nshallow\nIf \nTrue\n or \n1\n, the result is a set of things that match the top-level element\nof the \nquery\n.\nIf \n2\n or a bigger number \nn\n, return the set of truncated result tuples: only\nthe first \nn\n members of each tuple is retained.\nIf \nFalse\n or \n0\n, a sorted list of all result tuples will be returned.\nsets\nIf not \nNone\n, it should be a dictionary of sets, keyed by a names.\nIn \nquery\n you can refer to those names to invoke those sets.\nFor example, if you have a set \ngappedPhrases\n of all phrase nodes that have a gap,\nyou can pass \nsets=dict(gphrase=gappedPhrases)\n, and then in your query you can say\n1\n2\ngphrase function=Pred\n  word sp=verb\n\n\n\n\n\netc.\nsearch template reference\nSee the \nsearch template reference",
            "title": "Hebrew Bible"
        },
        {
            "location": "/Api/Bhsa/#bhsa",
            "text": "",
            "title": "BHSA"
        },
        {
            "location": "/Api/Bhsa/#about",
            "text": "The module  bhsa.py \ncontains a number of handy functions on top of Text-Fabric and especially its  Search  part.",
            "title": "About"
        },
        {
            "location": "/Api/Bhsa/#set-up",
            "text": "import Bhsa The  Bhsa  API is distributed with Text-Fabric.\nYou have to import it into your program: 1 from   tf.extra.bhsa   import   Bhsa ,   hasTf ,   getTf    The  hasTf  and  getTf  functions are only needed if you want to\nautomatically download TF data from one of the\nETCBC repositories on GitHub.",
            "title": "Set up"
        },
        {
            "location": "/Api/Bhsa/#get-data",
            "text": "hasTf(source, version, relative) Checks whether the TF data for a source/version of a release of an\nETCBC repository is locally present. If the data is already present under your local ~/github  directory,\nit will return the full path to  ~/github  on your machine. If not, but the data is already present under your local ~/text-fabric-data  directory,\nit will return the full path to  ~/text-fabric-data  on your machine. If no data is present locally, it will return  False . getTf(source, release, version, relative) Gets the TF data for a source/version of a release of an\nETCBC repository. If the data is already present under your local ~/github  directory or  ~/text-fabric-data  directory,\nit will not be downloaded.  If not, it will be downloaded to a location within ~/text-fabric-data . All arguments are optional, the defaults are: argument default description source bhsa repo within ETCBC organization release 1.3 release version of repo version c version of the BHSA relative {}/tf relative path of TF data within repo. The  {}  will be substituted with the value of  source . Main data Most recent version of the main BHSA data 1 getTf ()    Phono Most recent version of the  phono  features: 1 getTf ( source = 'phono' ,   release = '1.0.1' )   get and load data Here is an incantation to auto-load data: 1\n2\n3\n4\n5 from   tf.extra.bhsa   import   hasTf ,   getTf ,   Bhsa  getTf ( version = '2017' )  loc   =   hasTf ( version = '2017' )  TF   =   Fabric ( locations = [ f '{loc}/etcbc/bhsa' ],   modules = [ 'tf/2017' ])    And if you want to load phono data as well:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 from   tf.extra.bhsa   import   hasTf ,   getTf ,   Bhsa  getTf ( source = 'bhsa' ,   version = '2017' )  locMain   =   hasTf ( source = 'bhsa' ,   version = '2017' )  getTf ( source = 'phono' ,   version = '2017' )  locPhono   =   hasTf ( source = 'phono' ,   version = '2017' )  TF   =   Fabric ( \n   locations = [ f '{locMain}/etcbc/bhsa' ,   f '{locPhono}/etcbc/phono' ], \n   modules = [ 'tf/2017' ],  )    This will work also in cases where the main BHSA data is in your ~/github  directory and the phono data is in your  ~/text-fabric-data \ndirectory or vice versa.",
            "title": "Get data"
        },
        {
            "location": "/Api/Bhsa/#initialisation",
            "text": "Bhsa() 1 B   =   Bhsa ( api ,   'notebook' ,   version = VERSION )    Description Silently loads some additional features, and  B \nwill give access to some extra functions. api The API resulting from an earlier call  TF.load() Set up This module comes in action after you have set up TF and loaded some features, e.g. 1\n2\n3\n4\n5\n6 VERSION   =   '2017'  TF   =   Fabric ( locations = f '~/github/etcbc/bhsa/tf/{VERSION}' )  api   =   TF . load ( '''    function sp gn nu  ''' )  api . makeAvailableIn ( globals ())    Then we add the functionality of the  bhsa  module by a call to  Bhsa() . notebook This should be the name\nof your current notebook (without the  .ipynb  extension).\nThe Bhsa API will use this to generate a link to your notebook\non GitHub and NBViewer.",
            "title": "Initialisation"
        },
        {
            "location": "/Api/Bhsa/#linking",
            "text": "B.shbLink() 1 B . shbLink ( node ,   text = None )    Description Produces a link to SHEBANQ node node  can be an arbitrary node. The link targets the verse that\ncontains the first word contained by the node. text You may provide the text to be displayed as the link.\nThen the\npassage indicator (book chapter:verse) will be put\nin the tooltip (title) of the link.\nIf you do not provide a link text,\nthe passage indicator (book chapter:verse) will be chosen. Word 100000 on SHEBANQ 1 B . shbLink ( 100000 )",
            "title": "Linking"
        },
        {
            "location": "/Api/Bhsa/#plain-display",
            "text": "Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table. B.plain() 1 B . plain ( node ,   linked = True ,   withNodes = False ,   asString = False )    Description Displays the material that corresponds to a node in a simple way. node node  is a node of arbitrary type. linked linked  indicates whether the result should be a link to SHEBANQ\nto the appropriate book/chapter/verse. withNodes withNodes  indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say  asString=True . B.plainTuple() 1 B . plainTuple ( nodes ,   seqNumber ,   linked = 1 ,   withNodes = False ,   asString = False )    Description Displays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells. nodes nodes  is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber  is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes. linked linked=1  the column number where the cell contents is\nlinked to\nthe relevant passage in to SHEBANQ;\n(the first data column is column 1) withNodes, asString Same as in  B.plain() . B.table() 1\n2\n3\n4\n5\n6\n7 B . table ( \n   results , \n   start = 1 ,   end = len ( results ), \n   linked = 1 , \n   withNodes = False , \n   asString = False ,  )    Description Displays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable. results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. linked, withNodes, asString Same as in  B.plainTuple() .",
            "title": "Plain display"
        },
        {
            "location": "/Api/Bhsa/#pretty-display",
            "text": "Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way. B.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected\nset of features. \nWith this function you can add features to the display. features A string or iterable of feature names.\nThese features will be loaded automatically.\nIn pretty displays these features will show up as  feature=value ,\nprovided the value is not  None , or something like None. Automatic loading These features will load automatically, no explicit loading is\nnecessary. noneValues A set of values for which no display should be generated.\nThe default set is  None  and the strings  NA ,  none ,  unknown . None is useful Keep  None  in the set. If not, all custom features will be displayed\nfor all kinds of nodes. So you will see clause types on words,\n  and part of speech on clause atoms, al with value  None . Suppress common values You can use  noneValues  also to suppress the normal values of a feature,\nin order to attrect attention to the more special values, e.g. 1 noneValues = { None ,   'NA' ,   'unknown' ,   'm' ,   'sg' ,   'p3' }    None values affect all features Beware of putting to much in  noneValues .\nThe contents of  noneValues  affect the display of\nall features, not only the custom features. B.pretty() 1 B . pretty ( node ,   withNodes = False ,   suppress = set (),   highlights = {})    Description Displays the material that corresponds to a node in a graphical way. node node  is a node of arbitrary type. withNodes withNodes  indicates whether node numbers should be displayed. suppress suppress=set()  is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively. Highlighting When nodes such as verses and sentences are displayed by  pretty() ,\ntheir contents is also displayed. You can selectively highlight\nthose parts. highlights highlights={}  is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted. If  highlights  is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors.\nAny color that is a valid  CSS color \nqualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. B.prettyTuple() 1\n2\n3\n4\n5\n6\n7 B . prettyTuple ( \n   nodes ,   seqNumber , \n   withNodes = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes. By verse We examine all nodes in the tuple.\nWe collect and show all verses in which they\noccur and highlight the material corresponding to all the nodes in the tuple.\nThe highlighting can be tweaked by the optional  colorMap  parameter. nodes, seqNumber, withNodes Same as in  B.plainTuple() . suppress Same as in  B.pretty() . colorMap The nodes of the tuple will be highlighted.\nIf  colorMap  is  None  or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap  must be a dictionary that maps the positions in a tuple \nto a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid\n     CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in  B.pretty() . highlights takes precedence over colorMap If both  highlights  and  colorMap  are given,  colorMap  is ignored. If you need to micro-manage,  highlights  is your thing.\nWhenever possible, use  colorMap .   one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run  prettyTuple()  for many different tuples with the same  highlights .\nIt does not harm performance if  highlights  maps lots of nodes outside the tuple as well. B.show() 1\n2\n3\n4\n5\n6\n7\n8\n9 B . show ( \n   results , \n   condensed = True , \n   start = 1 ,   end = len ( results ), \n   withNodes = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays an iterable of tuples of nodes.\nThe elements of the list are displayed by  B.prettyTuple() . results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. condensed condensed  indicates one of two modes of displaying the result list: True : instead of showing all results one by one,\n    we show all verses with all results in it highlighted.\n    That way, we blur the distinction between the individual results,\n    but it is easier to oversee where the results are.\n    This is how SHEBANQ displays its query results. False : make a separate display for each result tuple.\n    This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring.\nIf a node occurs in two results, at different positions\nin the tuple, the  colorMap  wants to assign it two colors!\nYet one color will be chosen, and it is unpredictable which one. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. withNodes, suppress, colorMap, highlights Same as in  B.prettyTuple() .",
            "title": "Pretty display"
        },
        {
            "location": "/Api/Bhsa/#search",
            "text": "B.search() 1 B . search ( query ,   silent = False ,   shallow = False ,   sets = None )    Description Searches in the same way as the generic Text-Fabric  S.search() .\nBut whereas the  S  version returns a generator which yields the results\none by one, the  B  version collects all results and sorts them.\nIt then reports the number of results. query query  is the search template that has to be searched for. silent silent : if  True  it will suppress the reporting of the number of results. shallow If  True  or  1 , the result is a set of things that match the top-level element\nof the  query . If  2  or a bigger number  n , return the set of truncated result tuples: only\nthe first  n  members of each tuple is retained. If  False  or  0 , a sorted list of all result tuples will be returned. sets If not  None , it should be a dictionary of sets, keyed by a names.\nIn  query  you can refer to those names to invoke those sets. For example, if you have a set  gappedPhrases  of all phrase nodes that have a gap,\nyou can pass  sets=dict(gphrase=gappedPhrases) , and then in your query you can say 1\n2 gphrase function=Pred\n  word sp=verb   etc. search template reference See the  search template reference",
            "title": "Search"
        },
        {
            "location": "/Api/Cunei/",
            "text": "Cunei\n\u00b6\n\n\nAbout\n\u00b6\n\n\nThe module\n\ncunei.py\n\ncontains a number of handy functions to deal with TF nodes for cuneiform tablets\nand\n\nATF\n\ntranscriptions of them and \nCDLI\n photos and lineart.\n\n\nSee also\n\nabout\n,\n\nimages\n,\n\ntranscription\n.\n\n\nSet up\n\u00b6\n\n\nfrom tf.extra.cunei import Cunei\nimport Cunei\nThe \nCunei\n API is distributed with Text-Fabric.\nYou have to import it into your program.\nInitialisation\n\u00b6\n\n\nCunei()\n1\n2\n    \nCN\n \n=\n \nCunei\n(\n'~/github'\n,\n \n'Nino-cunei/uruk'\n,\n \n'notebook'\n)\n\n    \nCN\n.\napi\n.\nmakeAvailableIn\n(\nglobals\n())\n\n\n\n\n\n\nDescription\nText-Fabric will be started for you and load all features.\nWhen \nCunei\n is\ninitializing, it scans the image directory of the repo and reports how many\nphotos and lineart images it sees.\nlocal GitHub\nThe argument \n~/github\n\nshould point to the directory where your local\ngithub repositories reside.\nUruk location\nThe argument \nNino-cunei/uruk\n\nshould point to the local GitHub repository\nwhere the Uruk corpus resides.\nnotebook\nThe third argument of \nCunei()\n should be the name\nof your current notebook (without the \n.ipynb\n extension).\nThe Cunei API will use this to generate a link to your notebook\non GitHub and NBViewer.\nNote\nYour current notebook can be anywhere on your system.\n\nCunei()\n can find its\nlocation, but not its name, hence you have to pass its name.\nLinking\n\u00b6\n\n\nCN.cdli()\n1\nCN\n.\ncdli\n(\ntablet\n,\n \nlinkText\n=\nNone\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nProduces a link to a tablet page on CDLI,\nto be placed in an output cell.\ntablet\ntablet\n is either a node of type \ntablet\n\nor a P-number of a tablet.\nlinkText\nYou may provide the text to be displayed as the link.\nIf you do not provide any,\nthe P-number of the tablet will be used.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say \nasString=True\n.\nCN.tabletLink()\n1\nCN\n.\ntabletLink\n(\nnode\n,\n \ntext\n=\nNone\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nProduces a link to CDLI\nnode\nnode\n can be an arbitrary node. The link targets the tablet that\ncontains the material contained by the node.\ntext\nYou may provide the text to be displayed as the link.\nIf you do not provide a link text,\nthe P-number of the tablet will be chosen.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say \nasString=True\n.\nSign 10000 on CDLI\n1\nCN\n.\ntabletLink\n(\n100000\n)\n\n\n\n\n\n\nPlain display\n\u00b6\n\n\nStraightforward display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table.\nCN.plain()\n1\nCN\n.\nplain\n(\nnode\n,\n \nlinked\n=\nTrue\n,\n \nwithNodes\n=\nFalse\n,\n \nlineart\n=\nTrue\n,\n \nlineNumbers\n=\nFalse\n,\n \nasString\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a simple way.\nnode\nnode\n is a node of arbitrary type.\nlinked\nlinked\n indicates whether the result should be a link to CDLI\nto the tablet on which the node occurs.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nlineart\nlineart\n indicates whether to display a lineart image in addition\n(only relevant for signs and quads)\nlineNumbers\nlineNumbers\n indicates whether corresponding line numbers in the\nATF source should be displayed.\nasString\nInstead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say \nasString=True\n.\nCN.plainTuple()\n1\n2\n3\n4\n5\n6\nCN\n.\nplainTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n \nlineart\n=\nTrue\n,\n \nlineNumbers\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells.\nnodes\nnodes\n is an iterable (list, tuple, set, etc) of arbitrary nodes.\nseqNumber\nseqNumber\n is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes.\nlinked\nlinked=1\n the column number where the cell contents is\nlinked to\nthe CDLI page of the containing tablet;\n(the first data column is column 1)\nwithNodes, lineart, lineNumbers, asString\nSame as in \nCN.plain()\n.\nCN.table()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCN\n.\ntable\n(\n\n  \nresults\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nlinked\n=\n1\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nasString\n=\nFalse\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nlinked, withNodes, lineart, lineNumbers, asString\nSame as in \nCN.plainTuple()\n.\nPretty display\n\u00b6\n\n\nGraphical display of things\nThere are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way.\nCN.pretty()\n1\nCN\n.\npretty\n(\nnode\n,\n \nwithNodes\n=\nFalse\n,\n \nsuppress\n=\nset\n(),\n \nhighlights\n=\n{})\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a node in a graphical way.\nnode\nnode\n is a node of arbitrary type.\nwithNodes\nwithNodes\n indicates whether node numbers should be displayed.\nlineart, lineNumbers\nSame as in \nCN.plain()\n.\nsuppress\nsuppress=set()\n is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively.\nHighlighting\nWhen nodes such as tablets and cases are displayed by \npretty()\n,\ntheir contents is also displayed. You can selectively highlight\nthose parts.\nhighlights\nhighlights={}\n is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted.\nIf \nhighlights\n is a set, its nodes will be highlighted with a default color (yellow).\nIf it is a dictionary, it should map nodes to colors.\nAny color that is a valid \n\nCSS color\n\nqualifies.\nIf you map a node to the empty string, it will get the default highlight color.\ncolor names\nThe link above points to a series of handy color names and their previews.\nCN.prettyTuple()\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCN\n.\nprettyTuple\n(\n\n  \nnodes\n,\n \nseqNumber\n,\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes.\nBy tablet\nWe examine all nodes in the tuple.\nWe collect and show all tablets in which they\noccur and highlight the material corresponding to the all nodes in the tuple.\nThe highlighting can be tweaked by the optional \ncolorMap\n parameter.\nnodes, seqNumber, withNodes, lineart, lineNumbers\nSame as in \nCN.plainTuple()\n.\nsuppress\nSame as in \nCN.pretty()\n.\ncolorMap\nThe nodes of the tuple will be highlighted.\nIf \ncolorMap\n is \nNone\n or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow.\nBut you can assign different colors to the members of the tuple:\n\ncolorMap\n must be a dictionary that maps the positions in a tuple \nto a color:\nIf a position is not mapped, it will not be highlighted.\nIf it is mapped to the empty string, it gets the default highlight color.\nOtherwise, it should be mapped to a string that is a valid\n    \nCSS color\n.\ncolor names\nThe link above points to a series of handy color names and their previews.\nhighlights\nSame as in \nB.pretty()\n.\nhighlights takes precedence over colorMap\nIf both \nhighlights\n and \ncolorMap\n are given, \ncolorMap\n is ignored.\nIf you need to micro-manage, \nhighlights\n is your thing.\nWhenever possible, use \ncolorMap\n.  \none big highlights dictionary\nIt is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run \nprettyTuple()\n for many different tuples with the same \nhighlights\n.\nIt does not harm performance if \nhighlights\n maps lots of nodes outside the tuple as well.\nCN.show()\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nCN\n.\nshow\n(\n\n  \nresults\n,\n\n  \ncondensed\n=\nTrue\n,\n\n  \nstart\n=\n1\n,\n \nend\n=\nlen\n(\nresults\n),\n\n  \nwithNodes\n=\nFalse\n,\n\n  \nlineart\n=\nTrue\n,\n\n  \nlineNumbers\n=\nFalse\n,\n\n  \nsuppress\n=\nset\n(),\n\n  \ncolorMap\n=\nNone\n,\n\n  \nhighlights\n=\nNone\n,\n\n\n)\n\n\n\n\n\n\nDescription\nDisplays an iterable of tuples of nodes.\nThe elements of the list are displayed by \nCN.prettyTuple()\n.\nresults\nresults\n an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples.\ncondensed\ncondensed\n indicates one of two modes of displaying the result list:\nTrue\n: instead of showing all results one by one,\n    we show all tablets with all results in it highlighted.\n    That way, we blur the distinction between the individual results,\n    but it is easier to oversee where the results are.\nFalse\n: make a separate display for each result tuple.\n    This gives the best account of the exact result set.\nstart\nstart\n is the starting point in the results iterable (1 is the first one).\nDefault 1.\nend\nend\n is the end point in the results iterable.\nDefault the length of the iterable.\nwithNodes, lineart, lineNumbers, suppress, colorMap, highlights\nSame as in \nB.prettyTuple()\n.\nSearch\n\u00b6\n\n\nCN.search()\n1\nCN\n.\nsearch\n(\nquery\n,\n \nsilent\n=\nFalse\n,\n \nshallow\n=\nFalse\n,\n \nsets\n=\nNone\n)\n\n\n\n\n\n\nDescription\nSearches in the same way as the generic Text-Fabric \nS.search()\n.\nBut whereas the \nS\n version returns a generator which yields the results\none by one, the \nCN\n version collects all results and sorts them.\nIt then reports the number of results.\nquery\nquery\n is the search template that has to be searched for.\nsilent\nsilent\n: if \nTrue\n it will suppress the reporting of the number of results.\nshallow\nIf \nTrue\n or \n1\n, the result is a set of things that match the top-level element\nof the \nquery\n.\nIf \n2\n or a bigger number \nn\n, return the set of truncated result tuples: only\nthe first \nn\n members of each tuple is retained.\nIf \nFalse\n or \n0\n, a sorted list of all result tuples will be returned.\nsets\nIf not \nNone\n, it should be a dictionary of sets, keyed by a names.\nIn \nquery\n you can refer to those names to invoke those sets.\nFor example, if you have a set \nspecialCases\n of all case nodes that are special in some way,\nyou can pass \nsets=dict(scase=specialCases)\n, and then in your query you can say\n1\n2\nscase number=1a\n  sign grapheme=UKKIN\n\n\n\n\n\netc.\nsearch template reference\nSee the \nsearch template reference\nATF representation\n\u00b6\n\n\nGenerate ATF\nSigns and quads and clusters can be represented by an ascii string,\nin the so-called Ascii Text Format,\n\nATF\n.\nWe provide a bunch of function that, given a node, generate the appropriate ATF\nrepresentation.\nCN.atfFromSign()\n1\nCN\n.\natfFromSign\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a sign.\nnode\nnode\n must have node type \nsign\n.\nflags\nflags\n whether the \nflags\n associated with the sign\nwill be included in the ATF.\nCN.atfFromQuad()\n1\nCN\n.\natfFromQuad\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a quad.\nnode\nnode\n must have node type \nquad\n.\nflags\nflags\n whether the \nflags\n associated with the quad\nwill be included in the ATF.\nCN.atfFromOuterQuad()\n1\nCN\n.\natfFromOuterQuad\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a quad or sign.\nnode\nnode\n must have node type \nquad\n or \nsign\n.\nflags\nflags\n whether the \nflags\n associated with the quad\nwill be included in the ATF.\nouter quads\nIf you take an ATF transcription line with linguistic material on it, and you\nsplit it on white space, and you forget the brackets that cluster quads and\nsigns, then you get a sequence of outer quads and signs.\nIf you need to get the ATF representation for these items, this function does\nconveniently produce them. You do not have to worry yourself about the sign/quad\ndistinction here.\nCN.atfFromCluster()\n1\nCN\n.\natfFromCluster\n(\nnode\n,\n \nflags\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nReproduces the ATF representation of a cluster.\nnode\nnode\n must have node type \nquad\n.\nclusters\nClusters are bracketings of\nquads that indicate proper names, uncertainty, or supplied material. In ATF they\nlook like \n( )a\n or \n[ ]\n or \n< >\nSub-clusters\nSub-clusters will also be\nrepresented. Signs belonging to multiple nested clusters will only be\nrepresented once.\nCN.getSource()\n1\nCN\n.\ngetSource\n(\nnode\n,\n \nnodeType\n=\nNone\n,\n \nlineNumbers\n=\nFalse\n)\n\n\n\n\n\n\nDescription\nDelivers the transcription source of nodes that correspond to the\nATF source line level.\nThis in contrast with the \nCN.atfFromXxx()\n functions that\nwork for nodes that correspond to parts of the ATF source lines.\nnode\nnode\n must have a type in \ntablet\n, \nface\n, \ncolumn\n,\n\ncomment\n, \nline\n, \ncase\n.\nnodeType\nIf \nnodeType\n is passed, only source lines of this type are returned.\nlineNumbers\nlineNumbers\n: if \nTrue\n, add line numbers to the result,\nthese numbers say where the source line occurs in the source file.\nTF from ATF conversion\nThe conversion of ATF to Text-Fabric has saved the original source lines and\ntheir line numbers in the features \nsrcLn\n and \nsrcLnNum\n respectively. This\nfunction makes use of those features.\nSections\n\u00b6\n\n\nSections in tablets\nText-Fabric supports 3 section levels in general.\nThe Uruk corpus uses them for \ntablets\n, \ncolumns\n and \nlines\n.\nBut lines may be divided in cases and subcases, which are also numbered.\nWe need to mimick some functions of the Text-Fabric \nT\n Api for sections,\nso that we can retrieve cases more easily.\nConsider search\nText-Fabric Search is a generic and powerful mechanism for information retrieval.\nIn most cases it is easier to extract nodes by search than by hand-written\ncode using the functions here.\nCN.nodeFromCase()\n1\nCN\n.\nnodeFromCase\n((\nP\n-\nnumber\n,\n \nface\n:\ncolumnNumber\n,\n \nhLineNumber\n))\n\n\n\n\n\n\nDescription\nGives you a node, if you specify a terminal case, i.e. a\nnumbered transcription line.\nCompare \nT.nodeFromSection()\nThis function is analogous to\n\nT.nodeFromSection()\n of Text-Fabric.\ncase specification\nThis function takes a single argument which must be\na tuple\n(\ntabletNumber\n, \nface\n:\ncolumnNumber\n, \nhierarchical-line-number\n).\ndots\nThe hierarchical number may contain the original \n.\n that they\noften have in the transcriptions, but you may also leave them out.\nNot found\nIf no such node exists, you get \nNone\n back.\nCN.caseFromNode()\n1\nCN\n.\ncaseFromNode\n(\nnode\n)\n\n\n\n\n\n\nDescription\nGives you a terminal case specification,\nif you give a node of a case or something inside a case or line.\nCompare \nT.sectionFromNode()\nThis function is analogous to\n\nT.sectionFromNode()\n of Text-Fabric.\ncase specification\nA case specification is a tuple\n(\ntabletNumber\n, \nface\n:\ncolumnNumber\n, \nhierarchical-line-number\n).\nThe hierarchical line number will not contain dots.\nnode\nnode\n must be of a terminal case\n(these are the cases that have a full hierarchical\nnumber; these cases correspond to the individual numbered lines in the\ntranscription sources).\nother nodes\nIf \nnode\n corresponds to something inside a transcription line,\nthe node of the terminal case or line in which it is contained will be used.\nCN.lineFromNode()\n1\nCN\n.\nlineFromNode\n(\nnode\n)\n\n\n\n\n\n\nDescription\nIf called on a node corresponding to something inside a transcription line, it\nwill navigate to up to the terminal case or line in which it is contained, and\nreturn that node.\nnode\nnode\n must correspond to something inside a transcription line:\n\nsign\n, \nquad\n, \ncluster\n.\nCN.casesByLevel()\n1\nCN\n.\ncasesByLevel\n(\nk\n,\n \nterminal\n=\nTrue\n)\n\n\n\n\n\n\nDescription\nGrabs all (sub)cases of a specified level. You can choose to filter the result\nto those (sub)cases that are \nterminal\n, i.e. those which do not contain\nsubcases anymore. Such cases correspond to individual lines in the ATF.\nk\nk\n is an integer, indicating the level of (sub)cases you want.\n\n0\n is lines,\n\n1\n is top-level cases,\n\n2\n is subcases,\n\n3\n is subsubcases, and so on.\nterminal\nterminal\n: if \nTrue\n, only lines and cases that have the feature \nterminal\n\nare delivered.\nOtherwise, all lines/cases of that level will be delivered.\nCN.getOuterQuads()\n1\nCN\n.\ngetOuterQuads\n(\nnode\n)\n\n\n\n\n\n\nDescription\nCollects the outer quads and isolated signs under a node.\nnode\nnode\n is typically a tablet, face, column, line, or case.\nThis is the container of the outer quads.\nOuter quads\nOuter quads and isolated signs is what you get\nif you split line material by white space and\nremove cluster brackets.\nImages\n\u00b6\n\n\nCN.photo() and CN.lineart()\n1\n2\nCN\n.\nphoto\n(\nnodes\n,\n \nkey\n=\nNone\n,\n \nasLink\n=\nTrue\n,\n \nwithCaption\n=\n'bottom'\n,\n \n**\noptions\n)\n\n\nCN\n.\nlineart\n(\nnodes\n,\n \nkey\n=\nNone\n,\n \nasLink\n=\nTrue\n,\n \nwithCaption\n=\n'bottom'\n,\n \n**\noptions\n)\n\n\n\n\n\n\nDescription\nFetches photos or linearts for tablets, signs or quads, and returns it in a way\nthat it can be embedded in an output cell. The images that show up are clickable\nand link through to an online, higher resolution version on CDLI. Images will\nhave, by default, a caption that links to the relevant page on CDLI.\nPlacement\nThe result will be returned as a \nrow\n of images.\nSubsequent calls to \nphoto()\n and \nlineart()\n\nwill result in vertically stacked rows.\nnodes\nnodes\n is one or more \nnodes\n.\nAs far as they are of type \ntablet\n, \nquad\n or \nsign\n,\na photo or lineart will be looked up for them.\nby name\nInstead of a node you may also\nsupply the P-number of a tablet or the name of the sign or quad.\nkey\nkey\n is an optional string specifying which of the available images for\nthis node you want to use.\nlook up\nif you want to know which keys are available for a\nnode, supply \nkey='xxx'\n, or any non-existing key.\nasLink\nasLink=True\n: no image will be placed, only a link to the online\nimage at CDLI.\nIn this case the \ncaption\n will be suppressed, unless\nexplicitly given.\nwithCaption\nwithCaption='bottom'\n controls whether a CDLI link to the\ntablet page must be put under the image.\nYou can also specify \ntop\n, \nleft\n, \nright\n.\nIf left out, no caption will be placed.\noptions\noptions\n is a series of key=value arguments that\ncontrol the placement of the images,\nsuch as \nwidth=100\n, \nheight=200\n.\nCSS\nThe optional parameters \nheight\n and \nwidth\n control the height and width of the\nimages. The value should be a valid\n\nCSS\n length, such as\n\n100px\n, \n10em\n, \n32vw\n. If you pass an integer, or a decimal string without\nunit, your value will be converted to that many \npx\n.\nThese parameters are interpreted as setting a maximum value (in fact they will\nend up as \nmax-width\n and \nmax-height\n on the final \n<img/>\n element in the\nHTML.\nSo if you specify both \nwidth\n and \nheight\n, the image will be placed in tightly\nin a box of those dimensions without changing the aspect ratio.\nIf you want to force that the width of height you pass is completely consumed,\nyou can prefix your value with a \n!\n. In that case the aspect ratio maybe\nchanged. You can use the \n!\n also for both \nheight\n and \nwidth\n. In that case,\nthe rectangle will be completely filled, and the aspect ratio will be adjusted\nto that of the rectangle.\nThe way the effect of the \n!\n is achieved, is by adding \nmin-width\n and\n\nmin-height\n properties to the \n<img/>\n element.\nlocal images\nThe images will be called in by a little piece of generated HTML, using the\n\n<img/>\n tag. This only works if the image is within reach. To the images will\nbe copied to a sister directory of the notebook. The name of this directory is\n\ncdli-imagery\n. It will be created on-the-fly when needed. Copying will only be\ndone if needed. The names of the images will be changed, to prevent problems\nwith systems that cannot handle \n|\n and \n+\n characters in file names well.\nCN.imagery()\n1\nCN\n.\nimagery\n(\nobjectType\n,\n \nkind\n)\n\n\n\n\n\n\nDescription\nProvides the sets of locally available images by object type.\nfor tablets, it lists the P-numbers; for sign/quads: the ATF representations.\nobjectType\nobjectType\n is the type of thing: \nideograph\n or \ntablet\n.\nkind\nkind\n is \nphoto\n or \nlineart\n.",
            "title": "Cuneiform Tablets"
        },
        {
            "location": "/Api/Cunei/#cunei",
            "text": "",
            "title": "Cunei"
        },
        {
            "location": "/Api/Cunei/#about",
            "text": "The module cunei.py \ncontains a number of handy functions to deal with TF nodes for cuneiform tablets\nand ATF \ntranscriptions of them and  CDLI  photos and lineart.  See also about , images , transcription .",
            "title": "About"
        },
        {
            "location": "/Api/Cunei/#set-up",
            "text": "from tf.extra.cunei import Cunei import Cunei The  Cunei  API is distributed with Text-Fabric.\nYou have to import it into your program.",
            "title": "Set up"
        },
        {
            "location": "/Api/Cunei/#initialisation",
            "text": "Cunei() 1\n2      CN   =   Cunei ( '~/github' ,   'Nino-cunei/uruk' ,   'notebook' ) \n     CN . api . makeAvailableIn ( globals ())    Description Text-Fabric will be started for you and load all features.\nWhen  Cunei  is\ninitializing, it scans the image directory of the repo and reports how many\nphotos and lineart images it sees. local GitHub The argument  ~/github \nshould point to the directory where your local\ngithub repositories reside. Uruk location The argument  Nino-cunei/uruk \nshould point to the local GitHub repository\nwhere the Uruk corpus resides. notebook The third argument of  Cunei()  should be the name\nof your current notebook (without the  .ipynb  extension).\nThe Cunei API will use this to generate a link to your notebook\non GitHub and NBViewer. Note Your current notebook can be anywhere on your system. Cunei()  can find its\nlocation, but not its name, hence you have to pass its name.",
            "title": "Initialisation"
        },
        {
            "location": "/Api/Cunei/#linking",
            "text": "CN.cdli() 1 CN . cdli ( tablet ,   linkText = None ,   asString = False )    Description Produces a link to a tablet page on CDLI,\nto be placed in an output cell. tablet tablet  is either a node of type  tablet \nor a P-number of a tablet. linkText You may provide the text to be displayed as the link.\nIf you do not provide any,\nthe P-number of the tablet will be used. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say  asString=True . CN.tabletLink() 1 CN . tabletLink ( node ,   text = None ,   asString = False )    Description Produces a link to CDLI node node  can be an arbitrary node. The link targets the tablet that\ncontains the material contained by the node. text You may provide the text to be displayed as the link.\nIf you do not provide a link text,\nthe P-number of the tablet will be chosen. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the HTML as string,\njust say  asString=True . Sign 10000 on CDLI 1 CN . tabletLink ( 100000 )",
            "title": "Linking"
        },
        {
            "location": "/Api/Cunei/#plain-display",
            "text": "Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a simple way, as rows and as a table. CN.plain() 1 CN . plain ( node ,   linked = True ,   withNodes = False ,   lineart = True ,   lineNumbers = False ,   asString = False )    Description Displays the material that corresponds to a node in a simple way. node node  is a node of arbitrary type. linked linked  indicates whether the result should be a link to CDLI\nto the tablet on which the node occurs. withNodes withNodes  indicates whether node numbers should be displayed. lineart lineart  indicates whether to display a lineart image in addition\n(only relevant for signs and quads) lineNumbers lineNumbers  indicates whether corresponding line numbers in the\nATF source should be displayed. asString Instead of displaying the result directly in the output of your\ncode cell in a notebook, you can also deliver the markdown as string,\njust say  asString=True . CN.plainTuple() 1\n2\n3\n4\n5\n6 CN . plainTuple ( \n   nodes ,   seqNumber , \n   linked = 1 , \n   withNodes = False ,   lineart = True ,   lineNumbers = False , \n   asString = False ,  )    Description Displays the material that corresponds to a tuple of nodes\nin a simple way as a row of cells. nodes nodes  is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber  is an arbitrary number which will be displayed\nin the first cell.\nThis prepares the way for displaying query results, which come as\na sequence of tuples of nodes. linked linked=1  the column number where the cell contents is\nlinked to\nthe CDLI page of the containing tablet;\n(the first data column is column 1) withNodes, lineart, lineNumbers, asString Same as in  CN.plain() . CN.table() 1\n2\n3\n4\n5\n6\n7\n8\n9 CN . table ( \n   results , \n   start = 1 ,   end = len ( results ), \n   linked = 1 , \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   asString = False ,  )    Description Displays an iterable of tuples of nodes.\nThe list is displayed as a compact markdown table.\nEvery row is prepended with the sequence number in the iterable. results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. linked, withNodes, lineart, lineNumbers, asString Same as in  CN.plainTuple() .",
            "title": "Plain display"
        },
        {
            "location": "/Api/Cunei/#pretty-display",
            "text": "Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples\nof nodes in a graphical way. CN.pretty() 1 CN . pretty ( node ,   withNodes = False ,   suppress = set (),   highlights = {})    Description Displays the material that corresponds to a node in a graphical way. node node  is a node of arbitrary type. withNodes withNodes  indicates whether node numbers should be displayed. lineart, lineNumbers Same as in  CN.plain() . suppress suppress=set()  is a set of feature names that should NOT be displayed.\nBy default, quite a number of features is displayed for a node.\nIf you find they clutter the display, you can turn them off\nselectively. Highlighting When nodes such as tablets and cases are displayed by  pretty() ,\ntheir contents is also displayed. You can selectively highlight\nthose parts. highlights highlights={}  is a set or mapping of nodes that should be highlighted.\nOnly nodes that are involved in the display will be highlighted. If  highlights  is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors.\nAny color that is a valid  CSS color \nqualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. CN.prettyTuple() 1\n2\n3\n4\n5\n6\n7\n8\n9 CN . prettyTuple ( \n   nodes ,   seqNumber , \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays the material that corresponds to a tuple of nodes in a graphical way,\nwith customizable highlighting of nodes. By tablet We examine all nodes in the tuple.\nWe collect and show all tablets in which they\noccur and highlight the material corresponding to the all nodes in the tuple.\nThe highlighting can be tweaked by the optional  colorMap  parameter. nodes, seqNumber, withNodes, lineart, lineNumbers Same as in  CN.plainTuple() . suppress Same as in  CN.pretty() . colorMap The nodes of the tuple will be highlighted.\nIf  colorMap  is  None  or missing, all nodes will be highlighted with\nthe default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap  must be a dictionary that maps the positions in a tuple \nto a color: If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid\n     CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in  B.pretty() . highlights takes precedence over colorMap If both  highlights  and  colorMap  are given,  colorMap  is ignored. If you need to micro-manage,  highlights  is your thing.\nWhenever possible, use  colorMap .   one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes,\nand then run  prettyTuple()  for many different tuples with the same  highlights .\nIt does not harm performance if  highlights  maps lots of nodes outside the tuple as well. CN.show()  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 CN . show ( \n   results , \n   condensed = True , \n   start = 1 ,   end = len ( results ), \n   withNodes = False , \n   lineart = True , \n   lineNumbers = False , \n   suppress = set (), \n   colorMap = None , \n   highlights = None ,  )    Description Displays an iterable of tuples of nodes.\nThe elements of the list are displayed by  CN.prettyTuple() . results results  an iterable of tuples of nodes.\nThe results of a search qualify, but it works\nno matter which process has produced the tuples. condensed condensed  indicates one of two modes of displaying the result list: True : instead of showing all results one by one,\n    we show all tablets with all results in it highlighted.\n    That way, we blur the distinction between the individual results,\n    but it is easier to oversee where the results are. False : make a separate display for each result tuple.\n    This gives the best account of the exact result set. start start  is the starting point in the results iterable (1 is the first one).\nDefault 1. end end  is the end point in the results iterable.\nDefault the length of the iterable. withNodes, lineart, lineNumbers, suppress, colorMap, highlights Same as in  B.prettyTuple() .",
            "title": "Pretty display"
        },
        {
            "location": "/Api/Cunei/#search",
            "text": "CN.search() 1 CN . search ( query ,   silent = False ,   shallow = False ,   sets = None )    Description Searches in the same way as the generic Text-Fabric  S.search() .\nBut whereas the  S  version returns a generator which yields the results\none by one, the  CN  version collects all results and sorts them.\nIt then reports the number of results. query query  is the search template that has to be searched for. silent silent : if  True  it will suppress the reporting of the number of results. shallow If  True  or  1 , the result is a set of things that match the top-level element\nof the  query . If  2  or a bigger number  n , return the set of truncated result tuples: only\nthe first  n  members of each tuple is retained. If  False  or  0 , a sorted list of all result tuples will be returned. sets If not  None , it should be a dictionary of sets, keyed by a names.\nIn  query  you can refer to those names to invoke those sets. For example, if you have a set  specialCases  of all case nodes that are special in some way,\nyou can pass  sets=dict(scase=specialCases) , and then in your query you can say 1\n2 scase number=1a\n  sign grapheme=UKKIN   etc. search template reference See the  search template reference",
            "title": "Search"
        },
        {
            "location": "/Api/Cunei/#atf-representation",
            "text": "Generate ATF Signs and quads and clusters can be represented by an ascii string,\nin the so-called Ascii Text Format, ATF . We provide a bunch of function that, given a node, generate the appropriate ATF\nrepresentation. CN.atfFromSign() 1 CN . atfFromSign ( node ,   flags = False )    Description Reproduces the ATF representation of a sign. node node  must have node type  sign . flags flags  whether the  flags  associated with the sign\nwill be included in the ATF. CN.atfFromQuad() 1 CN . atfFromQuad ( node ,   flags = False )    Description Reproduces the ATF representation of a quad. node node  must have node type  quad . flags flags  whether the  flags  associated with the quad\nwill be included in the ATF. CN.atfFromOuterQuad() 1 CN . atfFromOuterQuad ( node ,   flags = False )    Description Reproduces the ATF representation of a quad or sign. node node  must have node type  quad  or  sign . flags flags  whether the  flags  associated with the quad\nwill be included in the ATF. outer quads If you take an ATF transcription line with linguistic material on it, and you\nsplit it on white space, and you forget the brackets that cluster quads and\nsigns, then you get a sequence of outer quads and signs. If you need to get the ATF representation for these items, this function does\nconveniently produce them. You do not have to worry yourself about the sign/quad\ndistinction here. CN.atfFromCluster() 1 CN . atfFromCluster ( node ,   flags = False )    Description Reproduces the ATF representation of a cluster. node node  must have node type  quad . clusters Clusters are bracketings of\nquads that indicate proper names, uncertainty, or supplied material. In ATF they\nlook like  ( )a  or  [ ]  or  < > Sub-clusters Sub-clusters will also be\nrepresented. Signs belonging to multiple nested clusters will only be\nrepresented once. CN.getSource() 1 CN . getSource ( node ,   nodeType = None ,   lineNumbers = False )    Description Delivers the transcription source of nodes that correspond to the\nATF source line level. This in contrast with the  CN.atfFromXxx()  functions that\nwork for nodes that correspond to parts of the ATF source lines. node node  must have a type in  tablet ,  face ,  column , comment ,  line ,  case . nodeType If  nodeType  is passed, only source lines of this type are returned. lineNumbers lineNumbers : if  True , add line numbers to the result,\nthese numbers say where the source line occurs in the source file. TF from ATF conversion The conversion of ATF to Text-Fabric has saved the original source lines and\ntheir line numbers in the features  srcLn  and  srcLnNum  respectively. This\nfunction makes use of those features.",
            "title": "ATF representation"
        },
        {
            "location": "/Api/Cunei/#sections",
            "text": "Sections in tablets Text-Fabric supports 3 section levels in general.\nThe Uruk corpus uses them for  tablets ,  columns  and  lines . But lines may be divided in cases and subcases, which are also numbered.\nWe need to mimick some functions of the Text-Fabric  T  Api for sections,\nso that we can retrieve cases more easily. Consider search Text-Fabric Search is a generic and powerful mechanism for information retrieval.\nIn most cases it is easier to extract nodes by search than by hand-written\ncode using the functions here. CN.nodeFromCase() 1 CN . nodeFromCase (( P - number ,   face : columnNumber ,   hLineNumber ))    Description Gives you a node, if you specify a terminal case, i.e. a\nnumbered transcription line. Compare  T.nodeFromSection() This function is analogous to T.nodeFromSection()  of Text-Fabric. case specification This function takes a single argument which must be\na tuple\n( tabletNumber ,  face : columnNumber ,  hierarchical-line-number ). dots The hierarchical number may contain the original  .  that they\noften have in the transcriptions, but you may also leave them out. Not found If no such node exists, you get  None  back. CN.caseFromNode() 1 CN . caseFromNode ( node )    Description Gives you a terminal case specification,\nif you give a node of a case or something inside a case or line. Compare  T.sectionFromNode() This function is analogous to T.sectionFromNode()  of Text-Fabric. case specification A case specification is a tuple\n( tabletNumber ,  face : columnNumber ,  hierarchical-line-number ).\nThe hierarchical line number will not contain dots. node node  must be of a terminal case\n(these are the cases that have a full hierarchical\nnumber; these cases correspond to the individual numbered lines in the\ntranscription sources). other nodes If  node  corresponds to something inside a transcription line,\nthe node of the terminal case or line in which it is contained will be used. CN.lineFromNode() 1 CN . lineFromNode ( node )    Description If called on a node corresponding to something inside a transcription line, it\nwill navigate to up to the terminal case or line in which it is contained, and\nreturn that node. node node  must correspond to something inside a transcription line: sign ,  quad ,  cluster . CN.casesByLevel() 1 CN . casesByLevel ( k ,   terminal = True )    Description Grabs all (sub)cases of a specified level. You can choose to filter the result\nto those (sub)cases that are  terminal , i.e. those which do not contain\nsubcases anymore. Such cases correspond to individual lines in the ATF. k k  is an integer, indicating the level of (sub)cases you want. 0  is lines, 1  is top-level cases, 2  is subcases, 3  is subsubcases, and so on. terminal terminal : if  True , only lines and cases that have the feature  terminal \nare delivered.\nOtherwise, all lines/cases of that level will be delivered. CN.getOuterQuads() 1 CN . getOuterQuads ( node )    Description Collects the outer quads and isolated signs under a node. node node  is typically a tablet, face, column, line, or case.\nThis is the container of the outer quads. Outer quads Outer quads and isolated signs is what you get\nif you split line material by white space and\nremove cluster brackets.",
            "title": "Sections"
        },
        {
            "location": "/Api/Cunei/#images",
            "text": "CN.photo() and CN.lineart() 1\n2 CN . photo ( nodes ,   key = None ,   asLink = True ,   withCaption = 'bottom' ,   ** options )  CN . lineart ( nodes ,   key = None ,   asLink = True ,   withCaption = 'bottom' ,   ** options )    Description Fetches photos or linearts for tablets, signs or quads, and returns it in a way\nthat it can be embedded in an output cell. The images that show up are clickable\nand link through to an online, higher resolution version on CDLI. Images will\nhave, by default, a caption that links to the relevant page on CDLI. Placement The result will be returned as a  row  of images.\nSubsequent calls to  photo()  and  lineart() \nwill result in vertically stacked rows. nodes nodes  is one or more  nodes .\nAs far as they are of type  tablet ,  quad  or  sign ,\na photo or lineart will be looked up for them. by name Instead of a node you may also\nsupply the P-number of a tablet or the name of the sign or quad. key key  is an optional string specifying which of the available images for\nthis node you want to use. look up if you want to know which keys are available for a\nnode, supply  key='xxx' , or any non-existing key. asLink asLink=True : no image will be placed, only a link to the online\nimage at CDLI.\nIn this case the  caption  will be suppressed, unless\nexplicitly given. withCaption withCaption='bottom'  controls whether a CDLI link to the\ntablet page must be put under the image.\nYou can also specify  top ,  left ,  right .\nIf left out, no caption will be placed. options options  is a series of key=value arguments that\ncontrol the placement of the images,\nsuch as  width=100 ,  height=200 . CSS The optional parameters  height  and  width  control the height and width of the\nimages. The value should be a valid CSS  length, such as 100px ,  10em ,  32vw . If you pass an integer, or a decimal string without\nunit, your value will be converted to that many  px . These parameters are interpreted as setting a maximum value (in fact they will\nend up as  max-width  and  max-height  on the final  <img/>  element in the\nHTML. So if you specify both  width  and  height , the image will be placed in tightly\nin a box of those dimensions without changing the aspect ratio. If you want to force that the width of height you pass is completely consumed,\nyou can prefix your value with a  ! . In that case the aspect ratio maybe\nchanged. You can use the  !  also for both  height  and  width . In that case,\nthe rectangle will be completely filled, and the aspect ratio will be adjusted\nto that of the rectangle. The way the effect of the  !  is achieved, is by adding  min-width  and min-height  properties to the  <img/>  element. local images The images will be called in by a little piece of generated HTML, using the <img/>  tag. This only works if the image is within reach. To the images will\nbe copied to a sister directory of the notebook. The name of this directory is cdli-imagery . It will be created on-the-fly when needed. Copying will only be\ndone if needed. The names of the images will be changed, to prevent problems\nwith systems that cannot handle  |  and  +  characters in file names well. CN.imagery() 1 CN . imagery ( objectType ,   kind )    Description Provides the sets of locally available images by object type.\nfor tablets, it lists the P-numbers; for sign/quads: the ATF representations. objectType objectType  is the type of thing:  ideograph  or  tablet . kind kind  is  photo  or  lineart .",
            "title": "Images"
        },
        {
            "location": "/Api/Lib/",
            "text": "Lib\n\u00b6\n\n\nWhile Text-Fabric is a generic package to deal with text and annotations\nin a model of nodes, edges, and features, there is need for some additions.\n\n\nTranscription\n\u00b6\n\n\ntranscription.py\n contains transliteration tables for Hebrew and Syriac that\nare being used in the \nBHSA\n.",
            "title": "Auxiliary"
        },
        {
            "location": "/Api/Lib/#lib",
            "text": "While Text-Fabric is a generic package to deal with text and annotations\nin a model of nodes, edges, and features, there is need for some additions.",
            "title": "Lib"
        },
        {
            "location": "/Api/Lib/#transcription",
            "text": "transcription.py  contains transliteration tables for Hebrew and Syriac that\nare being used in the  BHSA .",
            "title": "Transcription"
        },
        {
            "location": "/Server/Service/",
            "text": "Text-Fabric as a service\n\u00b6\n\n\nAbout\n\u00b6\n\n\nTF as a service\nText-Fabric can be used as a service.\nThe full API of Text-Fabric needs a lot of memory, which makes it unusably for\nrapid successions of loading and unloading, like when used in a web server context.\nHowever, you can start TF as a server, after which many clients can connect to it,\nall looking at the same (read-only) data.\nThe API that the TF server offers is limited, it is primarily template search that is offered.\nSee \nData service API\n below.\nSee the code in\n\ntf.server.service\n\nto get started.\nStart\n\u00b6\n\n\nRun\nYou can run the TF data server as follows:\n1\npython3 -m tf.server.service ddd\n\n\n\n\n\nwhere \nddd\n is one of the \nsupported apps\nExample\nSee the\n\nstart-up script\n\nof the text-fabric browser.\nConnect\n\u00b6\n\n\nConnect\nThe TF data service can be connected by an other Python program as follows:\n1\n2\n3\nfrom\n \ntf.server.service\n \nimport\n \nmakeTfConnection\n\n\nTF\n \n=\n \nmakeTfConnection\n(\nhost\n,\n \nport\n)\n\n\napi\n \n=\n \nTF\n.\nconnect\n()\n\n\n\n\n\n\nAfter this, \napi\n can be used to obtain information from the Text-Fabric data service.\nExample\nSee the\n\nweb server\n\nof the text-fabric browser.\nData service API\n\u00b6\n\n\nAbout\nThe API of the Text-Fabric data service is created\nby the function \nmakeTfServer\n in the \n\ndata\n\nmodule of the server subpackage.\nIt returns a class \nTfService\n with a number\nof exposed methods that can be called by other programs.\nFor the machinery of interprocess communication we rely on the\n\nrpyc\n module.\nSee especially the docs on\n\nservices\n.\nShadow objects\nThe way rpyc works in the case of data transmission has a pitfall.\nWhen server returns a Python object to the client, it\ndoes not return the object itself, but only a shadow object\nso called \nnetref\n objects. This strategy is called\n\nboxing\n.\nTo the client the shadow object looks like the real thing,\nbut when the client needs to access members, they will be fetched\non the fly.\nThis is a performance problem when the server sends a big list or dict,\nand the client iterates over all its items. Each item will be fetched in\na separate interprocess call, which causes an enormous overhead.\nBoxing only happens for mutable objects. And here lies the work-around:\nThe server must send big chunks of data as immutable objects,\nsuch as tuples. They are sent within a single interprocess call,\nand fly swiftly through the connecting pipe. \nheader()\nCalls the \nheader()\n method of the extraApi,\nwhich fetches all the stuff to create a header\non the page with links to data and documentation of the\ndata source.\nprovenance()\nCalls the \nprovenance()\n method of the extraApi,\nwhich fetches provenance metadata to be shown\non exported pages.\ncss()\nCalls the \nloadCSS()\n method of the extraApi,\nwhich delivers the CSS code to be inserted\non the browser page.\ncondenseTypes()\nFetches several things from the extraApi and the \ngeneric TF api:\ncondenseType\n: the default node type that acts\n  as a container for representing query results;\n  for Bhsa it is \nverse\n, for Cunei it is \ntablet\n;\nexampleSection\n: an example for the help text\n  for this data source;\nlevels\n: information about the node types in this\n  data source.\nsearch()\nThe work horse of this API.\nExecutes a TF search template, retrieves\nformatted results, retrieves \nformatted results for additional nodes and\nsections.\nParameters:\nquery\nSearch template to be executed.\nTypically coming\nfrom the \nsearch pad\n in the browser.\ntuples\nTuples of nodes to look up features for.\nTypically coming\nfrom the \nnode pad\n in the browser.\nsections\nSections to look up features for.\nTypically coming\nfrom the \nsection pad\n in the browser.\nFor the Bhsa these are \nverse\n references,\nfor Cunei these are tablets by \nP-number\n.\ncondensed\nWhether or not the results should be \ncondensed\n.\nNormally, results come as tuples of nodes, and each\ntuple is shown in a corresponding table row in\nplain or pretty display.\nBut you can also \ncondense\n results in container\nobjects. All tuples will be inspected, and the nodes\nof each tuple will be gathered in containers,\nand these containers will be displayed in table rows.\nWhat is lost is the notion of an individual result,\nand what is gained is a better overview of where\nthe parts of the results are.\ncondenseType\nWhen condensing results, you can choose the node type\nthat acts as container.\nNodes get suppressed\nNodes in result tuples that have a type\nthat is bigger than the condenseType, will\nbe skipped.\nE.g. if you have chapter nodes in your results,\nbut you condense to verses, the chapter nodes will \nnot show up.\nBut if you condense to books, they will show up.\nbatch\nThe number of table rows to show on one page\nin the browser.\nposition=1\nThe position that is central in the browser.\nThe navigation links take this position\nas the focus point, and enable the user\nto navigate to neighbouring results, in ever bigger\nstrides.\nopened=set()\nWhich results have been expanded and need extended results.\nNormally, only the information to provide a \nplain\n\nrepresentation of a result is being fetched,\nbut for the opened ones information is gathered for\npretty displays.\nwithNodes=False\nWhether to include the node numbers into the formatted\nresults.\nlinked=1\nWhich column in the results should be hyperlinked to\nonline representations closest to the objects\nin that column.\nCounting columns starts at 1.\noptions\nAdditional keyword arguments are passed\nas options to the underlying API.\nFor example, the Cunei API accepts \nlinenumbers\n\nand \nlineart\n, which will ask to include line numbers\nand lineart in the formatted results.\ncsvs()\nThis is an other workhorse.\nIt also asks for the things \nsearch()\n is asking\nfor, but it does not want formatted results.\nIt will get tabular data of result\nnodes, one for the \nsections\n, one for the \nnode tuples\n,\nand one for the \nsearch results\n.\nFor every node that occurs in this tabular data,\nfeatures will be looked up.\nAll loaded features will be looked up for those nodes.\nThe result is a big table of nodes and feature values.\nThe parameters are \nquery\n, \ntuples\n, \nsections\n,\n\ncondensed\n, \ncondenseType\n and have the same meaning as\nin \nsearch()\n above.",
            "title": "Service"
        },
        {
            "location": "/Server/Service/#text-fabric-as-a-service",
            "text": "",
            "title": "Text-Fabric as a service"
        },
        {
            "location": "/Server/Service/#about",
            "text": "TF as a service Text-Fabric can be used as a service.\nThe full API of Text-Fabric needs a lot of memory, which makes it unusably for\nrapid successions of loading and unloading, like when used in a web server context. However, you can start TF as a server, after which many clients can connect to it,\nall looking at the same (read-only) data. The API that the TF server offers is limited, it is primarily template search that is offered.\nSee  Data service API  below. See the code in tf.server.service \nto get started.",
            "title": "About"
        },
        {
            "location": "/Server/Service/#start",
            "text": "Run You can run the TF data server as follows: 1 python3 -m tf.server.service ddd   where  ddd  is one of the  supported apps Example See the start-up script \nof the text-fabric browser.",
            "title": "Start"
        },
        {
            "location": "/Server/Service/#connect",
            "text": "Connect The TF data service can be connected by an other Python program as follows: 1\n2\n3 from   tf.server.service   import   makeTfConnection  TF   =   makeTfConnection ( host ,   port )  api   =   TF . connect ()    After this,  api  can be used to obtain information from the Text-Fabric data service. Example See the web server \nof the text-fabric browser.",
            "title": "Connect"
        },
        {
            "location": "/Server/Service/#data-service-api",
            "text": "About The API of the Text-Fabric data service is created\nby the function  makeTfServer  in the  data \nmodule of the server subpackage. It returns a class  TfService  with a number\nof exposed methods that can be called by other programs. For the machinery of interprocess communication we rely on the rpyc  module.\nSee especially the docs on services . Shadow objects The way rpyc works in the case of data transmission has a pitfall.\nWhen server returns a Python object to the client, it\ndoes not return the object itself, but only a shadow object\nso called  netref  objects. This strategy is called boxing .\nTo the client the shadow object looks like the real thing,\nbut when the client needs to access members, they will be fetched\non the fly. This is a performance problem when the server sends a big list or dict,\nand the client iterates over all its items. Each item will be fetched in\na separate interprocess call, which causes an enormous overhead. Boxing only happens for mutable objects. And here lies the work-around: The server must send big chunks of data as immutable objects,\nsuch as tuples. They are sent within a single interprocess call,\nand fly swiftly through the connecting pipe.  header() Calls the  header()  method of the extraApi,\nwhich fetches all the stuff to create a header\non the page with links to data and documentation of the\ndata source. provenance() Calls the  provenance()  method of the extraApi,\nwhich fetches provenance metadata to be shown\non exported pages. css() Calls the  loadCSS()  method of the extraApi,\nwhich delivers the CSS code to be inserted\non the browser page. condenseTypes() Fetches several things from the extraApi and the \ngeneric TF api: condenseType : the default node type that acts\n  as a container for representing query results;\n  for Bhsa it is  verse , for Cunei it is  tablet ; exampleSection : an example for the help text\n  for this data source; levels : information about the node types in this\n  data source. search() The work horse of this API.\nExecutes a TF search template, retrieves\nformatted results, retrieves \nformatted results for additional nodes and\nsections. Parameters: query Search template to be executed.\nTypically coming\nfrom the  search pad  in the browser. tuples Tuples of nodes to look up features for.\nTypically coming\nfrom the  node pad  in the browser. sections Sections to look up features for.\nTypically coming\nfrom the  section pad  in the browser. For the Bhsa these are  verse  references,\nfor Cunei these are tablets by  P-number . condensed Whether or not the results should be  condensed .\nNormally, results come as tuples of nodes, and each\ntuple is shown in a corresponding table row in\nplain or pretty display. But you can also  condense  results in container\nobjects. All tuples will be inspected, and the nodes\nof each tuple will be gathered in containers,\nand these containers will be displayed in table rows.\nWhat is lost is the notion of an individual result,\nand what is gained is a better overview of where\nthe parts of the results are. condenseType When condensing results, you can choose the node type\nthat acts as container. Nodes get suppressed Nodes in result tuples that have a type\nthat is bigger than the condenseType, will\nbe skipped.\nE.g. if you have chapter nodes in your results,\nbut you condense to verses, the chapter nodes will \nnot show up.\nBut if you condense to books, they will show up. batch The number of table rows to show on one page\nin the browser. position=1 The position that is central in the browser.\nThe navigation links take this position\nas the focus point, and enable the user\nto navigate to neighbouring results, in ever bigger\nstrides. opened=set() Which results have been expanded and need extended results.\nNormally, only the information to provide a  plain \nrepresentation of a result is being fetched,\nbut for the opened ones information is gathered for\npretty displays. withNodes=False Whether to include the node numbers into the formatted\nresults. linked=1 Which column in the results should be hyperlinked to\nonline representations closest to the objects\nin that column. Counting columns starts at 1. options Additional keyword arguments are passed\nas options to the underlying API. For example, the Cunei API accepts  linenumbers \nand  lineart , which will ask to include line numbers\nand lineart in the formatted results. csvs() This is an other workhorse.\nIt also asks for the things  search()  is asking\nfor, but it does not want formatted results.\nIt will get tabular data of result\nnodes, one for the  sections , one for the  node tuples ,\nand one for the  search results . For every node that occurs in this tabular data,\nfeatures will be looked up.\nAll loaded features will be looked up for those nodes.\nThe result is a big table of nodes and feature values. The parameters are  query ,  tuples ,  sections , condensed ,  condenseType  and have the same meaning as\nin  search()  above.",
            "title": "Data service API"
        },
        {
            "location": "/Server/Web/",
            "text": "Web site\n\u00b6\n\n\nAbout\n\u00b6\n\n\nLocal web interface\nTF contains a local web interface\nin which you can enter a search template and view the results.\nThis is realized by a web app based on \n\nbottle\n.\nThis web app connects to the \nTF data service\n\nand merges the retrieved data into a set of \n\ntemplates\n.\nSee the code in\n\ntf.server.web\n.\nStart up\n\u00b6\n\n\nStart up\nData server, webserver and browser page are started\nup by means of a script called \ntext-fabric\n, which will be installed in an executable\ndirectory by the \npip\n installer.\nWhat the script does is the same as:\n1\npython3 -m tf.server.start\n\n\n\n\n\nProcess management\nDuring start up the following happens:\nKill previous processes\nThe system is searched for non-terminated incarnations of the processes\nit wants to start up.\nIf they are encountered, they will be killed, so that they cannot prevent\na successful start up.\nTF data server\nA TF data service process is started.\nThis process loads the bulk of the TF data, so it can take a while.\nWhen it has loaded the data, it sends out a message that loading is done,\nwhich is picked up by the script.\nTF web server\nA short while after receiving the \"data loading done\" message, the TF web server is started.\nDebug mode\nIf you have passed \n-d\n to the \ntext-fabric\n script, the \nbottle\n will be started\nin debug and reload mode.\nThat means that if you modify \nweb.py\n or a module it imports, the web server will\nreload itself automatically.\nWhen you refresh the browser you see the changes.\nIf you have changed templates, the css, or the javascript, you should do a \"refresh from origin\".\nLoad web page\nAfter a short while, the default web browser will be started with a url and port at which the\nwebserver will listen. You see your browser being started up and the TF page being loaded.\nWaiting\nThe script now waits till the web server is finished.\nYou finish it by pressing Ctrl-C, and if you have used the \n-d\n flag, you have to press it twice.\nTerminate the data server\nAt this point, the \ntext-fabric\n script will terminate the TF data service process.\nClean up\nNow all processes that have started up have been killed.\nIf something went wrong in this sequence, chances are that a process keeps running.\nIt will be terminated next time you call the \ntext-fabric\n.\nYou can kill too\nIf you run\n1\ntext-fabric -k\n\n\n\n\n\nall tf-browser-related processes will be killed.\n1\ntext-fabric -k ddd\n\n\n\n\n\nwill kill all such processes as far as they are for data source \nddd\n.\nRoutes\n\u00b6\n\n\nRoutes\nThere are 4 kinds of routes in the web app:\nurl pattern\neffect\n/server/static/...\nserves a static file from the server-wide \nstatic folder\n/data/static/...\nserves a static file from the app specific static folder\n/local/static/...\nserves a static file from a local directory specified by the app\nanything else\nsubmits the form with user data and return the processed request\nTemplates\n\u00b6\n\n\nTemplates\nThere are two templates in\n\nviews\n\n:\nindex\n: the normal template for returning responses\n  to user requests;\nexport\n: the template used for exporting results; it\n  has printer/PDF-friendly formatting: good page breaks.\n  Pretty displays always occur on a page by their own.\n  It has very few user interaction controls.\n  When saved as PDF from the browser, it is a neat record\n  of work done, with DOI links to the corpus and to Text-Fabric.\nCSS\n\u00b6\n\n\nCSS\nWe format the web pages with CSS, with extensive use\nof\n\nflexbox\n.\nThere are three sources of CSS formatting:\nthe CSS loaded from the app dependent extraApi, used\n  for pretty displays;\nmain.css\n: the formatting of the \n  \nindex\n web page with which the user interacts;\ninside the\n  \nexport\n\n  template, for formatting the exported page.\nJavascript\n\u00b6\n\n\nJavascript\nWe use a\n\nmodest amount of Javascript\n\non top of \n\nJQuery\n.\nFor collapsing and expanding elements we use the\n\ndetails\n\nelement. This is a convenient, Javascript-free way to manage\ncollapsing. Unfortunately it is not supported by the Microsoft\nbrowsers, not even Edge.\nOn Windows?\nWindows users should install Chrome of Firefox.\nSafari is fine.",
            "title": "Web"
        },
        {
            "location": "/Server/Web/#web-site",
            "text": "",
            "title": "Web site"
        },
        {
            "location": "/Server/Web/#about",
            "text": "Local web interface TF contains a local web interface\nin which you can enter a search template and view the results. This is realized by a web app based on  bottle . This web app connects to the  TF data service \nand merges the retrieved data into a set of  templates . See the code in tf.server.web .",
            "title": "About"
        },
        {
            "location": "/Server/Web/#start-up",
            "text": "Start up Data server, webserver and browser page are started\nup by means of a script called  text-fabric , which will be installed in an executable\ndirectory by the  pip  installer. What the script does is the same as: 1 python3 -m tf.server.start   Process management During start up the following happens: Kill previous processes The system is searched for non-terminated incarnations of the processes\nit wants to start up.\nIf they are encountered, they will be killed, so that they cannot prevent\na successful start up. TF data server A TF data service process is started.\nThis process loads the bulk of the TF data, so it can take a while.\nWhen it has loaded the data, it sends out a message that loading is done,\nwhich is picked up by the script. TF web server A short while after receiving the \"data loading done\" message, the TF web server is started. Debug mode If you have passed  -d  to the  text-fabric  script, the  bottle  will be started\nin debug and reload mode.\nThat means that if you modify  web.py  or a module it imports, the web server will\nreload itself automatically.\nWhen you refresh the browser you see the changes.\nIf you have changed templates, the css, or the javascript, you should do a \"refresh from origin\". Load web page After a short while, the default web browser will be started with a url and port at which the\nwebserver will listen. You see your browser being started up and the TF page being loaded. Waiting The script now waits till the web server is finished.\nYou finish it by pressing Ctrl-C, and if you have used the  -d  flag, you have to press it twice. Terminate the data server At this point, the  text-fabric  script will terminate the TF data service process. Clean up Now all processes that have started up have been killed. If something went wrong in this sequence, chances are that a process keeps running.\nIt will be terminated next time you call the  text-fabric . You can kill too If you run 1 text-fabric -k   all tf-browser-related processes will be killed. 1 text-fabric -k ddd   will kill all such processes as far as they are for data source  ddd .",
            "title": "Start up"
        },
        {
            "location": "/Server/Web/#routes",
            "text": "Routes There are 4 kinds of routes in the web app: url pattern effect /server/static/... serves a static file from the server-wide  static folder /data/static/... serves a static file from the app specific static folder /local/static/... serves a static file from a local directory specified by the app anything else submits the form with user data and return the processed request",
            "title": "Routes"
        },
        {
            "location": "/Server/Web/#templates",
            "text": "Templates There are two templates in views \n: index : the normal template for returning responses\n  to user requests; export : the template used for exporting results; it\n  has printer/PDF-friendly formatting: good page breaks.\n  Pretty displays always occur on a page by their own.\n  It has very few user interaction controls.\n  When saved as PDF from the browser, it is a neat record\n  of work done, with DOI links to the corpus and to Text-Fabric.",
            "title": "Templates"
        },
        {
            "location": "/Server/Web/#css",
            "text": "CSS We format the web pages with CSS, with extensive use\nof flexbox . There are three sources of CSS formatting: the CSS loaded from the app dependent extraApi, used\n  for pretty displays; main.css : the formatting of the \n   index  web page with which the user interacts; inside the\n   export \n  template, for formatting the exported page.",
            "title": "CSS"
        },
        {
            "location": "/Server/Web/#javascript",
            "text": "Javascript We use a modest amount of Javascript \non top of  JQuery . For collapsing and expanding elements we use the details \nelement. This is a convenient, Javascript-free way to manage\ncollapsing. Unfortunately it is not supported by the Microsoft\nbrowsers, not even Edge. On Windows? Windows users should install Chrome of Firefox. Safari is fine.",
            "title": "Javascript"
        },
        {
            "location": "/Server/Common/",
            "text": "Common Server Related Functions\n\u00b6\n\n\nAbout\n\u00b6\n\n\nAbout\nHere are functions that are being used by various parts of the\nTF browser infrastructure, such as \nservice.py\ndata.py\nweb.py\nstart.py\nArgument parsing\n\u00b6\n\n\nApologies\nReally, we should replace the whole adhoc argument parsing by a decent use\nof the Python module\n\nargparse\n. \ngetDebug()\nChecks whether one of the arguments with which the script is called is a \n-d\n.\ngetParam(interactive=False)\nChecks whether a \ndataSource\n parameter has been passed on the command line.\nIf so, it checks whether it specifies an existing app.\nIf no \ndataSource\n has been passed, and \ninteractive\n is true,\npresents the user with a list of valid choices and asks for input.\nLocating the app\n\u00b6\n\n\nThe problem\nThe data source specific apps are bundled inside the TF package.\nThe webserver of the TF browser needs the files in those apps,\nnot as Python modules, but just as files on disk.\nSo we have to tell the webserver where they are, and we really do not know that\nin advance, because it is dependent on how the text-fabric package has been\ninstalled by \npip3\n on your machine.\nYet we have found a way through the labyrinth!\ngetConfig(dataSource)\nRetrieves the \nconfig.py\n from the specified \ndataSource\n by\ndynamically importing it as a module from one of the\n\n*-app\n packages in\n\ntf.extra\nSee also \nApp structure\ngetAppdir(myDir, dataSource)\nThe code in\n\nweb.py\n\nwill pass its file location as \nmyDir\n.\nForm there this function computes the locstion of the file in which\nthe webapp of the \ndataSource\n resides: the location of the\n\ndataSource\n \n-app\n package in\n\ntf.extra\n.\nSee also \nApp structure\nGetting and setting form values\n\u00b6\n\n\nRequest and response\nThe TF browser user interacts with the web app by clicking and typing,\nas a result of which a HTML form gets filled in.\nThis form as regularly submitted to the server with a request\nfor a new incarnation of the page: a response.\nThe values that come with a request, must be peeled out of the form,\nand stored as logical values.\nMost of the data has a known function to the webserver,\nbut there is also a list of webapp dependent options.\nThe following functions deal with option values.\ngetValues(options, form)\nGiven a tuple of option specifications and form data from a web request,\nreturns a dictionary of filled in values for those options.\nThe options are specified in the \nconfig.py\n of an app.\nAn option specification is a tuple of the following bits of information:\nname of the input element in the HTML form\ntype of input (e.g. checkbox)\nvalue of html id attribute of the input element\nlabel for the input element\nCunei options\nThe options for the Cunei app are:\n1\n2\n3\n4\noptions\n \n=\n \n(\n\n    \n(\n'lineart'\n,\n \n'checkbox'\n,\n \n'linea'\n,\n \n'show lineart'\n),\n\n    \n(\n'lineNumbers'\n,\n \n'checkbox'\n,\n \n'linen'\n,\n \n'show line numbers'\n),\n\n\n)\n\n\n\n\n\n\nThis function isolates the option values from the rest of the form values,\nso that it can be passed as a whole (\n**values\n) to the app specific API.\nsetValues(options, source, form)\nFills in a \nform\n dictionary based on values in a \nsource\n dictionary,\nbut only insofar the keys to be filled out occur in the \noptions\n specs,\nand with a cast of checkbox values to booleans. \nThis function is used right after reading the form off a request.\nRaw form data is turned into logical data for further processing by the web server.\nHTML formatting\n\u00b6\n\n\nHTML generation\nHere we generate the HTML for bigger chunks on the page.\npageLinks(nResults, position, spread=10)\nProvide navigation links for results sets, big or small.\nIt creates links around \nposition\n in a set of \nnResults\n.\nThe spread indicates how many links before and after \nposition\n are generated\nin each column.\nThere will be multiple columns. The right most column contains links\nto results \nposition - spread\n to \nposition + spread\n.\nLeft of that there is a column for results \nposition - spread*spread\n\nto \nposition + spread*spread\n, stepping by \nspread\n.\nAnd so on, until the stepping factor becomes bigger than the result set.\nshapeMessages\nWraps error messages into HTML. The messages come from the TF API,\nthrough the TF data server, in response to wrong search templates\nand other mistaken user input.\nshapeOptions\nWraps the options, specified by the option specification in \nconfig.py\n\ninto HTML.\nSee also \nApp structure\nshapeCondense\nProvides a radio-buttoned chooser for the\n\ncondense types\n.",
            "title": "Common"
        },
        {
            "location": "/Server/Common/#common-server-related-functions",
            "text": "",
            "title": "Common Server Related Functions"
        },
        {
            "location": "/Server/Common/#about",
            "text": "About Here are functions that are being used by various parts of the\nTF browser infrastructure, such as  service.py data.py web.py start.py",
            "title": "About"
        },
        {
            "location": "/Server/Common/#argument-parsing",
            "text": "Apologies Really, we should replace the whole adhoc argument parsing by a decent use\nof the Python module argparse .  getDebug() Checks whether one of the arguments with which the script is called is a  -d . getParam(interactive=False) Checks whether a  dataSource  parameter has been passed on the command line.\nIf so, it checks whether it specifies an existing app.\nIf no  dataSource  has been passed, and  interactive  is true,\npresents the user with a list of valid choices and asks for input.",
            "title": "Argument parsing"
        },
        {
            "location": "/Server/Common/#locating-the-app",
            "text": "The problem The data source specific apps are bundled inside the TF package.\nThe webserver of the TF browser needs the files in those apps,\nnot as Python modules, but just as files on disk.\nSo we have to tell the webserver where they are, and we really do not know that\nin advance, because it is dependent on how the text-fabric package has been\ninstalled by  pip3  on your machine. Yet we have found a way through the labyrinth! getConfig(dataSource) Retrieves the  config.py  from the specified  dataSource  by\ndynamically importing it as a module from one of the *-app  packages in tf.extra See also  App structure getAppdir(myDir, dataSource) The code in web.py \nwill pass its file location as  myDir .\nForm there this function computes the locstion of the file in which\nthe webapp of the  dataSource  resides: the location of the dataSource   -app  package in tf.extra . See also  App structure",
            "title": "Locating the app"
        },
        {
            "location": "/Server/Common/#getting-and-setting-form-values",
            "text": "Request and response The TF browser user interacts with the web app by clicking and typing,\nas a result of which a HTML form gets filled in.\nThis form as regularly submitted to the server with a request\nfor a new incarnation of the page: a response. The values that come with a request, must be peeled out of the form,\nand stored as logical values. Most of the data has a known function to the webserver,\nbut there is also a list of webapp dependent options. The following functions deal with option values. getValues(options, form) Given a tuple of option specifications and form data from a web request,\nreturns a dictionary of filled in values for those options. The options are specified in the  config.py  of an app. An option specification is a tuple of the following bits of information: name of the input element in the HTML form type of input (e.g. checkbox) value of html id attribute of the input element label for the input element Cunei options The options for the Cunei app are: 1\n2\n3\n4 options   =   ( \n     ( 'lineart' ,   'checkbox' ,   'linea' ,   'show lineart' ), \n     ( 'lineNumbers' ,   'checkbox' ,   'linen' ,   'show line numbers' ),  )    This function isolates the option values from the rest of the form values,\nso that it can be passed as a whole ( **values ) to the app specific API. setValues(options, source, form) Fills in a  form  dictionary based on values in a  source  dictionary,\nbut only insofar the keys to be filled out occur in the  options  specs,\nand with a cast of checkbox values to booleans.  This function is used right after reading the form off a request.\nRaw form data is turned into logical data for further processing by the web server.",
            "title": "Getting and setting form values"
        },
        {
            "location": "/Server/Common/#html-formatting",
            "text": "HTML generation Here we generate the HTML for bigger chunks on the page. pageLinks(nResults, position, spread=10) Provide navigation links for results sets, big or small. It creates links around  position  in a set of  nResults .\nThe spread indicates how many links before and after  position  are generated\nin each column. There will be multiple columns. The right most column contains links\nto results  position - spread  to  position + spread . Left of that there is a column for results  position - spread*spread \nto  position + spread*spread , stepping by  spread . And so on, until the stepping factor becomes bigger than the result set. shapeMessages Wraps error messages into HTML. The messages come from the TF API,\nthrough the TF data server, in response to wrong search templates\nand other mistaken user input. shapeOptions Wraps the options, specified by the option specification in  config.py \ninto HTML.\nSee also  App structure shapeCondense Provides a radio-buttoned chooser for the condense types .",
            "title": "HTML formatting"
        },
        {
            "location": "/Model/Data-Model/",
            "text": "Text-Fabric Data Model\n\u00b6\n\n\n\n\nEverything about us, everything around us, everything we know and can know of\nis composed ultimately of patterns of nothing;\nthat\u2019s the bottom line, the final truth.\nSo where we find we have any control over those patterns,\nwhy not make the most elegant ones, the most enjoyable and good ones,\nin our own terms?\"\n\n\n-- \nIain M. Banks\n.\n\"\nConsider Phlebas\n:\nA Culture Novel (Culture series)\"\n\n\n\n\nAt a glance\n\u00b6\n\n\nTake a text, put a grid around the words, and then leave out the words. What is\nleft, are the textual positions, or \nslots\n.\n\n\n\n\nPieces of text correspond to phrases, clauses, sentences, verses, chapters,\nbooks. Draw circles around those pieces, and then leave out their contents. What\nis left, are the textual objects, or \nnodes\n.\n\n\n\n\nNodes can be connected to other nodes by \nedges\n. A basic function of edges is\nto indicate \ncontainment\n: \nthis\n node corresponds to a set of slots that is\ncontained in the slots of \nthat\n node. But edges can also denote more abstract,\nlinguistic relations between nodes.\n\n\nNodes have types. Types are just a label that we use to make distinctions\nbetween word nodes, phrase nodes, ..., book nodes. The type assignment is an\nexample of a \nfeature\n of nodes: a mapping that assigns a piece of information\nto each node. This type assignment has a name: \notype\n, and every Text-Fabric\ndataset has such a feature.\n\n\nNodes may be linked to textual positions or \nslots\n. Some nodes are linked to a\nsingle slot, others to a set of slots, and yet others to no slots at all.\n\n\nNodes of the first kind are identified with their slots, they have the same\nnumber as slot as they have as node.\n\n\nNodes of the second kind have an edge to every slot (which is also a node) that\nthey are linked to. The collection of these edges from nodes of the second kind\nto nodes of the first kind, is an example of a \nfeature\n of edges: a mapping\nthat assigns to each pair of nodes a boolean value: is this pair a link or not?\nThis particular edge feature is called \noslots\n, and every Text-Fabric dataset\nhas such a feature.\n\n\nNodes of the third kind represent information that is not part of the main body\nof text. We could represent the lexicon in this way. However, it is also\npossible to consider \nlexeme\n as a node type, where every lexeme node is linked\nto the set of slots that have an occurrence of that lexeme.\n\n\nFabric metaphor\n\u00b6\n\n\n\n\nAD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen\n\n\nBefore we go on, we invite you to look at a few basic terms in the craft of\n\nweaving\n.\n\n\nA weaver sets up a set of fixed, parallel threads, the \nwarp\n. He then picks a\nthread, usually a colourful one, and sends it in a perpendicular way through the\nwarp. This thread is called the \nweft\n.\n\n\n\n\nThe instrument that carries the wefts through the warp is called the \nloom\n. The\nweaver continues operating the loom, back and forth, occasionally selecting new\nwefts, until he has completed a rectangular piece of fabric, the \nweave\n.\n\n\n\n\nsource\n\n\nNow Text-Fabric, the tool, can be seen as the loom that sends features (the\nwefts) through a warp (the system of nodes and edges).\n\n\nThe features \notype\n and \noslots\n are the ones that set up the system of\nnodes and edges. That's why we call them \nwarp\n features. Every Text-Fabric\ndataset contains these two warp features. (Later on we'll see a third member of\nthe warp, \notext\n). They provide the structure of a text and its annotations,\nwithout any content. Even the text itself is left out!\n\n\n\n\nAll other information is added to the warp as \nfeatures\n (the wefts): node\nfeatures and edge features. A feature is a special aspect of the textual\ninformation, isolated as a kind of module. It is a collection of values which\ncan be woven as a weft into the warp.\n\n\nOne of the more basic things to add to the warp is the text itself. Ancient\ntexts often have several text representations, like original (Unicode)\ncharacters or transliterated characters, with or without the complete set of\ndiacritical marks. In Text-Fabric we do not have to choose between them: we can\npackage each representation into a feature, and add it to the fabric.\n\n\nA Text-Fabric data set is a warp (\notype\n, \noslots\n) plus a collection of wefts\n(all other features). We may add other features to the same warp. Data sets with\nonly wefts, but no warps, are called modules. When you use modules with a\ndataset, the modules must have been constructed around the warp of the dataset.\n\n\nWhenever you use Text-Fabric to generate new data, you are weaving a weave. The\nresulting dataset is a tight fabric of individual features (wefts), whose values\nare taken for a set of nodes (warp).\n\n\n\n\nSome features deserve a privileged place. After all, we are dealing with \ntext\n,\nso we need a bit of information about which features carry textual\nrepresentations and sectioning information (e.g. books, chapters, verses).\n\n\nThis information is not hard-wired into Text-Fabric, but it is given in the form\nof a \nconfig\n feature. A config feature has no data, only metadata. Every\nText-Fabric dataset may contain a config feature called \notext\n, which\nspecifies which node types and features correspond to sectional units such as\nbooks, chapters, and verses. It also contains templates for generating text\nrepresentations for the slots.\n\n\nThe \notext\n feature is optional, because not all Text-Fabric datasets are\nexpected to have extensive sectioning and text representation definitions.\nEspecially when you are in the process of converting a data source (such as a\ntreebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load\nthe data without bothering about these matters.\n\n\n\n\nModel\n\u00b6\n\n\nWe summarize in brief statements our data model, including ways to represent the\ndata, serialize it, and compute with it.\n\n\nText objects:\n\n\n\n\noccupy arbitrary compositions of slots;\n\n\ncarry a \ntype\n (just a label); all \nslots\n carry the same type, the \nslot\n    type\n; e.g. \nword\n or \ncharacter\n;\n\n\ncan be annotated by \nfeatures\n (key-value pairs)\n\n\ncan be connected by directed, labelled links to other text objects.\n\n\n\n\nThe model knows which feature assigned values to nodes and edges. If two\ndifferent features assign a value to an edge or node, both values can be read\noff later; one through the one feature, and one through the other.\n\n\nThe data in Text-Fabric is organized as an annotated directed graph with a bit\nof additional structure. The correspondence is\n\n\n\n\ntext positions => the first so many slot numbers\n\n\ntext objects => nodes\n\n\nlinks between text objects => edges\n\n\ninformation associated with text objects => node features\n\n\nlabels on links between text objects => edge features\n\n\nNB:\n since every link is specified by an edge feature, every link is\n    implicitly labelled by the name of the edge feature. If the edge feature\n    assigns values to edges, those values come on top of the implicit label.\n\n\ntypes of text objects => a special node feature called \notype\n (read: object\n    type)\n\n\nextent of text objects in terms of slots => a special edge feature called\n    \noslots\n (read: object slots)\n\n\noptional specifications for sectioning and representing text => a special\n    config feature called \notext\n (read: object text)\n\n\n\n\nTogether, the \notype\n, \noslots\n, and the optional \notext\n features are called\nthe \nwarp\n of a Text-Fabric dataset.\n\n\nRepresentation\n\u00b6\n\n\nWe represent the elements that make up such a graph as follows:\n\n\n\n\nnodes are integers, starting with 1, without gaps;\n\n\nthe first \nmaxSlot\n nodes correspond exactly with the slots, in the same\n    order, where \nmaxSlot\n is the number of slots;\n\n\nnodes greater than \nmaxSlot\n correspond to general text objects;\n\n\nnode features are mappings of integers to values;\n\n\nedge features are mappings of pairs of integers to values; i.e. edges are\n    ordered pairs of integers; labelled edges are ordered tuples of two nodes and\n    a value;\n\n\nvalues (for nodes and for edges) are strings (Unicode, utf8) or numbers;\n\n\nthe \notype\n feature maps\n\n\nthe integers \n1..maxSlot\n (including) to the \nslot type\n, where \nmaxSlot\n is\n    the last \nslot\n,\n\n\nthe integers \nmaxSlot+1..maxNode\n (including) to the relevant text object\n    types;\n\n\n\n\n\n\nthe \noslots\n feature is an valueless edge feature, mapping all non-slot nodes\n    to sets of slots; so there is an \noslots\n edge between each non-slot node and\n    each slot \ncontained\n by that node;\n\n\na Text-Fabric dataset is a collection of node features and edge features\n    containing at least the \nwarp\n features \notype\n, \noslots\n, and, optionally\n    \notext\n.\n\n\n\n\nMore about the warp\n\u00b6\n\n\nThe warp/weft distinction is a handy way of separating textual organisation from\ntextual content. Let us discuss the warp features a bit more.\n\n\notype: node feature\n\u00b6\n\n\nMaps each node to a label. The label typically is the kind of object that the\nnode represents, with values such as\n\n\n1\n2\n3\n4\n5\n6\n7\nbook\nchapter\nverse\nsentence\nclause\nphrase\nword\n\n\n\n\n\n\nThere is a special kind of object type, the \nslot type\n, which is the atomic\nbuilding block of the text objects. It is assumed that the complete text is\nbuilt from a sequence of \nslots\n, from slot \n1\n till slot \nmaxSlot\n (including),\nwhere the slots are numbered consecutively. There must be at least one slot.\n\n\nAll other objects are defined with respect to the \nslots\n they contain.\n\n\nThe \nslot type\n does not have to be called \nslot\n literally. If your basic\nentity is \nword\n, you may also call it \nword\n. Slots are then filled with\n\nwords\n. You can model text on the basis of another atomic entity, such as\n\ncharacter\n. In that case, slots are filled with \ncharacters\n. Other choices may\nbe equally viable.\n\n\nThe only requirement is that all slots correspond exactly with the first so many\nnodes.\n\n\nThe \notype\n feature will map node \n1\n to a node type, and this node type is the\ntype of all subsequent slots and also of the things that fill the slots.\n\n\nNote also the sectional features \nbook chapter verse\n here. They will play a\nrole in the third, optional, warp feature \notext\n.\n\n\noslots: edge feature\n\u00b6\n\n\nDefines which slots are occupied by which objects. It does so by specifying\nedges from nodes to the slots they contain.\n\n\nFrom the information in \noslots\n we can compute the embedding relationships\nbetween all nodes.\n\n\nIt gives also rise to a canonical \nordering\n of nodes.\n\n\notext: config feature (optional)\n\u00b6\n\n\nDeclares which node types correspond to the first three levels of sectioning,\nusually \nbook\n, \nchapter\n, \nverse\n. Also declares the corresponding features to\nget the names or numbers of the sections in those levels. Text-Fabric uses this\ninformation to construct the so-called Text-API, with functions to\n\n\n\n\nconvert nodes to section labels and vice versa,\n\n\nrepresent section names in multiple languages,\n\n\nprint formatted text for node sets.\n\n\n\n\nIf information about sections or text representations are missing, Text-Fabric\nwill build a reduced Text-API for you, but it will continue.\n\n\nSerializing and precomputing\n\u00b6\n\n\nWhen Text-Fabric works with a dataset, it reads feature data files, and offers\nan API to process that data. The main task of Text-Fabric is to make processing\nefficient, so that it can be done in interactive ways, such as in a Jupyter\nnotebook. To that end, Text-Fabric\n\n\n\n\noptimizes feature data after reading it for the first time and stores it in\n    binary form for fast loading in next invocations;\n\n\nprecomputes additional data from the warp features in order to provide\n    convenient API functions.\n\n\n\n\nIn Text-Fabric, we have various ways of encoding this model:\n\n\n\n\nas plain text in \n.tf\n feature files,\n\n\nas Python data structures in memory,\n\n\nas compressed serializations of the same data structures inside \n.tfx\n files\n    in \n.tf\n cache directories.",
            "title": "Data"
        },
        {
            "location": "/Model/Data-Model/#text-fabric-data-model",
            "text": "Everything about us, everything around us, everything we know and can know of\nis composed ultimately of patterns of nothing;\nthat\u2019s the bottom line, the final truth.\nSo where we find we have any control over those patterns,\nwhy not make the most elegant ones, the most enjoyable and good ones,\nin our own terms?\"  --  Iain M. Banks .\n\" Consider Phlebas :\nA Culture Novel (Culture series)\"",
            "title": "Text-Fabric Data Model"
        },
        {
            "location": "/Model/Data-Model/#at-a-glance",
            "text": "Take a text, put a grid around the words, and then leave out the words. What is\nleft, are the textual positions, or  slots .   Pieces of text correspond to phrases, clauses, sentences, verses, chapters,\nbooks. Draw circles around those pieces, and then leave out their contents. What\nis left, are the textual objects, or  nodes .   Nodes can be connected to other nodes by  edges . A basic function of edges is\nto indicate  containment :  this  node corresponds to a set of slots that is\ncontained in the slots of  that  node. But edges can also denote more abstract,\nlinguistic relations between nodes.  Nodes have types. Types are just a label that we use to make distinctions\nbetween word nodes, phrase nodes, ..., book nodes. The type assignment is an\nexample of a  feature  of nodes: a mapping that assigns a piece of information\nto each node. This type assignment has a name:  otype , and every Text-Fabric\ndataset has such a feature.  Nodes may be linked to textual positions or  slots . Some nodes are linked to a\nsingle slot, others to a set of slots, and yet others to no slots at all.  Nodes of the first kind are identified with their slots, they have the same\nnumber as slot as they have as node.  Nodes of the second kind have an edge to every slot (which is also a node) that\nthey are linked to. The collection of these edges from nodes of the second kind\nto nodes of the first kind, is an example of a  feature  of edges: a mapping\nthat assigns to each pair of nodes a boolean value: is this pair a link or not?\nThis particular edge feature is called  oslots , and every Text-Fabric dataset\nhas such a feature.  Nodes of the third kind represent information that is not part of the main body\nof text. We could represent the lexicon in this way. However, it is also\npossible to consider  lexeme  as a node type, where every lexeme node is linked\nto the set of slots that have an occurrence of that lexeme.",
            "title": "At a glance"
        },
        {
            "location": "/Model/Data-Model/#fabric-metaphor",
            "text": "AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen  Before we go on, we invite you to look at a few basic terms in the craft of weaving .  A weaver sets up a set of fixed, parallel threads, the  warp . He then picks a\nthread, usually a colourful one, and sends it in a perpendicular way through the\nwarp. This thread is called the  weft .   The instrument that carries the wefts through the warp is called the  loom . The\nweaver continues operating the loom, back and forth, occasionally selecting new\nwefts, until he has completed a rectangular piece of fabric, the  weave .   source  Now Text-Fabric, the tool, can be seen as the loom that sends features (the\nwefts) through a warp (the system of nodes and edges).  The features  otype  and  oslots  are the ones that set up the system of\nnodes and edges. That's why we call them  warp  features. Every Text-Fabric\ndataset contains these two warp features. (Later on we'll see a third member of\nthe warp,  otext ). They provide the structure of a text and its annotations,\nwithout any content. Even the text itself is left out!   All other information is added to the warp as  features  (the wefts): node\nfeatures and edge features. A feature is a special aspect of the textual\ninformation, isolated as a kind of module. It is a collection of values which\ncan be woven as a weft into the warp.  One of the more basic things to add to the warp is the text itself. Ancient\ntexts often have several text representations, like original (Unicode)\ncharacters or transliterated characters, with or without the complete set of\ndiacritical marks. In Text-Fabric we do not have to choose between them: we can\npackage each representation into a feature, and add it to the fabric.  A Text-Fabric data set is a warp ( otype ,  oslots ) plus a collection of wefts\n(all other features). We may add other features to the same warp. Data sets with\nonly wefts, but no warps, are called modules. When you use modules with a\ndataset, the modules must have been constructed around the warp of the dataset.  Whenever you use Text-Fabric to generate new data, you are weaving a weave. The\nresulting dataset is a tight fabric of individual features (wefts), whose values\nare taken for a set of nodes (warp).   Some features deserve a privileged place. After all, we are dealing with  text ,\nso we need a bit of information about which features carry textual\nrepresentations and sectioning information (e.g. books, chapters, verses).  This information is not hard-wired into Text-Fabric, but it is given in the form\nof a  config  feature. A config feature has no data, only metadata. Every\nText-Fabric dataset may contain a config feature called  otext , which\nspecifies which node types and features correspond to sectional units such as\nbooks, chapters, and verses. It also contains templates for generating text\nrepresentations for the slots.  The  otext  feature is optional, because not all Text-Fabric datasets are\nexpected to have extensive sectioning and text representation definitions.\nEspecially when you are in the process of converting a data source (such as a\ntreebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load\nthe data without bothering about these matters.",
            "title": "Fabric metaphor"
        },
        {
            "location": "/Model/Data-Model/#model",
            "text": "We summarize in brief statements our data model, including ways to represent the\ndata, serialize it, and compute with it.  Text objects:   occupy arbitrary compositions of slots;  carry a  type  (just a label); all  slots  carry the same type, the  slot\n    type ; e.g.  word  or  character ;  can be annotated by  features  (key-value pairs)  can be connected by directed, labelled links to other text objects.   The model knows which feature assigned values to nodes and edges. If two\ndifferent features assign a value to an edge or node, both values can be read\noff later; one through the one feature, and one through the other.  The data in Text-Fabric is organized as an annotated directed graph with a bit\nof additional structure. The correspondence is   text positions => the first so many slot numbers  text objects => nodes  links between text objects => edges  information associated with text objects => node features  labels on links between text objects => edge features  NB:  since every link is specified by an edge feature, every link is\n    implicitly labelled by the name of the edge feature. If the edge feature\n    assigns values to edges, those values come on top of the implicit label.  types of text objects => a special node feature called  otype  (read: object\n    type)  extent of text objects in terms of slots => a special edge feature called\n     oslots  (read: object slots)  optional specifications for sectioning and representing text => a special\n    config feature called  otext  (read: object text)   Together, the  otype ,  oslots , and the optional  otext  features are called\nthe  warp  of a Text-Fabric dataset.",
            "title": "Model"
        },
        {
            "location": "/Model/Data-Model/#representation",
            "text": "We represent the elements that make up such a graph as follows:   nodes are integers, starting with 1, without gaps;  the first  maxSlot  nodes correspond exactly with the slots, in the same\n    order, where  maxSlot  is the number of slots;  nodes greater than  maxSlot  correspond to general text objects;  node features are mappings of integers to values;  edge features are mappings of pairs of integers to values; i.e. edges are\n    ordered pairs of integers; labelled edges are ordered tuples of two nodes and\n    a value;  values (for nodes and for edges) are strings (Unicode, utf8) or numbers;  the  otype  feature maps  the integers  1..maxSlot  (including) to the  slot type , where  maxSlot  is\n    the last  slot ,  the integers  maxSlot+1..maxNode  (including) to the relevant text object\n    types;    the  oslots  feature is an valueless edge feature, mapping all non-slot nodes\n    to sets of slots; so there is an  oslots  edge between each non-slot node and\n    each slot  contained  by that node;  a Text-Fabric dataset is a collection of node features and edge features\n    containing at least the  warp  features  otype ,  oslots , and, optionally\n     otext .",
            "title": "Representation"
        },
        {
            "location": "/Model/Data-Model/#more-about-the-warp",
            "text": "The warp/weft distinction is a handy way of separating textual organisation from\ntextual content. Let us discuss the warp features a bit more.",
            "title": "More about the warp"
        },
        {
            "location": "/Model/Data-Model/#otype-node-feature",
            "text": "Maps each node to a label. The label typically is the kind of object that the\nnode represents, with values such as  1\n2\n3\n4\n5\n6\n7 book\nchapter\nverse\nsentence\nclause\nphrase\nword   There is a special kind of object type, the  slot type , which is the atomic\nbuilding block of the text objects. It is assumed that the complete text is\nbuilt from a sequence of  slots , from slot  1  till slot  maxSlot  (including),\nwhere the slots are numbered consecutively. There must be at least one slot.  All other objects are defined with respect to the  slots  they contain.  The  slot type  does not have to be called  slot  literally. If your basic\nentity is  word , you may also call it  word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with  characters . Other choices may\nbe equally viable.  The only requirement is that all slots correspond exactly with the first so many\nnodes.  The  otype  feature will map node  1  to a node type, and this node type is the\ntype of all subsequent slots and also of the things that fill the slots.  Note also the sectional features  book chapter verse  here. They will play a\nrole in the third, optional, warp feature  otext .",
            "title": "otype: node feature"
        },
        {
            "location": "/Model/Data-Model/#oslots-edge-feature",
            "text": "Defines which slots are occupied by which objects. It does so by specifying\nedges from nodes to the slots they contain.  From the information in  oslots  we can compute the embedding relationships\nbetween all nodes.  It gives also rise to a canonical  ordering  of nodes.",
            "title": "oslots: edge feature"
        },
        {
            "location": "/Model/Data-Model/#otext-config-feature-optional",
            "text": "Declares which node types correspond to the first three levels of sectioning,\nusually  book ,  chapter ,  verse . Also declares the corresponding features to\nget the names or numbers of the sections in those levels. Text-Fabric uses this\ninformation to construct the so-called Text-API, with functions to   convert nodes to section labels and vice versa,  represent section names in multiple languages,  print formatted text for node sets.   If information about sections or text representations are missing, Text-Fabric\nwill build a reduced Text-API for you, but it will continue.",
            "title": "otext: config feature (optional)"
        },
        {
            "location": "/Model/Data-Model/#serializing-and-precomputing",
            "text": "When Text-Fabric works with a dataset, it reads feature data files, and offers\nan API to process that data. The main task of Text-Fabric is to make processing\nefficient, so that it can be done in interactive ways, such as in a Jupyter\nnotebook. To that end, Text-Fabric   optimizes feature data after reading it for the first time and stores it in\n    binary form for fast loading in next invocations;  precomputes additional data from the warp features in order to provide\n    convenient API functions.   In Text-Fabric, we have various ways of encoding this model:   as plain text in  .tf  feature files,  as Python data structures in memory,  as compressed serializations of the same data structures inside  .tfx  files\n    in  .tf  cache directories.",
            "title": "Serializing and precomputing"
        },
        {
            "location": "/Model/File-formats/",
            "text": "Text-Fabric File Format\n\u00b6\n\n\nOverview\n\u00b6\n\n\nA \n.tf\n feature file starts with a \nheader\n, and is followed by the actual data.\nThe whole file is a plain text in UNICODE-utf8.\n\n\nHeader\n\u00b6\n\n\nA \n.tf\n feature file always starts with one or more metadata lines of the form\n\n\n1\n@key\n\n\n\n\n\n\nor\n\n\n1\n@key=value\n\n\n\n\n\n\nThe first line must be either\n\n\n1\n@node\n\n\n\n\n\n\nor\n\n\n1\n@edge\n\n\n\n\n\n\nor\n\n\n1\n@config\n\n\n\n\n\n\nThis tells Text-Fabric whether the data in the feature file is a \nnode\n feature\nor an \nedge\n feature. The value \n@config\n means that the file will be used as\nconfiguration info. It will only have metadata.\n\n\nThere \nmust\n also be a type declaration:\n\n\n1\n@valueType=type\n\n\n\n\n\n\nwhere type is \nstr\n or \nint\n. \n@valueType\n declares the type of the values in\nthis feature file. If it is anything other than \nstr\n (=\nstring\n), Text-Fabric\nwill convert it to that type when it reads the data from the file. Currently,\nthe only other supported type is \nint\n for integers.\n\n\nIn edge features, there \nmay\n also be a declaration\n\n\n1\n@edgeValues\n\n\n\n\n\n\nindicating that the edge feature carries values. The default is that an edge\ndoes not carry values.\n\n\nThe rest of the metadata is optional for now, but it is recommended to put a\ndate stamp in it like this\n\n\n1\n@dateCreated=2016-11-20T13:26:59Z\n\n\n\n\n\n\nThe time format should be \nISO 8601\n.\n\n\nData\n\u00b6\n\n\nAfter the metadata, there must be exactly one blank line, and every line after\nthat is data.\n\n\nData lines\n\u00b6\n\n\nThe form of a data line is\n\n\n1\nnode_spec value\n\n\n\n\n\n\nfor node features, and\n\n\n1\nnode_spec node_spec value\n\n\n\n\n\n\nfor edge features.\n\n\nThese fields are separated by single tabs.\n\n\nNB\n: This is the default format. Under \nOptimizations\n below we shall\ndescribe the bits that can be left out, which will lead to significant\nimprovement in space demands and processing speed.\n\n\nNode Specification\n\u00b6\n\n\nEvery line contains a feature value that pertains to all nodes defined by its\n\nnode_spec\n, or to all edges defined by its pair of \nnode_spec\ns.\n\n\nA node spec denotes a \nset\n of nodes.\n\n\nThe simplest form of a node spec is just a single integer. Examples:\n\n\n1\n2\n3\n3\n45\n425000\n\n\n\n\n\n\nRanges are also allowed. Examples\n\n\n1\n2\n3\n1-10\n5-13\n28-57045\n\n\n\n\n\n\nThe nodes denoted by a range are all numbers between the endpoints of the range\n(including at both sides). So\n\n\n1\n2-4\n\n\n\n\n\n\ndenotes the nodes \n2\n, \n3\n, and \n4\n.\n\n\nYou can also combine numbers and ranges arbitrarily by separating them with\ncommas. Examples\n\n\n1\n1-3,5-10,15,23-37\n\n\n\n\n\n\nSuch a specification denotes the union of what is denoted by each\ncomma-separated part.\n\n\nNB\n As node specs denote \nsets\n of nodes, the following node specs are in\nfact equivalent\n\n\n1\n2\n3\n1,1 and 1\n2-3 and 3,2\n1-5,2-7 and 1-7\n\n\n\n\n\n\nWe will be tolerant in that you may specify the end points of ranges in\narbitrary order:\n\n\n1\n1-3 is the same as 3-1\n\n\n\n\n\n\nEdges\n\u00b6\n\n\nAn edge is specified by an \nordered\n pair of nodes. The edge is \nfrom\n the first\nnode in the pair \nto\n the second one. An edge spec consists of two node specs.\nIt denotes all edges that are \nfrom\n a node denoted by the first node spec \nto\n\na node denoted by the second node spec. An edge might be labelled, in that case\nthe label of the edge is specified by the \nvalue\n after the two node specs.\n\n\nValue\n\u00b6\n\n\nThe value is arbitrary text. The type of the value must conform to the\n\n@valueType\n declaration in the feature file. If it is missing, it is assumed to\nbe \nstr\n, which is the type of Unicode-utf8 strings. If it is \nint\n, it should\nbe a valid representation of an integer number,\n\n\nThere are a few escapes:\n\n\n\n\n\\\\\n backslash\n\n\n\\t\n tab\n\n\n\\n\n newline These characters MUST always be escaped in a value string,\n    otherwise the line as a whole might be ambiguous.\n\n\n\n\nNB:\n There is no representation for the absence of a value. The empty string\nas value means that there is a value and it is the empty string. If you want to\ndescribe the fact that node \nn\n does not have a value for the feature in\nquestion, the node must be left out of the feature. In order words, there should\nbe no data line in the feature that targets this node.\n\n\nIf the declared value type (\n@valueType\n) of a feature is \nint\n, then its empty\nvalues will be taken as absence of values, though.\n\n\nConsistency requirements\n\u00b6\n\n\nThere are a few additional requirements on feature data, having to do with the\nfact that features annotate nodes or edges of a graph.\n\n\nSingle values\n\u00b6\n\n\nIt is assumed that a node feature assigns only one value to the same node. If\nthe data contains multiple assignments to a node, only the last assignment will\nbe honoured, the previous ones will be discarded.\n\n\nLikewise, it is assumed that an edge feature assigns only one value to the same\nedge. If the data contains multiple assignments to an edge, only the last\nassignment will be honoured.\n\n\nViolations maybe or may not be reported, and processing may continue without\nwarnings.",
            "title": "Format"
        },
        {
            "location": "/Model/File-formats/#text-fabric-file-format",
            "text": "",
            "title": "Text-Fabric File Format"
        },
        {
            "location": "/Model/File-formats/#overview",
            "text": "A  .tf  feature file starts with a  header , and is followed by the actual data.\nThe whole file is a plain text in UNICODE-utf8.",
            "title": "Overview"
        },
        {
            "location": "/Model/File-formats/#header",
            "text": "A  .tf  feature file always starts with one or more metadata lines of the form  1 @key   or  1 @key=value   The first line must be either  1 @node   or  1 @edge   or  1 @config   This tells Text-Fabric whether the data in the feature file is a  node  feature\nor an  edge  feature. The value  @config  means that the file will be used as\nconfiguration info. It will only have metadata.  There  must  also be a type declaration:  1 @valueType=type   where type is  str  or  int .  @valueType  declares the type of the values in\nthis feature file. If it is anything other than  str  (= string ), Text-Fabric\nwill convert it to that type when it reads the data from the file. Currently,\nthe only other supported type is  int  for integers.  In edge features, there  may  also be a declaration  1 @edgeValues   indicating that the edge feature carries values. The default is that an edge\ndoes not carry values.  The rest of the metadata is optional for now, but it is recommended to put a\ndate stamp in it like this  1 @dateCreated=2016-11-20T13:26:59Z   The time format should be  ISO 8601 .",
            "title": "Header"
        },
        {
            "location": "/Model/File-formats/#data",
            "text": "After the metadata, there must be exactly one blank line, and every line after\nthat is data.",
            "title": "Data"
        },
        {
            "location": "/Model/File-formats/#data-lines",
            "text": "The form of a data line is  1 node_spec value   for node features, and  1 node_spec node_spec value   for edge features.  These fields are separated by single tabs.  NB : This is the default format. Under  Optimizations  below we shall\ndescribe the bits that can be left out, which will lead to significant\nimprovement in space demands and processing speed.",
            "title": "Data lines"
        },
        {
            "location": "/Model/File-formats/#node-specification",
            "text": "Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of  node_spec s.  A node spec denotes a  set  of nodes.  The simplest form of a node spec is just a single integer. Examples:  1\n2\n3 3\n45\n425000   Ranges are also allowed. Examples  1\n2\n3 1-10\n5-13\n28-57045   The nodes denoted by a range are all numbers between the endpoints of the range\n(including at both sides). So  1 2-4   denotes the nodes  2 ,  3 , and  4 .  You can also combine numbers and ranges arbitrarily by separating them with\ncommas. Examples  1 1-3,5-10,15,23-37   Such a specification denotes the union of what is denoted by each\ncomma-separated part.  NB  As node specs denote  sets  of nodes, the following node specs are in\nfact equivalent  1\n2\n3 1,1 and 1\n2-3 and 3,2\n1-5,2-7 and 1-7   We will be tolerant in that you may specify the end points of ranges in\narbitrary order:  1 1-3 is the same as 3-1",
            "title": "Node Specification"
        },
        {
            "location": "/Model/File-formats/#edges",
            "text": "An edge is specified by an  ordered  pair of nodes. The edge is  from  the first\nnode in the pair  to  the second one. An edge spec consists of two node specs.\nIt denotes all edges that are  from  a node denoted by the first node spec  to \na node denoted by the second node spec. An edge might be labelled, in that case\nthe label of the edge is specified by the  value  after the two node specs.",
            "title": "Edges"
        },
        {
            "location": "/Model/File-formats/#value",
            "text": "The value is arbitrary text. The type of the value must conform to the @valueType  declaration in the feature file. If it is missing, it is assumed to\nbe  str , which is the type of Unicode-utf8 strings. If it is  int , it should\nbe a valid representation of an integer number,  There are a few escapes:   \\\\  backslash  \\t  tab  \\n  newline These characters MUST always be escaped in a value string,\n    otherwise the line as a whole might be ambiguous.   NB:  There is no representation for the absence of a value. The empty string\nas value means that there is a value and it is the empty string. If you want to\ndescribe the fact that node  n  does not have a value for the feature in\nquestion, the node must be left out of the feature. In order words, there should\nbe no data line in the feature that targets this node.  If the declared value type ( @valueType ) of a feature is  int , then its empty\nvalues will be taken as absence of values, though.",
            "title": "Value"
        },
        {
            "location": "/Model/File-formats/#consistency-requirements",
            "text": "There are a few additional requirements on feature data, having to do with the\nfact that features annotate nodes or edges of a graph.",
            "title": "Consistency requirements"
        },
        {
            "location": "/Model/File-formats/#single-values",
            "text": "It is assumed that a node feature assigns only one value to the same node. If\nthe data contains multiple assignments to a node, only the last assignment will\nbe honoured, the previous ones will be discarded.  Likewise, it is assumed that an edge feature assigns only one value to the same\nedge. If the data contains multiple assignments to an edge, only the last\nassignment will be honoured.  Violations maybe or may not be reported, and processing may continue without\nwarnings.",
            "title": "Single values"
        },
        {
            "location": "/Model/Optimizations/",
            "text": "File format Optimizations\n\u00b6\n\n\nRationale\n\u00b6\n\n\nIt is important to avoid an explosion of redundant data in \n.tf\n files. We want\nthe \n.tf\n format to be suitable for archiving, transparent to the human eye, and\neasy (i.e. fast) to process.\n\n\nUsing the implicit node\n\u00b6\n\n\nYou may leave out the node spec for node features, and the first node spec for\nedge features. When leaving out a node spec, you must also leave out the tab\nfollowing the node spec.\n\n\nA line with the first node spec left out denotes the singleton node set\nconsisting of the \nimplicit node\n. Here are the rules for implicit nodes.\n\n\n\n\nOn a line where there is an explicit node spec, the implicit node is equal to\n    the highest node denoted by the explicit node spec;\n\n\nOn a line without an explicit node spec, the implicit node is determined from\n    the previous line as follows:\n\n\nif there is no previous line, take \n1\n;\n\n\nelse take the implicit node of the previous line and increment it by \n1\n.\n\n\n\n\n\n\n\n\nFor edges, this optimization only happens for the \nfirst\n node spec. The second\nnode spec must always be explicit.\n\n\nThis optimizes some feature files greatly, e.g. the feature that contains the\nactual text of each word.\n\n\nInstead of\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n1 be\n2 reshit\n3 bara\n4 elohim\n5 et\n6 ha\n7 shamajim\n8 we\n9 et\n10 ha\n11 arets\n\n\n\n\n\n\nyou can just say\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nbe\nreshit\nbara\nelohim\net\nha\nshamajim\nwe\net\nha\narets\n\n\n\n\n\n\nThis optimization is not obligatory. It is a device that may be used if you want\nto optimize the size of data files that you want to distribute.\n\n\nOmitting empty values\n\u00b6\n\n\nIf the value is the empty string, you may also leave out the preceding tab (if\nthere is one). This is especially good for edge features, because most edges\njust consist of a node pair without any value.\n\n\nThis optimization will cause a conceptual ambiguity if there is only one field\npresent in a node feature, or if there are only two fields in an edge feature.\nIt could mean that the (first) node spec has been left out, or that the value\nhas been left out.\n\n\nIn those cases we will assume that the node spec has been left out for node\nfeatures.\n\n\nFor edge features, it depends on whether the edge is declared to have values\n(with \n@edgeValues\n). If the edge has values, then, as in the case of node\nfeatures, we assume that the first node spec has been left out. But if the edge\nhas no values, then we assume that both fields are node specs.\n\n\nSo, in a node feature a line like this\n\n\n1\n42\n\n\n\n\n\n\nmeans that the implicit node gets value \n42\n, and not that node \n42\n gets the\nempty value.\n\n\nLikewise, a line in an edge feature (without values) like this\n\n\n1\n42 43\n\n\n\n\n\n\nmeans that there is an edge from \n42\n to \n43\n with empty value, and not that\nthere is an edge from the implicit node to \n42\n with value 43.\n\n\nAnd, in the same edge, a line like this\n\n\n1\n42\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to \n42\n with the empty value.\n\n\nBut, in an edge with values, the same lines are interpreted thus:\n\n\n1\n42 43\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to node \n42\n with value \n43\n.\n\n\nAnd\n\n\n1\n42\n\n\n\n\n\n\nmeans that there is an edge from the implicit node to node \n42\n with empty\nvalue.\n\n\nThe reason for these conventions is practical: edge features usually have empty\nlabels, and there are many edges. In case of the Hebrew Text database, there are\n1.5 million edges, so every extra character that is needed on a data line means\nthat the file size increases with 1.5 MB.\n\n\nNodes on the other hand, usually do not have empty values, and they are often\nspecified in a consecutive way, especially slot (word) nodes. There are quite\nmany distinct word features, and it would be a waste to have a column of half a\nmillion incremental integers in those files.\n\n\nAbsence of values\n\u00b6\n\n\nSay you have a node feature assigning a value to only 2000 of 400,000 nodes.\n(The Hebrew \nqere\n would be an example). It is better to make sure that the\nabsent values are not coded as the empty string. So the feature data will look\nlike 2000 lines, each with a node spec, rather than a sequence of 400,000 lines,\nmost empty.\n\n\nIf you want to leave out just a few isolated cases in a feature where most nodes\nget a value, you can do it like this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n@node\n\nx0000\n...\nx1000\n1002 x1002\nx1003\n...\nx9999\n\n\n\n\n\n\nHere all 10,000 nodes get a value, except node \n1001\n.\n\n\nNote on redundancy\n\u00b6\n\n\nSome features assign the same value to many nodes. It is tempting to make a\nvalue definition facility, so that values are coded by short codes, the most\nfrequent values getting the shortest codes. After some experiments, it turned\nout that the overall gain was just 50%.\n\n\nI find this advantage too small to justify the increased code complexity, and\nabove all, the reduced transparency of the \n.tf\n files.\n\n\nExamples\n\u00b6\n\n\nHere are a few more and less contrived examples of legal feature data lines.\n\n\nNode features\n\u00b6\n\n\n\n\n\\t\\n\n\n\n2 2\\t3\n\n\nfoo\\nbar\n\n\n1 Escape \\t as \\\\t\n\n\n\n\nmeaning\n\n\n\n\nnode 1 has value: \ntab\n \nnewline\n\n\nnode 2 has value: 2 \ntab\n 3\n\n\nnode 3 has value: foo \nnewline\n bar\n\n\nnode 1 gets a new value: Escape \n as \\t\n\n\n\n\nEdge features\n\u00b6\n\n\n\n\n1\n\n\n1 2\n\n\n2 3 foo\n\n\n1-2 2-3 bar\n\n\n\n\nmeaning\n\n\n\n\nedge from 1 to 1 with no value\n\n\nedge from 1 to 2 with no value\n\n\nedge from 2 to 3 with value foo\n\n\nfour edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can\n    go from a node to itself. Note also that this line reassigns a value to two\n    edges: 1->2 and 2->3.",
            "title": "Tweaks"
        },
        {
            "location": "/Model/Optimizations/#file-format-optimizations",
            "text": "",
            "title": "File format Optimizations"
        },
        {
            "location": "/Model/Optimizations/#rationale",
            "text": "It is important to avoid an explosion of redundant data in  .tf  files. We want\nthe  .tf  format to be suitable for archiving, transparent to the human eye, and\neasy (i.e. fast) to process.",
            "title": "Rationale"
        },
        {
            "location": "/Model/Optimizations/#using-the-implicit-node",
            "text": "You may leave out the node spec for node features, and the first node spec for\nedge features. When leaving out a node spec, you must also leave out the tab\nfollowing the node spec.  A line with the first node spec left out denotes the singleton node set\nconsisting of the  implicit node . Here are the rules for implicit nodes.   On a line where there is an explicit node spec, the implicit node is equal to\n    the highest node denoted by the explicit node spec;  On a line without an explicit node spec, the implicit node is determined from\n    the previous line as follows:  if there is no previous line, take  1 ;  else take the implicit node of the previous line and increment it by  1 .     For edges, this optimization only happens for the  first  node spec. The second\nnode spec must always be explicit.  This optimizes some feature files greatly, e.g. the feature that contains the\nactual text of each word.  Instead of   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 1 be\n2 reshit\n3 bara\n4 elohim\n5 et\n6 ha\n7 shamajim\n8 we\n9 et\n10 ha\n11 arets   you can just say   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 be\nreshit\nbara\nelohim\net\nha\nshamajim\nwe\net\nha\narets   This optimization is not obligatory. It is a device that may be used if you want\nto optimize the size of data files that you want to distribute.",
            "title": "Using the implicit node"
        },
        {
            "location": "/Model/Optimizations/#omitting-empty-values",
            "text": "If the value is the empty string, you may also leave out the preceding tab (if\nthere is one). This is especially good for edge features, because most edges\njust consist of a node pair without any value.  This optimization will cause a conceptual ambiguity if there is only one field\npresent in a node feature, or if there are only two fields in an edge feature.\nIt could mean that the (first) node spec has been left out, or that the value\nhas been left out.  In those cases we will assume that the node spec has been left out for node\nfeatures.  For edge features, it depends on whether the edge is declared to have values\n(with  @edgeValues ). If the edge has values, then, as in the case of node\nfeatures, we assume that the first node spec has been left out. But if the edge\nhas no values, then we assume that both fields are node specs.  So, in a node feature a line like this  1 42   means that the implicit node gets value  42 , and not that node  42  gets the\nempty value.  Likewise, a line in an edge feature (without values) like this  1 42 43   means that there is an edge from  42  to  43  with empty value, and not that\nthere is an edge from the implicit node to  42  with value 43.  And, in the same edge, a line like this  1 42   means that there is an edge from the implicit node to  42  with the empty value.  But, in an edge with values, the same lines are interpreted thus:  1 42 43   means that there is an edge from the implicit node to node  42  with value  43 .  And  1 42   means that there is an edge from the implicit node to node  42  with empty\nvalue.  The reason for these conventions is practical: edge features usually have empty\nlabels, and there are many edges. In case of the Hebrew Text database, there are\n1.5 million edges, so every extra character that is needed on a data line means\nthat the file size increases with 1.5 MB.  Nodes on the other hand, usually do not have empty values, and they are often\nspecified in a consecutive way, especially slot (word) nodes. There are quite\nmany distinct word features, and it would be a waste to have a column of half a\nmillion incremental integers in those files.",
            "title": "Omitting empty values"
        },
        {
            "location": "/Model/Optimizations/#absence-of-values",
            "text": "Say you have a node feature assigning a value to only 2000 of 400,000 nodes.\n(The Hebrew  qere  would be an example). It is better to make sure that the\nabsent values are not coded as the empty string. So the feature data will look\nlike 2000 lines, each with a node spec, rather than a sequence of 400,000 lines,\nmost empty.  If you want to leave out just a few isolated cases in a feature where most nodes\nget a value, you can do it like this:  1\n2\n3\n4\n5\n6\n7\n8\n9 @node\n\nx0000\n...\nx1000\n1002 x1002\nx1003\n...\nx9999   Here all 10,000 nodes get a value, except node  1001 .",
            "title": "Absence of values"
        },
        {
            "location": "/Model/Optimizations/#note-on-redundancy",
            "text": "Some features assign the same value to many nodes. It is tempting to make a\nvalue definition facility, so that values are coded by short codes, the most\nfrequent values getting the shortest codes. After some experiments, it turned\nout that the overall gain was just 50%.  I find this advantage too small to justify the increased code complexity, and\nabove all, the reduced transparency of the  .tf  files.",
            "title": "Note on redundancy"
        },
        {
            "location": "/Model/Optimizations/#examples",
            "text": "Here are a few more and less contrived examples of legal feature data lines.",
            "title": "Examples"
        },
        {
            "location": "/Model/Optimizations/#node-features",
            "text": "\\t\\n  2 2\\t3  foo\\nbar  1 Escape \\t as \\\\t   meaning   node 1 has value:  tab   newline  node 2 has value: 2  tab  3  node 3 has value: foo  newline  bar  node 1 gets a new value: Escape   as \\t",
            "title": "Node features"
        },
        {
            "location": "/Model/Optimizations/#edge-features",
            "text": "1  1 2  2 3 foo  1-2 2-3 bar   meaning   edge from 1 to 1 with no value  edge from 1 to 2 with no value  edge from 2 to 3 with value foo  four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can\n    go from a node to itself. Note also that this line reassigns a value to two\n    edges: 1->2 and 2->3.",
            "title": "Edge features"
        },
        {
            "location": "/Create/CreateTF/",
            "text": "Create a TF dataset\n\u00b6\n\n\nWe describe a conversion of an \nexample text\n to Text-Fabric.\n\n\nThis is not meant as a recipe, but as a description of the pieces of information\nthat have to be assembled from the source text, and how to compose that into a\nText-Fabric resource, which is a set of features.\n\n\nHow you turn this insight into an executable program is dependent on how the\nsource text is encoded and organized. See some\n\nexamples\n.\n\n\nAnalysis\n\u00b6\n\n\nThe text is a string with a little bit of structure. Some of that structure\ngives us our node types, and other bits give us the features.\n\n\nNode types\n\u00b6\n\n\nThe text is divided into main sections, subsections, paragraphs, and sentences.\nThe sentences are divided into words by white-space and/or punctuation.\n\n\nStep 1: define slots\n\u00b6\n\n\nMake a copy of the text, strip out all headings and split the string on\nwhite-space. We get a sequence of \"words\". These words may contain punctuation or\nother non-alphabetical signs. We do not care for the moment.\n\n\nThe indexes in this sequence, from 1 till the number of \"words\", are our slots.\nLet's say we have \nS\n of them.\n\n\nWe start constructing a mapping from numbers to node types, called \notype\n.\n\n\nWe assign to numbers 1, ... ,\nS\n the string \nword\n.\n\n\nThat means, we have now \nS\n nodes, all of type \nword\n.\n\n\nStep 2: add higher level nodes\n\u00b6\n\n\nFor each level of \nsection\n, \nsubsection\n and \nparagraph\n, make new nodes. Nodes\nare numbers, and we start making new nodes directly after \nS\n.\n\n\nWe have 4 main sections, so we extend the \notype\n mapping as follows:\n\n\n\n\nS+1\n ~ \nsection\n\n\nS+2\n ~ \nsection\n\n\nS+3\n ~ \nsection\n\n\nS+4\n ~ \nsection\n\n\n\n\nLikewise, we have 11 subsections, so we continue extending:\n\n\n\n\nS+5\n ~ \nsubsection\n\n\nS+6\n ~ \nsubsection\n\n\n...\n\n\nS+16\n ~ \nsubsection\n\n\n\n\nWe do the same for \nparagraph\n.\n\n\nAnd after that, we break the paragraphs up into sentences (split on \n.\n), and we\nadd so many nodes of type \nsentence\n.\n\n\nThe mapping \notype\n is called a \nfeature\n of nodes. Any mapping that assigns\nvalues to nodes, is called a (node-)feature.\n\n\nContainment\n\u00b6\n\n\nWe also have to record which words belong to which nodes. This information takes\nthe shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we\nhave to lists pairs of nodes \n(n, w)\n such that the word \nw\n \"belongs\" to the\nnode \nn\n.\n\n\nThis is in fact a set of edges (pairs of nodes are edges), and a set of edges is\nan \nedge feature\n. In general it is possible to assign values to pairs of nodes,\nbut for our containment information we just assign the empty value to every pair\nwe have in our set.\n\n\nThe edge feature that records the containment of words in nodes, is called\n\noslots\n.\n\n\nStep 3: map nodes to sets of words\n\u00b6\n\n\nFor each of the higher level nodes \nn\n (the ones beyond \nS\n) we have to\nlookup/remember/compute which words \nw\n belong to it, and put that in the\n\noslots\n mapping:\n\n\n\n\nS+1\n ~ { 1, 2, 3, ... x, ..., y }\n\n\nS+2\n ~ { y+1, y+2, ... } ...\n\n\nS+5\n ~ { 1, 2, 3, ... x }\n\n\nS+6\n ~ { x+1, x+2, ...}\n\n\n...\n\n\n\n\nFeatures\n\u00b6\n\n\nNow we have two features, a node feature \notype\n and an edge feature \noslots\n.\nThis is merely the skeleton of our text, the \nwarp\n so to speak. It contains the\ntextual positions, and the information what the meaningful chunks are.\n\n\nNow it is time to weave the information in.\n\n\nStep 4: the actual text\n\u00b6\n\n\nRemember the words with punctuation attached? We can split every word into three\nparts:\n\n\n\n\ntext: the alphabetical characters in between\n\n\nprefix: the non-alphabetical leading characters\n\n\nsuffix: the non-alphabetical trailing characters\n\n\n\n\nWe can make three node features, \nprefix\n, \ntext\n, and \nsuffix\n. Remember that\nnode features are mappings from numbers to values.\n\n\nHere we go:\n\n\n\n\nprefix[1]\n is the prefix of word 1\n\n\nsuffix[1]\n is the suffix of word 1\n\n\ntext[1]\n is the text of word 1\n\n\n...\n\n\n\n\nAnd so for all words.\n\n\nStep 5: more features\n\u00b6\n\n\nFor the sections and subsections we can make a feature \nheading\n, in which we\nstore the headings of those sections.\n\n\n\n\nheading[S+1]\n is \nIntroduction\n\n\nheading[S+5]\n is \nBasic concepts\n\n\nheading[S+16]\n is \nIdentity\n\n\n...\n\n\n\n\nFor paragraphs we can figure out their sequence number within the subsection,\nand store that in a feature \nnumber\n:\n\n\n\n\nnumber[p]\n is 1 if \np\n is the node corresponding to the first paragraph in a\n    subsection.\n\n\n\n\nIf you want absolute paragraph numbers, you can just add a feature for that:\n\n\n\n\nabs_number[p]\n is 23 if \np\n is the node corresponding to the 23th paragraph\n    in the corpus.\n\n\n\n\nMetadata\n\u00b6\n\n\nYou can supply metadata to all node features and edge features. Metadata must be\ngiven as a dictionary, where the keys are the names of the features in your\ndataset, and the values are themselves key-value pairs, where the values are\njust strings.\n\n\nYou can mention where the source data comes from, who did the conversion, and\nyou can give a description of the intention of this feature and the shape of its\nvalues.\n\n\nLater, when you save the whole dataset as TF, Text-Fabric will insert a\n\ndatecreated\n key-value.\n\n\nYou can also supply metadata for \n''\n (the empty key). These key-values will be\nadded to all other features. Here you can put stuff that pertains to the dataset\nas a whole, such as information about decisions that have been taken.\n\n\nYou should also provide some special metadata to the key \notext\n. This feature\nhas no data, only metadata. It is not a node feature, not an edge feature, but a\n\nconfig\n feature. \notext\n is responsible for sectioning and text representation.\n\n\nIf you specify \notext\n well, the \nT-API\n can make use of it, so that\nyou have convenient, generic functions to get at your sections and to serialize\nyour text in different formats.\n\n\nStep 6: sectioning metadata\n\u00b6\n\n\n\n\nsectionTypes: 'section,subsection,paragraph'\n\n\nsectionFeatures: 'title,title,number'\n\n\n\n\nThis tells Text-Fabric that node type \nsection\n corresponds to section level 1,\n\nsubsection\n to level 2, and \nparagraph\n to level 3. Moreover, Text-Fabric knows\nthat the heading of sections at level 1 and 2 are in the feature \ntitle\n, and\nthat the heading at level 3 is in the feature \nnumber\n.\n\n\nStep 7: text formats\n\u00b6\n\n\n\n\nfmt:text-orig-plain: '{prefix}{text}{suffix}'\n\n\nfmt:text-orig-bare: '{text} '\n\n\nfmt:text-orig-angle: ' <{text}> '\n\n\n\n\nHere you have provided a bunch of text representation formats to Text-Fabric.\nThe names of those formats are up to you, and the values as well.\n\n\nIf you have a list of word nodes, say \nws\n, then a user of your corpus can ask\nText-Fabric:\n\n\n1\nT\n.\ntext\n(\nws\n,\n \nfmt\n=\n'text-orig-plain'\n)\n\n\n\n\n\n\n\nThis will spit out the full textual representation of those words, including\nthe non-alphabetical stuff in their prefixes and suffixes.\n\n\nThe second format, \ntext-orig-bare\n, will leave prefix and suffix out.\n\n\nAnd if for whatever reason you need to wrap each word in angle brackets, you can\nachieve that with \ntext-orig-angle\n.\n\n\nAs an example of how text formats come in handy, have a look at the text formats\nthat have been designed for Hebrew:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nfmt:lex-orig-full: '{g_lex_utf8} '\nfmt:lex-orig-plain: '{lex_utf8} '\nfmt:lex-trans-full: '{g_lex} '\nfmt:lex-trans-plain: '{lex0} '\nfmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}'\nfmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}'\nfmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}'\nfmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}'\nfmt:text-trans-full-ketiv: '{g_word}{trailer}'\nfmt:text-trans-plain: '{g_cons}{trailer}'\n\n\n\n\n\n\nNote that the actual text-formats are not baked in into TF, but are supplied by\nyou, the corpus designer.\n\n\nWriting out TF\n\u00b6\n\n\nOnce you have assembled your features and metadata as data structures in memory,\nyou can use \nTF.save()\n to write out your data as a bunch\nof Text-Fabric files.\n\n\nStep 8: invoke TF.save()\n\u00b6\n\n\nThe call to make is\n\n\n1\nTF\n.\nsave\n(\nnodeFeatures\n=\n{},\n \nedgeFeatures\n=\n{},\n \nmetaData\n=\n{},\n \nmodule\n=\nNone\n)\n\n\n\n\n\n\n\nHere you supply for \nnodeFeatures\n a dictionary keyed by your node feature\nnames and valued by the feature data of those features.\n\n\nLikewise for the edge features.\n\n\nAnd the metadata you have composed goes into the \nmetaData\n parameter.\n\n\nFinally, the \nmodule\n parameter dictates where on your system the TF-files will\nbe written.\n\n\nFirst time usage\n\u00b6\n\n\nWhen you start using your new dataset in Text-Fabric, you'll notice that there\nis some upfront computation going on. Text-Fabric computes derived data,\nespecially about the relationships between nodes based on the slots they occupy.\nAll that information comes from \noslots\n. The \noslots\n information is very\nterse, and using it directly would result in a hefty performance penalty.\nLikewise, all feature data will be read from the textual \n.tf\n files,\nrepresented in memory as a dictionary, and then that dictionary will be\nserialized and gzipped into a \n.tfx\n file in a hidden directory \n.tf\n. These\n\n.tfx\n files load an order of magnitude faster than the original \n.tf\n files.\nText-Fabric uses the timestamps of the files to determine whether the \n.tfx\n\nfiles are outdated and need to be regenerated again.\n\n\nThis whole machinery is invisible to you, the user, except for the delay at\nfirst time use.\n\n\nEnriching your corpus\n\u00b6\n\n\nMaybe a linguistic friend of yours has a tool to determine the part of speech of\neach word in the text.\n\n\nUsing TF itself it is not that hard to create a new feature \npos\n, that maps\neach word node to the part of speech of that word.\n\n\nSee for example how Cody Kingham\n\nadds\n\nthe notion of linguistic head to the BHSA\ndatasource of the Hebrew Bible.\n\n\nStep 9: add the new feature\n\u00b6\n\n\nOnce you have the feature \npos\n, provide a bit of metadata, and call\n\n\n1\n2\n3\n4\n5\nTF\n.\nsave\n(\n\n  \nnodeFeatures\n=\n{\n'pos'\n:\n \nposData\n},\n\n  \nmetaData\n=\n{\n'pos'\n:\n \nposMetaData\n},\n\n  \nmodule\n=\n'linguistics'\n,\n\n\n)\n\n\n\n\n\n\n\nYou get a TF module consisting of one feature \npos.tf\n in the \nlinguistics\n\ndirectory.\n\n\nMaybe you have more linguistic features to add. You do not have to create those\nfeatures alongside the original corpus. It is perfectly possible to leave the\ncorpus alone in its own GitHub repo, and write your new features in another\nrepo.\n\n\nUsers can just obtain the corpus and your linguistic module separately. When\nthey call their Text-Fabric, they can point it to both locations, and\nText-Fabric treats it as one dataset.\n\n\nStep 10: use the new feature\n\u00b6\n\n\nThe call to \nTF=Fabric()\n looks like this\n\n\n1\nTF\n \n=\n \nFabric\n(\nlocations\n=\n[\ncorpusLocation\n,\n \nmoduleLocation\n])\n\n\n\n\n\n\n\nAll feature files found at these locations are loadable in your session.",
            "title": "Make TF"
        },
        {
            "location": "/Create/CreateTF/#create-a-tf-dataset",
            "text": "We describe a conversion of an  example text  to Text-Fabric.  This is not meant as a recipe, but as a description of the pieces of information\nthat have to be assembled from the source text, and how to compose that into a\nText-Fabric resource, which is a set of features.  How you turn this insight into an executable program is dependent on how the\nsource text is encoded and organized. See some examples .",
            "title": "Create a TF dataset"
        },
        {
            "location": "/Create/CreateTF/#analysis",
            "text": "The text is a string with a little bit of structure. Some of that structure\ngives us our node types, and other bits give us the features.",
            "title": "Analysis"
        },
        {
            "location": "/Create/CreateTF/#node-types",
            "text": "The text is divided into main sections, subsections, paragraphs, and sentences.\nThe sentences are divided into words by white-space and/or punctuation.",
            "title": "Node types"
        },
        {
            "location": "/Create/CreateTF/#step-1-define-slots",
            "text": "Make a copy of the text, strip out all headings and split the string on\nwhite-space. We get a sequence of \"words\". These words may contain punctuation or\nother non-alphabetical signs. We do not care for the moment.  The indexes in this sequence, from 1 till the number of \"words\", are our slots.\nLet's say we have  S  of them.  We start constructing a mapping from numbers to node types, called  otype .  We assign to numbers 1, ... , S  the string  word .  That means, we have now  S  nodes, all of type  word .",
            "title": "Step 1: define slots"
        },
        {
            "location": "/Create/CreateTF/#step-2-add-higher-level-nodes",
            "text": "For each level of  section ,  subsection  and  paragraph , make new nodes. Nodes\nare numbers, and we start making new nodes directly after  S .  We have 4 main sections, so we extend the  otype  mapping as follows:   S+1  ~  section  S+2  ~  section  S+3  ~  section  S+4  ~  section   Likewise, we have 11 subsections, so we continue extending:   S+5  ~  subsection  S+6  ~  subsection  ...  S+16  ~  subsection   We do the same for  paragraph .  And after that, we break the paragraphs up into sentences (split on  . ), and we\nadd so many nodes of type  sentence .  The mapping  otype  is called a  feature  of nodes. Any mapping that assigns\nvalues to nodes, is called a (node-)feature.",
            "title": "Step 2: add higher level nodes"
        },
        {
            "location": "/Create/CreateTF/#containment",
            "text": "We also have to record which words belong to which nodes. This information takes\nthe shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we\nhave to lists pairs of nodes  (n, w)  such that the word  w  \"belongs\" to the\nnode  n .  This is in fact a set of edges (pairs of nodes are edges), and a set of edges is\nan  edge feature . In general it is possible to assign values to pairs of nodes,\nbut for our containment information we just assign the empty value to every pair\nwe have in our set.  The edge feature that records the containment of words in nodes, is called oslots .",
            "title": "Containment"
        },
        {
            "location": "/Create/CreateTF/#step-3-map-nodes-to-sets-of-words",
            "text": "For each of the higher level nodes  n  (the ones beyond  S ) we have to\nlookup/remember/compute which words  w  belong to it, and put that in the oslots  mapping:   S+1  ~ { 1, 2, 3, ... x, ..., y }  S+2  ~ { y+1, y+2, ... } ...  S+5  ~ { 1, 2, 3, ... x }  S+6  ~ { x+1, x+2, ...}  ...",
            "title": "Step 3: map nodes to sets of words"
        },
        {
            "location": "/Create/CreateTF/#features",
            "text": "Now we have two features, a node feature  otype  and an edge feature  oslots .\nThis is merely the skeleton of our text, the  warp  so to speak. It contains the\ntextual positions, and the information what the meaningful chunks are.  Now it is time to weave the information in.",
            "title": "Features"
        },
        {
            "location": "/Create/CreateTF/#step-4-the-actual-text",
            "text": "Remember the words with punctuation attached? We can split every word into three\nparts:   text: the alphabetical characters in between  prefix: the non-alphabetical leading characters  suffix: the non-alphabetical trailing characters   We can make three node features,  prefix ,  text , and  suffix . Remember that\nnode features are mappings from numbers to values.  Here we go:   prefix[1]  is the prefix of word 1  suffix[1]  is the suffix of word 1  text[1]  is the text of word 1  ...   And so for all words.",
            "title": "Step 4: the actual text"
        },
        {
            "location": "/Create/CreateTF/#step-5-more-features",
            "text": "For the sections and subsections we can make a feature  heading , in which we\nstore the headings of those sections.   heading[S+1]  is  Introduction  heading[S+5]  is  Basic concepts  heading[S+16]  is  Identity  ...   For paragraphs we can figure out their sequence number within the subsection,\nand store that in a feature  number :   number[p]  is 1 if  p  is the node corresponding to the first paragraph in a\n    subsection.   If you want absolute paragraph numbers, you can just add a feature for that:   abs_number[p]  is 23 if  p  is the node corresponding to the 23th paragraph\n    in the corpus.",
            "title": "Step 5: more features"
        },
        {
            "location": "/Create/CreateTF/#metadata",
            "text": "You can supply metadata to all node features and edge features. Metadata must be\ngiven as a dictionary, where the keys are the names of the features in your\ndataset, and the values are themselves key-value pairs, where the values are\njust strings.  You can mention where the source data comes from, who did the conversion, and\nyou can give a description of the intention of this feature and the shape of its\nvalues.  Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated  key-value.  You can also supply metadata for  ''  (the empty key). These key-values will be\nadded to all other features. Here you can put stuff that pertains to the dataset\nas a whole, such as information about decisions that have been taken.  You should also provide some special metadata to the key  otext . This feature\nhas no data, only metadata. It is not a node feature, not an edge feature, but a config  feature.  otext  is responsible for sectioning and text representation.  If you specify  otext  well, the  T-API  can make use of it, so that\nyou have convenient, generic functions to get at your sections and to serialize\nyour text in different formats.",
            "title": "Metadata"
        },
        {
            "location": "/Create/CreateTF/#step-6-sectioning-metadata",
            "text": "sectionTypes: 'section,subsection,paragraph'  sectionFeatures: 'title,title,number'   This tells Text-Fabric that node type  section  corresponds to section level 1, subsection  to level 2, and  paragraph  to level 3. Moreover, Text-Fabric knows\nthat the heading of sections at level 1 and 2 are in the feature  title , and\nthat the heading at level 3 is in the feature  number .",
            "title": "Step 6: sectioning metadata"
        },
        {
            "location": "/Create/CreateTF/#step-7-text-formats",
            "text": "fmt:text-orig-plain: '{prefix}{text}{suffix}'  fmt:text-orig-bare: '{text} '  fmt:text-orig-angle: ' <{text}> '   Here you have provided a bunch of text representation formats to Text-Fabric.\nThe names of those formats are up to you, and the values as well.  If you have a list of word nodes, say  ws , then a user of your corpus can ask\nText-Fabric:  1 T . text ( ws ,   fmt = 'text-orig-plain' )    This will spit out the full textual representation of those words, including\nthe non-alphabetical stuff in their prefixes and suffixes.  The second format,  text-orig-bare , will leave prefix and suffix out.  And if for whatever reason you need to wrap each word in angle brackets, you can\nachieve that with  text-orig-angle .  As an example of how text formats come in handy, have a look at the text formats\nthat have been designed for Hebrew:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 fmt:lex-orig-full: '{g_lex_utf8} '\nfmt:lex-orig-plain: '{lex_utf8} '\nfmt:lex-trans-full: '{g_lex} '\nfmt:lex-trans-plain: '{lex0} '\nfmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}'\nfmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}'\nfmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}'\nfmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}'\nfmt:text-trans-full-ketiv: '{g_word}{trailer}'\nfmt:text-trans-plain: '{g_cons}{trailer}'   Note that the actual text-formats are not baked in into TF, but are supplied by\nyou, the corpus designer.",
            "title": "Step 7: text formats"
        },
        {
            "location": "/Create/CreateTF/#writing-out-tf",
            "text": "Once you have assembled your features and metadata as data structures in memory,\nyou can use  TF.save()  to write out your data as a bunch\nof Text-Fabric files.",
            "title": "Writing out TF"
        },
        {
            "location": "/Create/CreateTF/#step-8-invoke-tfsave",
            "text": "The call to make is  1 TF . save ( nodeFeatures = {},   edgeFeatures = {},   metaData = {},   module = None )    Here you supply for  nodeFeatures  a dictionary keyed by your node feature\nnames and valued by the feature data of those features.  Likewise for the edge features.  And the metadata you have composed goes into the  metaData  parameter.  Finally, the  module  parameter dictates where on your system the TF-files will\nbe written.",
            "title": "Step 8: invoke TF.save()"
        },
        {
            "location": "/Create/CreateTF/#first-time-usage",
            "text": "When you start using your new dataset in Text-Fabric, you'll notice that there\nis some upfront computation going on. Text-Fabric computes derived data,\nespecially about the relationships between nodes based on the slots they occupy.\nAll that information comes from  oslots . The  oslots  information is very\nterse, and using it directly would result in a hefty performance penalty.\nLikewise, all feature data will be read from the textual  .tf  files,\nrepresented in memory as a dictionary, and then that dictionary will be\nserialized and gzipped into a  .tfx  file in a hidden directory  .tf . These .tfx  files load an order of magnitude faster than the original  .tf  files.\nText-Fabric uses the timestamps of the files to determine whether the  .tfx \nfiles are outdated and need to be regenerated again.  This whole machinery is invisible to you, the user, except for the delay at\nfirst time use.",
            "title": "First time usage"
        },
        {
            "location": "/Create/CreateTF/#enriching-your-corpus",
            "text": "Maybe a linguistic friend of yours has a tool to determine the part of speech of\neach word in the text.  Using TF itself it is not that hard to create a new feature  pos , that maps\neach word node to the part of speech of that word.  See for example how Cody Kingham adds \nthe notion of linguistic head to the BHSA\ndatasource of the Hebrew Bible.",
            "title": "Enriching your corpus"
        },
        {
            "location": "/Create/CreateTF/#step-9-add-the-new-feature",
            "text": "Once you have the feature  pos , provide a bit of metadata, and call  1\n2\n3\n4\n5 TF . save ( \n   nodeFeatures = { 'pos' :   posData }, \n   metaData = { 'pos' :   posMetaData }, \n   module = 'linguistics' ,  )    You get a TF module consisting of one feature  pos.tf  in the  linguistics \ndirectory.  Maybe you have more linguistic features to add. You do not have to create those\nfeatures alongside the original corpus. It is perfectly possible to leave the\ncorpus alone in its own GitHub repo, and write your new features in another\nrepo.  Users can just obtain the corpus and your linguistic module separately. When\nthey call their Text-Fabric, they can point it to both locations, and\nText-Fabric treats it as one dataset.",
            "title": "Step 9: add the new feature"
        },
        {
            "location": "/Create/CreateTF/#step-10-use-the-new-feature",
            "text": "The call to  TF=Fabric()  looks like this  1 TF   =   Fabric ( locations = [ corpusLocation ,   moduleLocation ])    All feature files found at these locations are loadable in your session.",
            "title": "Step 10: use the new feature"
        },
        {
            "location": "/Create/ExampleText/",
            "text": "Introduction\n\u00b6\n\n\nThe Social Construction of Reality is a 1966 book about the sociology of\nknowledge by the sociologists Peter L. Berger and Thomas Luckmann.\n\n\nBerger and Luckmann introduced the term \"social construction\" into the social\nsciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their\ncentral concept is that people and groups interacting in a social system create,\nover time, concepts or mental representations of each other's actions, and that\nthese concepts eventually become habituated into reciprocal roles played by the\nactors in relation to each other. When these roles are made available to other\nmembers of society to enter into and play out, the reciprocal interactions are\nsaid to be institutionalized. In the process, meaning is embedded in society.\nKnowledge and people's conceptions (and beliefs) of what reality is become\nembedded in the institutional fabric of society. Reality is therefore said to be\nsocially constructed.\n\n\nIn 1998 the International Sociological Association listed The Social\nConstruction of Reality as the fifth-most important sociological book of the\n20th century\n\n\nBasic concepts\n\u00b6\n\n\nSocial stock of knowledge\n\u00b6\n\n\nEarlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max\nWeber, etc.) often focused too much on scientific and theoretical knowledge, but\nthis is only a small part of social knowledge, concerning a very limited group.\nCustoms, common interpretations, institutions, shared routines,\nhabitualizations, the who-is-who and who-does-what in social processes and the\ndivision of labor, constitute a much larger part of knowledge in society.\n\n\n\u201c\u2026theoretical knowledge is only a small and by no means the most important part\nof what passed for knowledge in a society\u2026 the primary knowledge about the\ninstitutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019\nabout a social world, an assemblage of maxims, morals, proverbial nuggets of\nwisdom, values and beliefs, myths, and so forth\u201d (p.65)\n\n\nSemantic fields\n\u00b6\n\n\nThe general body of knowledge is socially distributed, and classified in\nsemantic fields. The dynamic distribution and inter dependencies of these\nknowledge sectors provide structure to the social stock of knowledge:\n\n\n\u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026\nmy knowledge of my own occupation and its world is very rich and specific, while\nI have only very sketchy knowledge of the occupational worlds of others\u201d (p.43)\n\u201cThe social distribution of knowledge thus begins with the simple fact that I do\nnot know everything known to my fellowmen, and vice versa, and culminates in\nexceedingly complex and esoteric systems of expertise. Knowledge of how the\nsocially available stock of knowledge is distributed, at least in outline, is an\nimportant element of that same stock of knowledge.\u201d (p.46)\n\n\nLanguage and signs\n\u00b6\n\n\nLanguage also plays an important role in the analysis of integration of everyday\nreality. Language links up commonsense knowledge with finite provinces of\nmeaning, thus enabling people, for example, to interpret dreams through\nunderstandings relevant in the daytime. \"Language is capable of transcending the\nreality of everyday life altogether. It can refer to experiences pertaining to\nfinite provinces of meaning, it can span discrete spheres of reality...Language\nsoars into regions that are not only de facto but also a priori unavailable to\neveryday experience.\"p. 40. Regarding the function of language and signs, Berger\nand Luckmann are indebted to George Herbert Mead and other figures in the field\nknown as symbolic interactionism, as acknowledged in their Introduction,\nespecially regarding the possibility of constructing objectivity.\n\n\nSigns and language provide interoperability for the construction of everyday\nreality:\n\n\n\u201cA sign [has the] explicit intention to serve as an index of subjective meanings\n\u2026 Language is capable of becoming the objective repository of vast accumulations\nof meaning and experience, which it can then preserve in time and transmit to\nfollowing generations\u2026 Language also typifies experiences, allowing me to\nsubsume them under broad categories in terms of which they have meaning not only\nto myself but also to my fellowmen\u201d (p.35-39)\n\n\nSocial everyday reality\n\u00b6\n\n\nSocial everyday reality is characterized by Intersubjectivity (which refers to\nthe coexistence of multiple realities in this context)(p. 23-25):\n\n\n\u201cCompared to the reality of everyday life, other realities appear as finite\nprovinces of meaning, enclaves within the paramount reality marked by\ncircumscribed meanings and modes of experience\u201d (p.25)\n\n\nThis is in contrast to other realities, such as dreams, theoretical constructs,\nreligious or mystic beliefs, artistic and imaginary worlds, etc. While\nindividuals may visit other realities (such as watching a film), they are always\nbrought back to everyday reality (once the film ends)(p. 25).\n\n\nSociety as objective reality\n\u00b6\n\n\n\u201c Social order is a human product, or more precisely, an ongoing human\nproduction \u201d Institutionalization[edit] Institutionalization of social processes\ngrows out of the habitualization and customs, gained through mutual observation\nwith subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces\nuncertainty and danger and allows our limited attention span to focus on more\nthings at the same time, while institutionalized routines can be expected to\ncontinue \u201cas previously agreed\u201d:\n\n\n\u201cHabitualization carries with it the important psychological gain that choices\nare narrowed\u2026 the background of habitualized activity opens up a foreground for\ndeliberation and innovation [which demand a higher level of attention]\u2026 The most\nimportant gain is that each [member of society] will be able to predict the\nother\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026\nMany actions are possible on a low level of attention. Each action of one is no\nlonger a source of astonishment and potential danger to the other\u201c (p.53-57).\n\n\nSocial objective worlds\n\u00b6\n\n\nSocial (or institutional) objective worlds are one consequence of\ninstitutionalization, and are created when institutions are passed on to a new\ngeneration. This creates a reality that is vulnerable to the ideas of a minority\nwhich will then form the basis of social expectations in the future. The\nunderlying reasoning is fully transparent to the creators of an institution, as\nthey can reconstruct the circumstances under which they made agreements; while\nthe second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and\n\u201cself-evident\u201d and they might not understand the underlying logic.\n\n\n\u201c\u2026a social world [is] a comprehensive and given reality confronting the\nindividual in a manner analogous to the reality of the natural world\u2026 In early\nphases of socialization the child is quite incapable of distinguishing between\nthe objectivity of natural phenomena and the objectivity of the social\nformations\u2026 The objective reality of institutions is not diminished if the\nindividual does not understand their purpose or their mode of operation\u2026 He must\n\u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)\n\n\nDivision of labor\n\u00b6\n\n\nDivision of labor is another consequence of institutionalization. Institutions\nassign \u201croles\u201d to be performed by various actors, through typification of\nperformances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As\nspecialization increases in number as well as in size and sophistication, a\ncivilization's culture contains more and more sections of knowledge specific to\ngiven roles or tasks, sections which become more and more esoteric to\nnon-specialists. These areas of knowledge do not belong anymore to the common\nsocial world and culture.\n\n\n\u201cA society\u2019s stock of knowledge is structured in terms of what is generally\nrelevant and what is relevant only to specific roles\u2026 the social distribution of\nknowledge entails a dichotomization in terms of general and role-specific\nrelevance\u2026 because of the division of labor, role-specific knowledge will grow\nat a faster rate than generally relevant and accessible knowledge\u2026 The\nincreasing number and complexity of [the resulting] sub universes [of\nspecialized knowledge] make them increasingly inaccessible to outsiders\n(p.77-87)\n\n\nSymbolic universes\n\u00b6\n\n\nSymbolic universes are created to provide legitimation to the created\ninstitutional structure. Symbolic universes are a set of beliefs \u201ceverybody\nknows\u201d that aim at making the institutionalized structure plausible and\nacceptable for the individual\u2014who might otherwise not understand or agree with\nthe underlying logic of the institution. As an ideological system, the symbolic\nuniverse \u201cputs everything in its right place\u201d. It provides explanations for why\nwe do things the way we do. Proverbs, moral maxims, wise sayings, mythology,\nreligions and other theological thought, metaphysical traditions and other value\nsystems are part of the symbolic universe. They are all (more or less\nsophisticated) ways to legitimize established institutions.\n\n\n\u201cThe function of legitimation is to make objectively available and subjectively\nplausible the \u2018first-order\u2019 objections that have been institutionalized\u2026\nProverbs, moral maxims and wise sayings are common on this level\u2026 [as well as]\nexplicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a\ngeneral theory of man\u2026 The symbolic universe also orders history. It locates all\ncollective events in a cohesive unity that includes past, present and future.\u201d\n(p. 92-104)\n\n\nUniverse-maintenance\n\u00b6\n\n\nUniverse-maintenance refers to specific procedures undertaken, often by an elite\ngroup, when the symbolic universe does not fulfill its purpose anymore, which is\nto legitimize the institutional structure in place. This happens, for example,\nin generational shifts, or when deviants create an internal movement against\nestablished institutions (e.g. against revolutions), or when a society is\nconfronted with another society with a greatly different history and\ninstitutional structures. In primitive societies this happened through\nmythological systems, later on through theological thought. Today, an extremely\ncomplex set of science has secularized universe-maintenance.\n\n\n\u201cSpecific procedures of universe-maintenance become necessary when the symbolic\nuniverse has become a problem. As long as this is not the case, the symbolic\nuniverse is self-maintaining, that is self-legitimating. An intrinsic problem\npresents itself with the process of transmission of the symbolic universe from\none generation to another\u2026 [additionally] two societies confronting each other\nwith conflicting universes will both develop conceptual machinery designed to\nmaintain their respective universes\u2026 mythology represents the most archaic form\nof universe-maintenance\u2026 theological thought may be distinguished from its\nmythological predecessor simply in terms of its greater degree of theoretical\nsystematization\u2026 Modern science is an extreme step in this development.\n(p.104-116)\n\n\nSociety as subjective reality\n\u00b6\n\n\nSocialization\n\u00b6\n\n\nSocialization is a two-step induction of the individual to participate in the\nsocial institutional structure, meaning in its objective reality.\n\n\n\"The individual\u2026 is not born a member of society. He\u2026 becomes a member of\nsociety. In the life of every individual\u2026 there is a temporal sequence, in the\ncourse of which he is inducted into participation in the social dialectic\"\n(p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high\ndegree of symmetry between objective and subjective reality\u201d (p. 163)\n\n\nPrimary Socialization takes place as a child. It is highly charged emotionally\nand is not questioned. Secondary Socialization includes the acquisition of\nrole-specific knowledge, thus taking one\u2019s place in the social division of\nlabor. It is learned through training and specific rituals, and is not\nemotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s\nteacher\u201d. Training for secondary socialization can be very complex and depends\non the complexity of division of labor in a society. Primary socialization is\nmuch less flexible than secondary socialization. E.g. shame for nudity comes\nfrom primary socialization, adequate dress code depends on secondary: A\nrelatively minor shift in the subjective definition of reality would suffice for\nan individual to take for granted that one may go to the office without a tie. A\nmuch more drastic shift would be necessary to have him go, as a matter of\ncourse, without any clothes at all.\n\n\n\u201cThe child does not internalize the world of his significant others as one of\nmany possible worlds\u2026 It is for this reason that the world internalized in\nprimary socialization is so much more firmly entrenched in consciousness than\nworlds internalized in secondary socialization\u2026. Secondary socialization is the\ninternalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of\nsecondary socialization carry a high degree of anonymity\u2026 The same knowledge\ntaught by one teacher could also be taught by another\u2026 The institutional\ndistribution of tasks between primary and secondary socialization varies with\nthe complexity of the social distribution of knowledge\u201d (p. 129-147)\n\n\nConversation\n\u00b6\n\n\nConversation or verbal communication aims at reality-maintenance of the\nsubjective reality. What seems to be a useless and unnecessary communication of\nredundant banalities is actually a constant mutual reconfirmation of each\nother's internal thoughts, in that it maintains subjective reality.\n\n\n\u201cOne may view the individual\u2019s everyday life in terms of the working away of a\nconversational apparatus that ongoingly maintains, modifies and reconstructs his\nsubjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the\nstation,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire\nworld within which these apparently simple propositions make sense\u2026 the exchange\nconfirms the subjective reality of this world\u2026 the great part, if not all, of\neveryday conversation maintains subjective reality\u2026 imagine the effect\u2026of an\nexchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine,\ndarling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)\n\n\nIdentity\n\u00b6\n\n\nIdentity of an individual is subject to a struggle of affiliation to sometimes\nconflicting realities. For example, the reality from primary socialization\n(mother tells child not to steal) can be in contrast with second socialization\n(gang members teach teenager that stealing is cool). Our final social location\nin the institutional structure of society will ultimately also influence our\nbody and organism.\n\n\n\u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines\nhow long and in what manner the individual organism shall live\u2026 Society also\ndirectly penetrates the organism in its functioning, most importantly in respect\nto sexuality and nutrition. While both sexuality and nutrition are grounded in\nbiological drives\u2026 biological constitution does not tell him where he should\nseek sexual release and what he should eat.\u201d (p. 163-183)",
            "title": "Example data"
        },
        {
            "location": "/Create/ExampleText/#introduction",
            "text": "The Social Construction of Reality is a 1966 book about the sociology of\nknowledge by the sociologists Peter L. Berger and Thomas Luckmann.  Berger and Luckmann introduced the term \"social construction\" into the social\nsciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their\ncentral concept is that people and groups interacting in a social system create,\nover time, concepts or mental representations of each other's actions, and that\nthese concepts eventually become habituated into reciprocal roles played by the\nactors in relation to each other. When these roles are made available to other\nmembers of society to enter into and play out, the reciprocal interactions are\nsaid to be institutionalized. In the process, meaning is embedded in society.\nKnowledge and people's conceptions (and beliefs) of what reality is become\nembedded in the institutional fabric of society. Reality is therefore said to be\nsocially constructed.  In 1998 the International Sociological Association listed The Social\nConstruction of Reality as the fifth-most important sociological book of the\n20th century",
            "title": "Introduction"
        },
        {
            "location": "/Create/ExampleText/#basic-concepts",
            "text": "",
            "title": "Basic concepts"
        },
        {
            "location": "/Create/ExampleText/#social-stock-of-knowledge",
            "text": "Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max\nWeber, etc.) often focused too much on scientific and theoretical knowledge, but\nthis is only a small part of social knowledge, concerning a very limited group.\nCustoms, common interpretations, institutions, shared routines,\nhabitualizations, the who-is-who and who-does-what in social processes and the\ndivision of labor, constitute a much larger part of knowledge in society.  \u201c\u2026theoretical knowledge is only a small and by no means the most important part\nof what passed for knowledge in a society\u2026 the primary knowledge about the\ninstitutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019\nabout a social world, an assemblage of maxims, morals, proverbial nuggets of\nwisdom, values and beliefs, myths, and so forth\u201d (p.65)",
            "title": "Social stock of knowledge"
        },
        {
            "location": "/Create/ExampleText/#semantic-fields",
            "text": "The general body of knowledge is socially distributed, and classified in\nsemantic fields. The dynamic distribution and inter dependencies of these\nknowledge sectors provide structure to the social stock of knowledge:  \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026\nmy knowledge of my own occupation and its world is very rich and specific, while\nI have only very sketchy knowledge of the occupational worlds of others\u201d (p.43)\n\u201cThe social distribution of knowledge thus begins with the simple fact that I do\nnot know everything known to my fellowmen, and vice versa, and culminates in\nexceedingly complex and esoteric systems of expertise. Knowledge of how the\nsocially available stock of knowledge is distributed, at least in outline, is an\nimportant element of that same stock of knowledge.\u201d (p.46)",
            "title": "Semantic fields"
        },
        {
            "location": "/Create/ExampleText/#language-and-signs",
            "text": "Language also plays an important role in the analysis of integration of everyday\nreality. Language links up commonsense knowledge with finite provinces of\nmeaning, thus enabling people, for example, to interpret dreams through\nunderstandings relevant in the daytime. \"Language is capable of transcending the\nreality of everyday life altogether. It can refer to experiences pertaining to\nfinite provinces of meaning, it can span discrete spheres of reality...Language\nsoars into regions that are not only de facto but also a priori unavailable to\neveryday experience.\"p. 40. Regarding the function of language and signs, Berger\nand Luckmann are indebted to George Herbert Mead and other figures in the field\nknown as symbolic interactionism, as acknowledged in their Introduction,\nespecially regarding the possibility of constructing objectivity.  Signs and language provide interoperability for the construction of everyday\nreality:  \u201cA sign [has the] explicit intention to serve as an index of subjective meanings\n\u2026 Language is capable of becoming the objective repository of vast accumulations\nof meaning and experience, which it can then preserve in time and transmit to\nfollowing generations\u2026 Language also typifies experiences, allowing me to\nsubsume them under broad categories in terms of which they have meaning not only\nto myself but also to my fellowmen\u201d (p.35-39)",
            "title": "Language and signs"
        },
        {
            "location": "/Create/ExampleText/#social-everyday-reality",
            "text": "Social everyday reality is characterized by Intersubjectivity (which refers to\nthe coexistence of multiple realities in this context)(p. 23-25):  \u201cCompared to the reality of everyday life, other realities appear as finite\nprovinces of meaning, enclaves within the paramount reality marked by\ncircumscribed meanings and modes of experience\u201d (p.25)  This is in contrast to other realities, such as dreams, theoretical constructs,\nreligious or mystic beliefs, artistic and imaginary worlds, etc. While\nindividuals may visit other realities (such as watching a film), they are always\nbrought back to everyday reality (once the film ends)(p. 25).",
            "title": "Social everyday reality"
        },
        {
            "location": "/Create/ExampleText/#society-as-objective-reality",
            "text": "\u201c Social order is a human product, or more precisely, an ongoing human\nproduction \u201d Institutionalization[edit] Institutionalization of social processes\ngrows out of the habitualization and customs, gained through mutual observation\nwith subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces\nuncertainty and danger and allows our limited attention span to focus on more\nthings at the same time, while institutionalized routines can be expected to\ncontinue \u201cas previously agreed\u201d:  \u201cHabitualization carries with it the important psychological gain that choices\nare narrowed\u2026 the background of habitualized activity opens up a foreground for\ndeliberation and innovation [which demand a higher level of attention]\u2026 The most\nimportant gain is that each [member of society] will be able to predict the\nother\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026\nMany actions are possible on a low level of attention. Each action of one is no\nlonger a source of astonishment and potential danger to the other\u201c (p.53-57).",
            "title": "Society as objective reality"
        },
        {
            "location": "/Create/ExampleText/#social-objective-worlds",
            "text": "Social (or institutional) objective worlds are one consequence of\ninstitutionalization, and are created when institutions are passed on to a new\ngeneration. This creates a reality that is vulnerable to the ideas of a minority\nwhich will then form the basis of social expectations in the future. The\nunderlying reasoning is fully transparent to the creators of an institution, as\nthey can reconstruct the circumstances under which they made agreements; while\nthe second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and\n\u201cself-evident\u201d and they might not understand the underlying logic.  \u201c\u2026a social world [is] a comprehensive and given reality confronting the\nindividual in a manner analogous to the reality of the natural world\u2026 In early\nphases of socialization the child is quite incapable of distinguishing between\nthe objectivity of natural phenomena and the objectivity of the social\nformations\u2026 The objective reality of institutions is not diminished if the\nindividual does not understand their purpose or their mode of operation\u2026 He must\n\u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)",
            "title": "Social objective worlds"
        },
        {
            "location": "/Create/ExampleText/#division-of-labor",
            "text": "Division of labor is another consequence of institutionalization. Institutions\nassign \u201croles\u201d to be performed by various actors, through typification of\nperformances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As\nspecialization increases in number as well as in size and sophistication, a\ncivilization's culture contains more and more sections of knowledge specific to\ngiven roles or tasks, sections which become more and more esoteric to\nnon-specialists. These areas of knowledge do not belong anymore to the common\nsocial world and culture.  \u201cA society\u2019s stock of knowledge is structured in terms of what is generally\nrelevant and what is relevant only to specific roles\u2026 the social distribution of\nknowledge entails a dichotomization in terms of general and role-specific\nrelevance\u2026 because of the division of labor, role-specific knowledge will grow\nat a faster rate than generally relevant and accessible knowledge\u2026 The\nincreasing number and complexity of [the resulting] sub universes [of\nspecialized knowledge] make them increasingly inaccessible to outsiders\n(p.77-87)",
            "title": "Division of labor"
        },
        {
            "location": "/Create/ExampleText/#symbolic-universes",
            "text": "Symbolic universes are created to provide legitimation to the created\ninstitutional structure. Symbolic universes are a set of beliefs \u201ceverybody\nknows\u201d that aim at making the institutionalized structure plausible and\nacceptable for the individual\u2014who might otherwise not understand or agree with\nthe underlying logic of the institution. As an ideological system, the symbolic\nuniverse \u201cputs everything in its right place\u201d. It provides explanations for why\nwe do things the way we do. Proverbs, moral maxims, wise sayings, mythology,\nreligions and other theological thought, metaphysical traditions and other value\nsystems are part of the symbolic universe. They are all (more or less\nsophisticated) ways to legitimize established institutions.  \u201cThe function of legitimation is to make objectively available and subjectively\nplausible the \u2018first-order\u2019 objections that have been institutionalized\u2026\nProverbs, moral maxims and wise sayings are common on this level\u2026 [as well as]\nexplicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a\ngeneral theory of man\u2026 The symbolic universe also orders history. It locates all\ncollective events in a cohesive unity that includes past, present and future.\u201d\n(p. 92-104)",
            "title": "Symbolic universes"
        },
        {
            "location": "/Create/ExampleText/#universe-maintenance",
            "text": "Universe-maintenance refers to specific procedures undertaken, often by an elite\ngroup, when the symbolic universe does not fulfill its purpose anymore, which is\nto legitimize the institutional structure in place. This happens, for example,\nin generational shifts, or when deviants create an internal movement against\nestablished institutions (e.g. against revolutions), or when a society is\nconfronted with another society with a greatly different history and\ninstitutional structures. In primitive societies this happened through\nmythological systems, later on through theological thought. Today, an extremely\ncomplex set of science has secularized universe-maintenance.  \u201cSpecific procedures of universe-maintenance become necessary when the symbolic\nuniverse has become a problem. As long as this is not the case, the symbolic\nuniverse is self-maintaining, that is self-legitimating. An intrinsic problem\npresents itself with the process of transmission of the symbolic universe from\none generation to another\u2026 [additionally] two societies confronting each other\nwith conflicting universes will both develop conceptual machinery designed to\nmaintain their respective universes\u2026 mythology represents the most archaic form\nof universe-maintenance\u2026 theological thought may be distinguished from its\nmythological predecessor simply in terms of its greater degree of theoretical\nsystematization\u2026 Modern science is an extreme step in this development.\n(p.104-116)",
            "title": "Universe-maintenance"
        },
        {
            "location": "/Create/ExampleText/#society-as-subjective-reality",
            "text": "",
            "title": "Society as subjective reality"
        },
        {
            "location": "/Create/ExampleText/#socialization",
            "text": "Socialization is a two-step induction of the individual to participate in the\nsocial institutional structure, meaning in its objective reality.  \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of\nsociety. In the life of every individual\u2026 there is a temporal sequence, in the\ncourse of which he is inducted into participation in the social dialectic\"\n(p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high\ndegree of symmetry between objective and subjective reality\u201d (p. 163)  Primary Socialization takes place as a child. It is highly charged emotionally\nand is not questioned. Secondary Socialization includes the acquisition of\nrole-specific knowledge, thus taking one\u2019s place in the social division of\nlabor. It is learned through training and specific rituals, and is not\nemotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s\nteacher\u201d. Training for secondary socialization can be very complex and depends\non the complexity of division of labor in a society. Primary socialization is\nmuch less flexible than secondary socialization. E.g. shame for nudity comes\nfrom primary socialization, adequate dress code depends on secondary: A\nrelatively minor shift in the subjective definition of reality would suffice for\nan individual to take for granted that one may go to the office without a tie. A\nmuch more drastic shift would be necessary to have him go, as a matter of\ncourse, without any clothes at all.  \u201cThe child does not internalize the world of his significant others as one of\nmany possible worlds\u2026 It is for this reason that the world internalized in\nprimary socialization is so much more firmly entrenched in consciousness than\nworlds internalized in secondary socialization\u2026. Secondary socialization is the\ninternalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of\nsecondary socialization carry a high degree of anonymity\u2026 The same knowledge\ntaught by one teacher could also be taught by another\u2026 The institutional\ndistribution of tasks between primary and secondary socialization varies with\nthe complexity of the social distribution of knowledge\u201d (p. 129-147)",
            "title": "Socialization"
        },
        {
            "location": "/Create/ExampleText/#conversation",
            "text": "Conversation or verbal communication aims at reality-maintenance of the\nsubjective reality. What seems to be a useless and unnecessary communication of\nredundant banalities is actually a constant mutual reconfirmation of each\nother's internal thoughts, in that it maintains subjective reality.  \u201cOne may view the individual\u2019s everyday life in terms of the working away of a\nconversational apparatus that ongoingly maintains, modifies and reconstructs his\nsubjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the\nstation,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire\nworld within which these apparently simple propositions make sense\u2026 the exchange\nconfirms the subjective reality of this world\u2026 the great part, if not all, of\neveryday conversation maintains subjective reality\u2026 imagine the effect\u2026of an\nexchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine,\ndarling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)",
            "title": "Conversation"
        },
        {
            "location": "/Create/ExampleText/#identity",
            "text": "Identity of an individual is subject to a struggle of affiliation to sometimes\nconflicting realities. For example, the reality from primary socialization\n(mother tells child not to steal) can be in contrast with second socialization\n(gang members teach teenager that stealing is cool). Our final social location\nin the institutional structure of society will ultimately also influence our\nbody and organism.  \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines\nhow long and in what manner the individual organism shall live\u2026 Society also\ndirectly penetrates the organism in its functioning, most importantly in respect\nto sexuality and nutrition. While both sexuality and nutrition are grounded in\nbiological drives\u2026 biological constitution does not tell him where he should\nseek sexual release and what he should eat.\u201d (p. 163-183)",
            "title": "Identity"
        },
        {
            "location": "/Search/",
            "text": "Search Design\n\u00b6\n\n\nFabric metaphor\n\u00b6\n\n\n\n\nThe search space is a massive fabric of interconnected material. In it we\ndiscern the structures we are interested in: little pieces of fabric, also with\ninterconnected material.\n\n\nWhen we search, we have a fabric in mind, woven from specific material, stitched\ntogether in a specific manner.\n\n\nSearch in Text-Fabric works exactly like this: you give a sample patch, and\nText-Fabric fetches all pieces of the big fabric that match your patch.\n\n\n\n\nThe textile metaphor is particularly suited for grasping the search part of\nText-Fabric, so I'm going to stick to it for a while. I have used it in the\nactual code as well, and even in the proofs that certain parts of the algorithm\nterminate and are correct. Yet it remains a metaphor, and the fit is not exact.\n\n\nThe basic pattern of search is this:\n\n\n\n\n\n\n\n\ntextile\n\n\ntext\n\n\nexample\n\n\n\n\n\n\n\n\n\n\ntake several fleeces\n\n\npick the nodes corresponding to a node type\n\n\nword\ns, \nphrase\ns, \nclause\ns, \nverse\ns\n\n\n\n\n\n\nspin thick yarns from them\n\n\nfilter by feature conditions\n\n\npart-of-speech=verb\n gender=\nf\n \nbook=Genesis\n \nvt\n\n\n\n\n\n\nspin the yarns further into thin yarns\n\n\nthrow away nodes that do not have the right connections\n\n\nfeature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa\n\n\n\n\n\n\nstitch the yarns together with thread\n\n\nbuild results by selecting a member for every filtered node set\n\n\nword\n node \n123456\n in \nphrase\n node \n657890\n in \nclause\n node \n490567\n in \nverse\n node \n1403456\n\n\n\n\n\n\n\n\nWe will explain the stages of the fabrication process in detail.\n\n\nFleece\n\u00b6\n\n\n\n\nA fleece corresponds with a very simple search template that asks for all\nobjects of a given type:\n\n\n1\nword\n\n\n\n\n\n\nor\n\n\n1\nclause\n\n\n\n\n\n\nor, asking for multiple types:\n\n\n1\n2\n3\n4\n5\nverse\nclause\nphrase\nlex\nword\n\n\n\n\n\n\nFleeces are the raw material from which we fabricate our search results.\n\n\nEvery node type, such as \nword\n, \nsentence\n, \nbook\n corresponds to a fleece. In\nText-Fabric, every node has exactly one node type, so the whole space is neatly\ndivided into a small set of fleeces.\n\n\nThe most important characteristic of a fleece is it size: the number of nodes in\na node type.\n\n\nSpinning thick yarn\n\u00b6\n\n\n\n\nConsider search templates where we ask for specific members of a node type, by\ngiving feature constraints:\n\n\n1\n2\n3\n4\n5\n6\nverse book=Genesis\nclause type=rela\nphrase determined=yes\nlex id=jc/\nword number=pl vt\nvt\n\n\n\n\n\n\nEvery line in this search templates we call an \natom\n: a node type plus a\nfeature specification. The result of an atom is the set of all nodes in that\nnode type that satisfy those feature conditions. Finding the results of an atom\ncorresponds with first thing that we do with a fleece: \nspin\n a thick \nyarn\n\nfrom it. Yarns in general are obtained by spinning fleeces, i.e. by filtering\nnode sets that correspond to a node type.\n\n\nA search template may contain multiple atoms. Text-Fabric collects all atoms of\na template, grabs the corresponding fleeces, and spins thick yarns from them.\nFor each atom it will spin a yarn, and if there are several atoms referring to\nthe same node type, there will be several yarns spun from that fleece. This\nspinning of thick yarns out of fleeces happens in just one go. All fleeces\ntogether contain exactly all nodes, so Text-Fabric walks in one pass over all\nnodes, applies the feature conditions, and puts the nodes into the yarns\ndepending on which conditions apply.\n\n\nBy the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical\npass requires a fraction of a second.\n\n\nThe most important characteristic of a yarn is its \nthickness\n, which we define\nas the number of nodes in the yarn divided by the number of nodes in the fleece.\n\n\nFor example, a yarn consisting of the \nbook\n node of Genesis only, has a\nthickness of 1/39, because there are 39 books in the fleece. A much thicker yarn\nis that of the verbs, which has a thickness of roughly 1/6. A very thin thread\nis that of the word \n<CQH\n (which occurs only once) with a thickness of only\n1/400,000.\n\n\nSpinning thin yarns\n\u00b6\n\n\nIn order to find results, we have to further narrow down the search space. In\nother words, we are going to spin our thick yarns into thinner and thinner\nyarns. Before we can do that, we should make one thing clear.\n\n\nConnected by constraints\n\u00b6\n\n\nIf the template above were complete, it would lead to a monstrous number of\nresults. Because a result of a template like this is any combination of verse-,\nclause-, phrase-, lex-, word nodes that individually satisfy their own atom\ncondition. So the number of results is the product of the number of results of\nthe individual atoms, which is pretty enormous. It is hard to imagine a\nsituation where these results could be consumed.\n\n\nUsually, there are \nconstraints\n active between the atoms. For example in a\ntemplate like this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n1 verse book=Genesis\n2    clause type=rela\n3        phrase determined=yes\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l\n\n\n\n\n\n\nThe meaning of this template is that we look for a \nverse\n that\n\n\n\n\n(1) claims to be in Genesis\n\n\n(2) has a clause whose type is \nrela\n\n\n(3) which in turn has a phrase of a determined character\n\n\n(4) which contains a word in the plural and that has a verbal tense (vt)\n\n\n\n\nThere should also be a\n\n\n\n\n(6) lex object, identified by \njc/\n\n\n\n\nwhich is connected to the rest by the constraint that\n\n\n\n\n(8) the word of line 4 is contained in it.\n\n\n\n\nNote that all atoms are linked by constraints into one network. In graph\ntheoretical terms: this template consists of exactly one\n\nconnected component\n.\n\n\nIf this were not so, we would have in fact two independent search tasks, where\nthe result set would be the (cartesian) product of the result sets of the\nseparate components.\n\n\nFor example, if line 8 were missing, we would effectively search for things that\nmatch lines 1-4, and, independently, for things that match line 6. And every\nresult of the first part, combined with any result of the second part, would be\na valid result of the whole.\n\n\nWell, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the\nsame time, let alone that it wants to waste time to generate all the results in\nthe product. You will have to fire those search tasks one by one, and it is up\nto you how you combine the results.\n\n\nThe upshot it: \nthe atoms in the search template should form a network,\nconnected by constraints\n.\n\n\nText-Fabric will check this, and will only work with search templates that have\nonly one connected component.\n\n\nTerminology\n\u00b6\n\n\nBy now we have arrived at the idea that our search template is a graph\nunderneath: what we have called \natoms\n are in fact the nodes, and what we have\ncalled \nconstraints\n, are the edges.\n\n\nFrom now on, we will call the \natoms\n \nqnodes\n and the constraints \nqedges\n.\nThe \nq\n is to distinguish the nodes and the edges from the nodes and the edges\nof your dataset, the \ntext\n nodes and \ntext\n edges. When we use the term \nnodes\n\nand \nedges\n we will always refer to \ntext\n nodes and edges.\n\n\nWhen we are searching, we maintain a \nyarn\n for every \nqnode\n. This yarn starts\nout to be the thick yarn as described above, but we are going to thin them.\n\n\nWe can also see how our query templates are really \ntopographic\n: a query\ntemplate is a piece of local geography that we want to match against the data.\nFinding results is nothing else than instantiating \nqnodes\n of the search\ntemplate by text nodes in such a way that the \nqedges\n hold between the text\nedges.\n\n\nSpinning a qedge\n\u00b6\n\n\n\n\nSo, where were we? We have spun thick threads based on the \nqnodes\n\nindividually, but we have not done anything with the \nqedges\n. That is going to\nchange now.\n\n\nConsider this piece of search template:\n\n\n1\n2\n3\n4\n5\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l\n\n\n\n\n\n\nSo our \nqnodes\n are \nw\n and \nl\n, and our \nqedge\n is \nw ]] l\n. Note that a \nlex\n\nobject is the set of all occurrences of a lexeme. So \nw ]] l\n says that \nw\n is\nembedded in \nl\n, in other words, \nw\n is a slot contained in the slots of \nl\n. It\nis nothing else than the word \nw\n is an instance of the lexeme \njc/\n.\n\n\nWe will now check the pairs of (lex, word)-nodes in the text, where the lex node\nis taken from the yarn of the qnode \nl\n, and the word from the yarn of the qnode\n\nw\n.\n\n\nWe can throw away some words from the yarn of \nw\n, namely those words that do\nnot lie in a lexeme that is in the yarn of \nl\n. In other words: the words that\nare not instances of lexeme \njc/\n are out!\n\n\nConversely, if the lexeme \njc/\n does not have occurrences in the plural and with\na verbal tense, we must kick it out of the yarn of \nl\n, leaving no members in\nit. If a yarn gets empty, we have an early detection that the search yields no\nresults, and the whole process stops.\n\n\nIn our case, however, this is not so, and we continue.\n\n\nThis pass over the yarns at both sides of a qedge is a \nspin\n action. We spin\nthis qedge, and the result is that the two yarns become spun more thinly,\nhopefully.\n\n\n\n\nWith the yarn of words severely spun out, we are going to the next qedge, the\none between words and phrases.\n\n\n1\n2\n3        phrase determined=yes\n4            w:word number=pl vt\n\n\n\n\n\n\nThe indent is an implicit way of saying that the \"embeds\" relation \n[[\n holds\nbetween the \nphrase\n and the \nword\n. An equivalent formulation of the template\nis\n\n\n1\n2\n3\n4\np:phrase determined=yes\nw:word number=pl vt\n\np [[ w\n\n\n\n\n\n\nWe race along the yarn \nw\n of remaining words and check for each word if it is\ncontained in a phrase in the yarn of \np\n, the determined phrases. If it is not,\nwe throw the word out of the yarn of \nw\n. Similarly, we can throw out some\nphrases from the yarn of \np\n, namely those phrases that do not contain words in\nthe yarn of \nw\n. In other words: the phrases without plural words and verbal\ntense are also out.\n\n\n\n\nWe continue spinning, now between phrases and clauses.\n\n\n1\n2\n2    clause type=rela\n3        phrase determined=yes\n\n\n\n\n\n\nHere we loose the phrases that are not contained in a clause of \ntype=rela\n, and\nwe loose all clauses that do not embed one of the few phrases left.\n\n\n\n\nThe last spin action corresponds with\n\n\n1\n2\n1 verse book=Genesis\n2    clause type=rela\n\n\n\n\n\n\nSo we throw away all our results if they are outside Genesis.\n\n\nWe end up with a set of thin yarns, severely thinned out, even. This will be a\ngood starting point for the last stage: picking members from each yarn to form\nresults. We call this \nstitching\n and we'll get there in a moment.\n\n\nThe spread of a qedge\n\u00b6\n\n\nA very important property of a qedge is its \nspread\n. A qedge links a every node\n\nn\n in its \nfrom\n-yarn to zero, one, or more nodes in its \nto\n-yarn. The number\nof nodes in the \nto\n-yarn is a key property. The average number of nodes \nm\n in\nthe \nto\n-yarn per linked node \nn\n in the \nfrom\n-yarn is the \nspread\n of the\nedge.\n\n\nA few examples:\n\n\n\n\nAn edge that corresponds to \n]]\n, \nn\n embeds \nm\n. If this edge goes from books\n    to words, then every book node \nn\n is linked to every one of its words. So\n    very \nn\n has hundreds or thousands \nm\ns. The spread will roughly be 425,000 /\n    39 =~ 10,000\n\n\nThe opposite edge has a spread of exactly 1, because every word belongs to\n    exactly one book. Edges with spread 1 are very pleasant for our stitching\n    algorithm later on.\n\n\nAn edge corresponding to \n=\n. These qedges are super efficient, because their\n    relation \n=\n is a breeze to compute, and they have always a spread 1 in both\n    directions.\n\n\nAn edge corresponding to \n#\n, the node inequality relation. The relation \n#\n\n    is still a breeze to compute, but the result is heavy: the set of all nodes\n    not equal to a given node. The spread is nearly 100% of the yarn length, in\n    both directions. These edges are not worth to spin, because if you have two\n    yarns, no node will be excluded: if you have an \nn\n in the \nfrom\n-yarn, you\n    will always be able to find a different \nn\n in the \nto\n-yarn (except when bot\n    yarns are equal, and contain just one node).\n\n\nAn edge corresponding to \n==\n, the relation between nodes that are linked to\n    the same set of slots. The spread of this relation is not too big, but the\n    cost of computing it adds up quickly when applied to many cases.\n\n\n\n\nSpinning all qedges\n\u00b6\n\n\nLet us describe the spinning of yarns along edges in a bit more general way, and\nreflect on what it does for us.\n\n\nWe spin all qedges of a template. But after spinning a qedge, the yarns involved\nmay have changed. If that is the case, it makes sense to re-spin other qedges\nthat are involved in the changed yarns.\n\n\nThat is exactly what we do. We keep spinning, until the yarns have stabilized.\n\n\nA few key questions need to be addressed:\n\n\n\n\nDo the yarns stabilize?\n\n\nIf they stabilize, what have we got?\n\n\nIs this an efficient process?\n\n\n\n\nTermination of spinning\n\u00b6\n\n\nYes, spinning qedges until nothing changes any more, terminates, provided you do\nnot try to spin qedges that are up-to-date. If the yarns around an edge have not\nchanged, it does not make sense to spin that qedge. See\n\nhere\n for proof.\n\n\nWhat have we got?\n\u00b6\n\n\nAfter spinning, it is guaranteed that we have not thrown away results. All nodes\nthat are parts of valid results, are still in the yarns.\n\n\nBut, conversely, can it be that there are still nodes in the yarns that are not\npart of a result? Yes, that is possible.\n\n\nOnly when the graph of qnodes and qedges does not have a cycle, we know that all\nmembers of all yarns occur at least once in a result. See\n\nhere\n for proof.\n\n\nQuite a few interesting queries, however, have cycles in in their graphs. So, in\nthose cases, spinning qedges will not cause the maximal narrowing down of the\nsearch space.\n\n\nEfficiency\n\u00b6\n\n\nAnd that raises the question: how effective is the process of spinning qedges?\n\n\nThe answer is: it depends. If your qnodes have strong conditions on them, so\nthat the first yarn is already very thin, then every yarn that is connected to\nthis one by a qedge has also the chance to get very thin after spinning. In this\ncase, the combined filtering effect of all edges can produce a rapid narrowing\nof the search space.\n\n\nEspecially if we can implement edge spinning in an optimized way, this works\nlike a charm. When we come to stitching results (which is potentially very\nexpensive), we have already achieved a massive reduction of work.\n\n\nBut if none of the yarns is thin at the outset, spinning qedges will not result\nin appreciable thinning of the yarns, while it might be an enormous amount of\nwork, depending on the actual relations involved.\n\n\nThe good news is that it is possible to detect those situations. Text-Fabric\nestimates whether it makes sense to spin a qedge, and if not, it will just skip\nspinning that edge. Which will make the final result gathering (stitching) more\nexpensive.\n\n\nThere is more to efficiency than this. It turns out that the strategy by which\nyou select the next qedge to be spun, influences the efficiency. In general, it\nis best to always start with the thinnest yarns, and select edges that affect\nthem. Also here there is complication: not every qedge is equally expensive when\ncomputed over a yarn. It might be better to compute a cheaper edge over a\nthicker yarn.\n\n\nStitching\n\u00b6\n\n\n\n\nThe last step is actually getting results. A result is a bunch of nodes, one\nfrom each yarn, in such a way that result nodes on yarns fulfil the\nrelationships that the qedges of the search template dictate. If we can find\nsuch a set of nodes, we have stitched the yarns together. We call such a result\na \nstitch\n. A stitch is a tuple of text nodes, each corresponding to exactly one\nqnode.\n\n\nIt is not completely trivial to find stitches, let alone to collect them\nefficiently. The general procedure is as follows:\n\n\n\n\nchoose a yarn to start with;\n\n\ntry a node in that yarn as starting point\n\n\npick a qedge from the qnode associated with the yarn (the source yarn), to\n    another qnode and consider that yarn (the target yarn),\n\n\nfind a node in the target yarn that is in the right relationship with the node\n    selected in the source yarn,\n\n\nand so on, until all qedges have been used,\n\n\nif all has gone well, deliver the nodes found as a result.\n\n\n\n\nLet us look to these steps in a bit more detail. There is an element of choice,\nand it is very important to study how big this element of choice is in the\nvarious stages.\n\n\nFirst we select a yarn, and in that yarn a node. Usually we have many choices\nand at least one, because result seeking only makes sense if all yarns are\nnon-empty. The third choice is the related node in the target yarn. Here we may\nencounter anything from zero, one or many choices.\n\n\nIf there are zero choices, then we know that our provisional stitching of yarns\nso far cannot be completed into a full stitching of all yarns. If we have made\nchoices to get this far, then some of these choices have not been lucky. We have\nto back-track and try other alternatives.\n\n\nIf there is just one choice, it is easy: we pick the one and only possible node\nin the target yarn, without introducing new points of choice.\n\n\nIf there are many choices, we have to try them all, one by one. Some might lead\nto a full stitch, others not.\n\n\nAn important situation to be aware of, is when a qedge leads the stitching\nprocess to a yarn, out of which a node has already been chosen by an earlier\nstep. This is very well possible, since the search template might have cycles in\nthe qedges, or multiple qedges arrive at the same qnode.\n\n\nWhen this happens, we do not have to select a target node, we only have to check\nwhether the target node that has been selected before, stands in the right\nrelationship to the current source node. The relationship, that is, which is\ndictated by the current qedge that we are at. If so, we can stitch on with other\nedges, without introducing choice points (very much like the one-choice above).\nIf the relation fails to hold, this stitch is doomed, and we have to back-track\n(very much like the zero-choice above).\n\n\nStrategy of stitching\n\u00b6\n\n\nThe steps involved in stitching as described above are clear, but less clear is\nwhat yarn we shall select to start with, and in which order we shall follow the\nedges. We need a strategy, and multiple strategies might lead to the same\nresults, albeit with varying efficiency.\n\n\nIn Text-Fabric we employ a strategy, that makes the \nnarrowest\n choices first.\nWe call a choice narrow if there are few alternatives to choose from, and broad\nif there are many alternatives.\n\n\nBy giving precedence to narrow choices, we prune larger parts of the search tree\nwhen we fail. If we are stitching, the more nodes we have gathered in our\nstitch, the greater the chance that a blocking relationship is encountered, i.e.\na relationship that should hold between the nodes gathered so far, but which in\nfact does not hold.\n\n\nSo we want to get as many nodes in our stitch as quickly as possible.\n\n\nIf our search tree is narrowly branching near the root, and broadly branching\nnear the leaves, the top \nn\n levels of the tree contain relatively few nodes. So\nwe have relatively few possibilities to stitch n nodes together, and most\nreasons to fail will happen while visiting these \nn\n levels.\n\n\nIf on the other hand our search tree is broadly branching near the root, and\nnarrowly branching near the leaves, the top \nn\n levels of the tree contain many\nnodes. We will visit many nodes and try many stitchings of length \nn\n, of which\na lot will fail.\n\n\nI have also tried a different, more complicated strategy, which is still\nimplemented, and which can be used by means of an optional argument to\n\nS.study()\n,\nbut results of this strategy were not particularly good.\n\n\nSmall-first strategy\n\u00b6\n\n\nHere is the small-first strategy in a bit more detail.\n\n\n\n\nwe choose the smallest yarn to start with;\n\n\nfor every qedge we estimate its current \nspread\n, i.e. how many targets it has\n    per source on average, relative to the current source and target yarns;\n\n\nat every step there are three kinds of qedges:\n\n\nqedges that go between qnodes of which we have already stitched the yarns\n\n\nqedges that go from a yarn that is already part of the stitch to a yarn\n    outside the stitch\n\n\nqedges that do not start at a yarn in the current stitch\n\n\n\n\n\n\nat every step,\n\n\nwe first process all qedges of type (i), in arbitrary order;\n\n\nwe select one edge with minimal spread out of type (ii) and process it;\n\n\nwe postpone all edges of type (iii);\n\n\nwe redetermine which edges are in all types.\n\n\n\n\n\n\n\n\nIt cannot happen that at the end we have not visited all qnodes and yarns,\nbecause we have assumed that our search template consists of one connected\ncomponent. Every qnode can be reached from every other through a series of\nqedges. So, as we perform step after step, as long as there are qnodes in type\n(iii), we can be sure that there are also qnodes in a path from the qnodes we\nhave visited to the type (iii) qnodes. At least one of the qnodes in that path\nwill be a type (ii) node. In the end there will no type (iii) nodes be left.\n\n\nWe have added a few more things to optimize the process.\n\n\nA relationship between a source yarn and a target yarn can also be considered in\nthe opposite direction. If its spread in the opposite direction is less than its\nspread in the normal direction, we use the opposite direction.\n\n\nSecondly, before we start stitching, we can compute the order of qedges that we\nwill use for every stitch. We then sort the qnodes according to the order by\nwhich they will be encountered when we work through the qedges. When we are\nstitching, in the midst of a partial stitch, it is always the case that we have\nstitched qnodes 1 .. \nn\n for some \nn\n, and we still have to stitch all qnodes\nabove \nn\n. That means that when we try to finish partial stitches of which an\ninitial part has been fixed, the search process will not change that initial\npart of the stitch. Only when the algorithm has exhausted all possibilities\nbased on that initial part, it will change the last node of the initial part,\nreplace it by other options, and start searching further.\n\n\nThis means that we just can maintain our partial stitch in a single list. We do\nnot have to assemble many partial stitches as separate immutable tuples.\n\n\nWe have implemented our deliver function as a generator, that walks over all\nstitch possibilities while maintaining just one current stitch. When the stitch\nhas been completely filled in, a copy of it will be yielded, after which\nback-tracking occurs, by which the current stitch will get partly undefined,\nonly to be filled up again by further searching.\n\n\nRead it all in the source code:\n\ndef stitchOn(e)\n.",
            "title": "Search"
        },
        {
            "location": "/Search/#search-design",
            "text": "",
            "title": "Search Design"
        },
        {
            "location": "/Search/#fabric-metaphor",
            "text": "The search space is a massive fabric of interconnected material. In it we\ndiscern the structures we are interested in: little pieces of fabric, also with\ninterconnected material.  When we search, we have a fabric in mind, woven from specific material, stitched\ntogether in a specific manner.  Search in Text-Fabric works exactly like this: you give a sample patch, and\nText-Fabric fetches all pieces of the big fabric that match your patch.   The textile metaphor is particularly suited for grasping the search part of\nText-Fabric, so I'm going to stick to it for a while. I have used it in the\nactual code as well, and even in the proofs that certain parts of the algorithm\nterminate and are correct. Yet it remains a metaphor, and the fit is not exact.  The basic pattern of search is this:     textile  text  example      take several fleeces  pick the nodes corresponding to a node type  word s,  phrase s,  clause s,  verse s    spin thick yarns from them  filter by feature conditions  part-of-speech=verb  gender= f   book=Genesis   vt    spin the yarns further into thin yarns  throw away nodes that do not have the right connections  feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa    stitch the yarns together with thread  build results by selecting a member for every filtered node set  word  node  123456  in  phrase  node  657890  in  clause  node  490567  in  verse  node  1403456     We will explain the stages of the fabrication process in detail.",
            "title": "Fabric metaphor"
        },
        {
            "location": "/Search/#fleece",
            "text": "A fleece corresponds with a very simple search template that asks for all\nobjects of a given type:  1 word   or  1 clause   or, asking for multiple types:  1\n2\n3\n4\n5 verse\nclause\nphrase\nlex\nword   Fleeces are the raw material from which we fabricate our search results.  Every node type, such as  word ,  sentence ,  book  corresponds to a fleece. In\nText-Fabric, every node has exactly one node type, so the whole space is neatly\ndivided into a small set of fleeces.  The most important characteristic of a fleece is it size: the number of nodes in\na node type.",
            "title": "Fleece"
        },
        {
            "location": "/Search/#spinning-thick-yarn",
            "text": "Consider search templates where we ask for specific members of a node type, by\ngiving feature constraints:  1\n2\n3\n4\n5\n6 verse book=Genesis\nclause type=rela\nphrase determined=yes\nlex id=jc/\nword number=pl vt\nvt   Every line in this search templates we call an  atom : a node type plus a\nfeature specification. The result of an atom is the set of all nodes in that\nnode type that satisfy those feature conditions. Finding the results of an atom\ncorresponds with first thing that we do with a fleece:  spin  a thick  yarn \nfrom it. Yarns in general are obtained by spinning fleeces, i.e. by filtering\nnode sets that correspond to a node type.  A search template may contain multiple atoms. Text-Fabric collects all atoms of\na template, grabs the corresponding fleeces, and spins thick yarns from them.\nFor each atom it will spin a yarn, and if there are several atoms referring to\nthe same node type, there will be several yarns spun from that fleece. This\nspinning of thick yarns out of fleeces happens in just one go. All fleeces\ntogether contain exactly all nodes, so Text-Fabric walks in one pass over all\nnodes, applies the feature conditions, and puts the nodes into the yarns\ndepending on which conditions apply.  By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical\npass requires a fraction of a second.  The most important characteristic of a yarn is its  thickness , which we define\nas the number of nodes in the yarn divided by the number of nodes in the fleece.  For example, a yarn consisting of the  book  node of Genesis only, has a\nthickness of 1/39, because there are 39 books in the fleece. A much thicker yarn\nis that of the verbs, which has a thickness of roughly 1/6. A very thin thread\nis that of the word  <CQH  (which occurs only once) with a thickness of only\n1/400,000.",
            "title": "Spinning thick yarn"
        },
        {
            "location": "/Search/#spinning-thin-yarns",
            "text": "In order to find results, we have to further narrow down the search space. In\nother words, we are going to spin our thick yarns into thinner and thinner\nyarns. Before we can do that, we should make one thing clear.",
            "title": "Spinning thin yarns"
        },
        {
            "location": "/Search/#connected-by-constraints",
            "text": "If the template above were complete, it would lead to a monstrous number of\nresults. Because a result of a template like this is any combination of verse-,\nclause-, phrase-, lex-, word nodes that individually satisfy their own atom\ncondition. So the number of results is the product of the number of results of\nthe individual atoms, which is pretty enormous. It is hard to imagine a\nsituation where these results could be consumed.  Usually, there are  constraints  active between the atoms. For example in a\ntemplate like this:  1\n2\n3\n4\n5\n6\n7\n8 1 verse book=Genesis\n2    clause type=rela\n3        phrase determined=yes\n4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l   The meaning of this template is that we look for a  verse  that   (1) claims to be in Genesis  (2) has a clause whose type is  rela  (3) which in turn has a phrase of a determined character  (4) which contains a word in the plural and that has a verbal tense (vt)   There should also be a   (6) lex object, identified by  jc/   which is connected to the rest by the constraint that   (8) the word of line 4 is contained in it.   Note that all atoms are linked by constraints into one network. In graph\ntheoretical terms: this template consists of exactly one connected component .  If this were not so, we would have in fact two independent search tasks, where\nthe result set would be the (cartesian) product of the result sets of the\nseparate components.  For example, if line 8 were missing, we would effectively search for things that\nmatch lines 1-4, and, independently, for things that match line 6. And every\nresult of the first part, combined with any result of the second part, would be\na valid result of the whole.  Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the\nsame time, let alone that it wants to waste time to generate all the results in\nthe product. You will have to fire those search tasks one by one, and it is up\nto you how you combine the results.  The upshot it:  the atoms in the search template should form a network,\nconnected by constraints .  Text-Fabric will check this, and will only work with search templates that have\nonly one connected component.",
            "title": "Connected by constraints"
        },
        {
            "location": "/Search/#terminology",
            "text": "By now we have arrived at the idea that our search template is a graph\nunderneath: what we have called  atoms  are in fact the nodes, and what we have\ncalled  constraints , are the edges.  From now on, we will call the  atoms   qnodes  and the constraints  qedges .\nThe  q  is to distinguish the nodes and the edges from the nodes and the edges\nof your dataset, the  text  nodes and  text  edges. When we use the term  nodes \nand  edges  we will always refer to  text  nodes and edges.  When we are searching, we maintain a  yarn  for every  qnode . This yarn starts\nout to be the thick yarn as described above, but we are going to thin them.  We can also see how our query templates are really  topographic : a query\ntemplate is a piece of local geography that we want to match against the data.\nFinding results is nothing else than instantiating  qnodes  of the search\ntemplate by text nodes in such a way that the  qedges  hold between the text\nedges.",
            "title": "Terminology"
        },
        {
            "location": "/Search/#spinning-a-qedge",
            "text": "So, where were we? We have spun thick threads based on the  qnodes \nindividually, but we have not done anything with the  qedges . That is going to\nchange now.  Consider this piece of search template:  1\n2\n3\n4\n5 4            w:word number=pl vt\n5\n6 l:lex id=jc/\n7\n8 w ]] l   So our  qnodes  are  w  and  l , and our  qedge  is  w ]] l . Note that a  lex \nobject is the set of all occurrences of a lexeme. So  w ]] l  says that  w  is\nembedded in  l , in other words,  w  is a slot contained in the slots of  l . It\nis nothing else than the word  w  is an instance of the lexeme  jc/ .  We will now check the pairs of (lex, word)-nodes in the text, where the lex node\nis taken from the yarn of the qnode  l , and the word from the yarn of the qnode w .  We can throw away some words from the yarn of  w , namely those words that do\nnot lie in a lexeme that is in the yarn of  l . In other words: the words that\nare not instances of lexeme  jc/  are out!  Conversely, if the lexeme  jc/  does not have occurrences in the plural and with\na verbal tense, we must kick it out of the yarn of  l , leaving no members in\nit. If a yarn gets empty, we have an early detection that the search yields no\nresults, and the whole process stops.  In our case, however, this is not so, and we continue.  This pass over the yarns at both sides of a qedge is a  spin  action. We spin\nthis qedge, and the result is that the two yarns become spun more thinly,\nhopefully.   With the yarn of words severely spun out, we are going to the next qedge, the\none between words and phrases.  1\n2 3        phrase determined=yes\n4            w:word number=pl vt   The indent is an implicit way of saying that the \"embeds\" relation  [[  holds\nbetween the  phrase  and the  word . An equivalent formulation of the template\nis  1\n2\n3\n4 p:phrase determined=yes\nw:word number=pl vt\n\np [[ w   We race along the yarn  w  of remaining words and check for each word if it is\ncontained in a phrase in the yarn of  p , the determined phrases. If it is not,\nwe throw the word out of the yarn of  w . Similarly, we can throw out some\nphrases from the yarn of  p , namely those phrases that do not contain words in\nthe yarn of  w . In other words: the phrases without plural words and verbal\ntense are also out.   We continue spinning, now between phrases and clauses.  1\n2 2    clause type=rela\n3        phrase determined=yes   Here we loose the phrases that are not contained in a clause of  type=rela , and\nwe loose all clauses that do not embed one of the few phrases left.   The last spin action corresponds with  1\n2 1 verse book=Genesis\n2    clause type=rela   So we throw away all our results if they are outside Genesis.  We end up with a set of thin yarns, severely thinned out, even. This will be a\ngood starting point for the last stage: picking members from each yarn to form\nresults. We call this  stitching  and we'll get there in a moment.",
            "title": "Spinning a qedge"
        },
        {
            "location": "/Search/#the-spread-of-a-qedge",
            "text": "A very important property of a qedge is its  spread . A qedge links a every node n  in its  from -yarn to zero, one, or more nodes in its  to -yarn. The number\nof nodes in the  to -yarn is a key property. The average number of nodes  m  in\nthe  to -yarn per linked node  n  in the  from -yarn is the  spread  of the\nedge.  A few examples:   An edge that corresponds to  ]] ,  n  embeds  m . If this edge goes from books\n    to words, then every book node  n  is linked to every one of its words. So\n    very  n  has hundreds or thousands  m s. The spread will roughly be 425,000 /\n    39 =~ 10,000  The opposite edge has a spread of exactly 1, because every word belongs to\n    exactly one book. Edges with spread 1 are very pleasant for our stitching\n    algorithm later on.  An edge corresponding to  = . These qedges are super efficient, because their\n    relation  =  is a breeze to compute, and they have always a spread 1 in both\n    directions.  An edge corresponding to  # , the node inequality relation. The relation  # \n    is still a breeze to compute, but the result is heavy: the set of all nodes\n    not equal to a given node. The spread is nearly 100% of the yarn length, in\n    both directions. These edges are not worth to spin, because if you have two\n    yarns, no node will be excluded: if you have an  n  in the  from -yarn, you\n    will always be able to find a different  n  in the  to -yarn (except when bot\n    yarns are equal, and contain just one node).  An edge corresponding to  == , the relation between nodes that are linked to\n    the same set of slots. The spread of this relation is not too big, but the\n    cost of computing it adds up quickly when applied to many cases.",
            "title": "The spread of a qedge"
        },
        {
            "location": "/Search/#spinning-all-qedges",
            "text": "Let us describe the spinning of yarns along edges in a bit more general way, and\nreflect on what it does for us.  We spin all qedges of a template. But after spinning a qedge, the yarns involved\nmay have changed. If that is the case, it makes sense to re-spin other qedges\nthat are involved in the changed yarns.  That is exactly what we do. We keep spinning, until the yarns have stabilized.  A few key questions need to be addressed:   Do the yarns stabilize?  If they stabilize, what have we got?  Is this an efficient process?",
            "title": "Spinning all qedges"
        },
        {
            "location": "/Search/#termination-of-spinning",
            "text": "Yes, spinning qedges until nothing changes any more, terminates, provided you do\nnot try to spin qedges that are up-to-date. If the yarns around an edge have not\nchanged, it does not make sense to spin that qedge. See here  for proof.",
            "title": "Termination of spinning"
        },
        {
            "location": "/Search/#what-have-we-got",
            "text": "After spinning, it is guaranteed that we have not thrown away results. All nodes\nthat are parts of valid results, are still in the yarns.  But, conversely, can it be that there are still nodes in the yarns that are not\npart of a result? Yes, that is possible.  Only when the graph of qnodes and qedges does not have a cycle, we know that all\nmembers of all yarns occur at least once in a result. See here  for proof.  Quite a few interesting queries, however, have cycles in in their graphs. So, in\nthose cases, spinning qedges will not cause the maximal narrowing down of the\nsearch space.",
            "title": "What have we got?"
        },
        {
            "location": "/Search/#efficiency",
            "text": "And that raises the question: how effective is the process of spinning qedges?  The answer is: it depends. If your qnodes have strong conditions on them, so\nthat the first yarn is already very thin, then every yarn that is connected to\nthis one by a qedge has also the chance to get very thin after spinning. In this\ncase, the combined filtering effect of all edges can produce a rapid narrowing\nof the search space.  Especially if we can implement edge spinning in an optimized way, this works\nlike a charm. When we come to stitching results (which is potentially very\nexpensive), we have already achieved a massive reduction of work.  But if none of the yarns is thin at the outset, spinning qedges will not result\nin appreciable thinning of the yarns, while it might be an enormous amount of\nwork, depending on the actual relations involved.  The good news is that it is possible to detect those situations. Text-Fabric\nestimates whether it makes sense to spin a qedge, and if not, it will just skip\nspinning that edge. Which will make the final result gathering (stitching) more\nexpensive.  There is more to efficiency than this. It turns out that the strategy by which\nyou select the next qedge to be spun, influences the efficiency. In general, it\nis best to always start with the thinnest yarns, and select edges that affect\nthem. Also here there is complication: not every qedge is equally expensive when\ncomputed over a yarn. It might be better to compute a cheaper edge over a\nthicker yarn.",
            "title": "Efficiency"
        },
        {
            "location": "/Search/#stitching",
            "text": "The last step is actually getting results. A result is a bunch of nodes, one\nfrom each yarn, in such a way that result nodes on yarns fulfil the\nrelationships that the qedges of the search template dictate. If we can find\nsuch a set of nodes, we have stitched the yarns together. We call such a result\na  stitch . A stitch is a tuple of text nodes, each corresponding to exactly one\nqnode.  It is not completely trivial to find stitches, let alone to collect them\nefficiently. The general procedure is as follows:   choose a yarn to start with;  try a node in that yarn as starting point  pick a qedge from the qnode associated with the yarn (the source yarn), to\n    another qnode and consider that yarn (the target yarn),  find a node in the target yarn that is in the right relationship with the node\n    selected in the source yarn,  and so on, until all qedges have been used,  if all has gone well, deliver the nodes found as a result.   Let us look to these steps in a bit more detail. There is an element of choice,\nand it is very important to study how big this element of choice is in the\nvarious stages.  First we select a yarn, and in that yarn a node. Usually we have many choices\nand at least one, because result seeking only makes sense if all yarns are\nnon-empty. The third choice is the related node in the target yarn. Here we may\nencounter anything from zero, one or many choices.  If there are zero choices, then we know that our provisional stitching of yarns\nso far cannot be completed into a full stitching of all yarns. If we have made\nchoices to get this far, then some of these choices have not been lucky. We have\nto back-track and try other alternatives.  If there is just one choice, it is easy: we pick the one and only possible node\nin the target yarn, without introducing new points of choice.  If there are many choices, we have to try them all, one by one. Some might lead\nto a full stitch, others not.  An important situation to be aware of, is when a qedge leads the stitching\nprocess to a yarn, out of which a node has already been chosen by an earlier\nstep. This is very well possible, since the search template might have cycles in\nthe qedges, or multiple qedges arrive at the same qnode.  When this happens, we do not have to select a target node, we only have to check\nwhether the target node that has been selected before, stands in the right\nrelationship to the current source node. The relationship, that is, which is\ndictated by the current qedge that we are at. If so, we can stitch on with other\nedges, without introducing choice points (very much like the one-choice above).\nIf the relation fails to hold, this stitch is doomed, and we have to back-track\n(very much like the zero-choice above).",
            "title": "Stitching"
        },
        {
            "location": "/Search/#strategy-of-stitching",
            "text": "The steps involved in stitching as described above are clear, but less clear is\nwhat yarn we shall select to start with, and in which order we shall follow the\nedges. We need a strategy, and multiple strategies might lead to the same\nresults, albeit with varying efficiency.  In Text-Fabric we employ a strategy, that makes the  narrowest  choices first.\nWe call a choice narrow if there are few alternatives to choose from, and broad\nif there are many alternatives.  By giving precedence to narrow choices, we prune larger parts of the search tree\nwhen we fail. If we are stitching, the more nodes we have gathered in our\nstitch, the greater the chance that a blocking relationship is encountered, i.e.\na relationship that should hold between the nodes gathered so far, but which in\nfact does not hold.  So we want to get as many nodes in our stitch as quickly as possible.  If our search tree is narrowly branching near the root, and broadly branching\nnear the leaves, the top  n  levels of the tree contain relatively few nodes. So\nwe have relatively few possibilities to stitch n nodes together, and most\nreasons to fail will happen while visiting these  n  levels.  If on the other hand our search tree is broadly branching near the root, and\nnarrowly branching near the leaves, the top  n  levels of the tree contain many\nnodes. We will visit many nodes and try many stitchings of length  n , of which\na lot will fail.  I have also tried a different, more complicated strategy, which is still\nimplemented, and which can be used by means of an optional argument to S.study() ,\nbut results of this strategy were not particularly good.",
            "title": "Strategy of stitching"
        },
        {
            "location": "/Search/#small-first-strategy",
            "text": "Here is the small-first strategy in a bit more detail.   we choose the smallest yarn to start with;  for every qedge we estimate its current  spread , i.e. how many targets it has\n    per source on average, relative to the current source and target yarns;  at every step there are three kinds of qedges:  qedges that go between qnodes of which we have already stitched the yarns  qedges that go from a yarn that is already part of the stitch to a yarn\n    outside the stitch  qedges that do not start at a yarn in the current stitch    at every step,  we first process all qedges of type (i), in arbitrary order;  we select one edge with minimal spread out of type (ii) and process it;  we postpone all edges of type (iii);  we redetermine which edges are in all types.     It cannot happen that at the end we have not visited all qnodes and yarns,\nbecause we have assumed that our search template consists of one connected\ncomponent. Every qnode can be reached from every other through a series of\nqedges. So, as we perform step after step, as long as there are qnodes in type\n(iii), we can be sure that there are also qnodes in a path from the qnodes we\nhave visited to the type (iii) qnodes. At least one of the qnodes in that path\nwill be a type (ii) node. In the end there will no type (iii) nodes be left.  We have added a few more things to optimize the process.  A relationship between a source yarn and a target yarn can also be considered in\nthe opposite direction. If its spread in the opposite direction is less than its\nspread in the normal direction, we use the opposite direction.  Secondly, before we start stitching, we can compute the order of qedges that we\nwill use for every stitch. We then sort the qnodes according to the order by\nwhich they will be encountered when we work through the qedges. When we are\nstitching, in the midst of a partial stitch, it is always the case that we have\nstitched qnodes 1 ..  n  for some  n , and we still have to stitch all qnodes\nabove  n . That means that when we try to finish partial stitches of which an\ninitial part has been fixed, the search process will not change that initial\npart of the stitch. Only when the algorithm has exhausted all possibilities\nbased on that initial part, it will change the last node of the initial part,\nreplace it by other options, and start searching further.  This means that we just can maintain our partial stitch in a single list. We do\nnot have to assemble many partial stitches as separate immutable tuples.  We have implemented our deliver function as a generator, that walks over all\nstitch possibilities while maintaining just one current stitch. When the stitch\nhas been completely filled in, a copy of it will be yielded, after which\nback-tracking occurs, by which the current stitch will get partly undefined,\nonly to be filled up again by further searching.  Read it all in the source code: def stitchOn(e) .",
            "title": "Small-first strategy"
        }
    ]
}