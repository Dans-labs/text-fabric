{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Text-Fabric \u00b6 About \u00b6 Before diving head-on into Text-Fabric, you might want to read a bit more about what it is and where it came from. And after it, if you want to cite it, use this DOI: 10.5281/zenodo.592193 . Intro Text-Fabric is several things: a browser for ancient text corpora a Python3 package for processing ancient corpora a tool to contribute additional research data to corpora A corpus of ancient texts and linguistic annotations represents a large body of knowledge. Text-Fabric makes that knowledge accessible to non-programmers by means of a built-in a search interface that runs in your browser. From there the step to program your own analytics is not so big anymore. You can export your results to Excel and work with them from there. And if that is not enough, you can call the Text-Fabric API from your Python programs. This works really well in Jupyter notebooks. Apps When the generic functions of TF are not enough, apps come into play. TF offers an API for apps that contain custom code for the display of specific corpora. Apps can also make use of functions to retrieve online data that has been added later as the results of research. current TF apps app-api implementation Capabilities Search for patterns The search API works with search templates that define relational patterns which will be instantiated by nodes and edges of the big fabric of the corpus. Pick and choose data Students can selectively load the feature data they need. When the time comes to share the fruits of their thought, they can do so in various ways: when using the TF browser, results can be exported as a PDF plus a zip archive of tab separated files and stored in a repository; when programming in a notebook, these notebooks can easily be shared online by using GitHub or NBViewer. Contributing data Researchers can easily produce new data features of text-fabric data out of their findings. They can package their new data into modules and distribute it to GitHub. Other people can use that data just by mentioning the GitHub location. Text-Fabric will auto-load it for them. Factory Text-Fabric can be and has been used to construct websites, for example SHEBANQ . In the case of SHEBANQ, data has been converted to mysql databases. However, with the built-in TF kernel , it is also possible to TF itself as a database to serve multiple connections and requests. Design principles There are a number of things that set Text-Fabric apart from most other ways to encode corpora. Minimalistic model Text-Fabric is based on a minimalistic data model for text plus annotations. A defining characteristic is that Text-Fabric stores text as a bunch of features in plain text files. These features are interpreted against a graph of nodes and edges, which make up the abstract fabric of the text. A graph is a more general concept than a tree. Whilst trees are ubiquitous in linguistic analysis, there is much structure in a corpus that is not strictly tree-like. Therefore, we do not adopt technologies that have the tree as their first class data model. Hence, almost by definition, Text-Fabric does not make use of XML technology. Performance matters Based on this model, Text-Fabric offers a processing API to search, navigate and process text and its annotations. A lot of care has been taken to make this API work as fast as possible. Efficiency in data processing has been a design criterion from the start. Comparisons See e.g. the comparisons between the Text-Fabric way of serializing (pickle + gzip) and avro , joblib , and marshal . Code organization and statistics To get an impression of the software that is Text-Fabric, in terms of organization and size, see Code and lines . History The foundational ideas derive from work done in and around the ETCBC avant-la-lettre from 1970 onwards by Eep Talstra, Crist-Jan Doedens, ( Ph.D. thesis ), Henk Harmsen, Ulrik Sandborg-Petersen ( Emdros ), and many others. I entered in that world in 2007 as a DANS employee, doing a joint small data project, and a bigger project SHEBANQ in 2013/2014. In 2013 I developed LAF-Fabric as a tool for constructing the website SHEBANQ . House cleaning LAF-Fabric is based on the ISO standard Linguistic Annotation Framework (LAF) . LAF is an attempt to marry graph models to the Text Encoding Initiative (TEI) which lives in XML. It is a good try, but it turns out that using XML technology for graphs is a pain. All the usual advantages of using the XML toolchain evaporate. So I decided to leave XML and its associated syntactical complexity. While I was at it, I took out everything that makes LAF-Fabric complicated and all things that are not essential for the sake of raw data processing. That became Text-Fabric version 1 at the end of 2016. It turned out that this move has freed the way to work towards higher-level goals: a new search engine (inspired by MQL and support for research data workflows. Version 7 capitalizes on the latter. State of the art Time moves on, and nowhere is that felt as keenly as in computing science. Programming has become easier, humanists become better programmers, and personal computers have become powerful enough to do a sizable amount of data science on them. That leads to exciting tipping points : In sociology, a tipping point is a point in time when a group - or a large number of group members \u2014 rapidly and dramatically changes its behavior by widely adopting a previously rare practice. WikiPedia Text-Fabric is an attempt to tip the scales by providing digital humanists with the functions they need now , based on technology that appeals now . Hence, my implementation of Text-Fabric search has been done from the ground up, and uses a strategy that is very different from Ulrik's MQL search engine. Author and acknowledgements Author : Dirk Roorda Text-Fabric is a matter of putting a few good ideas by others into practice. Co-creation In version 7, the idea of co-creation becomes fully tangible: Text-Fabric does not only work with a few curated corpora, but it allows you to add your own data in a seamless way. So you when you do research, you have the fruits of many people's work at your finger tips. Acknowledgements While I wrote most of the code, a product like Text-Fabric is unthinkable without the contributions of avid users that take the trouble to give feedback and file issues, and have the zeal and stamina to hold on when things are frustrating and bugs overwhelming, and give encouragement when they are happy. In particular I thank Andrea Scharnhorst Cale Johnson Camil Staps Christian H\u00f8ygaard-Jensen Christiaan Erwich Cody Kingham Ernst Boogert Eliran Wong Gyusang Jin Henk Harmsen James Cu\u00e9nod Johan de Joode Kyoungsik Kim Martijn Naaijer Stephen Ku Wido van Peursen Getting started \u00b6 Installation Use Advanced Documentation \u00b6 There is extensive documentation here. If you start using the Text-Fabric API in your programs, you'll definitely need it. If you are just starting with the Text-Fabric browser, consult the Search guide first. It might also help to look at the online tutorials for the app-supported annotated corpora to see what Text-Fabric can reveal about the data. Concepts The conceptual model of Text-Fabric and how it is realized in a data model and an optimized file format. data model file format optimizations search design API Reference Text-Fabric offers lots of functionality that works for all corpora. Corpus designers can add apps to Text-Fabric that enhance its behaviours, especially in displaying the corpus in ways that make sense to people that study the corpus. TF apps TF api See also Corpora Papers Papers (preprints on arxiv ), most of them published: Coding the Hebrew Bible Parallel Texts in the Hebrew Bible, New Methods and Visualizations The Hebrew Bible as Data: Laboratory - Sharing - Experiences (preprint: arxiv ) LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible Annotation as a New Paradigm in Research Archiving Presentation Here is a motivational presentation , given just before SBL 2016 in the Lutheran Church of San Antonio.","title":"Home"},{"location":"#text-fabric","text":"","title":"Text-Fabric"},{"location":"#about","text":"Before diving head-on into Text-Fabric, you might want to read a bit more about what it is and where it came from. And after it, if you want to cite it, use this DOI: 10.5281/zenodo.592193 . Intro Text-Fabric is several things: a browser for ancient text corpora a Python3 package for processing ancient corpora a tool to contribute additional research data to corpora A corpus of ancient texts and linguistic annotations represents a large body of knowledge. Text-Fabric makes that knowledge accessible to non-programmers by means of a built-in a search interface that runs in your browser. From there the step to program your own analytics is not so big anymore. You can export your results to Excel and work with them from there. And if that is not enough, you can call the Text-Fabric API from your Python programs. This works really well in Jupyter notebooks. Apps When the generic functions of TF are not enough, apps come into play. TF offers an API for apps that contain custom code for the display of specific corpora. Apps can also make use of functions to retrieve online data that has been added later as the results of research. current TF apps app-api implementation Capabilities Search for patterns The search API works with search templates that define relational patterns which will be instantiated by nodes and edges of the big fabric of the corpus. Pick and choose data Students can selectively load the feature data they need. When the time comes to share the fruits of their thought, they can do so in various ways: when using the TF browser, results can be exported as a PDF plus a zip archive of tab separated files and stored in a repository; when programming in a notebook, these notebooks can easily be shared online by using GitHub or NBViewer. Contributing data Researchers can easily produce new data features of text-fabric data out of their findings. They can package their new data into modules and distribute it to GitHub. Other people can use that data just by mentioning the GitHub location. Text-Fabric will auto-load it for them. Factory Text-Fabric can be and has been used to construct websites, for example SHEBANQ . In the case of SHEBANQ, data has been converted to mysql databases. However, with the built-in TF kernel , it is also possible to TF itself as a database to serve multiple connections and requests. Design principles There are a number of things that set Text-Fabric apart from most other ways to encode corpora. Minimalistic model Text-Fabric is based on a minimalistic data model for text plus annotations. A defining characteristic is that Text-Fabric stores text as a bunch of features in plain text files. These features are interpreted against a graph of nodes and edges, which make up the abstract fabric of the text. A graph is a more general concept than a tree. Whilst trees are ubiquitous in linguistic analysis, there is much structure in a corpus that is not strictly tree-like. Therefore, we do not adopt technologies that have the tree as their first class data model. Hence, almost by definition, Text-Fabric does not make use of XML technology. Performance matters Based on this model, Text-Fabric offers a processing API to search, navigate and process text and its annotations. A lot of care has been taken to make this API work as fast as possible. Efficiency in data processing has been a design criterion from the start. Comparisons See e.g. the comparisons between the Text-Fabric way of serializing (pickle + gzip) and avro , joblib , and marshal . Code organization and statistics To get an impression of the software that is Text-Fabric, in terms of organization and size, see Code and lines . History The foundational ideas derive from work done in and around the ETCBC avant-la-lettre from 1970 onwards by Eep Talstra, Crist-Jan Doedens, ( Ph.D. thesis ), Henk Harmsen, Ulrik Sandborg-Petersen ( Emdros ), and many others. I entered in that world in 2007 as a DANS employee, doing a joint small data project, and a bigger project SHEBANQ in 2013/2014. In 2013 I developed LAF-Fabric as a tool for constructing the website SHEBANQ . House cleaning LAF-Fabric is based on the ISO standard Linguistic Annotation Framework (LAF) . LAF is an attempt to marry graph models to the Text Encoding Initiative (TEI) which lives in XML. It is a good try, but it turns out that using XML technology for graphs is a pain. All the usual advantages of using the XML toolchain evaporate. So I decided to leave XML and its associated syntactical complexity. While I was at it, I took out everything that makes LAF-Fabric complicated and all things that are not essential for the sake of raw data processing. That became Text-Fabric version 1 at the end of 2016. It turned out that this move has freed the way to work towards higher-level goals: a new search engine (inspired by MQL and support for research data workflows. Version 7 capitalizes on the latter. State of the art Time moves on, and nowhere is that felt as keenly as in computing science. Programming has become easier, humanists become better programmers, and personal computers have become powerful enough to do a sizable amount of data science on them. That leads to exciting tipping points : In sociology, a tipping point is a point in time when a group - or a large number of group members \u2014 rapidly and dramatically changes its behavior by widely adopting a previously rare practice. WikiPedia Text-Fabric is an attempt to tip the scales by providing digital humanists with the functions they need now , based on technology that appeals now . Hence, my implementation of Text-Fabric search has been done from the ground up, and uses a strategy that is very different from Ulrik's MQL search engine. Author and acknowledgements Author : Dirk Roorda Text-Fabric is a matter of putting a few good ideas by others into practice. Co-creation In version 7, the idea of co-creation becomes fully tangible: Text-Fabric does not only work with a few curated corpora, but it allows you to add your own data in a seamless way. So you when you do research, you have the fruits of many people's work at your finger tips. Acknowledgements While I wrote most of the code, a product like Text-Fabric is unthinkable without the contributions of avid users that take the trouble to give feedback and file issues, and have the zeal and stamina to hold on when things are frustrating and bugs overwhelming, and give encouragement when they are happy. In particular I thank Andrea Scharnhorst Cale Johnson Camil Staps Christian H\u00f8ygaard-Jensen Christiaan Erwich Cody Kingham Ernst Boogert Eliran Wong Gyusang Jin Henk Harmsen James Cu\u00e9nod Johan de Joode Kyoungsik Kim Martijn Naaijer Stephen Ku Wido van Peursen","title":"About"},{"location":"#getting-started","text":"Installation Use Advanced","title":"Getting started"},{"location":"#documentation","text":"There is extensive documentation here. If you start using the Text-Fabric API in your programs, you'll definitely need it. If you are just starting with the Text-Fabric browser, consult the Search guide first. It might also help to look at the online tutorials for the app-supported annotated corpora to see what Text-Fabric can reveal about the data. Concepts The conceptual model of Text-Fabric and how it is realized in a data model and an optimized file format. data model file format optimizations search design API Reference Text-Fabric offers lots of functionality that works for all corpora. Corpus designers can add apps to Text-Fabric that enhance its behaviours, especially in displaying the corpus in ways that make sense to people that study the corpus. TF apps TF api See also Corpora Papers Papers (preprints on arxiv ), most of them published: Coding the Hebrew Bible Parallel Texts in the Hebrew Bible, New Methods and Visualizations The Hebrew Bible as Data: Laboratory - Sharing - Experiences (preprint: arxiv ) LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible Annotation as a New Paradigm in Research Archiving Presentation Here is a motivational presentation , given just before SBL 2016 in the Lutheran Church of San Antonio.","title":"Documentation"},{"location":"About/Corpora/","text":"Corpora \u00b6 Corpora are usually stored in an online repository, such as GitHub or a research data archive such as DANS . Some corpora are supported by Text-Fabric apps . These apps provide a browser interface for the corpus, and they enhance the API for working with them programmatically. A TF app can also download and update the corpus data . Current TF apps \u00b6 acronym language/writing system name period description converted by athenaeus Greek Works of Athenaeus 80 - 170 Deipnosophistae Ernst Boogert banks modern english Iain M. Banks 1984 - 1987 99 words from the SF novel Consider Phlebas Dirk Roorda bhsa Hebrew Hebrew Bible 1000 BC - 900 AD Biblia Hebraica Stuttgartensia (Amstelodamensis) Dirk Roorda + ETCBC dss Hebrew Dead Sea Scrolls 300 BC - 100 AD Transcriptions with morphology based on Martin Abegg's data files Dirk Roorda, Jarod Jacobs nena Aramaic North Eastern Neo-Aramaic Corpus 2000 Nena Cambridge Cody Kingham oldbabylonian Akkadian / cuneiform Old Babylonian letters 1900 - 1600 BC Altbabylonische Briefe in Umschrift und \u00dcbersetzung Dirk Roorda, Cale Johnson peshitta Syriac Syriac Old Testament 1000 BC - 900 AD Vetus Testamentum Syriace Dirk Roorda, Hannes Vlaardingerbroek quran Arabic Quran 600 - 900 Quranic Arabic Corpus Dirk Roorda, Cornelis van Lit syrnt Syriac Syriac New Testament 0 - 1000 Novum Testamentum Syriace Dirk Roorda, Hannes Vlaardingerbroek tisch Greek New Testament 50 - 450 Greek New Testament in Tischendorf 8 th Edition Cody Kingham uruk proto-cuneiform Uruk 4000 - 3100 BC Archaic tablets from Uruk Dirk Roorda, Cale Johnson All these apps can be found in annotation on GitHub. Each repo named app- appName hosts the app named appName . Current TF corpora without an app \u00b6 acronym language/writing system name period description converted by patristics Greek Works by Church Fathers 200 - 600 see PThU Ernst Boogert Intentions \u00b6 acronym language/writing system name period description converted by oldassyrian Akkadian / cuneiform Old Assyrian documents 2000 - 1900 see About Dirk Roorda, Cale Johnson, Alba de Ridder, Martijn Kokken oldroyal Akkadian-Sumerian cuneiform Bilingual royal inscriptions 2000 - 1600 more info to come Martijn Kokken, Dirk Roorda Get apps \u00b6 Automatically Text-Fabric downloads apps from annotation automatically when you use them. So when you say in a notebook 1 use ( 'xxxx' ) Text-Fabric will fetch the xxxx app for you, if it exists. And if you use the Text-Fabric browser, and say 1 text-fabric xxxx the same thing happens. Once you have the app, Text-Fabric will use your offline copy. It will not check for newer versions by default. But if you pass -c resp check=True , Text-Fabric will check online for newer versions of the app and if there are, it will download the newest version and run it. 1 use ( 'xxxx' , check = True ) Text-Fabric will fetch the xxxx app for you, if it exists. And if you use the Text-Fabric browser, and say 1 text-fabric xxxx -c Apps end up in text-fabric-data/__apps__ relative your home directory. If you have trouble with an app xxxx , just remove the entire directory text-fabric-data/__apps__/xxxx and run either of the commands above. Get data \u00b6 Automatically Text-Fabric apps download the corpus data for you automatically. When you use the browser, it happens when you start it up. And from within a Python program, you get the data when you do the incantation . In a program (e.g. a Jupyter notebook): 1 use ( 'xxxx' ) Browser: 1 text-fabric xxxx This will also automatically upgrade your data if there are new releases. If you want to avoid upgrades, add the :local specifier. 1 use ( 'xxxx' , checkout = 'local' ) 1 text-fabric xxxx --checkout = local There are more options, see the incantation . Data ends up in text-fabric-data/ orgName / repoName relative your home directory, where orgName is the organization or person on GitHub that has the repo repoName that contains the data. The TF data is fairly compact. Size of data There might be sizable additional data for some corpora, images for example. In that case, take care to have a good internet connection when you use a TF app for the first time. Manually Corpus data of app-supported corpora reside in a GitHub repo. You can manually clone such a data repository and point Text-Fabric to that data. First, take care that your clone ends up in github/ orgName` (relative your home directory) where *orgName is the organization or person on GitHub under which you have found the repo. Then, when you invoke the app, pass the specifier :clone . This instructs Text-Fabric to look in your local GitHub clone, rather than online or in text-fabric-data , where downloaded data is stored. 1 use ( 'xxxx:clone' ) 1 text-fabric xxxx --checkout = clone In this way, you can work with data that is under your control. Size of data Cloning a data repository is more costly then letting Text-Fabric download the data. A data repository may contain several versions and representations of the data, including the their change histories. There might also be other material in the repo, such as source data, tutorials, programs. For example, the etcbc/bhsa repo is several GB, but the TF data for a specific version is only 25MB. Extra data Researchers are continually adding new insights in the form of new feature data. TF apps make it easy to use that data alongside the main data source. Read more about the data life cycle in Data More corpora \u00b6 text-fabric-data The text-fabric-data repo has some corpora that have been converted to TF, but for which no supporting TF-apps have been written.","title":"Corpora"},{"location":"About/Corpora/#corpora","text":"Corpora are usually stored in an online repository, such as GitHub or a research data archive such as DANS . Some corpora are supported by Text-Fabric apps . These apps provide a browser interface for the corpus, and they enhance the API for working with them programmatically. A TF app can also download and update the corpus data .","title":"Corpora"},{"location":"About/Corpora/#current-tf-apps","text":"acronym language/writing system name period description converted by athenaeus Greek Works of Athenaeus 80 - 170 Deipnosophistae Ernst Boogert banks modern english Iain M. Banks 1984 - 1987 99 words from the SF novel Consider Phlebas Dirk Roorda bhsa Hebrew Hebrew Bible 1000 BC - 900 AD Biblia Hebraica Stuttgartensia (Amstelodamensis) Dirk Roorda + ETCBC dss Hebrew Dead Sea Scrolls 300 BC - 100 AD Transcriptions with morphology based on Martin Abegg's data files Dirk Roorda, Jarod Jacobs nena Aramaic North Eastern Neo-Aramaic Corpus 2000 Nena Cambridge Cody Kingham oldbabylonian Akkadian / cuneiform Old Babylonian letters 1900 - 1600 BC Altbabylonische Briefe in Umschrift und \u00dcbersetzung Dirk Roorda, Cale Johnson peshitta Syriac Syriac Old Testament 1000 BC - 900 AD Vetus Testamentum Syriace Dirk Roorda, Hannes Vlaardingerbroek quran Arabic Quran 600 - 900 Quranic Arabic Corpus Dirk Roorda, Cornelis van Lit syrnt Syriac Syriac New Testament 0 - 1000 Novum Testamentum Syriace Dirk Roorda, Hannes Vlaardingerbroek tisch Greek New Testament 50 - 450 Greek New Testament in Tischendorf 8 th Edition Cody Kingham uruk proto-cuneiform Uruk 4000 - 3100 BC Archaic tablets from Uruk Dirk Roorda, Cale Johnson All these apps can be found in annotation on GitHub. Each repo named app- appName hosts the app named appName .","title":"Current TF apps"},{"location":"About/Corpora/#current-tf-corpora-without-an-app","text":"acronym language/writing system name period description converted by patristics Greek Works by Church Fathers 200 - 600 see PThU Ernst Boogert","title":"Current TF corpora without an app"},{"location":"About/Corpora/#intentions","text":"acronym language/writing system name period description converted by oldassyrian Akkadian / cuneiform Old Assyrian documents 2000 - 1900 see About Dirk Roorda, Cale Johnson, Alba de Ridder, Martijn Kokken oldroyal Akkadian-Sumerian cuneiform Bilingual royal inscriptions 2000 - 1600 more info to come Martijn Kokken, Dirk Roorda","title":"Intentions"},{"location":"About/Corpora/#get-apps","text":"Automatically Text-Fabric downloads apps from annotation automatically when you use them. So when you say in a notebook 1 use ( 'xxxx' ) Text-Fabric will fetch the xxxx app for you, if it exists. And if you use the Text-Fabric browser, and say 1 text-fabric xxxx the same thing happens. Once you have the app, Text-Fabric will use your offline copy. It will not check for newer versions by default. But if you pass -c resp check=True , Text-Fabric will check online for newer versions of the app and if there are, it will download the newest version and run it. 1 use ( 'xxxx' , check = True ) Text-Fabric will fetch the xxxx app for you, if it exists. And if you use the Text-Fabric browser, and say 1 text-fabric xxxx -c Apps end up in text-fabric-data/__apps__ relative your home directory. If you have trouble with an app xxxx , just remove the entire directory text-fabric-data/__apps__/xxxx and run either of the commands above.","title":"Get apps"},{"location":"About/Corpora/#get-data","text":"Automatically Text-Fabric apps download the corpus data for you automatically. When you use the browser, it happens when you start it up. And from within a Python program, you get the data when you do the incantation . In a program (e.g. a Jupyter notebook): 1 use ( 'xxxx' ) Browser: 1 text-fabric xxxx This will also automatically upgrade your data if there are new releases. If you want to avoid upgrades, add the :local specifier. 1 use ( 'xxxx' , checkout = 'local' ) 1 text-fabric xxxx --checkout = local There are more options, see the incantation . Data ends up in text-fabric-data/ orgName / repoName relative your home directory, where orgName is the organization or person on GitHub that has the repo repoName that contains the data. The TF data is fairly compact. Size of data There might be sizable additional data for some corpora, images for example. In that case, take care to have a good internet connection when you use a TF app for the first time. Manually Corpus data of app-supported corpora reside in a GitHub repo. You can manually clone such a data repository and point Text-Fabric to that data. First, take care that your clone ends up in github/ orgName` (relative your home directory) where *orgName is the organization or person on GitHub under which you have found the repo. Then, when you invoke the app, pass the specifier :clone . This instructs Text-Fabric to look in your local GitHub clone, rather than online or in text-fabric-data , where downloaded data is stored. 1 use ( 'xxxx:clone' ) 1 text-fabric xxxx --checkout = clone In this way, you can work with data that is under your control. Size of data Cloning a data repository is more costly then letting Text-Fabric download the data. A data repository may contain several versions and representations of the data, including the their change histories. There might also be other material in the repo, such as source data, tutorials, programs. For example, the etcbc/bhsa repo is several GB, but the TF data for a specific version is only 25MB. Extra data Researchers are continually adding new insights in the form of new feature data. TF apps make it easy to use that data alongside the main data source. Read more about the data life cycle in Data","title":"Get data"},{"location":"About/Corpora/#more-corpora","text":"text-fabric-data The text-fabric-data repo has some corpora that have been converted to TF, but for which no supporting TF-apps have been written.","title":"More corpora"},{"location":"About/Faq/","text":"FAQ \u00b6 It does not work. Why? Stay up to date! \u00b6 Always use the latest version of Text-Fabric, because there is still a lot of development going on. A working installation contains three parts that are updated occasionally, sometinmes slowly, other times rapidly: text-fabric itself, the Python library that you obtained by pip3 install text-fabric ; TF apps, the apps that are specialized in a specific corpus; you obtained it when you said text-fabric appName or A = use(appName) ; TF data, which was downloaded by that same statement that downloaded the app. See Install for instructions how to upgrade these things. Latest Text-Fabric \u00b6 I have installed Text-Fabric, yet I get complaints that it cannot be found! Most likely, you installed Text-Fabric into another Python than you use when you run your Python programs. See Python Setup below. Why do I not get the latest version of Text-Fabric? When you get errors doing pip3 install text-fabric , there is probably an older version around. You have to say 1 pip3 install --upgrade text-fabric If this still does not download the most recent version of text-fabric , it may have been caused by caching. Then say: 1 pip3 install --upgrade --no-cache-dir text-fabric You can check what the newest distributed version of Text-Fabric is on PyPi . Why do I still not get the latest version of Text-Fabric!?!? Old versions on your system might get in the way. Sometimes pip3 uninstall text-fabric fails to remove all traces of Text-Fabric. Here is how you can remove them manually: locate the bin directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/bin (Windows Anaconda) C:\\Users\\You\\Anaconda3\\Scripts Remove the file text-fabric from this directory if it exists. locate the site-packages directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages Remove the subdirectory tf from this location, plus all files with text-fabric in the name. After this, you can make a fresh install of text-fabric : 1 pip3 install text-fabric Python setup \u00b6 If you are new to Python, it might be tricky to set up Python the right way. If you make unlucky choices, and work with trial and error, things might get messed up. Most of the times when text-fabric does not appear to work, it is because of this. Here are some hints to recover from that. Older versions do not get away! Older versions of Python may be in the way. The following hygeneic measures are known to be beneficial: When you have upgraded Python, remove PATH statements for older versions from your system startup files. For the Macos: look at .bashrc , .bash_profile in your home directory. For Windows: on the command prompt, say echo %path% to see what the content of your PATH variable is. If you see references to older versions of python than you actually work with, they need to be removed. Here is how Only for Python3 Do not remove references to Python 2.* , but only outdated Python 3.* versions. Text-Fabric browser \u00b6 I get an Internal Server Error ! When the TF browser opens with an Internal Server error, the most likely reason is that the TF kernel has not started up without errors. Look back at the terminal or command prompt where you started text-fabric . Error If somewhere down the road you see Error , I offer you my apologies! Copy and paste that error and send it to me , and I'll fix it as soon as I can, and I let you know on the issue list . Out of memory If TF has run out of memory, you might be able to do something about it. In this case, during loading TF did not have access too enough RAM memory. Maybe you had too many programs (or browser tabs) open at that time. Close as many programs as possible (even better, restart your machine) and try again. TF is know to work on Windows 10 machines with only 3GB RAM on board, but only in the best of circumstances. If your machine has 4GB of RAM, it should be possible to run TF, with care.","title":"Faq"},{"location":"About/Faq/#faq","text":"It does not work. Why?","title":"FAQ"},{"location":"About/Faq/#stay-up-to-date","text":"Always use the latest version of Text-Fabric, because there is still a lot of development going on. A working installation contains three parts that are updated occasionally, sometinmes slowly, other times rapidly: text-fabric itself, the Python library that you obtained by pip3 install text-fabric ; TF apps, the apps that are specialized in a specific corpus; you obtained it when you said text-fabric appName or A = use(appName) ; TF data, which was downloaded by that same statement that downloaded the app. See Install for instructions how to upgrade these things.","title":"Stay up to date!"},{"location":"About/Faq/#latest-text-fabric","text":"I have installed Text-Fabric, yet I get complaints that it cannot be found! Most likely, you installed Text-Fabric into another Python than you use when you run your Python programs. See Python Setup below. Why do I not get the latest version of Text-Fabric? When you get errors doing pip3 install text-fabric , there is probably an older version around. You have to say 1 pip3 install --upgrade text-fabric If this still does not download the most recent version of text-fabric , it may have been caused by caching. Then say: 1 pip3 install --upgrade --no-cache-dir text-fabric You can check what the newest distributed version of Text-Fabric is on PyPi . Why do I still not get the latest version of Text-Fabric!?!? Old versions on your system might get in the way. Sometimes pip3 uninstall text-fabric fails to remove all traces of Text-Fabric. Here is how you can remove them manually: locate the bin directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/bin (Windows Anaconda) C:\\Users\\You\\Anaconda3\\Scripts Remove the file text-fabric from this directory if it exists. locate the site-packages directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages Remove the subdirectory tf from this location, plus all files with text-fabric in the name. After this, you can make a fresh install of text-fabric : 1 pip3 install text-fabric","title":"Latest Text-Fabric"},{"location":"About/Faq/#python-setup","text":"If you are new to Python, it might be tricky to set up Python the right way. If you make unlucky choices, and work with trial and error, things might get messed up. Most of the times when text-fabric does not appear to work, it is because of this. Here are some hints to recover from that. Older versions do not get away! Older versions of Python may be in the way. The following hygeneic measures are known to be beneficial: When you have upgraded Python, remove PATH statements for older versions from your system startup files. For the Macos: look at .bashrc , .bash_profile in your home directory. For Windows: on the command prompt, say echo %path% to see what the content of your PATH variable is. If you see references to older versions of python than you actually work with, they need to be removed. Here is how Only for Python3 Do not remove references to Python 2.* , but only outdated Python 3.* versions.","title":"Python setup"},{"location":"About/Faq/#text-fabric-browser","text":"I get an Internal Server Error ! When the TF browser opens with an Internal Server error, the most likely reason is that the TF kernel has not started up without errors. Look back at the terminal or command prompt where you started text-fabric . Error If somewhere down the road you see Error , I offer you my apologies! Copy and paste that error and send it to me , and I'll fix it as soon as I can, and I let you know on the issue list . Out of memory If TF has run out of memory, you might be able to do something about it. In this case, during loading TF did not have access too enough RAM memory. Maybe you had too many programs (or browser tabs) open at that time. Close as many programs as possible (even better, restart your machine) and try again. TF is know to work on Windows 10 machines with only 3GB RAM on board, but only in the best of circumstances. If your machine has 4GB of RAM, it should be possible to run TF, with care.","title":"Text-Fabric browser"},{"location":"About/Install/","text":"Install \u00b6 Text Fabric is a Python(3) package on the Python Package Index, so you can install it directly with pip3 or pip from the command line. Prerequisites \u00b6 Computer Your computer should be a 64-bit machine and it needs at least 3 GB RAM memory. It should run Linux, Macos, or Windows. close other programs When you run the Text-Fabric browser for the first time, make sure that most of that minimum of 3GB RAM is actually available, and not in use by other programs. Python \u00b6 3.6+ 64 bit Install or upgrade Python on your system to at least version 3.6.3. Go for the 64-bit version. Otherwise Python may not be able to address all the memory it needs. Distro The leanest install is provided by python.org . You can also install it from anaconda.com . on Windows? Choose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix, you need to have additional developers' tools installed. The Anaconda distribution has Jupyter out of the box. When installing Python, make sure that the installer adds the path to Python to your environment variables. Install Chrome of Firefox and set it as your default browser. The Text-Fabric browser does not display well in Microsoft Edge, for Edge does not support the details element. on Linux? On Ubuntu 18.04 Python 3.6 is already installed. But you have to install pip3 in order to add the Python package text-fabric. 1 sudo apt install python3-pip Text-Fabric \u00b6 Install Text-Fabric: \u00b6 1 pip3 install text-fabric On Windows: 1 pip install text-fabric to 3 or not to 3? From now on we always use python3 and pip3 in instructions. On Windows you have to say python and pip . There are more differences, when you go further with programming. Advice: if you are serious on programming, consider switching to a Unix -like platform, such as Linux or the Mac (macos). on Linux? On Ubuntu the text-fabric script ends up in your ~/.local/bin directory, but this is not on your PATH. You need to execute your .profile file first by: 1 source ~/.profile You need to do this every time when you open a new terminal and want to run Text-Fabric. If you get tired of this, you can add this to your .bashrc file: 1 2 PATH = \"~/.local/bin: ${ PATH } \" export PATH Upgrade Text-Fabric: \u00b6 1 pip3 install --upgrade text-fabric TF apps and data \u00b6 In order to work with a corpus you need its data. For some corpora there is a TF app, which takes care of downloading and updating that data. Corpora tells you how to install and update a TF app and get the corpus data. Jupyter notebook \u00b6 Optionally install Jupyter as well: 1 2 3 ```sh pip3 install jupyter ``` Jupyter Lab Jupyter lab is a nice context to work with Jupyter notebooks. Recommended for working with the the tutorials of Text-Fabric. Also when you want to copy and paste cells from one notebook to another. 1 2 pip3 install jupyterlab jupyter labextension install jupyterlab-toc The toc-extension is handy to get an overview when working with the lengthy tutorial. It will create an extra tab in the Jupyter Lab interface with a table of contents of the current notebook. Node version Jupyter lab is an interesting off-spring from the marriage between the Python world and the Javascript world. It is still in beta, and there are rough edges. In order to install lab extensions you need to have Node installed.","title":"Install"},{"location":"About/Install/#install","text":"Text Fabric is a Python(3) package on the Python Package Index, so you can install it directly with pip3 or pip from the command line.","title":"Install"},{"location":"About/Install/#prerequisites","text":"Computer Your computer should be a 64-bit machine and it needs at least 3 GB RAM memory. It should run Linux, Macos, or Windows. close other programs When you run the Text-Fabric browser for the first time, make sure that most of that minimum of 3GB RAM is actually available, and not in use by other programs.","title":"Prerequisites"},{"location":"About/Install/#python","text":"3.6+ 64 bit Install or upgrade Python on your system to at least version 3.6.3. Go for the 64-bit version. Otherwise Python may not be able to address all the memory it needs. Distro The leanest install is provided by python.org . You can also install it from anaconda.com . on Windows? Choose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix, you need to have additional developers' tools installed. The Anaconda distribution has Jupyter out of the box. When installing Python, make sure that the installer adds the path to Python to your environment variables. Install Chrome of Firefox and set it as your default browser. The Text-Fabric browser does not display well in Microsoft Edge, for Edge does not support the details element. on Linux? On Ubuntu 18.04 Python 3.6 is already installed. But you have to install pip3 in order to add the Python package text-fabric. 1 sudo apt install python3-pip","title":"Python"},{"location":"About/Install/#text-fabric","text":"","title":"Text-Fabric"},{"location":"About/Install/#install-text-fabric","text":"1 pip3 install text-fabric On Windows: 1 pip install text-fabric to 3 or not to 3? From now on we always use python3 and pip3 in instructions. On Windows you have to say python and pip . There are more differences, when you go further with programming. Advice: if you are serious on programming, consider switching to a Unix -like platform, such as Linux or the Mac (macos). on Linux? On Ubuntu the text-fabric script ends up in your ~/.local/bin directory, but this is not on your PATH. You need to execute your .profile file first by: 1 source ~/.profile You need to do this every time when you open a new terminal and want to run Text-Fabric. If you get tired of this, you can add this to your .bashrc file: 1 2 PATH = \"~/.local/bin: ${ PATH } \" export PATH","title":"Install Text-Fabric:"},{"location":"About/Install/#upgrade-text-fabric","text":"1 pip3 install --upgrade text-fabric","title":"Upgrade Text-Fabric:"},{"location":"About/Install/#tf-apps-and-data","text":"In order to work with a corpus you need its data. For some corpora there is a TF app, which takes care of downloading and updating that data. Corpora tells you how to install and update a TF app and get the corpus data.","title":"TF apps and data"},{"location":"About/Install/#jupyter-notebook","text":"Optionally install Jupyter as well: 1 2 3 ```sh pip3 install jupyter ``` Jupyter Lab Jupyter lab is a nice context to work with Jupyter notebooks. Recommended for working with the the tutorials of Text-Fabric. Also when you want to copy and paste cells from one notebook to another. 1 2 pip3 install jupyterlab jupyter labextension install jupyterlab-toc The toc-extension is handy to get an overview when working with the lengthy tutorial. It will create an extra tab in the Jupyter Lab interface with a table of contents of the current notebook. Node version Jupyter lab is an interesting off-spring from the marriage between the Python world and the Javascript world. It is still in beta, and there are rough edges. In order to install lab extensions you need to have Node installed.","title":"Jupyter notebook"},{"location":"About/News/","text":"Changes in this major version \u00b6 Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off all possibilities. See the app-specific tutorials in annotation . Auto-update When TF apps have been updated, they will be autoloaded to the newest version provided you call the app as follows: In a program: 1 use ( 'appName' , ... ) Calling the browser 1 text-fabric appName This will get you the newest stable version. To get the newest unstable version: 1 use ( 'appName:hot' , ... ) 1 text-fabric appName:hot What's going on See the issue list on GitHub . Queued for next release: Support for workflows where TF data is exported to be annotated by other tools whose results are to be imported as TF features. The first step has been set: the Recorder. 7.8.12 \u00b6 2019-07-24 Fix a bug spotted by Robert Voogdgeert: in search templates with qunatifiers: if the line before the quantifier is not an atom line but a feature line, TF crashes. Not anymore. The fix is at the syntactical level of queries. I have tested most known queries and they gave identical results as before. 7.8.11 \u00b6 2019-07-23 Following a suggestion by Camil Staps: In search templates, the comment sign % does not have to be at the start of a line, it may also be indented by white space. Still, you cannot use % to comment out trailing parts of lines after non-blank parts. 7.8.9-10 \u00b6 2019-07-11 When TF wants to fetch data from GitHub, but cannot get connection, it will give some sort of message as to why. 7.8.8 \u00b6 2019-07-05 Something new: Recorder , a device to export plain text from TF in such a way that the position of nodes in that text is stored. Then you can annotate the plain text in some tool, e.g. BRAT, and after that, the Recorder can turn those annotations into TF features. It is not documented yet, but this notebook shows you a complete examnple. 7.8.7 \u00b6 2019-07-03 Fixed adding multiple click events in the javascript of the TF browser. 7.8.6 \u00b6 2019-07-02 Unmentionable fixes. 7.8.5 \u00b6 2019-06-21 Added fonts for the upcoming NENA corpus with TF app by Cody Kingham. Updated docs for app writers. 7.8.4 \u00b6 2019-06-14 All queries go a tad faster. Additional small fixes. 7.8.3 \u00b6 2019-06-13 Performance tweaks in querying. Especially long running queries perform better. The query planning can now handle multiple relationships of the kind a < b and b < c . Formerly, every b after a was searched, including the ones after c , and they then failed. Now the ones after c are not tried anymore. Yet the gain is not as high as I had hoped, because finding the right b -s between a and b turns out to be tricky. The machinery for getting that in place and then walking in the right direction worked, but was so costly itself, that it defeated the purpose of a performance gain. Have a look at some profiling results . 7.8.2 \u00b6 2019-06-11 The performance of the new feature comparison relations turned out to be bad. They have been greatly improved. Now they are workable. But it is possible that new cases will turn up with a bad performance. 7.8.1 \u00b6 2019-06-10 Main thing in this release: new relations in queries, based on feature comparison, as asked for by Oliver Glanz. For more info: see #50 Key examples: 1 2 3 phrase word .nu. word which gives the pairs of words in phrases that agree in nu (= grammatical number), provided both words are marked for number. 1 2 3 phrase word .nu#nu. word which gives the pairs of words in phrases that disagree in nu , provided both words are marked for number. 1 2 3 phrase word .nu=prs_nu. word which gives the pairs of words in phrases of which the number of the first word agrees with the number of the pronominal suffix of the second word, provided feature nu is present on the first word and feature prs_nu is present on the second word. These are only examples, the new relations work for any combination of node features. You can also test on > and < if the node features are integer valued. And for string valued features, you can also reduce the values before comparing by means of a regular expression, which specifies the parts of the value that will be stripped. See also the docs , jump to Based on node features . The working of silent=True has been fine-tuned (i.e. it is easier to silence TF in more cases.) There is also a silent parameter for the walker conversion . The info() function always checks whether it should be silent or not. There is a new warning() function that is silent if silent='deep' . So you can use warning() to issue messages that you do not want to be silenced by silent=True . 7.8 \u00b6 2019-05-30 Compose \u00b6 The biggest addition is a new tf.compose package with operators to manipulate TF data sets: combine() and modify() . See specs and the compose chapter in the Banks tutorial, where you can see it in action on (2 copies of) the nice little 100-word example corpus. Minor goodies: New TF.loadAll() function to load all features in one go. New method items() for all features, which yields all pairs in the mapping of the feature one by one. See [../Api/Features.md#generics-for-features]. 7.7.11 \u00b6 2019-05-27 Small fixes: tweaks in edge spinning (part of the search engine), but no real performance improvements nothing in TF relies on Python's glob module anymore, which turned out to miss file names with characters such as [ ] in it. 7.7.10 \u00b6 2019-05-23 Fixed a bug in fabric.py spotted by Ernst Boogert, where there was a confusion between sections and structure If a TF-app needs to import its own modules, there is the risk of conflicts when several TF-apps get loaded in the same program and they import modules with the same name. TF offers a function loadModule() by which an app can dynamically load a module, and this function makes sure that the imported module gets an app-dependent internal name. 7.7.9 \u00b6 2019-05-21 Some queries perform much better now. Especially the ones with == (same slots), && (overlapping slots), and :: (same boundaries). The performance of the machinery has been tuned with new parameters, and all BHSA queries in the tutorials have been tested. There was a pair of queries in searchGaps that either took 9 seconds or 40, randomly. Now it is consistently 9 seconds. See searchRough at the end where the performance parameters are tweaked. 7.7.6-8 \u00b6 2019-05-20 New functions cv.active() and cv.activeTypes() in the walker conversion (requested by Ernst Boogert). 7.7.5 \u00b6 2019-05-18 Another 20% of the original memory footprint has been shaved off. Method: using arrays instead of tuples for sequences of integers. 7.7.4 \u00b6 2019-05-16 Optimization: the memory footprint of the features has been reduced by ca 30%. Method: reusing readonly objects with the same value. The BHSA now needs 2.2 GB of RAM, instead of the 3.4 before. Bug fixes: * silent means silent again in A.use() * the walk converter will not stop if there is no structure configured 7.7.3 \u00b6 2019-05-13 Added more checks for the new structure API when using the walk converter. Made the pre-computing for structure more robust. 7.7.2 \u00b6 2019-05-12 The T API has been extended with structure types . Structure types is a flexible sectioning system with unlimited levels. It can be configured next to the more rigid sections that T already supported. The rigid system is meant to be used by the TF browser for chunking up the material in decent portions. The new, flexible system is meant to reflect the structure of the corpus, and will give you means to navigate the copus accordingly. Quick examples: banks . Documentation: structure . 7.7.1 \u00b6 2019-05-10 You can ask the meta data of any feature by TF.features['featureName'].metaData . That is not new. You can get it also by F.featureName.meta , for node features and E.featureName.meta for edge features. Both only work for loaded features. This is a bit more crisp. Thanks to Ernst Boogert for bringing this up. In the TF browser, in the control where you select a book/document/scroll: the chosen item disappeared from the view if you narrowed down the list by typing a capital letter. Fixed. 7.7.0 \u00b6 2019-05-08 Big improvement on T.text() . It now accepts one or more nodes of arbitrary types and produces text for them all. Largely backward compatible, in that: it takes the same arguments when it produced sensisble results, it will produce the same results when it produced nothing, it will now produce sensible things, in many cases. You have to use the descend parameter a lot less. See the docs 7.6.8 \u00b6 2019-05-02 There is an extra cv.occurs() function to check whether a feature actually occurs in the result data. cv.meta(feature) without more arguments deletes the feature from the metadata, 7.6.7 \u00b6 2019-04-27 Added the option force=True to the cv.walk() function, to continue conversion after errors. 7.6.5-6 \u00b6 2019-04-26 Added punctation geresh and gershayim to the Hebrew mapping from unicode to ETCBC transcription. The ETCBC transcription only mapped the accents but not the punctuation characters of these. Fixed a bug in cv.meta() in the conversion walker. 7.6.4 \u00b6 2019-04-25 The walker conversion module has an extra check: if you assign features to None, it will be reported. There is an extra cv.meta() function to accomodate a use case brought in by Ernst Boogert. 7.6.3 \u00b6 2019-04-14 Small addition to search templates. You could already use edges in search by means of the relational operator 1 -edgeFeature> that look for n and m such that there is an edgeFeature edge from n to m , and likewise 1 <edgeFeature- for edges in the opposite direction. Now you can also use 1 <edgeFeature> that look for n and m such that there is an edgeFeature edge from n to m , or from m to n , or both. See the docs This corresponds to E. edgeFeature .b() . See also the Banks example . 7.6.2 \u00b6 2019-04-12 Small but important fix in the display logic of the pretty() function. The bug is not in the particular TF-apps that partly implementt pretty() , but in the generic tf.applib.display library that implements the other part. Thanks to Gyusang Jin, Christiaan Erwich and Cody Kingham for spottting it. I wrote an account of the bug and its fixing in this notebook . 7.6.1 \u00b6 2019-04-10 Small fix in reporting of the location of data being used. 7.6 \u00b6 2019-04-09 Simplified sharing: pushing to GitHub is enough. It is still recommended to make a release on GitHub now and them, but it is not necessary. The use() function and the calling of the TF browser undergo an API change. API addition: \u00b6 When calling up data and a TF-app, you can go back in history: to previous releases and previous commits, using a checkout parameter. You can specify the checkout parameter separately for the TF-app code (so you can go back to previous instantiations of the TF-app) the main data of the app plus its standard data modules every data-module that you include by means of the --mod= parameter. The values of the checkout parameters tell you to use data that is: clone : locally present under ~/github in the appropriate place local : locally present under ~/text-fabric-data in the appropriate place latest : from the latest online release hot : from the latest online commit '' : (default): from the latest online release, or if there are no releases, from the latest online commit 2387abc78f9de... : a concrete commit hash found on GitHub (under Commits) v1.3 : a release tag found on GitHub (under Releases) Or consult the repo notebook. API deletion (backwards incompatible): \u00b6 The parameters check=... and lgc=... of use() and -lgc and -c when calling the TF browser have been removed. These parameters were all-or-nothing, they were applied TF app code, main data, and included data modules. Advice \u00b6 In most cases, just do not use the checkout parameters at all. Then the TF-app will be kept updated, and you keep using the newest data. If you want to producing fixed output, not influenced by future changes, run TF once with a particular version or commit, and after that supply the value local as long as you wish. If you are developing data yourself, place the data in your repository under ~/github , and use the value clone for checkout. Sharing \u00b6 If you create your own features and want to share them, it is no longer needed to zip the data and attach it to a newly created release on GitHub. Just pushing your repo to GitHub is sufficient. Still it is a good practice to make a release every now and then. Even then, you do not need to attach your data as a binary. But, if you have much data or many files, doing so makes the downloading more efficient for the users. checkoutRepo() \u00b6 There is a new utility function checkoutRepo() , by which you can maintain a local copy of any subdirectory of any repo on Github. See Repo This is yet another step in making your scholarly work reproducible. Fix in query parsing \u00b6 Queries like 1 2 sentence <: sentence caused TF to complain erroneously about disconnected components. You had to say 1 2 3 s1:sentence s2:sentence s1 <: s2 instead. That workaround is not needed anymore. Thanks to Oliver Glanz for mentioning this behaviour. 7.5.4 \u00b6 2019-03-28 The TF browser now displays the total number of results clearly. 7.5.3 \u00b6 2019-03-27 Small fix in Excel export when called by the TF kernel. 7.5.2 \u00b6 2019-03-26 Small fix: a TF app that did not define its own text-formats caused an error. Now the generic TF applib is robust against this. 7.5.1 \u00b6 2019-03-14 Modified E.feature.b() so that it gives precedence to outgoing edges. Further tweaks in layout of plain() . 7.5 \u00b6 2019-03-13 API addition for E (edges): E.feature.b() gives the symmetrical closure of the edges under feature . That means it combines the results of E.feature.f() and E.feature.t() . In plain speech: E.feature.b(m) collects the nodes that have an incoming edge from m and the nodes that have an outgoing edge to m . 7.4.11 \u00b6 2019-03-11 TF.save() an now write to any absolute location by means of the optional parameter location . 7.4.10 \u00b6 2019-03-10 The markdown display in online notebooks showed many spurious </span> . This is a bug in the Markdown renderer used by Github and NBViewer. It happens if table cells have doubly nested <span> elements. It did not show up in local notebooks. In order to avoid it, TF does no longer work with the Markdown renderer. Instead, it produces output in HTML and uses the HTML renderer in notebooks. That fixes the issue. When using A.export() to export data to Excel-friendly CSV files, some node types will get their text exported, and some just a label. It depended on whether the node type was a section or not. Now it depends on whether the node type is small or big. We export text for small node types. A node type is small if it is not bigger than the condense type. This behaviour is now the same as for pretty displays. 7.4.9 \u00b6 2019-03-08 Changes in font handling New flag in pretty() : full=False . See the docs 7.4.8 \u00b6 2019-03-07 When looking for data in lgc=True mode, TF will report clearly when data cannot be found in local github clones. In such cases TF will look for an online release of the repo with the desired data attached. Before it was not clear enough that TF was looking online, despite the lgc flag, because of missing data. So if you misspelled a module path, you got messages that did not point you to the root cause. Some fixes in the plain display having to do with the passage label. 7.4.7 \u00b6 2019-02-28 When converting a new corpus, Old Babylonian Letters (cuneiform), I tuned the conversion module a bit. Several improvements in the conversion program. Better warnings for potential problems. Several other small changes have been applied here and there. 7.4.6 \u00b6 2019-02-07 When querying integer valued features with inequality conditions, such as 1 word level>0 an unpleasant error was raised if not all words have a level, or if some words have level None . That has been fixed now. Missing values and None values always cause the > and < comparisons to be False . 7.4.5 \u00b6 2019-01-31 Bug fix in data pre-computation. The bug was introduced in version 7.4.2 . If you have been running that version or a newer one, you might need to recompute your features. Here is how. Manually: delete the .tf directory in ~/github/.../.../tf/version or in ~/text-fabric-data/.../.../tf/version/ . This directory is hidden on the Mac and Linux and you can make it visible by pressing Cmd+Shift+. on the Mac, or you can navigate to this directory in a terminal and do ls -al (Mac and Linux). The other method can be used in a Jupyter notebook: 1 2 3 from tf.app import Fabric A = use(...) TF.clearCache After this, restart the notebook, and run it again, except the TF.clearCache . If you are still pre 7.4.2, you're out of trouble. You can upgrade to 7.4.5 7.4.4 \u00b6 2019-01-30 Added checks to the converter for section structure. 7.4.3 \u00b6 2019-01-30 A much simpler implementation of conversions from source data to Text-Fabric. Especially the code that the conversion writer has to produce is simplified. 7.4.1-2 \u00b6 2019-01-29 Small fixes in the token converter. 7.4 \u00b6 2019-01-25 Easier conversion of data sources to TF: via an intermediate token stream. For more info: see #45 7.3.14-15 \u00b6 2019-01-16 Make sure it works. 7.3.13 \u00b6 2019-01-16 Feature display within pretty displays: a newline in a feature value will cause a line break in the display, by means of a <br/> element. 7.3.12 \u00b6 2019-01-16 Small fix in oslots validation. You can save a data set without the oslots feature (a module). The previous release wrongly flagged a oslots validation error because of a missing oslots feature. That has been remedied. 7.3.11 \u00b6 2019-01-16 If the oslots feature is not valid, weird error messages used to occur when TF tried to load a dataset containing it. The oslots feature was loaded, but the computing of derived data threw a deep error. Not anymore. When TF saves the oslots feature it checks whether it is valid: It should map all non-slot nodes and only non-slot nodes to slots. So, right after you have converted a data source to TF you can check whether the oslots is valid, during TF.save() . And further down the line, if you somehow have let a faulty oslots pass, and try to load a dataset containing such a oslots feature, TF checks whether the range of nodes mapped by oslots does not have holes in it. If so, it generates a clear error and stops processing. 7.3.10 \u00b6 2019-01-10 Moved the app tutorials from the annotation/app-appName repos into a new annotation/tutorials repo. The reason: the app-appName are used for downloading the app code. It should not be burdened with extra material, which is also often updated, giving rise to many spurious redownloads of the app code. Additionally, for education purposes it is handy to have the tutorials for all apps inside one repo. For example, to use in a Microsoft Azure environment. 7.3.9 \u00b6 2019-01-09 Better browsing for corpora with very many top level sections, such as Uruk. For more info: see #38 7.3.8 \u00b6 2019-01-07 Small fix. 7.3.7 \u00b6 2019-01-07 Small fixes in the core: the Text API can now work with corpora with only two levels of sections, such as the Quran. 7.3.6 \u00b6 2019-01-04 Arabic transcription functions 7.3.5 \u00b6 2018-12-19 TF-browser: Fixed a performance bottleneck in showing passages. The computation of the highlights took too much time if the query in question has many results. 7.3.4 \u00b6 2018-12-18 In the plain() representation NBconvert has a glitch. We can prevent that by directly outputting the plain representation as HTML, instead of going through Markdown. Fixed that. 7.3.3 \u00b6 2018-12-17 The TF browser could not fiund its templates, because I had forgotten to include the template files in the Python package. (More precisely, I had renamed the templates folder from views , which was included, to templates , and I had forgotten to adapt the MANIFEST.in file). 7.3.1 \u00b6 2018-12-14 Glitch in the Uruk app: it imports other modules, but because of the dynamic way it is imported itself, a trick is needed to let it import its submodules correctly. 2018-12-13 7.3 \u00b6 2018-12-13 Text-Fabric has moved house from Dans-labs to annotation on GitHub. The TF-apps have been moved to separate repos with name app- xxxx within annotation The tutorials have been moved from the repos that store the corpus data to the app - xxxx repositories. 7.2.3 \u00b6 2018-12-13 The TF-browser exports an Excel export of results. Now you can also export to Excel from a notebook, using A.export(results) . Jump to the tutorial: exportExcel For more info: see #38 7.2.2 \u00b6 2018-12-12 Web framework: Bottle => Flask The dependency on Bottle as webserver has been replaced by Flask because Bottle is lagging behind in support for Python 3.7. Plain display in Uruk The plain display of lines and cases now outputs their ATF source, instead of merely line 1 or case a . ??? abstract \"Further code reorganization Most Python files are now less than 200 lines, although there is still a code file of more than 1000 lines. 7.2.1 \u00b6 2018-12-10 Fix broken links to the documentation of the TF API members, after the incantation. Fix in the Uruk lineart option: it could not be un-checked. 7.2 \u00b6 2018-12-08 TF Browser The TF kernel/server/website is also fit to be served over the internet There is query result highlighting in passage view (like in SHEBANQ) Various tweaks TF app API prettySetup() has been replaced with displaySetup() and displayReset() , by which you can configure a whole bunch of display parameters selectively. Display All display functions ( pretty plain prettyTuple plainTuple show table ) accept a new optional parameter withPassage which will add a section label to the display. This parameter can be regulated in displaySetup . Display A.search() accepts a new optional parameter: sort=... by which you can ask for canonically sorted results ( True ), custom sorted results (pass your onw key function), or unsorted results ( False ). A.search New functions A.nodeFromSectionStr() and A.sectionStrFromNode() which give the passage string of any kind of node, if possible. Section support for apps The function A.plain() now responds to the highlights parameter: you can highlight material inside plain displays. A.plain and display tutorial New function T.sectionTuple(n) which gives the tuple of section nodes in which n is embedded T.sectionTuple Modified function T.sectionFromNode(n, fillup=False) It used to give a tuple (section1, section2, section3), also for nodes of type section1 and section2 (like book and chapter). The new behaviour is the same if fillup=True . But if fillup=False (default), it returns a 1-tuple for section1 nodes and a 2-tuple for section2 nodes. T.sectionFromNode New API member sortKeyTuple to sort tuples of nodes in the canonical ordering. sortKeyTuple The code to detect the file name and path of the script/notebook you are running in, is inherently brittle. It is unwise to base decisions on that. This code has been removed from TF. So TF no longer knows whether you are in a notebook or not. And it will no longer produce links to the online notebook on GitHub or NBViewer. Various other fixes Documentation The entry points and paths from superficial to in-depth information have been adapted. Writing docs is an uphill battle. Under the hood As TF keeps growing, the need arises over and over again to reorganize the code, weed out duplicate pieces of near identical functionality, and abstract from concrete details to generic patterns. This release has seen a lot of that. 7.1.1 \u00b6 2018-11-21 Queries in the TF browser are limited to three minutes, after that a graceful error message is shown. Other small fixes. 7.1 \u00b6 2018-11-19 You can use custom sets in queries in the TF browser Reorganized the docs of the individual apps, took the common parts together New functions writeSets and readSets in tf.lib 7.0.3 \u00b6 2018-11-17 In the BHSA, feature values on the atom-types and subphrases are now shown too, and that includes extra features from foreign data sets The feature listing after the incantation in a notebook now lists the loaded modules in a meaningful order. 7.0.2 \u00b6 2018-11-16 Small fixes in text-fabric-zip Internal reorgnization of the code Documentation updates (but internal docs are still lagging behind) 7.0.1 \u00b6 2018-11-15 Fixed messages and logic in finding data and checking for updates (thanks to feedback of Christian H\u00f8ygaard-Jensen) Fixed issue #30 Improved the doc links under features after the incantation. Typos in the documentation 7.0.0 \u00b6 2018-11-14 Just before SBL Denver, two years after SBL San Antonio, where I started writing Text-Fabric, here is major version 7. Here is what is new: you can call in \"foreign data\": tf feature files made by yourself and other researchers; the foreign data shows up in the text-fabric browser; all features that are used in a query, show up in the pretty displays in the TF browser, also the foreign features; there is a command to prepare your own data for distribution via GitHub; the incantantion is simpler, but ut has changed in a backwards-incompatible way; after the incantation, for each feature it is shown where it comes from. Under the hood: apps (bhsa, peshitta, syrnt, uruk) have been refactored thoroughly; a lot of repeated code inside apps has been factored out it is easier to turn corpora into new text-fabric apps. Quick start: the new share See the advanced guide for concrete and detailed hints how to make most of this version.","title":"News"},{"location":"About/News/#changes-in-this-major-version","text":"Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off all possibilities. See the app-specific tutorials in annotation . Auto-update When TF apps have been updated, they will be autoloaded to the newest version provided you call the app as follows: In a program: 1 use ( 'appName' , ... ) Calling the browser 1 text-fabric appName This will get you the newest stable version. To get the newest unstable version: 1 use ( 'appName:hot' , ... ) 1 text-fabric appName:hot What's going on See the issue list on GitHub . Queued for next release: Support for workflows where TF data is exported to be annotated by other tools whose results are to be imported as TF features. The first step has been set: the Recorder.","title":"Changes in this major version"},{"location":"About/News/#7812","text":"2019-07-24 Fix a bug spotted by Robert Voogdgeert: in search templates with qunatifiers: if the line before the quantifier is not an atom line but a feature line, TF crashes. Not anymore. The fix is at the syntactical level of queries. I have tested most known queries and they gave identical results as before.","title":"7.8.12"},{"location":"About/News/#7811","text":"2019-07-23 Following a suggestion by Camil Staps: In search templates, the comment sign % does not have to be at the start of a line, it may also be indented by white space. Still, you cannot use % to comment out trailing parts of lines after non-blank parts.","title":"7.8.11"},{"location":"About/News/#789-10","text":"2019-07-11 When TF wants to fetch data from GitHub, but cannot get connection, it will give some sort of message as to why.","title":"7.8.9-10"},{"location":"About/News/#788","text":"2019-07-05 Something new: Recorder , a device to export plain text from TF in such a way that the position of nodes in that text is stored. Then you can annotate the plain text in some tool, e.g. BRAT, and after that, the Recorder can turn those annotations into TF features. It is not documented yet, but this notebook shows you a complete examnple.","title":"7.8.8"},{"location":"About/News/#787","text":"2019-07-03 Fixed adding multiple click events in the javascript of the TF browser.","title":"7.8.7"},{"location":"About/News/#786","text":"2019-07-02 Unmentionable fixes.","title":"7.8.6"},{"location":"About/News/#785","text":"2019-06-21 Added fonts for the upcoming NENA corpus with TF app by Cody Kingham. Updated docs for app writers.","title":"7.8.5"},{"location":"About/News/#784","text":"2019-06-14 All queries go a tad faster. Additional small fixes.","title":"7.8.4"},{"location":"About/News/#783","text":"2019-06-13 Performance tweaks in querying. Especially long running queries perform better. The query planning can now handle multiple relationships of the kind a < b and b < c . Formerly, every b after a was searched, including the ones after c , and they then failed. Now the ones after c are not tried anymore. Yet the gain is not as high as I had hoped, because finding the right b -s between a and b turns out to be tricky. The machinery for getting that in place and then walking in the right direction worked, but was so costly itself, that it defeated the purpose of a performance gain. Have a look at some profiling results .","title":"7.8.3"},{"location":"About/News/#782","text":"2019-06-11 The performance of the new feature comparison relations turned out to be bad. They have been greatly improved. Now they are workable. But it is possible that new cases will turn up with a bad performance.","title":"7.8.2"},{"location":"About/News/#781","text":"2019-06-10 Main thing in this release: new relations in queries, based on feature comparison, as asked for by Oliver Glanz. For more info: see #50 Key examples: 1 2 3 phrase word .nu. word which gives the pairs of words in phrases that agree in nu (= grammatical number), provided both words are marked for number. 1 2 3 phrase word .nu#nu. word which gives the pairs of words in phrases that disagree in nu , provided both words are marked for number. 1 2 3 phrase word .nu=prs_nu. word which gives the pairs of words in phrases of which the number of the first word agrees with the number of the pronominal suffix of the second word, provided feature nu is present on the first word and feature prs_nu is present on the second word. These are only examples, the new relations work for any combination of node features. You can also test on > and < if the node features are integer valued. And for string valued features, you can also reduce the values before comparing by means of a regular expression, which specifies the parts of the value that will be stripped. See also the docs , jump to Based on node features . The working of silent=True has been fine-tuned (i.e. it is easier to silence TF in more cases.) There is also a silent parameter for the walker conversion . The info() function always checks whether it should be silent or not. There is a new warning() function that is silent if silent='deep' . So you can use warning() to issue messages that you do not want to be silenced by silent=True .","title":"7.8.1"},{"location":"About/News/#78","text":"2019-05-30","title":"7.8"},{"location":"About/News/#compose","text":"The biggest addition is a new tf.compose package with operators to manipulate TF data sets: combine() and modify() . See specs and the compose chapter in the Banks tutorial, where you can see it in action on (2 copies of) the nice little 100-word example corpus. Minor goodies: New TF.loadAll() function to load all features in one go. New method items() for all features, which yields all pairs in the mapping of the feature one by one. See [../Api/Features.md#generics-for-features].","title":"Compose"},{"location":"About/News/#7711","text":"2019-05-27 Small fixes: tweaks in edge spinning (part of the search engine), but no real performance improvements nothing in TF relies on Python's glob module anymore, which turned out to miss file names with characters such as [ ] in it.","title":"7.7.11"},{"location":"About/News/#7710","text":"2019-05-23 Fixed a bug in fabric.py spotted by Ernst Boogert, where there was a confusion between sections and structure If a TF-app needs to import its own modules, there is the risk of conflicts when several TF-apps get loaded in the same program and they import modules with the same name. TF offers a function loadModule() by which an app can dynamically load a module, and this function makes sure that the imported module gets an app-dependent internal name.","title":"7.7.10"},{"location":"About/News/#779","text":"2019-05-21 Some queries perform much better now. Especially the ones with == (same slots), && (overlapping slots), and :: (same boundaries). The performance of the machinery has been tuned with new parameters, and all BHSA queries in the tutorials have been tested. There was a pair of queries in searchGaps that either took 9 seconds or 40, randomly. Now it is consistently 9 seconds. See searchRough at the end where the performance parameters are tweaked.","title":"7.7.9"},{"location":"About/News/#776-8","text":"2019-05-20 New functions cv.active() and cv.activeTypes() in the walker conversion (requested by Ernst Boogert).","title":"7.7.6-8"},{"location":"About/News/#775","text":"2019-05-18 Another 20% of the original memory footprint has been shaved off. Method: using arrays instead of tuples for sequences of integers.","title":"7.7.5"},{"location":"About/News/#774","text":"2019-05-16 Optimization: the memory footprint of the features has been reduced by ca 30%. Method: reusing readonly objects with the same value. The BHSA now needs 2.2 GB of RAM, instead of the 3.4 before. Bug fixes: * silent means silent again in A.use() * the walk converter will not stop if there is no structure configured","title":"7.7.4"},{"location":"About/News/#773","text":"2019-05-13 Added more checks for the new structure API when using the walk converter. Made the pre-computing for structure more robust.","title":"7.7.3"},{"location":"About/News/#772","text":"2019-05-12 The T API has been extended with structure types . Structure types is a flexible sectioning system with unlimited levels. It can be configured next to the more rigid sections that T already supported. The rigid system is meant to be used by the TF browser for chunking up the material in decent portions. The new, flexible system is meant to reflect the structure of the corpus, and will give you means to navigate the copus accordingly. Quick examples: banks . Documentation: structure .","title":"7.7.2"},{"location":"About/News/#771","text":"2019-05-10 You can ask the meta data of any feature by TF.features['featureName'].metaData . That is not new. You can get it also by F.featureName.meta , for node features and E.featureName.meta for edge features. Both only work for loaded features. This is a bit more crisp. Thanks to Ernst Boogert for bringing this up. In the TF browser, in the control where you select a book/document/scroll: the chosen item disappeared from the view if you narrowed down the list by typing a capital letter. Fixed.","title":"7.7.1"},{"location":"About/News/#770","text":"2019-05-08 Big improvement on T.text() . It now accepts one or more nodes of arbitrary types and produces text for them all. Largely backward compatible, in that: it takes the same arguments when it produced sensisble results, it will produce the same results when it produced nothing, it will now produce sensible things, in many cases. You have to use the descend parameter a lot less. See the docs","title":"7.7.0"},{"location":"About/News/#768","text":"2019-05-02 There is an extra cv.occurs() function to check whether a feature actually occurs in the result data. cv.meta(feature) without more arguments deletes the feature from the metadata,","title":"7.6.8"},{"location":"About/News/#767","text":"2019-04-27 Added the option force=True to the cv.walk() function, to continue conversion after errors.","title":"7.6.7"},{"location":"About/News/#765-6","text":"2019-04-26 Added punctation geresh and gershayim to the Hebrew mapping from unicode to ETCBC transcription. The ETCBC transcription only mapped the accents but not the punctuation characters of these. Fixed a bug in cv.meta() in the conversion walker.","title":"7.6.5-6"},{"location":"About/News/#764","text":"2019-04-25 The walker conversion module has an extra check: if you assign features to None, it will be reported. There is an extra cv.meta() function to accomodate a use case brought in by Ernst Boogert.","title":"7.6.4"},{"location":"About/News/#763","text":"2019-04-14 Small addition to search templates. You could already use edges in search by means of the relational operator 1 -edgeFeature> that look for n and m such that there is an edgeFeature edge from n to m , and likewise 1 <edgeFeature- for edges in the opposite direction. Now you can also use 1 <edgeFeature> that look for n and m such that there is an edgeFeature edge from n to m , or from m to n , or both. See the docs This corresponds to E. edgeFeature .b() . See also the Banks example .","title":"7.6.3"},{"location":"About/News/#762","text":"2019-04-12 Small but important fix in the display logic of the pretty() function. The bug is not in the particular TF-apps that partly implementt pretty() , but in the generic tf.applib.display library that implements the other part. Thanks to Gyusang Jin, Christiaan Erwich and Cody Kingham for spottting it. I wrote an account of the bug and its fixing in this notebook .","title":"7.6.2"},{"location":"About/News/#761","text":"2019-04-10 Small fix in reporting of the location of data being used.","title":"7.6.1"},{"location":"About/News/#76","text":"2019-04-09 Simplified sharing: pushing to GitHub is enough. It is still recommended to make a release on GitHub now and them, but it is not necessary. The use() function and the calling of the TF browser undergo an API change.","title":"7.6"},{"location":"About/News/#api-addition","text":"When calling up data and a TF-app, you can go back in history: to previous releases and previous commits, using a checkout parameter. You can specify the checkout parameter separately for the TF-app code (so you can go back to previous instantiations of the TF-app) the main data of the app plus its standard data modules every data-module that you include by means of the --mod= parameter. The values of the checkout parameters tell you to use data that is: clone : locally present under ~/github in the appropriate place local : locally present under ~/text-fabric-data in the appropriate place latest : from the latest online release hot : from the latest online commit '' : (default): from the latest online release, or if there are no releases, from the latest online commit 2387abc78f9de... : a concrete commit hash found on GitHub (under Commits) v1.3 : a release tag found on GitHub (under Releases) Or consult the repo notebook.","title":"API addition:"},{"location":"About/News/#api-deletion-backwards-incompatible","text":"The parameters check=... and lgc=... of use() and -lgc and -c when calling the TF browser have been removed. These parameters were all-or-nothing, they were applied TF app code, main data, and included data modules.","title":"API deletion (backwards incompatible):"},{"location":"About/News/#advice","text":"In most cases, just do not use the checkout parameters at all. Then the TF-app will be kept updated, and you keep using the newest data. If you want to producing fixed output, not influenced by future changes, run TF once with a particular version or commit, and after that supply the value local as long as you wish. If you are developing data yourself, place the data in your repository under ~/github , and use the value clone for checkout.","title":"Advice"},{"location":"About/News/#sharing","text":"If you create your own features and want to share them, it is no longer needed to zip the data and attach it to a newly created release on GitHub. Just pushing your repo to GitHub is sufficient. Still it is a good practice to make a release every now and then. Even then, you do not need to attach your data as a binary. But, if you have much data or many files, doing so makes the downloading more efficient for the users.","title":"Sharing"},{"location":"About/News/#checkoutrepo","text":"There is a new utility function checkoutRepo() , by which you can maintain a local copy of any subdirectory of any repo on Github. See Repo This is yet another step in making your scholarly work reproducible.","title":"checkoutRepo()"},{"location":"About/News/#fix-in-query-parsing","text":"Queries like 1 2 sentence <: sentence caused TF to complain erroneously about disconnected components. You had to say 1 2 3 s1:sentence s2:sentence s1 <: s2 instead. That workaround is not needed anymore. Thanks to Oliver Glanz for mentioning this behaviour.","title":"Fix in query parsing"},{"location":"About/News/#754","text":"2019-03-28 The TF browser now displays the total number of results clearly.","title":"7.5.4"},{"location":"About/News/#753","text":"2019-03-27 Small fix in Excel export when called by the TF kernel.","title":"7.5.3"},{"location":"About/News/#752","text":"2019-03-26 Small fix: a TF app that did not define its own text-formats caused an error. Now the generic TF applib is robust against this.","title":"7.5.2"},{"location":"About/News/#751","text":"2019-03-14 Modified E.feature.b() so that it gives precedence to outgoing edges. Further tweaks in layout of plain() .","title":"7.5.1"},{"location":"About/News/#75","text":"2019-03-13 API addition for E (edges): E.feature.b() gives the symmetrical closure of the edges under feature . That means it combines the results of E.feature.f() and E.feature.t() . In plain speech: E.feature.b(m) collects the nodes that have an incoming edge from m and the nodes that have an outgoing edge to m .","title":"7.5"},{"location":"About/News/#7411","text":"2019-03-11 TF.save() an now write to any absolute location by means of the optional parameter location .","title":"7.4.11"},{"location":"About/News/#7410","text":"2019-03-10 The markdown display in online notebooks showed many spurious </span> . This is a bug in the Markdown renderer used by Github and NBViewer. It happens if table cells have doubly nested <span> elements. It did not show up in local notebooks. In order to avoid it, TF does no longer work with the Markdown renderer. Instead, it produces output in HTML and uses the HTML renderer in notebooks. That fixes the issue. When using A.export() to export data to Excel-friendly CSV files, some node types will get their text exported, and some just a label. It depended on whether the node type was a section or not. Now it depends on whether the node type is small or big. We export text for small node types. A node type is small if it is not bigger than the condense type. This behaviour is now the same as for pretty displays.","title":"7.4.10"},{"location":"About/News/#749","text":"2019-03-08 Changes in font handling New flag in pretty() : full=False . See the docs","title":"7.4.9"},{"location":"About/News/#748","text":"2019-03-07 When looking for data in lgc=True mode, TF will report clearly when data cannot be found in local github clones. In such cases TF will look for an online release of the repo with the desired data attached. Before it was not clear enough that TF was looking online, despite the lgc flag, because of missing data. So if you misspelled a module path, you got messages that did not point you to the root cause. Some fixes in the plain display having to do with the passage label.","title":"7.4.8"},{"location":"About/News/#747","text":"2019-02-28 When converting a new corpus, Old Babylonian Letters (cuneiform), I tuned the conversion module a bit. Several improvements in the conversion program. Better warnings for potential problems. Several other small changes have been applied here and there.","title":"7.4.7"},{"location":"About/News/#746","text":"2019-02-07 When querying integer valued features with inequality conditions, such as 1 word level>0 an unpleasant error was raised if not all words have a level, or if some words have level None . That has been fixed now. Missing values and None values always cause the > and < comparisons to be False .","title":"7.4.6"},{"location":"About/News/#745","text":"2019-01-31 Bug fix in data pre-computation. The bug was introduced in version 7.4.2 . If you have been running that version or a newer one, you might need to recompute your features. Here is how. Manually: delete the .tf directory in ~/github/.../.../tf/version or in ~/text-fabric-data/.../.../tf/version/ . This directory is hidden on the Mac and Linux and you can make it visible by pressing Cmd+Shift+. on the Mac, or you can navigate to this directory in a terminal and do ls -al (Mac and Linux). The other method can be used in a Jupyter notebook: 1 2 3 from tf.app import Fabric A = use(...) TF.clearCache After this, restart the notebook, and run it again, except the TF.clearCache . If you are still pre 7.4.2, you're out of trouble. You can upgrade to 7.4.5","title":"7.4.5"},{"location":"About/News/#744","text":"2019-01-30 Added checks to the converter for section structure.","title":"7.4.4"},{"location":"About/News/#743","text":"2019-01-30 A much simpler implementation of conversions from source data to Text-Fabric. Especially the code that the conversion writer has to produce is simplified.","title":"7.4.3"},{"location":"About/News/#741-2","text":"2019-01-29 Small fixes in the token converter.","title":"7.4.1-2"},{"location":"About/News/#74","text":"2019-01-25 Easier conversion of data sources to TF: via an intermediate token stream. For more info: see #45","title":"7.4"},{"location":"About/News/#7314-15","text":"2019-01-16 Make sure it works.","title":"7.3.14-15"},{"location":"About/News/#7313","text":"2019-01-16 Feature display within pretty displays: a newline in a feature value will cause a line break in the display, by means of a <br/> element.","title":"7.3.13"},{"location":"About/News/#7312","text":"2019-01-16 Small fix in oslots validation. You can save a data set without the oslots feature (a module). The previous release wrongly flagged a oslots validation error because of a missing oslots feature. That has been remedied.","title":"7.3.12"},{"location":"About/News/#7311","text":"2019-01-16 If the oslots feature is not valid, weird error messages used to occur when TF tried to load a dataset containing it. The oslots feature was loaded, but the computing of derived data threw a deep error. Not anymore. When TF saves the oslots feature it checks whether it is valid: It should map all non-slot nodes and only non-slot nodes to slots. So, right after you have converted a data source to TF you can check whether the oslots is valid, during TF.save() . And further down the line, if you somehow have let a faulty oslots pass, and try to load a dataset containing such a oslots feature, TF checks whether the range of nodes mapped by oslots does not have holes in it. If so, it generates a clear error and stops processing.","title":"7.3.11"},{"location":"About/News/#7310","text":"2019-01-10 Moved the app tutorials from the annotation/app-appName repos into a new annotation/tutorials repo. The reason: the app-appName are used for downloading the app code. It should not be burdened with extra material, which is also often updated, giving rise to many spurious redownloads of the app code. Additionally, for education purposes it is handy to have the tutorials for all apps inside one repo. For example, to use in a Microsoft Azure environment.","title":"7.3.10"},{"location":"About/News/#739","text":"2019-01-09 Better browsing for corpora with very many top level sections, such as Uruk. For more info: see #38","title":"7.3.9"},{"location":"About/News/#738","text":"2019-01-07 Small fix.","title":"7.3.8"},{"location":"About/News/#737","text":"2019-01-07 Small fixes in the core: the Text API can now work with corpora with only two levels of sections, such as the Quran.","title":"7.3.7"},{"location":"About/News/#736","text":"2019-01-04 Arabic transcription functions","title":"7.3.6"},{"location":"About/News/#735","text":"2018-12-19 TF-browser: Fixed a performance bottleneck in showing passages. The computation of the highlights took too much time if the query in question has many results.","title":"7.3.5"},{"location":"About/News/#734","text":"2018-12-18 In the plain() representation NBconvert has a glitch. We can prevent that by directly outputting the plain representation as HTML, instead of going through Markdown. Fixed that.","title":"7.3.4"},{"location":"About/News/#733","text":"2018-12-17 The TF browser could not fiund its templates, because I had forgotten to include the template files in the Python package. (More precisely, I had renamed the templates folder from views , which was included, to templates , and I had forgotten to adapt the MANIFEST.in file).","title":"7.3.3"},{"location":"About/News/#731","text":"2018-12-14 Glitch in the Uruk app: it imports other modules, but because of the dynamic way it is imported itself, a trick is needed to let it import its submodules correctly. 2018-12-13","title":"7.3.1"},{"location":"About/News/#73","text":"2018-12-13 Text-Fabric has moved house from Dans-labs to annotation on GitHub. The TF-apps have been moved to separate repos with name app- xxxx within annotation The tutorials have been moved from the repos that store the corpus data to the app - xxxx repositories.","title":"7.3"},{"location":"About/News/#723","text":"2018-12-13 The TF-browser exports an Excel export of results. Now you can also export to Excel from a notebook, using A.export(results) . Jump to the tutorial: exportExcel For more info: see #38","title":"7.2.3"},{"location":"About/News/#722","text":"2018-12-12 Web framework: Bottle => Flask The dependency on Bottle as webserver has been replaced by Flask because Bottle is lagging behind in support for Python 3.7. Plain display in Uruk The plain display of lines and cases now outputs their ATF source, instead of merely line 1 or case a . ??? abstract \"Further code reorganization Most Python files are now less than 200 lines, although there is still a code file of more than 1000 lines.","title":"7.2.2"},{"location":"About/News/#721","text":"2018-12-10 Fix broken links to the documentation of the TF API members, after the incantation. Fix in the Uruk lineart option: it could not be un-checked.","title":"7.2.1"},{"location":"About/News/#72","text":"2018-12-08 TF Browser The TF kernel/server/website is also fit to be served over the internet There is query result highlighting in passage view (like in SHEBANQ) Various tweaks TF app API prettySetup() has been replaced with displaySetup() and displayReset() , by which you can configure a whole bunch of display parameters selectively. Display All display functions ( pretty plain prettyTuple plainTuple show table ) accept a new optional parameter withPassage which will add a section label to the display. This parameter can be regulated in displaySetup . Display A.search() accepts a new optional parameter: sort=... by which you can ask for canonically sorted results ( True ), custom sorted results (pass your onw key function), or unsorted results ( False ). A.search New functions A.nodeFromSectionStr() and A.sectionStrFromNode() which give the passage string of any kind of node, if possible. Section support for apps The function A.plain() now responds to the highlights parameter: you can highlight material inside plain displays. A.plain and display tutorial New function T.sectionTuple(n) which gives the tuple of section nodes in which n is embedded T.sectionTuple Modified function T.sectionFromNode(n, fillup=False) It used to give a tuple (section1, section2, section3), also for nodes of type section1 and section2 (like book and chapter). The new behaviour is the same if fillup=True . But if fillup=False (default), it returns a 1-tuple for section1 nodes and a 2-tuple for section2 nodes. T.sectionFromNode New API member sortKeyTuple to sort tuples of nodes in the canonical ordering. sortKeyTuple The code to detect the file name and path of the script/notebook you are running in, is inherently brittle. It is unwise to base decisions on that. This code has been removed from TF. So TF no longer knows whether you are in a notebook or not. And it will no longer produce links to the online notebook on GitHub or NBViewer. Various other fixes Documentation The entry points and paths from superficial to in-depth information have been adapted. Writing docs is an uphill battle. Under the hood As TF keeps growing, the need arises over and over again to reorganize the code, weed out duplicate pieces of near identical functionality, and abstract from concrete details to generic patterns. This release has seen a lot of that.","title":"7.2"},{"location":"About/News/#711","text":"2018-11-21 Queries in the TF browser are limited to three minutes, after that a graceful error message is shown. Other small fixes.","title":"7.1.1"},{"location":"About/News/#71","text":"2018-11-19 You can use custom sets in queries in the TF browser Reorganized the docs of the individual apps, took the common parts together New functions writeSets and readSets in tf.lib","title":"7.1"},{"location":"About/News/#703","text":"2018-11-17 In the BHSA, feature values on the atom-types and subphrases are now shown too, and that includes extra features from foreign data sets The feature listing after the incantation in a notebook now lists the loaded modules in a meaningful order.","title":"7.0.3"},{"location":"About/News/#702","text":"2018-11-16 Small fixes in text-fabric-zip Internal reorgnization of the code Documentation updates (but internal docs are still lagging behind)","title":"7.0.2"},{"location":"About/News/#701","text":"2018-11-15 Fixed messages and logic in finding data and checking for updates (thanks to feedback of Christian H\u00f8ygaard-Jensen) Fixed issue #30 Improved the doc links under features after the incantation. Typos in the documentation","title":"7.0.1"},{"location":"About/News/#700","text":"2018-11-14 Just before SBL Denver, two years after SBL San Antonio, where I started writing Text-Fabric, here is major version 7. Here is what is new: you can call in \"foreign data\": tf feature files made by yourself and other researchers; the foreign data shows up in the text-fabric browser; all features that are used in a query, show up in the pretty displays in the TF browser, also the foreign features; there is a command to prepare your own data for distribution via GitHub; the incantantion is simpler, but ut has changed in a backwards-incompatible way; after the incantation, for each feature it is shown where it comes from. Under the hood: apps (bhsa, peshitta, syrnt, uruk) have been refactored thoroughly; a lot of repeated code inside apps has been factored out it is easier to turn corpora into new text-fabric apps. Quick start: the new share See the advanced guide for concrete and detailed hints how to make most of this version.","title":"7.0.0"},{"location":"About/Older/","text":"Older changes \u00b6 6.4.5-6 \u00b6 2018-11-08 Bug fix: Now TF can truly work if you do not have a feature text.tf in your dataset. Tests added for basic relations in search: all relations are rigorously tested, a few small bugs fixed. The comment sign in queries is now % , only at the start of a line. 6.4.3-4 \u00b6 2018-11-06 Big bug fix in queries: basic relationships in combination with custom sets. The implementation of the basic relationships did not reckon with custom sets that contains both slot nodes and non-slot nodes. And it did not trigger the right code when a custom set has only slot nodes. That has been remedied. Some of the search tutorials have been expanded to include a number of these critical cases. A more complete test suite outside the tutorials is still on my to do list. Thanks to Cody Kingham for spotting and reporting this bug. 6.4, 6.4.1-2 \u00b6 2018-11-02 A passage browsing interface that interacts with the search results. The interface scrolls to the highlighted row. Minor things: More refined warnings when you run out of memory TF checks whether you are running 64 bit Python. If not, a warning is issued. 6.3.2 \u00b6 2018-10-27 Better documentation for installation of Text-Fabric on Ubuntu. Added new module requirements: ipykernel and notebook. 6.3.1 \u00b6 2018-10-24 An optional parameter silent=False has been added to the initialisation calls of the specific app APIs: you can say now 1 A = Xxxx ( silent = True ) where Xxxx is a know corpus. and then all non-error messages will be suppressed. If the underlying TF API needs to precompute data, it will still be shown, because this may cause an otherwise unexpected delay. Since this is a releatively rare case, and since this can be remedied by running the call again, I leave this behaviour as it is. 6.3 \u00b6 2018-10-19 Character tables for Hebrew abd Syriac, with links to them from the TF browser Better font handling In the pretty and plain functions you can pass a fmt parameter, to control the text representation (original script, transcription, phonetics) You can also control the text representation in the Text-Fabric browser. 6.2.2 \u00b6 2018-10-18 Added ETCBC/WIT transcriptions to the SyrNT data source. Now both Peshitta and Syriac New Testament have ETCBC transcriptions. The older, rectangular logo makes place for the more crisp, circular one 6.2.1 \u00b6 2018-10-17 New app: Syrnt (Syriac New Testament. It works much like the Peshitta, but the SyrNT data has linguistic annotations at the word and lexeme levels. After this upgrade you can browse the SyrNT by saying text-fabric syrnt on the command line. 6.2 \u00b6 2018-10-16 New app: Peshitta. It works much like the BHSA, but there is one big difference: the current Peshitta data does not have linguistic annotations. There are just books, chapters, verses and words. We expect to add lemmatizations of words shortly. After this upgrade you can browse the peshitta by saying text-fabric peshitta on the command line. Fixed a bug in exportMQL: when there are no enumerated values, do not write out an empty CREATE ENUMERATION statement to the MQL file. 6.1 \u00b6 2018-10-12 More precise provenance data when you export results from the Text-Fabric data; Under the hood reorganization of configuration data of apps like Bhsa and Uruk; App-specific parts of the code have moved to more generic parts: a big cleanup has performed; This will make it easier to add new apps. 6.0.7-8-9 \u00b6 2018-10-11 Avoid computing the notebook name when the user passes a name for the notebook to Uruk() or Bhsa() . And when the computing needs to be done, all exceptions will be caught, because the code for determining the notebook name is brittle, and may crash if the Jupyter version does not match. Fixed the bug that the Bhsa and Uruk did not run properly outside a notebook or outside a github repo. In Bhsa and Uruk, the generated info after the incantation can be collapsed (features, API members). 6.0.6 \u00b6 2018-10-10 In the BHSA, the edge features are now shown too after the incantation. If you hoist the API members into your namespace, you will get a list of hoisted names, linked to the API documentation. 6.0.5 \u00b6 2018-10-09 When using BHSA and Uruk in a notebook, there is an even simpler incantation which auto downloads features. In the BHSA it is shown which features are loaded, with direct links to the feature docs. 6.0.4 \u00b6 2018-10-09 When using BHSA and Uruk in a notebook, there is a simpler incantation which auto downloads features. Some issues concerning paths in zipfiles of downloaded data have been solved. 6.0.3 \u00b6 Easier incantations for Bhsa() and Uruk() . It is no longer needed to pass the name of the notebook, but you can still do so: name='mynotebook' You can leave out the api argument in Bhsa() . Then you do not have to load features by means of TF.load() , Bhsa() will load a standard set of features, and if the BHSA data is missing, it will download them first. The former ways of calling Bhsa() and Uruk() are still valid. Note that all arguments have become optional. 2018-10-08 The Text-Fabric browser will always print a banner with its name and version. If you pass it the argument --help or -h or --version or -v it will show the relevant information and stop executing. 6.0.2 \u00b6 2018-10-07 The Text-Fabric browser takes it data by default from ~/text-fabric-data . It will not check local github clones for data. But if you pass the option -lgc , it will first check your local github clones. So it you do nothing special, the TF browser always works with the auto-downloaded data. 6.0.1 \u00b6 2018-10-06 Not only the core BHSA data will auto load, also the related PHONO and PARALLELS data. A new release has been made of the related data, and they are now in sync with the release of the core data. If you use auto load already, you do not have to do anything. But if you have the etcbc/phono and etcbc/parallels repos in your ~/github folder, you should do a git pull origin master on those repos. N.B. : I am contemplating to have the Text-Fabric browser always use data from ~/text-fabric-data and no longer from ~/github/etcbc . Then the TF browser always controls its own data, and it will not occur that the version of the TF browser is not compatible with the version of the TF data in your github repos, or that the main data and the related data are out of synch. The disadvantage is that if you have the github repos on your system, you get redundant data in ~/text-fabric-data . However, there is only one version kept in ~/text-fabric-data , so this is not much. 6.0 \u00b6 2018-10-05 A big update with several changes: API change: \u00b6 T.text() has got more behaviours. This change was needed for the Text-Fabric browser, in order to represent lexemes in exported files. Showcase: BHSA dictionary Here is how you can collect the BHSA lexemes in an Excel sheet. about.md RESULTSX.tsv It might also be handy for the programmers amongst you. See the updated API doc on T , expand the T.text() item. Auto update \u00b6 The Text-Fabric browser checks if you are using the most recent release of the data. Font rendering \u00b6 A font rendering issue in Safari 12 in macos Mojave prevented the use of Ezra SIL for Hebrew in notebooks. We now work around this by relying on the distribution of Ezra SIL as webfont in the font library . Additional small fixes. \u00b6 Not worth telling. update Text-Fabric To update Text-Fabric itself to version 6.0, consult Upgrade . Perform this step first, because the new TF may download the new data for you. Data update needed In order to work successfully with the new T.text() function, you need a newer release (1.4) of the BHSA data . (In fact, only one line in one feature has changed ( otext.tf ). Here is how you get the new data release: Automatically \u00b6 If previously your Text-Fabric browser has automatically downloaded the data for you, it will detect the new release and download it automatically. You do not have to do anything, except increase your patience. The download (24 MB) takes some time and after that Text-Fabric will precompute related data, which may take a few minutes. This is a one-time-step after a data update. Manually \u00b6 If you have a clone of the BHSA repository, then go to that directory and say git pull origin master . If you get error messages, then you have local changes in your local BHSA repository that conflict with the github version. Probably you have run the tutorials in place. Best thing to do is: copy your BHSA tutorial directory to somehwere else; remove your local BHSA repository entirely; decide whether you really want the whole repo back (nearly 4 GB). If not: you're done, and TF will download automatically the data it needs. If you still need it: move one directory up (into the etcbc directory) and do git clone https://github.com/etcbc/bhsa . If you want to consult the tutorials, either: view them on nbviewer ; or run them in a directory outside the BHSA repo (where you have copied it a minute ago). 5.6.4 \u00b6 2018-10-04 Solved a font-rendering issue on Safari 12 (Macos Mojave): locally installed fonts, such as Ezra SIL are not being honored. So I linked to a stylesheet of the fontlibrary which has a webfont version of Ezra SIL. That worked. 5.6.3 \u00b6 2018-10-04 Exported tab-separated files get extension .tsv instead of .csv , because then they render better in GitHub. 5.6.2 \u00b6 2018-10-04 Small optimization. More docs about reading and writing Excel compatible CSV files with Hebrew characters in it. 5.6.1 \u00b6 2018-10-04 Better exporting from TF browser: a good RESULTSX.tsv with results, sensibly augmented with information, directly openable in Excel, even when non-latin unicode code characters are present . All features that occur in the searchTemplate are drawn in into the RESULTSX.tsv, onto the nodes they filter. An additonal feature filtering is now possible in searchTemplates: feature* . This acts as \"no additional constraint\", so it does not influence the result set. But it will be picked up and used to add information into the RESULTSX.tsv. 5.5.25 \u00b6 2018-10-03 The Text-Fabric browser exports results as node lists and produces also a CONTEXT.tsv with all feature values for all nodes in the results. However, it does not contain full text representations of the nodes and it is also not possible to see in what verses the nodes occur. That has changed. The last column of CONTEXT.tsv contains the full text of a node. And there are three columns at the beginning that contain the references to the sections the node is in. For the BHSA that is the book, chapter and verse. 5.5.24 \u00b6 2018-09-25 BHSA app in Text-Fabric Browser: the book names on the verse pad should be the English book names. That is now in the help texts, including a link to the list of English book names. 5.5.23 \u00b6 2018-09-21 Problem in use of msgCache in the search engine, which caused fetch() to fail in some cases. Fixed. 5.5.22 \u00b6 2018-09-13 Fix in left-to-right displays of extra features in pretty() displays in the BHSA. 5.5.21 \u00b6 2018-08-30 Bug fix in transcription.py w.r.t. to Syriac transcriptions. 5.5.20 \u00b6 2018-08-16 BHSA app: adjusted the color of the gloss attribute: darker. 5.5.19 \u00b6 2018-07-19 Fixed: when opening files for reading and writing for an export of a TF browser session: specify that the encoding is utf8 . This is needed on those windowses where the default encoding is something else, usually cp1252 . 5.5.18 \u00b6 2018-07-19 No change, only in the build script. This is a test whether after uploading to PyPi, users can upgrade without using the --no-cache-dir in their pip commands. 5.5.17 \u00b6 2018-07-19 The main functions in kernel and web can be passed arguments, instead that they always read from sys.argv. So that it can be used packaged apps. 5.5.16 \u00b6 2018-07-17 Extra option when starting up the text-fabric web interface: -docker to let the web server listen at 0.0.0.0 instead of localhost . So that it can be used in a Docker container. 5.5.15 \u00b6 2018-07-16 Extra option when starting up the text-fabric web interface: -noweb to not start the web browser. So that it can be used in a Docker container. 5.5.13-14 \u00b6 2018-07-12 Better error reporting of quantified queries. 5.5.12 \u00b6 2018-07-11 Faster export of big csv lists. Tweaks in the web interface. Cleaner termination of processes. The concept TF data server is now called TF kernel 5.5.8-11 \u00b6 2018-07-10 Better in catching out-of-memory errors. Prevents creation of corrupt compiled binary TF data. Prevents starting the web server if the TF kernel fails to load. 5.5.7 \u00b6 2018-07-09 Optimization is export from TF browser. 5.5.6 \u00b6 2018-07-09 Better help display. The opened-state of help sections is remembered. You can open help next to an other open section in the sidebar. 5.5.5 \u00b6 2018-07-08 Crisper icon. 5.5.4 \u00b6 2018-07-6 Docs updated. Little bit of refactoring. 5.5.1-3 \u00b6 2018-07-4 In the TF browser, use a selection of all the features when working with the BHSA. Otherwise in Windows you might run out of memory, even if you have 8GB RAM. 5.5 \u00b6 2018-07-4 Text-Fabric can download data for BHSA and Uruk. You do not have to clone github repositories for that. The data downloaded by Text-Fabric ends up in text-fabric-data under your home directory. 5.4.5-7 \u00b6 2018-07-03 Experimenting with setuptools to get the text-fabric script working on Windows. 5.4.4 \u00b6 2018-07-02 Added renaming/duplicating of jobs and change of directory. 5.4.3 \u00b6 2018-06-29 Small fixes in error reporting in search. 5.4.1-2 \u00b6 2018-06-28 Text-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata. Small fixes. 5.4 \u00b6 2018-06-26 Improved interface and functionality of the text-fabric browser: you can save your work you can enter verse references and tablet P numbers there is help there is a side bar Docs not up to date The API docs are not up-to-date: there are new functions in the Bhsa and Uruk APIs. The server/kernel/client apis are not completely spelled out. However, the help for the text-fabric browser is included in the interface itself. 5.3.3 \u00b6 2018-06-23 Small fix: command line args for text-fabric. 5.3.0-2 \u00b6 2018-06-22 Better process management When the TF web interface is started, it cleans up remnant process that might get in the way otherwise. You can also say 1 text-fabric -k to kill all remnant processes, or 1 text-fabric -k corpus to kill the processes for a specific corpus only. Manual node entry You can enter nodes manually in the TF browser. Handy for quick inspection. You can click on the sequence number to append the node tuple of that row to the tuple input field. That way you can collect interesting results. Name and Description You can enter a name which will be used as title and file name during export. You can enter a description in Markdown. When you export your query, the description appears formatted on top. Provenance If you export a query, provenance is added, using DOIs. Small fixes No more blank pages due to double page breaks. 5.2.1 \u00b6 2018-06-21 Added an expand all checkbox in the text-fabric browser, to expand all shown rows or to collapse them. Export function for search results in the text-fabric browser. What you see is what you get, 1 pretty display per page if you have the browser save it to pdf. Small tweaks 5.1 \u00b6 2018-06-21 When displaying results in condensed mode, you can now choose the level of the container in which results are highlighted. So far it was fixed to verse for the bhsa and tablet for Uruk. The docs are lagging behind! But it is shown in the tutorials and you can observer it in the text-fabric browser. 5.0.1,2,3,4 \u00b6 2018-06-19 Addressed start-up problems. 5.0 \u00b6 2018-06-18 Built in web server and client for local query running. It is implemented for Bhsa and Uruk. 4.4.2,3 \u00b6 2018-06-13 New distribution method with setuptools. Text-Fabric has now dependencies on modules rpyc and bottle, because it contains a built-in TF kernel and web server. This website is still barely functional, though. 4.4.1 \u00b6 2018-06-10 Search API: Escapes in regular expression search was buggy and convoluted. If a feature value contains a | then in an RE you have to enter \\| to match it. But to have that work in a TF search, you needed to say \\\\\\| . On the other hand, in the same case for . instead of | , you could just sat \\. In the new situation you do not have to double escape in REs anymore. You can just say \\| and \\. . 4.4 \u00b6 2018-06-06 Search API: S.search() accepts a new optional parameter: withContext . It triggers the output of context information for nodes in the result tuples. 4.3.4, 4.3.5 \u00b6 2018-06-05 Search API: The /with/ /or/ /or/ /-/' quantifier is also allowed with zero /or/` s. Small fix in the /with/ quantifier if there are quantifiers between this one and its parent atom. 4.3.3 \u00b6 2018-06-01 Search API: Improved quantifiers in search: /where/ /have/ /without/ /with/ /or/ /-/ ; much clearer indentation rules (no caret anymore); better reporting by S.study() . 4.3.2 \u00b6 2018-05-31 Search API: quantifiers may use the name .. to refer to their parents you may use names in the place of atoms, which lessens the need for constructs with p = q stricter checks on the syntax and position of quantifiers 4.3.1 \u00b6 2018-05-30 Docs and metadata update 4.3.0 \u00b6 2018-05-30 API Change in Search. In search templates I recently added things like 1 word vt! which checks for words that do not have a value for feature vt . The syntax for this has now changed to 1 word vt# Unequal (#) in feature value conditions. Now you can say things like 1 word vt#infa|infc meaning that the value of feature is not one of infa , infc . So, in addition to = we have # for \"not equal\". * Quantifiers. You can now use quantifiers in search. One of them is like NOTEXIST in MQL. See the docs A number of minor fixes. 4.2.1 \u00b6 2018-05-25 Several improvements in the pretty display in Bhsa and Uruk APIs Under the hood changes in S.search() to prepare for quantifiers in search templates. Tokenisation of quantifiers already works Searches can now spawn auxiliary searches without polluting intermediate data This has been done by promoting the S API to a factory of search engines. By deafault, S creates and maintains a single factory, so to the user it is the same S . But when it needs to run a query in the middle of processing another query it can just spawn another search engine to do that, without interfering with the original search. NB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten. 4.2 \u00b6 2018-05-23 The Search API has been extended: you can use custom sets in your query templates you can search in shallow mode: instead of full result tuples, you just get a set of the top-level thing you mention in your template. This functionality is a precursor for quantifiers in search templates but is also a powerful addition to search in its own right. 4.1.2 \u00b6 2018-05-17 Bhsa and Uruk APIs: custom highlight colours also work for condensed results. you can pass the highlights parameter also to show and prettyTuple 4.1.1 \u00b6 2018-05-16 Bhsa API: you can customize the features that are shown in pretty displays. 4.1 \u00b6 2018-05-16 Bhsa and Uruk APIs: you can customize the highlighting of search results: different colours for different parts of the results you can choose your colours freely from all that CSS has to offer. See the updated search tutorials. 4.0.3 \u00b6 2018-05-11 No changes, just quirks in the update process to get a new version of TF out. 4.0.1 \u00b6 2018-05-11 Documentation updates. 4.0.0 \u00b6 2018-05-11 Additions to Search. You can now include the values of edges in your search templates. F. feature .freqList() accepts a new parameter: nodeTypes . It will restrict its results to nodes in one of the types in nodeTypes . You can now also do E. feature .freqList() . It will count the number of edges if the edge is declared to be without values, or it will give a frequency list of the edges by value if the edge has values. Like F.freqList , you can pass parameters to constrain the frequency list to certain node types. You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive ( nodeTypesTo ). New documentation system based on MkDocs . 3.4.12 \u00b6 2018-05-02 The Uruk and Bhsa APIs show the version of Text-Fabric that is being called. 3.4.11 \u00b6 2018-05-01 Uruk cases are divided horizontally and vertically, alternating with their nesting level; cases have a feature depth now, indicating at which level of nesting they are. 3.4.8-9-10 \u00b6 2018-04-30 Various small fixes, such as: Bhsa: Lexeme links in pretty displays. Uruk: Prevented spurious </div> in NbViewer. 3.4.7 \u00b6 Uruk: Modified local image names 3.4.6 \u00b6 Small tweaks in search. 3.4.5 \u00b6 2018-04-28 Bhsa API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() . 3.4.4 \u00b6 2018-04-27 Uruk API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() . 3.4.2 \u00b6 2018-04-26 Better search documentation. Uruk API: small fixes. 3.4.1 \u00b6 2018-04-25 Bhsa API: Search/show: you can now show results condensed: i.e. a list of passages with highlighted results is returned. This is how SHEBANQ represents the results of a query. If you have two results in the same verse, with condensed=True you get one verse display with two highlights, with condensed=False you get two verse displays with one highlight each. Uruk API: Search/show: the pretty , prettyTuple , show functions of the Bhsa API have bee translated to the Uruk API. You can now get very pretty displays of search results. 3.4 \u00b6 2018-04-23 Search : You can use regular expressions to specify feature values in queries. You could already search for nodes which have a non-None value for a certain feature. Now you can also search for the complement: nodes that do not have a certain feature. Bhsa API: The display of query results also works with lexeme nodes. 3.3.4 \u00b6 2018-04-20 Uruk API: Better height and width control for images. Leaner captions. 3.3.3 \u00b6 2018-04-19 Uruk API: casesByLevel() returns case nodes in corpus order. 3.3.2 \u00b6 2018-04-18 Change in the Uruk api reflecting that undivided lines have no cases now (was: they had a single case with the same material as the line). Also: the feature fullNumber on cases is now called number , and contains the full hierarchical part leading to a case. There is an extra feature terminal on lines and cases if they are not subdivided. Changes in Uruk and Bhsa api: fixed a bug that occurred when working outside a GitHub repository. 3.3.1 \u00b6 2018-04-18 Change in the Uruk api. casesByLevel() now takes an optional argument terminal instead of withChildren , with opposite values. withChildren=False is ambiguous: will it deliver only cases that have no children (intended), or will it deliver cases and their children (understood, but not intended). terminal=True : delivers only cases that are terminal. terminal=False : delivers all cases at that level. 3.3 \u00b6 2018-04-14 Small fix in the bhsa api. Bumped the version number because of the inclusion of corpus specific APIs. 3.2.6 \u00b6 2018-04-14 Text-Fabric now contains corpus specific extras: bhsa.py for the Hebrew Bible (BHSA) uruk.py for the Proto-Cuneiform corpus Uruk The Fabric(locations=locations, modules=modules) constructor now uses [''] as default value for modules. Now you can use the locations parameter on its own to specify the search paths for TF features, leaving the modules parameter undefined, if you wish. 3.2.5 \u00b6 2018-03-23 Enhancement in search templates: you can now test for the presence of features. Till now, you could only test for one or more concrete values of features. So, in addition to things like 1 word number=plural tense=yiqtol you can also say things like 1 word number=plural tense and it will give you words in the plural that have a tense. 3.2.4 \u00b6 2018-03-20 The short API names F , T , L etc. have been aliased to longer names: Feature , Text , Locality , etc. 3.2.2 \u00b6 2018-02-27 Removed the sub module uruk.py . It is better to keep corpus dependent modules in outside the TF package. 3.2.1 \u00b6 2018-02-26 Added a sub module uruk.py , which contains methods to produce ATF transcriptions for nodes of certain types. 3.2 \u00b6 2018-02-19 API change Previously, the functions L.d() and L.u() took rank into account. In the Hebrew Bible, that means that L.d(sentence) will not return a verse, even if the verse is contained in the sentence. This might be handy for sentences and verses, but in general this behaviour causes problems. It also disturbs the expectation that with these functions you get all embedders and embeddees. So we have lifted this restriction. Still, the results of the L functions have an ordering that takes into account the levels of the returned nodes. Enhancement Previously, Text-Fabric determined the levels of node types automatically, based on the average slot-size of nodes within the node types. So books get a lower level than chapters than verses than phrases, etc. However, working with cuneiform tablets revealed that containing node types may have a smaller average size than contained node types. This happens when a container type only contains small instances of the contained type and not the bigger ones. Now you can override the computation by text-fabric by means of a key-value in the otext feature. See the api . 3.1.5 \u00b6 2018-02-15 Fixed a small problem in sectionFromNode(n) when n is a node within a primary section but outside secondary/tertiary sections. 3.1.4 \u00b6 2018-02-15 Small fix in the Text API. If your data set does not have language dependent features, for section level 1 headings, such as book@en , book@sw , the Text API will not break, and the plain book feature will be taken always. We also reformatted all code with a PEP8 code formatter. 3.1.3 \u00b6 2018-01-29 Small adaptions in conversion from MQL to TF, it can now also convert the MQL coming from CALAP dataset (Syriac). 3.1.2 \u00b6 2018-01-27 Nothing changed, only the names of some variables and the text of some messages. The terminology has been made more consistent with the fabric metaphor, in particular, grid has been replaced by warp . 3.1.1 \u00b6 2017-10-21 The exportMQL() function now generates one single enumeration type that serves for all enumeration features. That makes it possible to compare values of different enumeration features with each other, such as ps and prs_ps . 3.1 \u00b6 2017-10-20 The exportMQL() function now generates enumeration types for features, if certain conditions are fulfilled. That makes it possible to query those features with the IN relationship of MQL, like [chapter book IN (Genesis, Exodus)] . 3.0.8 \u00b6 2017-10-07 When reading edges with values, also the edges without a value are taken in. 3.0.7 \u00b6 2017-10-07 Edges with edge values did not allow for the absence of values. Now they do. 3.0.6 \u00b6 2017-10-05 A major tweak in the importMQL() function so that it can handle gaps in the monad sequence. The issue arose when converting MQL for version 3 of the BHSA . In that version there are somewhat arbitrary gaps in the monad sequence between the books of the Hebrew Bible. I transform a gapped sequence of monads into a continuous sequence of slots. 3.0.5 \u00b6 2017-10-05 Another little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for version 3 of the BHSA . 3.0.4 \u00b6 2017-10-04 Little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical material. 3.0.2, 3.0.3 \u00b6 2017-10-03 No changes, only an update of the package metadata, to reflect that Text-Fabric has moved from ETCBC to Dans-labs . 3.0.1 \u00b6 2017-10-02 Bug fix in reading edge features with values. 3.0.0 \u00b6 2017-10-02 MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already exportMQL() . The consequence is that we can operate with much agility between the worlds of MQL and TF. We can start with source data in MQL, convert it to TF, combine it with other TF data sources, compute additional stuff and add it, and then finally export it as enriched MQL, so that the enriched data can be queried by MQL. 2.3.15 \u00b6 2017-09-29 Completion: TF defines the concept of edges that carry a value. But so far we have not used them. It turned out that it was impossible to let TF know that an edge carries values, when saving data as a new feature. Now it is possible. 2.3.14 \u00b6 2017-09-29 Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible. This is the consequence of a bug in precomputing the sections sections . The preparation step used 1 range ( firstVerse , lastVerse ) somewhere, which should of course have been 1 range ( firstVerse , lastVerse + 1 ) 2.3.13 \u00b6 2017-09-28 Loading TF was not completely silent if silent=True was passed. Better now. 2.3.12 \u00b6 2017-09-18 Small fix in TF.save() . The spec says that the metadata under the empty key will be inserted into all features, but in fact this did not happen. Instead it was used as a default when some feature did not have metadata specified. From now on, that metadata will spread through all features. New API function explore , to get a list of all known features in a dataset. 2.3.11 \u00b6 2017-09-18 Small fix in Search: the implementation of the relation operator || (disjoint slot sets) was faulty. Repaired. The search tutorial got an extra example: how to look for gaps. Gaps are not a primitive in the TF search language. Yet the language turns out to be powerful enough to look for gaps. This answers a question by Cody Kingham. 2.3.10 \u00b6 2017-08-24 When defining text formats in the otext.tf feature, you can now include newlines and tabs in the formats. Enter them as \\n and \\t . 2.3.9 \u00b6 2017-07-24 TF has a list of default locations to look for data sources: ~/Downloads , ~/github , etc. Now ~/Dropbox has been added to that list. 2.3.8 \u00b6 2017-07-24 The section levels (book, chapter, verse) were supposed to be customizable through the otext feature. But in fact, up till version 2.3.7 this did not work. From now on the names of the section types and the features that name/number them, are given in the otext feature. It is still the case that exactly three levels must be specified, otherwise it does not work. 2.3.7 \u00b6 2017-05-12 Fixes. Added an extra default location for looking for text-fabric-data sources, for the benefit of running text-fabric within a shared notebook service. 2.3.5, 2.3.6 \u00b6 2017-03-01 Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms in the template got discarded after an outdent. 2.3.4 \u00b6 2017-02-12 Also the Fabric() call can be made silent now. 2.3.3 \u00b6 2017-02-11 Improvements: you can load features more silently. See TF.load() ; you can search more silently. See S.study() ; you can search more concisely. See the new S.search() ; when fetching results, the amount parameter of S.fetch() has been renamed to limit ; the tutorial notebooks (see links on top) have been updated. 2.3.2 \u00b6 2017-02-03 Bug fix: the results of F.feature.s() , E.feature.f() , and E.features.t() are now all tuples. They were a mixture of tuples and lists. 2.3.1 \u00b6 2017-01-23 Bug fix: when searching simple queries with only one query node, the result nodes were delivered as integers, instead of 1-tuples of integers. 2.3 \u00b6 2017-01-13 We start archiving releases of Text-Fabric at Zenodo . 2.2.1 \u00b6 2017-01-09 Small fixes. 2.2.0 \u00b6 2017-01-06 New: sortKey \u00b6 The API has a new member: sortKey New relationships in templates: nearness . See for examples the end of the searchTutorial . Thanks to James Cu\u00e9nod for requesting nearness operators. Fixes \u00b6 in S.glean() word nodes were not printed; the check whether the search graph consists of a single connected component did not handle the case of one node without edges well; 2.1.3 \u00b6 2017-01-04 Various fixes. 2.1.0 \u00b6 2017-01-04 New: relations \u00b6 Some relations have been added to search templates: =: and := and :: : start at same slot , end at same slot , start at same slot and end at same slot <: and :> : adjacent before and adjacent next . The latter two can also be used through the L -api: L.p() and L.n() . The data that feeds them is precomputed and available as C.boundary . New: enhanced search templates \u00b6 You can now easily make extra constraints in search templates without naming atoms. See the searchTutorial for an updated exposition on searching. 2.0.0 \u00b6 2016-12-23 New: Search \u00b6 Want to feel cosy with Christmas? Put your laptop on your lap, update Text-Fabric, and start playing with search. Your laptop will spin itself warm with your queries! Text-Fabric just got a powerful search facility, based on (graph)-templates. It is still very fresh, and more experimentation will be needed. Feedback is welcome. Start with the tutorial . The implementation of this search engine can be nicely explained with a textile metaphor: spinning wool into yarn and then stitching the yarns together. That will be explained further in a document that I'll love to write during Xmas. 1.2.7 \u00b6 2016-12-14 New \u00b6 F.otype.sInterval() 1.2.6 \u00b6 2016-12-14 bug fix \u00b6 There was an error in computing the order of nodes. One of the consequences was that objects that occupy the same slots were not ordered properly. And that had as consequence that you could not go up from words in one-word phrases to their containing phrase. It has been remedied. Note Your computed data needs to be refreshed. This can be done by calling a new function TF.clearCache() . When you use TF after this, you will see it working quite hard to recompute a bunch of data. 1.2.5 \u00b6 2016-12-13 Documentation update 1.2.0 \u00b6 2016-12-08 Note Data update needed New \u00b6 Frequency lists \u00b6 F.feature.freqList() : get a sorted frequency list for any feature. Handy as a first step in exploring a feature. Export to MQL \u00b6 TF.exportMQL() : export a whole dataset as a MQL database. Including all modules that you have loaded with it. Changed \u00b6 The slot numbers start at 0, no longer at 1. Personally I prefer the zero starting point, but Emdros insists on positive monads and objects ids. Most important is that users do not have to add/subtract one from the numbers they see in TF if they want to use it in MQL and vice versa. Because of this you need to update your data too: 1 2 cd ~/github/text-fabric-data git pull origin master","title":"Older"},{"location":"About/Older/#older-changes","text":"","title":"Older changes"},{"location":"About/Older/#645-6","text":"2018-11-08 Bug fix: Now TF can truly work if you do not have a feature text.tf in your dataset. Tests added for basic relations in search: all relations are rigorously tested, a few small bugs fixed. The comment sign in queries is now % , only at the start of a line.","title":"6.4.5-6"},{"location":"About/Older/#643-4","text":"2018-11-06 Big bug fix in queries: basic relationships in combination with custom sets. The implementation of the basic relationships did not reckon with custom sets that contains both slot nodes and non-slot nodes. And it did not trigger the right code when a custom set has only slot nodes. That has been remedied. Some of the search tutorials have been expanded to include a number of these critical cases. A more complete test suite outside the tutorials is still on my to do list. Thanks to Cody Kingham for spotting and reporting this bug.","title":"6.4.3-4"},{"location":"About/Older/#64-641-2","text":"2018-11-02 A passage browsing interface that interacts with the search results. The interface scrolls to the highlighted row. Minor things: More refined warnings when you run out of memory TF checks whether you are running 64 bit Python. If not, a warning is issued.","title":"6.4, 6.4.1-2"},{"location":"About/Older/#632","text":"2018-10-27 Better documentation for installation of Text-Fabric on Ubuntu. Added new module requirements: ipykernel and notebook.","title":"6.3.2"},{"location":"About/Older/#631","text":"2018-10-24 An optional parameter silent=False has been added to the initialisation calls of the specific app APIs: you can say now 1 A = Xxxx ( silent = True ) where Xxxx is a know corpus. and then all non-error messages will be suppressed. If the underlying TF API needs to precompute data, it will still be shown, because this may cause an otherwise unexpected delay. Since this is a releatively rare case, and since this can be remedied by running the call again, I leave this behaviour as it is.","title":"6.3.1"},{"location":"About/Older/#63","text":"2018-10-19 Character tables for Hebrew abd Syriac, with links to them from the TF browser Better font handling In the pretty and plain functions you can pass a fmt parameter, to control the text representation (original script, transcription, phonetics) You can also control the text representation in the Text-Fabric browser.","title":"6.3"},{"location":"About/Older/#622","text":"2018-10-18 Added ETCBC/WIT transcriptions to the SyrNT data source. Now both Peshitta and Syriac New Testament have ETCBC transcriptions. The older, rectangular logo makes place for the more crisp, circular one","title":"6.2.2"},{"location":"About/Older/#621","text":"2018-10-17 New app: Syrnt (Syriac New Testament. It works much like the Peshitta, but the SyrNT data has linguistic annotations at the word and lexeme levels. After this upgrade you can browse the SyrNT by saying text-fabric syrnt on the command line.","title":"6.2.1"},{"location":"About/Older/#62","text":"2018-10-16 New app: Peshitta. It works much like the BHSA, but there is one big difference: the current Peshitta data does not have linguistic annotations. There are just books, chapters, verses and words. We expect to add lemmatizations of words shortly. After this upgrade you can browse the peshitta by saying text-fabric peshitta on the command line. Fixed a bug in exportMQL: when there are no enumerated values, do not write out an empty CREATE ENUMERATION statement to the MQL file.","title":"6.2"},{"location":"About/Older/#61","text":"2018-10-12 More precise provenance data when you export results from the Text-Fabric data; Under the hood reorganization of configuration data of apps like Bhsa and Uruk; App-specific parts of the code have moved to more generic parts: a big cleanup has performed; This will make it easier to add new apps.","title":"6.1"},{"location":"About/Older/#607-8-9","text":"2018-10-11 Avoid computing the notebook name when the user passes a name for the notebook to Uruk() or Bhsa() . And when the computing needs to be done, all exceptions will be caught, because the code for determining the notebook name is brittle, and may crash if the Jupyter version does not match. Fixed the bug that the Bhsa and Uruk did not run properly outside a notebook or outside a github repo. In Bhsa and Uruk, the generated info after the incantation can be collapsed (features, API members).","title":"6.0.7-8-9"},{"location":"About/Older/#606","text":"2018-10-10 In the BHSA, the edge features are now shown too after the incantation. If you hoist the API members into your namespace, you will get a list of hoisted names, linked to the API documentation.","title":"6.0.6"},{"location":"About/Older/#605","text":"2018-10-09 When using BHSA and Uruk in a notebook, there is an even simpler incantation which auto downloads features. In the BHSA it is shown which features are loaded, with direct links to the feature docs.","title":"6.0.5"},{"location":"About/Older/#604","text":"2018-10-09 When using BHSA and Uruk in a notebook, there is a simpler incantation which auto downloads features. Some issues concerning paths in zipfiles of downloaded data have been solved.","title":"6.0.4"},{"location":"About/Older/#603","text":"Easier incantations for Bhsa() and Uruk() . It is no longer needed to pass the name of the notebook, but you can still do so: name='mynotebook' You can leave out the api argument in Bhsa() . Then you do not have to load features by means of TF.load() , Bhsa() will load a standard set of features, and if the BHSA data is missing, it will download them first. The former ways of calling Bhsa() and Uruk() are still valid. Note that all arguments have become optional. 2018-10-08 The Text-Fabric browser will always print a banner with its name and version. If you pass it the argument --help or -h or --version or -v it will show the relevant information and stop executing.","title":"6.0.3"},{"location":"About/Older/#602","text":"2018-10-07 The Text-Fabric browser takes it data by default from ~/text-fabric-data . It will not check local github clones for data. But if you pass the option -lgc , it will first check your local github clones. So it you do nothing special, the TF browser always works with the auto-downloaded data.","title":"6.0.2"},{"location":"About/Older/#601","text":"2018-10-06 Not only the core BHSA data will auto load, also the related PHONO and PARALLELS data. A new release has been made of the related data, and they are now in sync with the release of the core data. If you use auto load already, you do not have to do anything. But if you have the etcbc/phono and etcbc/parallels repos in your ~/github folder, you should do a git pull origin master on those repos. N.B. : I am contemplating to have the Text-Fabric browser always use data from ~/text-fabric-data and no longer from ~/github/etcbc . Then the TF browser always controls its own data, and it will not occur that the version of the TF browser is not compatible with the version of the TF data in your github repos, or that the main data and the related data are out of synch. The disadvantage is that if you have the github repos on your system, you get redundant data in ~/text-fabric-data . However, there is only one version kept in ~/text-fabric-data , so this is not much.","title":"6.0.1"},{"location":"About/Older/#60","text":"2018-10-05 A big update with several changes:","title":"6.0"},{"location":"About/Older/#api-change","text":"T.text() has got more behaviours. This change was needed for the Text-Fabric browser, in order to represent lexemes in exported files. Showcase: BHSA dictionary Here is how you can collect the BHSA lexemes in an Excel sheet. about.md RESULTSX.tsv It might also be handy for the programmers amongst you. See the updated API doc on T , expand the T.text() item.","title":"API change:"},{"location":"About/Older/#auto-update","text":"The Text-Fabric browser checks if you are using the most recent release of the data.","title":"Auto update"},{"location":"About/Older/#font-rendering","text":"A font rendering issue in Safari 12 in macos Mojave prevented the use of Ezra SIL for Hebrew in notebooks. We now work around this by relying on the distribution of Ezra SIL as webfont in the font library .","title":"Font rendering"},{"location":"About/Older/#additional-small-fixes","text":"Not worth telling. update Text-Fabric To update Text-Fabric itself to version 6.0, consult Upgrade . Perform this step first, because the new TF may download the new data for you. Data update needed In order to work successfully with the new T.text() function, you need a newer release (1.4) of the BHSA data . (In fact, only one line in one feature has changed ( otext.tf ). Here is how you get the new data release:","title":"Additional small fixes."},{"location":"About/Older/#automatically","text":"If previously your Text-Fabric browser has automatically downloaded the data for you, it will detect the new release and download it automatically. You do not have to do anything, except increase your patience. The download (24 MB) takes some time and after that Text-Fabric will precompute related data, which may take a few minutes. This is a one-time-step after a data update.","title":"Automatically"},{"location":"About/Older/#manually","text":"If you have a clone of the BHSA repository, then go to that directory and say git pull origin master . If you get error messages, then you have local changes in your local BHSA repository that conflict with the github version. Probably you have run the tutorials in place. Best thing to do is: copy your BHSA tutorial directory to somehwere else; remove your local BHSA repository entirely; decide whether you really want the whole repo back (nearly 4 GB). If not: you're done, and TF will download automatically the data it needs. If you still need it: move one directory up (into the etcbc directory) and do git clone https://github.com/etcbc/bhsa . If you want to consult the tutorials, either: view them on nbviewer ; or run them in a directory outside the BHSA repo (where you have copied it a minute ago).","title":"Manually"},{"location":"About/Older/#564","text":"2018-10-04 Solved a font-rendering issue on Safari 12 (Macos Mojave): locally installed fonts, such as Ezra SIL are not being honored. So I linked to a stylesheet of the fontlibrary which has a webfont version of Ezra SIL. That worked.","title":"5.6.4"},{"location":"About/Older/#563","text":"2018-10-04 Exported tab-separated files get extension .tsv instead of .csv , because then they render better in GitHub.","title":"5.6.3"},{"location":"About/Older/#562","text":"2018-10-04 Small optimization. More docs about reading and writing Excel compatible CSV files with Hebrew characters in it.","title":"5.6.2"},{"location":"About/Older/#561","text":"2018-10-04 Better exporting from TF browser: a good RESULTSX.tsv with results, sensibly augmented with information, directly openable in Excel, even when non-latin unicode code characters are present . All features that occur in the searchTemplate are drawn in into the RESULTSX.tsv, onto the nodes they filter. An additonal feature filtering is now possible in searchTemplates: feature* . This acts as \"no additional constraint\", so it does not influence the result set. But it will be picked up and used to add information into the RESULTSX.tsv.","title":"5.6.1"},{"location":"About/Older/#5525","text":"2018-10-03 The Text-Fabric browser exports results as node lists and produces also a CONTEXT.tsv with all feature values for all nodes in the results. However, it does not contain full text representations of the nodes and it is also not possible to see in what verses the nodes occur. That has changed. The last column of CONTEXT.tsv contains the full text of a node. And there are three columns at the beginning that contain the references to the sections the node is in. For the BHSA that is the book, chapter and verse.","title":"5.5.25"},{"location":"About/Older/#5524","text":"2018-09-25 BHSA app in Text-Fabric Browser: the book names on the verse pad should be the English book names. That is now in the help texts, including a link to the list of English book names.","title":"5.5.24"},{"location":"About/Older/#5523","text":"2018-09-21 Problem in use of msgCache in the search engine, which caused fetch() to fail in some cases. Fixed.","title":"5.5.23"},{"location":"About/Older/#5522","text":"2018-09-13 Fix in left-to-right displays of extra features in pretty() displays in the BHSA.","title":"5.5.22"},{"location":"About/Older/#5521","text":"2018-08-30 Bug fix in transcription.py w.r.t. to Syriac transcriptions.","title":"5.5.21"},{"location":"About/Older/#5520","text":"2018-08-16 BHSA app: adjusted the color of the gloss attribute: darker.","title":"5.5.20"},{"location":"About/Older/#5519","text":"2018-07-19 Fixed: when opening files for reading and writing for an export of a TF browser session: specify that the encoding is utf8 . This is needed on those windowses where the default encoding is something else, usually cp1252 .","title":"5.5.19"},{"location":"About/Older/#5518","text":"2018-07-19 No change, only in the build script. This is a test whether after uploading to PyPi, users can upgrade without using the --no-cache-dir in their pip commands.","title":"5.5.18"},{"location":"About/Older/#5517","text":"2018-07-19 The main functions in kernel and web can be passed arguments, instead that they always read from sys.argv. So that it can be used packaged apps.","title":"5.5.17"},{"location":"About/Older/#5516","text":"2018-07-17 Extra option when starting up the text-fabric web interface: -docker to let the web server listen at 0.0.0.0 instead of localhost . So that it can be used in a Docker container.","title":"5.5.16"},{"location":"About/Older/#5515","text":"2018-07-16 Extra option when starting up the text-fabric web interface: -noweb to not start the web browser. So that it can be used in a Docker container.","title":"5.5.15"},{"location":"About/Older/#5513-14","text":"2018-07-12 Better error reporting of quantified queries.","title":"5.5.13-14"},{"location":"About/Older/#5512","text":"2018-07-11 Faster export of big csv lists. Tweaks in the web interface. Cleaner termination of processes. The concept TF data server is now called TF kernel","title":"5.5.12"},{"location":"About/Older/#558-11","text":"2018-07-10 Better in catching out-of-memory errors. Prevents creation of corrupt compiled binary TF data. Prevents starting the web server if the TF kernel fails to load.","title":"5.5.8-11"},{"location":"About/Older/#557","text":"2018-07-09 Optimization is export from TF browser.","title":"5.5.7"},{"location":"About/Older/#556","text":"2018-07-09 Better help display. The opened-state of help sections is remembered. You can open help next to an other open section in the sidebar.","title":"5.5.6"},{"location":"About/Older/#555","text":"2018-07-08 Crisper icon.","title":"5.5.5"},{"location":"About/Older/#554","text":"2018-07-6 Docs updated. Little bit of refactoring.","title":"5.5.4"},{"location":"About/Older/#551-3","text":"2018-07-4 In the TF browser, use a selection of all the features when working with the BHSA. Otherwise in Windows you might run out of memory, even if you have 8GB RAM.","title":"5.5.1-3"},{"location":"About/Older/#55","text":"2018-07-4 Text-Fabric can download data for BHSA and Uruk. You do not have to clone github repositories for that. The data downloaded by Text-Fabric ends up in text-fabric-data under your home directory.","title":"5.5"},{"location":"About/Older/#545-7","text":"2018-07-03 Experimenting with setuptools to get the text-fabric script working on Windows.","title":"5.4.5-7"},{"location":"About/Older/#544","text":"2018-07-02 Added renaming/duplicating of jobs and change of directory.","title":"5.4.4"},{"location":"About/Older/#543","text":"2018-06-29 Small fixes in error reporting in search.","title":"5.4.3"},{"location":"About/Older/#541-2","text":"2018-06-28 Text-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata. Small fixes.","title":"5.4.1-2"},{"location":"About/Older/#54","text":"2018-06-26 Improved interface and functionality of the text-fabric browser: you can save your work you can enter verse references and tablet P numbers there is help there is a side bar Docs not up to date The API docs are not up-to-date: there are new functions in the Bhsa and Uruk APIs. The server/kernel/client apis are not completely spelled out. However, the help for the text-fabric browser is included in the interface itself.","title":"5.4"},{"location":"About/Older/#533","text":"2018-06-23 Small fix: command line args for text-fabric.","title":"5.3.3"},{"location":"About/Older/#530-2","text":"2018-06-22 Better process management When the TF web interface is started, it cleans up remnant process that might get in the way otherwise. You can also say 1 text-fabric -k to kill all remnant processes, or 1 text-fabric -k corpus to kill the processes for a specific corpus only. Manual node entry You can enter nodes manually in the TF browser. Handy for quick inspection. You can click on the sequence number to append the node tuple of that row to the tuple input field. That way you can collect interesting results. Name and Description You can enter a name which will be used as title and file name during export. You can enter a description in Markdown. When you export your query, the description appears formatted on top. Provenance If you export a query, provenance is added, using DOIs. Small fixes No more blank pages due to double page breaks.","title":"5.3.0-2"},{"location":"About/Older/#521","text":"2018-06-21 Added an expand all checkbox in the text-fabric browser, to expand all shown rows or to collapse them. Export function for search results in the text-fabric browser. What you see is what you get, 1 pretty display per page if you have the browser save it to pdf. Small tweaks","title":"5.2.1"},{"location":"About/Older/#51","text":"2018-06-21 When displaying results in condensed mode, you can now choose the level of the container in which results are highlighted. So far it was fixed to verse for the bhsa and tablet for Uruk. The docs are lagging behind! But it is shown in the tutorials and you can observer it in the text-fabric browser.","title":"5.1"},{"location":"About/Older/#501234","text":"2018-06-19 Addressed start-up problems.","title":"5.0.1,2,3,4"},{"location":"About/Older/#50","text":"2018-06-18 Built in web server and client for local query running. It is implemented for Bhsa and Uruk.","title":"5.0"},{"location":"About/Older/#4423","text":"2018-06-13 New distribution method with setuptools. Text-Fabric has now dependencies on modules rpyc and bottle, because it contains a built-in TF kernel and web server. This website is still barely functional, though.","title":"4.4.2,3"},{"location":"About/Older/#441","text":"2018-06-10 Search API: Escapes in regular expression search was buggy and convoluted. If a feature value contains a | then in an RE you have to enter \\| to match it. But to have that work in a TF search, you needed to say \\\\\\| . On the other hand, in the same case for . instead of | , you could just sat \\. In the new situation you do not have to double escape in REs anymore. You can just say \\| and \\. .","title":"4.4.1"},{"location":"About/Older/#44","text":"2018-06-06 Search API: S.search() accepts a new optional parameter: withContext . It triggers the output of context information for nodes in the result tuples.","title":"4.4"},{"location":"About/Older/#434-435","text":"2018-06-05 Search API: The /with/ /or/ /or/ /-/' quantifier is also allowed with zero /or/` s. Small fix in the /with/ quantifier if there are quantifiers between this one and its parent atom.","title":"4.3.4, 4.3.5"},{"location":"About/Older/#433","text":"2018-06-01 Search API: Improved quantifiers in search: /where/ /have/ /without/ /with/ /or/ /-/ ; much clearer indentation rules (no caret anymore); better reporting by S.study() .","title":"4.3.3"},{"location":"About/Older/#432","text":"2018-05-31 Search API: quantifiers may use the name .. to refer to their parents you may use names in the place of atoms, which lessens the need for constructs with p = q stricter checks on the syntax and position of quantifiers","title":"4.3.2"},{"location":"About/Older/#431","text":"2018-05-30 Docs and metadata update","title":"4.3.1"},{"location":"About/Older/#430","text":"2018-05-30 API Change in Search. In search templates I recently added things like 1 word vt! which checks for words that do not have a value for feature vt . The syntax for this has now changed to 1 word vt# Unequal (#) in feature value conditions. Now you can say things like 1 word vt#infa|infc meaning that the value of feature is not one of infa , infc . So, in addition to = we have # for \"not equal\". * Quantifiers. You can now use quantifiers in search. One of them is like NOTEXIST in MQL. See the docs A number of minor fixes.","title":"4.3.0"},{"location":"About/Older/#421","text":"2018-05-25 Several improvements in the pretty display in Bhsa and Uruk APIs Under the hood changes in S.search() to prepare for quantifiers in search templates. Tokenisation of quantifiers already works Searches can now spawn auxiliary searches without polluting intermediate data This has been done by promoting the S API to a factory of search engines. By deafault, S creates and maintains a single factory, so to the user it is the same S . But when it needs to run a query in the middle of processing another query it can just spawn another search engine to do that, without interfering with the original search. NB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten.","title":"4.2.1"},{"location":"About/Older/#42","text":"2018-05-23 The Search API has been extended: you can use custom sets in your query templates you can search in shallow mode: instead of full result tuples, you just get a set of the top-level thing you mention in your template. This functionality is a precursor for quantifiers in search templates but is also a powerful addition to search in its own right.","title":"4.2"},{"location":"About/Older/#412","text":"2018-05-17 Bhsa and Uruk APIs: custom highlight colours also work for condensed results. you can pass the highlights parameter also to show and prettyTuple","title":"4.1.2"},{"location":"About/Older/#411","text":"2018-05-16 Bhsa API: you can customize the features that are shown in pretty displays.","title":"4.1.1"},{"location":"About/Older/#41","text":"2018-05-16 Bhsa and Uruk APIs: you can customize the highlighting of search results: different colours for different parts of the results you can choose your colours freely from all that CSS has to offer. See the updated search tutorials.","title":"4.1"},{"location":"About/Older/#403","text":"2018-05-11 No changes, just quirks in the update process to get a new version of TF out.","title":"4.0.3"},{"location":"About/Older/#401","text":"2018-05-11 Documentation updates.","title":"4.0.1"},{"location":"About/Older/#400","text":"2018-05-11 Additions to Search. You can now include the values of edges in your search templates. F. feature .freqList() accepts a new parameter: nodeTypes . It will restrict its results to nodes in one of the types in nodeTypes . You can now also do E. feature .freqList() . It will count the number of edges if the edge is declared to be without values, or it will give a frequency list of the edges by value if the edge has values. Like F.freqList , you can pass parameters to constrain the frequency list to certain node types. You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive ( nodeTypesTo ). New documentation system based on MkDocs .","title":"4.0.0"},{"location":"About/Older/#3412","text":"2018-05-02 The Uruk and Bhsa APIs show the version of Text-Fabric that is being called.","title":"3.4.12"},{"location":"About/Older/#3411","text":"2018-05-01 Uruk cases are divided horizontally and vertically, alternating with their nesting level; cases have a feature depth now, indicating at which level of nesting they are.","title":"3.4.11"},{"location":"About/Older/#348-9-10","text":"2018-04-30 Various small fixes, such as: Bhsa: Lexeme links in pretty displays. Uruk: Prevented spurious </div> in NbViewer.","title":"3.4.8-9-10"},{"location":"About/Older/#347","text":"Uruk: Modified local image names","title":"3.4.7"},{"location":"About/Older/#346","text":"Small tweaks in search.","title":"3.4.6"},{"location":"About/Older/#345","text":"2018-04-28 Bhsa API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() .","title":"3.4.5"},{"location":"About/Older/#344","text":"2018-04-27 Uruk API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() .","title":"3.4.4"},{"location":"About/Older/#342","text":"2018-04-26 Better search documentation. Uruk API: small fixes.","title":"3.4.2"},{"location":"About/Older/#341","text":"2018-04-25 Bhsa API: Search/show: you can now show results condensed: i.e. a list of passages with highlighted results is returned. This is how SHEBANQ represents the results of a query. If you have two results in the same verse, with condensed=True you get one verse display with two highlights, with condensed=False you get two verse displays with one highlight each. Uruk API: Search/show: the pretty , prettyTuple , show functions of the Bhsa API have bee translated to the Uruk API. You can now get very pretty displays of search results.","title":"3.4.1"},{"location":"About/Older/#34","text":"2018-04-23 Search : You can use regular expressions to specify feature values in queries. You could already search for nodes which have a non-None value for a certain feature. Now you can also search for the complement: nodes that do not have a certain feature. Bhsa API: The display of query results also works with lexeme nodes.","title":"3.4"},{"location":"About/Older/#334","text":"2018-04-20 Uruk API: Better height and width control for images. Leaner captions.","title":"3.3.4"},{"location":"About/Older/#333","text":"2018-04-19 Uruk API: casesByLevel() returns case nodes in corpus order.","title":"3.3.3"},{"location":"About/Older/#332","text":"2018-04-18 Change in the Uruk api reflecting that undivided lines have no cases now (was: they had a single case with the same material as the line). Also: the feature fullNumber on cases is now called number , and contains the full hierarchical part leading to a case. There is an extra feature terminal on lines and cases if they are not subdivided. Changes in Uruk and Bhsa api: fixed a bug that occurred when working outside a GitHub repository.","title":"3.3.2"},{"location":"About/Older/#331","text":"2018-04-18 Change in the Uruk api. casesByLevel() now takes an optional argument terminal instead of withChildren , with opposite values. withChildren=False is ambiguous: will it deliver only cases that have no children (intended), or will it deliver cases and their children (understood, but not intended). terminal=True : delivers only cases that are terminal. terminal=False : delivers all cases at that level.","title":"3.3.1"},{"location":"About/Older/#33","text":"2018-04-14 Small fix in the bhsa api. Bumped the version number because of the inclusion of corpus specific APIs.","title":"3.3"},{"location":"About/Older/#326","text":"2018-04-14 Text-Fabric now contains corpus specific extras: bhsa.py for the Hebrew Bible (BHSA) uruk.py for the Proto-Cuneiform corpus Uruk The Fabric(locations=locations, modules=modules) constructor now uses [''] as default value for modules. Now you can use the locations parameter on its own to specify the search paths for TF features, leaving the modules parameter undefined, if you wish.","title":"3.2.6"},{"location":"About/Older/#325","text":"2018-03-23 Enhancement in search templates: you can now test for the presence of features. Till now, you could only test for one or more concrete values of features. So, in addition to things like 1 word number=plural tense=yiqtol you can also say things like 1 word number=plural tense and it will give you words in the plural that have a tense.","title":"3.2.5"},{"location":"About/Older/#324","text":"2018-03-20 The short API names F , T , L etc. have been aliased to longer names: Feature , Text , Locality , etc.","title":"3.2.4"},{"location":"About/Older/#322","text":"2018-02-27 Removed the sub module uruk.py . It is better to keep corpus dependent modules in outside the TF package.","title":"3.2.2"},{"location":"About/Older/#321","text":"2018-02-26 Added a sub module uruk.py , which contains methods to produce ATF transcriptions for nodes of certain types.","title":"3.2.1"},{"location":"About/Older/#32","text":"2018-02-19 API change Previously, the functions L.d() and L.u() took rank into account. In the Hebrew Bible, that means that L.d(sentence) will not return a verse, even if the verse is contained in the sentence. This might be handy for sentences and verses, but in general this behaviour causes problems. It also disturbs the expectation that with these functions you get all embedders and embeddees. So we have lifted this restriction. Still, the results of the L functions have an ordering that takes into account the levels of the returned nodes. Enhancement Previously, Text-Fabric determined the levels of node types automatically, based on the average slot-size of nodes within the node types. So books get a lower level than chapters than verses than phrases, etc. However, working with cuneiform tablets revealed that containing node types may have a smaller average size than contained node types. This happens when a container type only contains small instances of the contained type and not the bigger ones. Now you can override the computation by text-fabric by means of a key-value in the otext feature. See the api .","title":"3.2"},{"location":"About/Older/#315","text":"2018-02-15 Fixed a small problem in sectionFromNode(n) when n is a node within a primary section but outside secondary/tertiary sections.","title":"3.1.5"},{"location":"About/Older/#314","text":"2018-02-15 Small fix in the Text API. If your data set does not have language dependent features, for section level 1 headings, such as book@en , book@sw , the Text API will not break, and the plain book feature will be taken always. We also reformatted all code with a PEP8 code formatter.","title":"3.1.4"},{"location":"About/Older/#313","text":"2018-01-29 Small adaptions in conversion from MQL to TF, it can now also convert the MQL coming from CALAP dataset (Syriac).","title":"3.1.3"},{"location":"About/Older/#312","text":"2018-01-27 Nothing changed, only the names of some variables and the text of some messages. The terminology has been made more consistent with the fabric metaphor, in particular, grid has been replaced by warp .","title":"3.1.2"},{"location":"About/Older/#311","text":"2017-10-21 The exportMQL() function now generates one single enumeration type that serves for all enumeration features. That makes it possible to compare values of different enumeration features with each other, such as ps and prs_ps .","title":"3.1.1"},{"location":"About/Older/#31","text":"2017-10-20 The exportMQL() function now generates enumeration types for features, if certain conditions are fulfilled. That makes it possible to query those features with the IN relationship of MQL, like [chapter book IN (Genesis, Exodus)] .","title":"3.1"},{"location":"About/Older/#308","text":"2017-10-07 When reading edges with values, also the edges without a value are taken in.","title":"3.0.8"},{"location":"About/Older/#307","text":"2017-10-07 Edges with edge values did not allow for the absence of values. Now they do.","title":"3.0.7"},{"location":"About/Older/#306","text":"2017-10-05 A major tweak in the importMQL() function so that it can handle gaps in the monad sequence. The issue arose when converting MQL for version 3 of the BHSA . In that version there are somewhat arbitrary gaps in the monad sequence between the books of the Hebrew Bible. I transform a gapped sequence of monads into a continuous sequence of slots.","title":"3.0.6"},{"location":"About/Older/#305","text":"2017-10-05 Another little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for version 3 of the BHSA .","title":"3.0.5"},{"location":"About/Older/#304","text":"2017-10-04 Little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical material.","title":"3.0.4"},{"location":"About/Older/#302-303","text":"2017-10-03 No changes, only an update of the package metadata, to reflect that Text-Fabric has moved from ETCBC to Dans-labs .","title":"3.0.2, 3.0.3"},{"location":"About/Older/#301","text":"2017-10-02 Bug fix in reading edge features with values.","title":"3.0.1"},{"location":"About/Older/#300","text":"2017-10-02 MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already exportMQL() . The consequence is that we can operate with much agility between the worlds of MQL and TF. We can start with source data in MQL, convert it to TF, combine it with other TF data sources, compute additional stuff and add it, and then finally export it as enriched MQL, so that the enriched data can be queried by MQL.","title":"3.0.0"},{"location":"About/Older/#2315","text":"2017-09-29 Completion: TF defines the concept of edges that carry a value. But so far we have not used them. It turned out that it was impossible to let TF know that an edge carries values, when saving data as a new feature. Now it is possible.","title":"2.3.15"},{"location":"About/Older/#2314","text":"2017-09-29 Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible. This is the consequence of a bug in precomputing the sections sections . The preparation step used 1 range ( firstVerse , lastVerse ) somewhere, which should of course have been 1 range ( firstVerse , lastVerse + 1 )","title":"2.3.14"},{"location":"About/Older/#2313","text":"2017-09-28 Loading TF was not completely silent if silent=True was passed. Better now.","title":"2.3.13"},{"location":"About/Older/#2312","text":"2017-09-18 Small fix in TF.save() . The spec says that the metadata under the empty key will be inserted into all features, but in fact this did not happen. Instead it was used as a default when some feature did not have metadata specified. From now on, that metadata will spread through all features. New API function explore , to get a list of all known features in a dataset.","title":"2.3.12"},{"location":"About/Older/#2311","text":"2017-09-18 Small fix in Search: the implementation of the relation operator || (disjoint slot sets) was faulty. Repaired. The search tutorial got an extra example: how to look for gaps. Gaps are not a primitive in the TF search language. Yet the language turns out to be powerful enough to look for gaps. This answers a question by Cody Kingham.","title":"2.3.11"},{"location":"About/Older/#2310","text":"2017-08-24 When defining text formats in the otext.tf feature, you can now include newlines and tabs in the formats. Enter them as \\n and \\t .","title":"2.3.10"},{"location":"About/Older/#239","text":"2017-07-24 TF has a list of default locations to look for data sources: ~/Downloads , ~/github , etc. Now ~/Dropbox has been added to that list.","title":"2.3.9"},{"location":"About/Older/#238","text":"2017-07-24 The section levels (book, chapter, verse) were supposed to be customizable through the otext feature. But in fact, up till version 2.3.7 this did not work. From now on the names of the section types and the features that name/number them, are given in the otext feature. It is still the case that exactly three levels must be specified, otherwise it does not work.","title":"2.3.8"},{"location":"About/Older/#237","text":"2017-05-12 Fixes. Added an extra default location for looking for text-fabric-data sources, for the benefit of running text-fabric within a shared notebook service.","title":"2.3.7"},{"location":"About/Older/#235-236","text":"2017-03-01 Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms in the template got discarded after an outdent.","title":"2.3.5, 2.3.6"},{"location":"About/Older/#234","text":"2017-02-12 Also the Fabric() call can be made silent now.","title":"2.3.4"},{"location":"About/Older/#233","text":"2017-02-11 Improvements: you can load features more silently. See TF.load() ; you can search more silently. See S.study() ; you can search more concisely. See the new S.search() ; when fetching results, the amount parameter of S.fetch() has been renamed to limit ; the tutorial notebooks (see links on top) have been updated.","title":"2.3.3"},{"location":"About/Older/#232","text":"2017-02-03 Bug fix: the results of F.feature.s() , E.feature.f() , and E.features.t() are now all tuples. They were a mixture of tuples and lists.","title":"2.3.2"},{"location":"About/Older/#231","text":"2017-01-23 Bug fix: when searching simple queries with only one query node, the result nodes were delivered as integers, instead of 1-tuples of integers.","title":"2.3.1"},{"location":"About/Older/#23","text":"2017-01-13 We start archiving releases of Text-Fabric at Zenodo .","title":"2.3"},{"location":"About/Older/#221","text":"2017-01-09 Small fixes.","title":"2.2.1"},{"location":"About/Older/#220","text":"2017-01-06","title":"2.2.0"},{"location":"About/Older/#new-sortkey","text":"The API has a new member: sortKey New relationships in templates: nearness . See for examples the end of the searchTutorial . Thanks to James Cu\u00e9nod for requesting nearness operators.","title":"New: sortKey"},{"location":"About/Older/#fixes","text":"in S.glean() word nodes were not printed; the check whether the search graph consists of a single connected component did not handle the case of one node without edges well;","title":"Fixes"},{"location":"About/Older/#213","text":"2017-01-04 Various fixes.","title":"2.1.3"},{"location":"About/Older/#210","text":"2017-01-04","title":"2.1.0"},{"location":"About/Older/#new-relations","text":"Some relations have been added to search templates: =: and := and :: : start at same slot , end at same slot , start at same slot and end at same slot <: and :> : adjacent before and adjacent next . The latter two can also be used through the L -api: L.p() and L.n() . The data that feeds them is precomputed and available as C.boundary .","title":"New: relations"},{"location":"About/Older/#new-enhanced-search-templates","text":"You can now easily make extra constraints in search templates without naming atoms. See the searchTutorial for an updated exposition on searching.","title":"New: enhanced search templates"},{"location":"About/Older/#200","text":"2016-12-23","title":"2.0.0"},{"location":"About/Older/#new-search","text":"Want to feel cosy with Christmas? Put your laptop on your lap, update Text-Fabric, and start playing with search. Your laptop will spin itself warm with your queries! Text-Fabric just got a powerful search facility, based on (graph)-templates. It is still very fresh, and more experimentation will be needed. Feedback is welcome. Start with the tutorial . The implementation of this search engine can be nicely explained with a textile metaphor: spinning wool into yarn and then stitching the yarns together. That will be explained further in a document that I'll love to write during Xmas.","title":"New: Search"},{"location":"About/Older/#127","text":"2016-12-14","title":"1.2.7"},{"location":"About/Older/#new","text":"F.otype.sInterval()","title":"New"},{"location":"About/Older/#126","text":"2016-12-14","title":"1.2.6"},{"location":"About/Older/#bug-fix","text":"There was an error in computing the order of nodes. One of the consequences was that objects that occupy the same slots were not ordered properly. And that had as consequence that you could not go up from words in one-word phrases to their containing phrase. It has been remedied. Note Your computed data needs to be refreshed. This can be done by calling a new function TF.clearCache() . When you use TF after this, you will see it working quite hard to recompute a bunch of data.","title":"bug fix"},{"location":"About/Older/#125","text":"2016-12-13 Documentation update","title":"1.2.5"},{"location":"About/Older/#120","text":"2016-12-08 Note Data update needed","title":"1.2.0"},{"location":"About/Older/#new_1","text":"","title":"New"},{"location":"About/Older/#frequency-lists","text":"F.feature.freqList() : get a sorted frequency list for any feature. Handy as a first step in exploring a feature.","title":"Frequency lists"},{"location":"About/Older/#export-to-mql","text":"TF.exportMQL() : export a whole dataset as a MQL database. Including all modules that you have loaded with it.","title":"Export to MQL"},{"location":"About/Older/#changed","text":"The slot numbers start at 0, no longer at 1. Personally I prefer the zero starting point, but Emdros insists on positive monads and objects ids. Most important is that users do not have to add/subtract one from the numbers they see in TF if they want to use it in MQL and vice versa. Because of this you need to update your data too: 1 2 cd ~/github/text-fabric-data git pull origin master","title":"Changed"},{"location":"Api/App/","text":"App API \u00b6 About The app-API provides extra functionality of top of the core of TF. The most notable thing is display. The functions plain() and pretty() are able to display nodes using extra information about what those nodes typically represent in a specific corpus. The app-API further knows how to download the data, and can be invoked by a simple incantation. Incantation \u00b6 A.use() 1 2 from tf.app import use A = use ( appName ) See Corpora for what you can use. Hint: think of the use {database} statements in MySQL and MongoDb. Without further arguments, this will set up an TF core API with most features loaded, wrapped in an app-specific API. Start up sequence During start-up the following happens: (1) the corpus data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations is executed; (3) most features of the corpus are loaded. There are many scenarios in which you can work with a TF app: in a Python script or in a notebook. If you start the TF browser, a TF app is started in a kernel , a process that can communicate with other processes, such as web servers, much like how a database communicates with programs that need data. Sometimes you want to load all features without thinking, at other times you want to be selective. You may want to load downloadable features from the internet, or you want to experiment with features you are developing. A TF app can be invoked for all these scenarios by supplying additional arguments to the incantation. appName:specifier, checkData=specifier The specifiers let you use a specific point in the history of the app and data. appName:specifier is for the TF-app application code. checkoutData=specifier is for the main data of the TF-app. '' (empty string or absent) ( default ): use the latest release; if there are no releases, use the latest commit. latest : use the latest release. If there are commits after the commit that has been tagged with the latest release, these will not be used. hot : use the latest commit, even if it comes after the latest commit of the latest release. release tag , e.g. v1.3 : use exactly this release. More precisely, this is the commit that has been tagged with that release tag. commit hash , e.g. 2d0ca1f593805af0c13c4a62ed7405b94d870045 : use exactly this commit. local : use local data from your ~/text-fabric-data directory if it is present, otherwise fail. clone : use local data from your ~/github directory if it is present, otherwise fail. hoist=globals() This makes the API elements directly available as global names in your script or notebook: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you pass this argument, TF will show you what names will be inserted in your namespace. And if you are in a Jupyter notebook, these names are linked to their documentation. Without further arguments, TF thinks you are a user that wants to work with as much data possible without hassle. It makes some standard choices for you, and it will auto-download data. The following options are for people who want increasingly finer control over the features that are being loaded. version=VERSION If you do not want to work with the default version of your main corpus, you can specify a different version here. versions 1 A = use ( 'xxxx' , version = '2017' ) In fact, this single statement will download that Xxxx version if you do not already have it. Modules If you also ask for extra data modules by means of the mod argument, then the corresponding version of those modules will be chosen. Every properly designed data module must refer to a specific version of the main source! mod=None mod is a comma-separated list of modules in one of the forms 1 2 {org}/{repo}/{path} {org}/{repo}/{path}:specifier All features of all those modules will be loaded. If they are not yet present, they will be downloaded from GitHub first. For example, there is an easter egg module on GitHub, and you can obtain it by 1 mod='etcbc/lingo/easter/tf' Here the {org} is etcbc , the {repo} is lingo , and the {path} is easter/tf under which version c of the feature egg is available in TF format. You can point to any such directory om the entire GitHub if you know that it contains relevant features. The specifier is as in appName:specifier and checkData=specifier above: for using a different point in the history. Your TF app might be configured to download specific modules. See MODULE_SPECS in the app's config.py , Let TF manage your text-fabric-data directory It is better not to fiddle with your ~/text-fabric-data directory manually. Let it be filled with auto-downloaded data. You can then delete data sources and modules when needed, and have them redownloaded at your wish, without any hassle or data loss. locations and modules arguments If you want to add other search locations for TF features manually, you can pass optional locations and modules parameters, which will be passed to the Fabric() call to the core of TF. More, not less Using these arguments will load features on top of the default selection of features. You cannot use these arguments to prevent features from being loaded. api=None So far, the TF app will construct a generic TF API with a more or less standard set of features loaded, and make that API avaible to you, under A.api . But you can also setup an API yourself by using the core TF machinery : 1 2 3 from tf.fabric import Fabric TF = Fabric ( locations =... , modules =... ) api = TF . load ( features ) Here you have full control over what you load and what not. If you want the extra power of the TF app, you can wrap this api : 1 A = use ( 'xxxx' , api = api ) Unloaded features Some apps do not load all available features of the corpus by default. This happens when a corpus contains quite a number of features that most people never need. Loading them cost time and takes a lot of RAM. In the case where you need an available feature that has not been loaded, you can load it by demanding 1 TF . load ( 'feature1 feature2' , add = True ) provided you have used the hoist=globals() parameter earlier. If not, you have to say 1 A . api . TF . load ( 'feature1 feature2' , add = True ) silent=False If True , nearly all output of this call will be suppressed, including the links to the loaded data, features, and the API methods. Error messages will still come through. Search \u00b6 A.search() 1 A . search ( query , silent = False , shallow = False , sets = None , sort = True ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them in the canonical order . (but you can change the sorting, see the sort parameter). It then reports the number of results. It will also set the display parameter tupleFeatures (see below) in such a way that subsequent calls to A.export() emit the features that have been used in the query. query the search template that has to be searched for. silent if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple are retained. If False or 0 , a list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set gappedPhrases of all phrase nodes that have a gap, you can pass sets=dict(gphrase=gappedPhrases) , and then in your query you can say 1 2 gphrase function=Pred word sp=verb etc. This is handy when you need node sets that cannot be conveniently queried. You can produce them by hand-coding. Once you got them, you can use them over and over again in queries. Or you can save them with writeSets and use them in the TF browser. sort If True (default), search results will be returned in canonical order . canonical sort key for tuples This sort is achieved by using the function sortKeyTuple as sort key. If it is a sort key , i.e. function that can be applied to tuples of nodes returning values, then these values will be used to sort the results. If it is a False value, no sorting will be applied. search template reference See the search template reference Linking \u00b6 A.webLink() 1 A . webLink ( node , text = None ) Description Produces a link to an online website dedicated to the main corpus. node node can be an arbitrary node. The link targets the verse/tablet that contains the first word/sign contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse or tablet column line) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator will be chosen. Ezra 3:4 on SHEBANQ First we obtain the node 1 z = A . nodeFromSectionStr ( 'Ezra 3:4' ) then we call webLink 1 A . webLink ( z ) with this result: Ezra 3:4 Sections \u00b6 A.nodeFromSectionStr() 1 A . nodeFromSectionStr ( sectionStr , lang = 'en' ) Description Given a section string pointing to a section, return the corresponding node (or an error message). Compare T.nodeFromSection . sectionStr sectionStr must be a valid section specficiation in the language specified in lang . The string may specific a section 0 level only (book/tablet), or section 0 and 1 levels (book/tablet chapter/column), or all levels (book/tablet chapter/column verse/line). Examples: 1 2 3 Genesis Genesis 1 Genesis 1:1 1 2 3 P005381 P005381 1 P005381 1:1 Depending on what is passed, the result is a node of section level 0, 1, or 2. lang The language assumed for the section parts, as far as they are language dependent. A.sectionStrFromNode() 1 A . sectionStrFromNode ( node , lang = 'en' , lastSlot = False , fillup = False ) Description Returns the section label (a string) that correspond to the reference node, which is the first or last slot belonging to node , dependent on lastSlot . The result is a string, built from the labels of the individual section levels (in language lang where applicable). But see the fillup parameter. Compare T.sectionFromNode . node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. fillup Same as for T.sectionTuple() Display \u00b6 About Where a TF app really shines is in displaying nodes. There are basically two ways of displaying a node: plain : just the associated text of a node, or if that would be too much, an identifying label of that node (e.g. for books, chapters and lexemes). pretty : a display of the internal structure of the textual object a node stands for. That structure is adorned with relevant feature values. A TF app offers these display methods for nodes, tuples of nodes, and iterables of tuples of nodes (think: query results). The names of these methods are 1 2 3 plain () plainTuple () table () pretty () prettyTuple () show () Setting up display parameters There is a bunch of parameters that govern how the display functions arrive at their results. You can pass them as optional arguments to these functions, or you can set up them in advance, and reset them to their original state when you are done. All calls to the display functions look for the values for these parameters in the following order: optional parameters passed directly to the function, values as set up by previous calls to displaySetup() , default values configured by the app. List of display parameters These are the parameters and their default values: colorMap=None Which nodes of a tuple (or list of tuples) will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . condensed=False indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show container nodes with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. This is how SHEBANQ displays its query results. See also the parameter condenseType . False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. condenseType=None The type of container to be used for condensing results. The default is app dependent, usually verse or tablet . end=None end is the end point in the iterable of results. If None , displaying will stop after the end of the iterable. extraFeatures=() A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. full=False For pretty displays: indicates that the whole object should be displayed, even if it is big. Big objects Big objects are objects of a type that is bigger than the default condense type. fmt=None fmt is the text format that will be used for the representation. E.g. text-orig-full . Text formats Use T.formats to inspect what text formats are available in your corpus. highlights={} When nodes such as verses and sentences and lines and cases are displayed by plain() or pretty() , their contents is also displayed. You can selectively highlight those parts. highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. linked=1 the number of the column whose cell contents is web-linked to the relevant passage (the first column is column 1). noneValues=None A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values, e.g. 1 noneValues = { None , 'NA' , 'unknown' , 'm' , 'sg' , 'p3' } None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. start=None start is the starting point for displaying the iterable of results. (1 is the first one). If None , displaying starts at the first element of the iterable. suppress=set() a set of names of features that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. tupleFeatures=() A bit like \"extraFeatures\" above, but more intricate. Only meant to steer the A.export() function below into outputting the features you choose. It should be a tuple of pairs (i, features) which means that to member i of a result tuple we assign extra features . features may be given as an iterable or a space separated string of feature names. withNodes=False indicates whether node numbers should be displayed. zooming in If you are in a Jupyter notebook, you can inspect in a powerful way by setting withNodes=True . Then every part of a pretty display shows its node number, and you can use L F T E to look up all information about each node that the corpus has to offer. withPassage=True indicates whether a passage label should be put next to a displayed node or tuple of nodes. A.displaySetup() 1 A . displaySetup ( parameter1 = value1 , parameter2 = value2 , ... ) Assigns working values to display parameters (see the section above). All subsequent calls to the display functions will use these values, unless they are passed overriding values as arguments. These working values remain in effect until a new call to displaySetup() assigns new values, or a call to displayReset() resets the values to the defaults. extra features 1 A . displayReset ( extraFeatures = 'egg' ) will cause displays to show the values of the egg feature for each node that has egg values: 1 A . pretty ( n ) But if you want to suppress the egg for a particular display, you can say 1 A . pretty ( n , extraFeatures = '' ) And if you want to turn it off for future displays, you can say 1 A . displayReset ( 'extraFeatures' ) A.displayReset() 1 A . displayReset ( parameter1 , parameter2 , ... ) Reset the given display parameters to their default value and let the others retain their current value. So you can reset the display parameters selectively. Reset all If you call this function without parameters at all, the effect is that all display parameters are reset to their default values: 1 A . displayReset () A.plain() 1 A . plain ( node , isLinked = False , ** displayParameters ) Description Displays the material that corresponds to a node in a compact way. Nodes with little content will be represented by their text content, nodes with large content will be represented by an identifying label. node a node of arbitrary type. isLinked indicates whether the result should be a web-link to the appropriate passage. A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , ** displayParameters ) Description Displays the material that corresponds to a tuple of nodes as a row of cells, each displaying a member of the tuple by means of plain() . nodes an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. A.table() 1 A . table ( results , ** displayParameters ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable, and then displayed by plainTuple() results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condense, condenseType You can condense the list first to containers of condenseType , before displaying the list. Pass the display parameters condense and condenseType (documented above). A.export() 1 A . export ( results , toDir = None , toFile = None , ** displayParameters ) Description Exports an iterable of tuples of nodes to an Excel friendly .tsv file. There will be a row for each tuple. The columns are: R the sequence number of the result tuple in the result list S1 S2 S3 the section as book, chapter, verse, in separate columns; the section is the section of the first non book/chapter node in the tuple NODEi TYPEi the node and its type, for each node i in the result tuple TEXTi the full text of node i , if the node type admits a concise text representation; the criterion is whether the node type has a type not bigger than the default condense type, which is app specific. If you pass an explicit condenseType= xxx as display parameter, then this is the reference condenseType on which the decision is based. XFi the value of extra feature XF for node i , where these features have been declared by a previous displaySetup(tupleFeatures=...)` See for detailed examples the exportExcel and exportExcel notebooks. results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. toDir The destination directory for the exported file. By default it is your Downloads folder. If the directory does not exist, it will be created. toFile The name of the exported file. By default it is results.tsv condensed= Has no effect. Exports to Excel will not be condensed, because the number of columns is variable per row in that case. Excel itself as nice possibilities for grouping rows. You can also filter your tuples by means of hand-coding before exporting them. condenseType= The condense type influences for which nodes the full text will be exported. Only nodes that are \"smaller\" than the condense type will have their full text exported. fmt= This display parameter specifies the text format for any nodes that trigger a text value to be exported. tupleFeatures= This is a display parameter that steers which features are exported with each member of the tuples in the list. See above under \"Setting up display parameters\" If the iterable of tuples are the results of a query you have just run, then an appropriate call to displaySetup(tupleFeatures=...) has already been issued, so you can just say: 1 2 results = A . search ( query ) A . export ( results ) Encoding The exported file is written in the utf_16_le encoding. This ensures that Excel can open it without hassle, even if there are non-latin characters inside. When you want to read the exported file programmatically, open it with encoding=utf_16 . A.pretty() 1 A . pretty ( node , ** displayParameters ) Description Displays the material that corresponds to a node in a graphical way. The internal structure of the nodes that are involved is also revealed. node a node of arbitrary type. full True or False. Normally pretty(n) only displays a summary of n if n is big, i.e. it has a type bigger than the condense type . If you still want to see the whole big object, pass True: pretty(n, full=True) . condense type Alternatively, you could have said pretty(n, condenseType='xxx') with xxx a big enough type, but this is a bit clumsy. extraFeatures, tupleFeatures These display parameters govern which extra features will be displayed in pretty displays. The union of what these parameters specify will be taken. A.prettyTuple() 1 A . prettyTuple ( tuple , seqNumber , ** displayParameters ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. The member nodes of the tuple will be collected into containers, which will be displayed with pretty() , and the nodes of the tuple will be highlighted in the containers. tuple tuple is an arbitrary tuple of nodes of arbitrary types. A.show() 1 A . show ( results , ** displayParameters ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condense, condenseType You can condense the list first to containers of condenseType , before displaying the list. Pass the display parameters condense and condenseType (documented above).","title":"Apps"},{"location":"Api/App/#app-api","text":"About The app-API provides extra functionality of top of the core of TF. The most notable thing is display. The functions plain() and pretty() are able to display nodes using extra information about what those nodes typically represent in a specific corpus. The app-API further knows how to download the data, and can be invoked by a simple incantation.","title":"App API"},{"location":"Api/App/#incantation","text":"A.use() 1 2 from tf.app import use A = use ( appName ) See Corpora for what you can use. Hint: think of the use {database} statements in MySQL and MongoDb. Without further arguments, this will set up an TF core API with most features loaded, wrapped in an app-specific API. Start up sequence During start-up the following happens: (1) the corpus data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations is executed; (3) most features of the corpus are loaded. There are many scenarios in which you can work with a TF app: in a Python script or in a notebook. If you start the TF browser, a TF app is started in a kernel , a process that can communicate with other processes, such as web servers, much like how a database communicates with programs that need data. Sometimes you want to load all features without thinking, at other times you want to be selective. You may want to load downloadable features from the internet, or you want to experiment with features you are developing. A TF app can be invoked for all these scenarios by supplying additional arguments to the incantation. appName:specifier, checkData=specifier The specifiers let you use a specific point in the history of the app and data. appName:specifier is for the TF-app application code. checkoutData=specifier is for the main data of the TF-app. '' (empty string or absent) ( default ): use the latest release; if there are no releases, use the latest commit. latest : use the latest release. If there are commits after the commit that has been tagged with the latest release, these will not be used. hot : use the latest commit, even if it comes after the latest commit of the latest release. release tag , e.g. v1.3 : use exactly this release. More precisely, this is the commit that has been tagged with that release tag. commit hash , e.g. 2d0ca1f593805af0c13c4a62ed7405b94d870045 : use exactly this commit. local : use local data from your ~/text-fabric-data directory if it is present, otherwise fail. clone : use local data from your ~/github directory if it is present, otherwise fail. hoist=globals() This makes the API elements directly available as global names in your script or notebook: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you pass this argument, TF will show you what names will be inserted in your namespace. And if you are in a Jupyter notebook, these names are linked to their documentation. Without further arguments, TF thinks you are a user that wants to work with as much data possible without hassle. It makes some standard choices for you, and it will auto-download data. The following options are for people who want increasingly finer control over the features that are being loaded. version=VERSION If you do not want to work with the default version of your main corpus, you can specify a different version here. versions 1 A = use ( 'xxxx' , version = '2017' ) In fact, this single statement will download that Xxxx version if you do not already have it. Modules If you also ask for extra data modules by means of the mod argument, then the corresponding version of those modules will be chosen. Every properly designed data module must refer to a specific version of the main source! mod=None mod is a comma-separated list of modules in one of the forms 1 2 {org}/{repo}/{path} {org}/{repo}/{path}:specifier All features of all those modules will be loaded. If they are not yet present, they will be downloaded from GitHub first. For example, there is an easter egg module on GitHub, and you can obtain it by 1 mod='etcbc/lingo/easter/tf' Here the {org} is etcbc , the {repo} is lingo , and the {path} is easter/tf under which version c of the feature egg is available in TF format. You can point to any such directory om the entire GitHub if you know that it contains relevant features. The specifier is as in appName:specifier and checkData=specifier above: for using a different point in the history. Your TF app might be configured to download specific modules. See MODULE_SPECS in the app's config.py , Let TF manage your text-fabric-data directory It is better not to fiddle with your ~/text-fabric-data directory manually. Let it be filled with auto-downloaded data. You can then delete data sources and modules when needed, and have them redownloaded at your wish, without any hassle or data loss. locations and modules arguments If you want to add other search locations for TF features manually, you can pass optional locations and modules parameters, which will be passed to the Fabric() call to the core of TF. More, not less Using these arguments will load features on top of the default selection of features. You cannot use these arguments to prevent features from being loaded. api=None So far, the TF app will construct a generic TF API with a more or less standard set of features loaded, and make that API avaible to you, under A.api . But you can also setup an API yourself by using the core TF machinery : 1 2 3 from tf.fabric import Fabric TF = Fabric ( locations =... , modules =... ) api = TF . load ( features ) Here you have full control over what you load and what not. If you want the extra power of the TF app, you can wrap this api : 1 A = use ( 'xxxx' , api = api ) Unloaded features Some apps do not load all available features of the corpus by default. This happens when a corpus contains quite a number of features that most people never need. Loading them cost time and takes a lot of RAM. In the case where you need an available feature that has not been loaded, you can load it by demanding 1 TF . load ( 'feature1 feature2' , add = True ) provided you have used the hoist=globals() parameter earlier. If not, you have to say 1 A . api . TF . load ( 'feature1 feature2' , add = True ) silent=False If True , nearly all output of this call will be suppressed, including the links to the loaded data, features, and the API methods. Error messages will still come through.","title":"Incantation"},{"location":"Api/App/#search","text":"A.search() 1 A . search ( query , silent = False , shallow = False , sets = None , sort = True ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them in the canonical order . (but you can change the sorting, see the sort parameter). It then reports the number of results. It will also set the display parameter tupleFeatures (see below) in such a way that subsequent calls to A.export() emit the features that have been used in the query. query the search template that has to be searched for. silent if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple are retained. If False or 0 , a list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set gappedPhrases of all phrase nodes that have a gap, you can pass sets=dict(gphrase=gappedPhrases) , and then in your query you can say 1 2 gphrase function=Pred word sp=verb etc. This is handy when you need node sets that cannot be conveniently queried. You can produce them by hand-coding. Once you got them, you can use them over and over again in queries. Or you can save them with writeSets and use them in the TF browser. sort If True (default), search results will be returned in canonical order . canonical sort key for tuples This sort is achieved by using the function sortKeyTuple as sort key. If it is a sort key , i.e. function that can be applied to tuples of nodes returning values, then these values will be used to sort the results. If it is a False value, no sorting will be applied. search template reference See the search template reference","title":"Search"},{"location":"Api/App/#linking","text":"A.webLink() 1 A . webLink ( node , text = None ) Description Produces a link to an online website dedicated to the main corpus. node node can be an arbitrary node. The link targets the verse/tablet that contains the first word/sign contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse or tablet column line) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator will be chosen. Ezra 3:4 on SHEBANQ First we obtain the node 1 z = A . nodeFromSectionStr ( 'Ezra 3:4' ) then we call webLink 1 A . webLink ( z ) with this result: Ezra 3:4","title":"Linking"},{"location":"Api/App/#sections","text":"A.nodeFromSectionStr() 1 A . nodeFromSectionStr ( sectionStr , lang = 'en' ) Description Given a section string pointing to a section, return the corresponding node (or an error message). Compare T.nodeFromSection . sectionStr sectionStr must be a valid section specficiation in the language specified in lang . The string may specific a section 0 level only (book/tablet), or section 0 and 1 levels (book/tablet chapter/column), or all levels (book/tablet chapter/column verse/line). Examples: 1 2 3 Genesis Genesis 1 Genesis 1:1 1 2 3 P005381 P005381 1 P005381 1:1 Depending on what is passed, the result is a node of section level 0, 1, or 2. lang The language assumed for the section parts, as far as they are language dependent. A.sectionStrFromNode() 1 A . sectionStrFromNode ( node , lang = 'en' , lastSlot = False , fillup = False ) Description Returns the section label (a string) that correspond to the reference node, which is the first or last slot belonging to node , dependent on lastSlot . The result is a string, built from the labels of the individual section levels (in language lang where applicable). But see the fillup parameter. Compare T.sectionFromNode . node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. fillup Same as for T.sectionTuple()","title":"Sections"},{"location":"Api/App/#display","text":"About Where a TF app really shines is in displaying nodes. There are basically two ways of displaying a node: plain : just the associated text of a node, or if that would be too much, an identifying label of that node (e.g. for books, chapters and lexemes). pretty : a display of the internal structure of the textual object a node stands for. That structure is adorned with relevant feature values. A TF app offers these display methods for nodes, tuples of nodes, and iterables of tuples of nodes (think: query results). The names of these methods are 1 2 3 plain () plainTuple () table () pretty () prettyTuple () show () Setting up display parameters There is a bunch of parameters that govern how the display functions arrive at their results. You can pass them as optional arguments to these functions, or you can set up them in advance, and reset them to their original state when you are done. All calls to the display functions look for the values for these parameters in the following order: optional parameters passed directly to the function, values as set up by previous calls to displaySetup() , default values configured by the app. List of display parameters These are the parameters and their default values: colorMap=None Which nodes of a tuple (or list of tuples) will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . condensed=False indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show container nodes with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. This is how SHEBANQ displays its query results. See also the parameter condenseType . False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. condenseType=None The type of container to be used for condensing results. The default is app dependent, usually verse or tablet . end=None end is the end point in the iterable of results. If None , displaying will stop after the end of the iterable. extraFeatures=() A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. full=False For pretty displays: indicates that the whole object should be displayed, even if it is big. Big objects Big objects are objects of a type that is bigger than the default condense type. fmt=None fmt is the text format that will be used for the representation. E.g. text-orig-full . Text formats Use T.formats to inspect what text formats are available in your corpus. highlights={} When nodes such as verses and sentences and lines and cases are displayed by plain() or pretty() , their contents is also displayed. You can selectively highlight those parts. highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. linked=1 the number of the column whose cell contents is web-linked to the relevant passage (the first column is column 1). noneValues=None A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values, e.g. 1 noneValues = { None , 'NA' , 'unknown' , 'm' , 'sg' , 'p3' } None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. start=None start is the starting point for displaying the iterable of results. (1 is the first one). If None , displaying starts at the first element of the iterable. suppress=set() a set of names of features that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. tupleFeatures=() A bit like \"extraFeatures\" above, but more intricate. Only meant to steer the A.export() function below into outputting the features you choose. It should be a tuple of pairs (i, features) which means that to member i of a result tuple we assign extra features . features may be given as an iterable or a space separated string of feature names. withNodes=False indicates whether node numbers should be displayed. zooming in If you are in a Jupyter notebook, you can inspect in a powerful way by setting withNodes=True . Then every part of a pretty display shows its node number, and you can use L F T E to look up all information about each node that the corpus has to offer. withPassage=True indicates whether a passage label should be put next to a displayed node or tuple of nodes. A.displaySetup() 1 A . displaySetup ( parameter1 = value1 , parameter2 = value2 , ... ) Assigns working values to display parameters (see the section above). All subsequent calls to the display functions will use these values, unless they are passed overriding values as arguments. These working values remain in effect until a new call to displaySetup() assigns new values, or a call to displayReset() resets the values to the defaults. extra features 1 A . displayReset ( extraFeatures = 'egg' ) will cause displays to show the values of the egg feature for each node that has egg values: 1 A . pretty ( n ) But if you want to suppress the egg for a particular display, you can say 1 A . pretty ( n , extraFeatures = '' ) And if you want to turn it off for future displays, you can say 1 A . displayReset ( 'extraFeatures' ) A.displayReset() 1 A . displayReset ( parameter1 , parameter2 , ... ) Reset the given display parameters to their default value and let the others retain their current value. So you can reset the display parameters selectively. Reset all If you call this function without parameters at all, the effect is that all display parameters are reset to their default values: 1 A . displayReset () A.plain() 1 A . plain ( node , isLinked = False , ** displayParameters ) Description Displays the material that corresponds to a node in a compact way. Nodes with little content will be represented by their text content, nodes with large content will be represented by an identifying label. node a node of arbitrary type. isLinked indicates whether the result should be a web-link to the appropriate passage. A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , ** displayParameters ) Description Displays the material that corresponds to a tuple of nodes as a row of cells, each displaying a member of the tuple by means of plain() . nodes an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. A.table() 1 A . table ( results , ** displayParameters ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable, and then displayed by plainTuple() results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condense, condenseType You can condense the list first to containers of condenseType , before displaying the list. Pass the display parameters condense and condenseType (documented above). A.export() 1 A . export ( results , toDir = None , toFile = None , ** displayParameters ) Description Exports an iterable of tuples of nodes to an Excel friendly .tsv file. There will be a row for each tuple. The columns are: R the sequence number of the result tuple in the result list S1 S2 S3 the section as book, chapter, verse, in separate columns; the section is the section of the first non book/chapter node in the tuple NODEi TYPEi the node and its type, for each node i in the result tuple TEXTi the full text of node i , if the node type admits a concise text representation; the criterion is whether the node type has a type not bigger than the default condense type, which is app specific. If you pass an explicit condenseType= xxx as display parameter, then this is the reference condenseType on which the decision is based. XFi the value of extra feature XF for node i , where these features have been declared by a previous displaySetup(tupleFeatures=...)` See for detailed examples the exportExcel and exportExcel notebooks. results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. toDir The destination directory for the exported file. By default it is your Downloads folder. If the directory does not exist, it will be created. toFile The name of the exported file. By default it is results.tsv condensed= Has no effect. Exports to Excel will not be condensed, because the number of columns is variable per row in that case. Excel itself as nice possibilities for grouping rows. You can also filter your tuples by means of hand-coding before exporting them. condenseType= The condense type influences for which nodes the full text will be exported. Only nodes that are \"smaller\" than the condense type will have their full text exported. fmt= This display parameter specifies the text format for any nodes that trigger a text value to be exported. tupleFeatures= This is a display parameter that steers which features are exported with each member of the tuples in the list. See above under \"Setting up display parameters\" If the iterable of tuples are the results of a query you have just run, then an appropriate call to displaySetup(tupleFeatures=...) has already been issued, so you can just say: 1 2 results = A . search ( query ) A . export ( results ) Encoding The exported file is written in the utf_16_le encoding. This ensures that Excel can open it without hassle, even if there are non-latin characters inside. When you want to read the exported file programmatically, open it with encoding=utf_16 . A.pretty() 1 A . pretty ( node , ** displayParameters ) Description Displays the material that corresponds to a node in a graphical way. The internal structure of the nodes that are involved is also revealed. node a node of arbitrary type. full True or False. Normally pretty(n) only displays a summary of n if n is big, i.e. it has a type bigger than the condense type . If you still want to see the whole big object, pass True: pretty(n, full=True) . condense type Alternatively, you could have said pretty(n, condenseType='xxx') with xxx a big enough type, but this is a bit clumsy. extraFeatures, tupleFeatures These display parameters govern which extra features will be displayed in pretty displays. The union of what these parameters specify will be taken. A.prettyTuple() 1 A . prettyTuple ( tuple , seqNumber , ** displayParameters ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. The member nodes of the tuple will be collected into containers, which will be displayed with pretty() , and the nodes of the tuple will be highlighted in the containers. tuple tuple is an arbitrary tuple of nodes of arbitrary types. A.show() 1 A . show ( results , ** displayParameters ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condense, condenseType You can condense the list first to containers of condenseType , before displaying the list. Pass the display parameters condense and condenseType (documented above).","title":"Display"},{"location":"Api/Compose/","text":"Combining data \u00b6 Explanation This package contains functions to operate on TF datasets as a whole There are the following basic operations: add/merge/delete types and features to/from a single data source combine several data sources into one See also the compose chapter in the Banks tutorial. Modify \u00b6 usage 1 2 3 4 5 6 7 8 9 10 11 12 13 from tf.compose import add modify ( location , targetLocation , mergeFeatures = None , deleteFeatures = None , addFeatures = None , mergeTypes = None , deleteTypes = None , addTypes = None , featureMeta = None , silent = False , ) overview Modifies the supply of node types and features in a single data set. Dependent on the presence of the parameters, the following steps will be executed before the result is written out as a new TF dataset: merge existing features into an other feature, removing the features that went in; delete any number of existing features; add any number of features and their data; merge existing node types into a new one, removing the types that went in, without loss of nodes; So far, no new nodes have been added or removed. But then: delete any number of node types with their nodes; add any number of new node types, with nodes and features. The last two action lead to a shifting of nodes, and all features that map them, will be shifted accordingly. You can also pass meta data to be merged in. Finally, the resulting features will be written to disk. location You can pass just the location of the original dataset in the file system, i.e. the directory that contains the .tf files. targetLocation The directory into which the result dataset will be written. mergeFeatures You can merge several features into one. This is especially useful if there are many features each operating on different node types, and you want to unify them into one feature. The situation may occur that several of the features to be merged supply conflicting values for a node. Then the last feature in the merge list wins. The result feature may exist already. Also then there is a risk of conflict. Again, the merge result wins. 1 2 3 4 mergeFeatures = dict ( resultFeature1 = [ feat1 , feat2 ], resultFeature2 = [ feat3 , feat4 ], ), If the resulting feature is new, or needs a new description, you can provide metadata in the featureMeta argument. For new features you may want to set the valueType , although we try hard to deduce it from the data available. deleteFeatures This should be an iterable or space separated string of features that you want to delete from the result. addFeatures You can add as many features as you want, assigning values to all types, including new nodes of new types that have been generated in the steps before. You can also use this parameter to override existing features: if a feature that you are adding already exists, the new data will be merged in, overriding assignments of the existing feature if there is a conflict. The meta data of the old and new feature will also be merged. The parameter must have this shape: 1 2 3 4 5 6 7 8 9 addFeatures = dict ( nodeFeatures = dict ( feat1 = data1 , feat2 = data2 , ), edgeFeatures = dict ( feat3 = data3 , feat4 = data4 , ), If the resulting features are new, or need a new description, you can provide metadata in the featureMeta argument. For new features you may want to set the valueType , although we try hard to deduce it from the data available. mergeTypes You can merge several node types into one. The merged node type will have the union of nodes of the types that are merged. All relevant features will stay the same, except the otype feature of course. You can pass additional information to be added as features to nodes of the new node type. These features can be used to discriminate between the merged types. The parameter you pass takes this shape 1 2 3 4 5 6 7 8 9 10 mergeTypes = dict ( outTypeA = ( 'inType1' , 'inType2' , ), outTypeB = ( 'inType3' , 'inType4' , ), ) or 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 mergeTypes = dict ( outTypeA = dict ( inType1 = dict ( featureI = valueI , featureK = valueK , ), inType2 = dict ( featureL = valueL , featureM = valueM , ), ), outTypeB = dict ( inType3 = dict ( featureN = valueN , featureO = valueO , ), inType4 = dict ( featureP = valueP , featureQ = valueQ , ), ), ) It does not matter if these types and features already occur. The outTypes may be existing types of really new types. The new features may be existing or new features. Do not forget to provide meta data for new features in the featureMeta argument. This will migrate nodes of type inType1 or inType2 to nodes of outTypeA . In the extended form, when there are feature specifications associated with the old types, after merging the following assignments will be made: featureI = valueI to nodes coming from inType1 and featureK = valueK to nodes coming from inType2 . No nodes will be removed! slot types Merging is all about non-slot types. It is an error if a new type or an old type is a slot type. deleteTypes You can delete node types from the result altogether. You can specify a list of node types as an iterable or as a space separated string. If a node type has to be deleted, all nodes in that type will be removed, and features that assign values to these nodes will have those assignments removed. Example 1 deleteTypes = ( 'line' , 'sentence' ) 1 deleteTypes = ( ' line sentence ' ) slot types Deleting is all about non-slot types. It is an error to attempt to delete slot type. addTypes You may add as many node types as you want. Per node type that you add, you need to specify the current boundaries of that type and how all those nodes map to slots. You can also add features that assign values to those nodes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 dict ( nodeType1 = dict ( nodeFrom = from1 , nodeTo = to1 , nodeSlots = slots1 , nodeFeatures = nFeatures1 , edgeFeatures = eFeatures1 , ), nodeType2 = dict ( nodeFrom = from2 , nodeTo = to2 , nodeSlots = slots2 , nodeFeatures = nFeatures2 , edgeFeatures = eFeatures2 , ), ), The boundaries may be completely arbitrary, so if you get your nodes from another TF data source, you do not need to align their values. If you also add features about those nodes, the only thing that matters is that the features assign the right values to the nodes within the boundaries. Assignments to nodes outside the boundaries will be ignored. The slots that you link the new nodes to, must exist in the original. You cannot use this function to add slots to your data set. existing node types It is an error if a new node type already exists in the original. nodeFeatures, edgeFeatures You can add any number of features. Per feature you have to provide the mapping that defines the feature. These features may be new, or they may already be present in the original data. If these features have values to nodes that are not within the boundaries of the new node type, those values will not be assigned but silently discarded. Node features are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = val1 , n2 = val2 , ), feat2 = dict ( n1 = val1 , n2 = val2 , ), ), Edge features without values are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = { m1 , m2 }, n2 = { m3 , m4 }, ), feat2 = dict ( n1 = { m5 , m6 }, n2 = { m7 , m8 }, ), ), Edge features with values are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = { m1 : v1 , m2 : v2 }, n2 = { m3 : v3 , m4 : v4 }, ), feat2 = dict ( n1 = { m5 : v5 , m6 : v6 }, n2 = { m7 : v7 , m8 : v8 }, ), ), featureMeta If the features you have specified in one of the paramers above are new, do not forget to pass metadata for them in this parameter It is especially important to state the value type: 1 2 3 4 5 6 7 8 9 10 featureMeta = dict ( featureI = dict ( valueType = 'int' , description = 'level of node' ), featureK = dict ( valueType = 'str' , description = 'subtype of node' ), ), You can also tweak the section/structure configuration and the text-formats that are specified in the otext feature. Just specify them as keys and values to the otext feature. The logic of tweaking meta data is this: what you provide in this parameter will be merged into existing meta data. If you want to remove a key from a feature, give it the value None. silent Suppress or enable informational messages. Combine \u00b6 usage 1 2 3 4 5 6 7 8 from tf.compose import combine combine ( ( location1 , location2 , ), targetLocation , ) 1 2 3 4 5 6 7 8 9 10 11 combine( ( (name1, location1), (name2, location2), ), targetLocation, componentType=None, componentFeature=None, featureMeta=None, **otext, ) overview Creates a new TF data source out of a number of other ones. You may pass as many component data sources as you want. The combination will be the union of all nodes of the components, rearranged according to their types, where node types with the same names will be merged. The slots of the result are the concatenation of the slots of the components, which must all have the same slot type. The node and edge features will be remapped, so that they have the same values in the combined data as they had in the individual components. Optionally, nodes corresponding to the components themselves will be added to the combined result. Care will be taken of the metadata of the features and the contents of the otext.tf feature, which consists of metadata only. All details of the combination can be steered by means of parameters. locations You can either pass just the locations of the components, or you can give them a name. If you do not give names, the locations will be used as names. targetLocation The directory into which the feature files of the combined dataset will be written. componentType, componentFeature If a string value for componentType and/or componentFeature is passed, a new node type will be added, with nodes for each component. There will also be a new feature, that assigns the name of a component to the node of that component. The name of the new node type is the value of componentType if it is a non-empty string, else it is the value of componentFeature . The name of the new feature is componentFeature if it is a non-empty string, else it is the value of componentType . componentType must be fresh It is an error if the componentType is a node type that already occurs in one of the components. componentFeature may exist The componentFeature may already exist in one or more components. In that case the new feature values for nodes of componentType will just be added to it. Example 1 2 3 4 5 6 7 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , componentType = 'volume' , componentFeature = 'vol' , ) This results of a dataset with nodes and features from the components found at the indicated places on your file system. After combination, the components are visible in the data set as nodes of type volume , and the feature vol provides the names banks and river for those nodes. featureMeta The meta data of the components involved will be merged. If feature metadata of the same feature is encountered in different components, and if components specify different values for the same keys, the different values will be stored under a key with the name of the component appended to the key, separated by a ! . The special metadata field valueType will just be reduced to one single value str if some components have it as str and others as int . If the components assign the same value type to a feature, that value type will be assigned to the combined feature. If you want to assign other meta data to specific features, or pass meta data for new features that orginate from the merging process, you can pass them in the parameter featureMeta as in the following example, where we pass meta data for a feature called level with integer values. Example 1 2 3 4 5 6 7 8 9 10 11 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , featureMeta = dict ( level = dict ( valueType = 'int' , description = 'level of a section node' , ), ), ) The contents of the otext.tf features are also metadata, and their contents will be merged in exactly the same way. So if the section/structure specifications and the formats are not the same for all components, you will see them spread out in fields qualified by the component name with a ! sign between the key and the component. But you can add new specifications explicitly, as meta data of the otext feature. by passing them as keyword arguments. They will be passed directly to the combined otext.tf feature and will override anything with the same key that is already in one of the components. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , featureMeta = dict ( otext = dict ( componentType = 'volume' , componentFeature = 'vol' , sectionTypes = 'volume,chapter,line' , sectionFeatures = 'title,number,number' , ), ), silent = False , ) will give rise to something like this (assuming that banks and rivers have some deviating material in their otext.tf : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @config @compiler=Dirk Roorda @dateWritten=2019-05-20T19:12:23Z @fmt:line-default={letters:XXX}{terminator} @fmt:line-term=line#{terminator} @fmt:text-orig-extra={letters}{punc}{gap} @fmt:text-orig-full={letters} @fmt:text-orig-full!banks={letters}{punc} @fmt:text-orig-full!rivers={letters}{gap} @name=Culture quotes from Iain Banks @purpose=exposition @sectionFeatures=title,number,number @sectionFeatures!banks=title,number,number @sectionFeatures!rivers=number,number,number @sectionTypes=volume,chapter,line @sectionTypes!banks=book,chapter,sentence @sectionTypes!rivers=chapter,sentence,line @source=Good Reads @status=with for similarities in a separate module @structureFeatures!banks=title,number,number,number @structureFeatures!rivers=title,number,number @structureTypes!banks=book,chapter,sentence,line @structureTypes!rivers=book,chapter,sentence @url=https://www.goodreads.com/work/quotes/14366-consider-phlebas @version=0.2 @writtenBy=Text-Fabric @writtenBy=Text-Fabric @dateWritten=2019-05-28T10:55:06Z silent Suppress or enable informational messages.","title":"Combining"},{"location":"Api/Compose/#combining-data","text":"Explanation This package contains functions to operate on TF datasets as a whole There are the following basic operations: add/merge/delete types and features to/from a single data source combine several data sources into one See also the compose chapter in the Banks tutorial.","title":"Combining data"},{"location":"Api/Compose/#modify","text":"usage 1 2 3 4 5 6 7 8 9 10 11 12 13 from tf.compose import add modify ( location , targetLocation , mergeFeatures = None , deleteFeatures = None , addFeatures = None , mergeTypes = None , deleteTypes = None , addTypes = None , featureMeta = None , silent = False , ) overview Modifies the supply of node types and features in a single data set. Dependent on the presence of the parameters, the following steps will be executed before the result is written out as a new TF dataset: merge existing features into an other feature, removing the features that went in; delete any number of existing features; add any number of features and their data; merge existing node types into a new one, removing the types that went in, without loss of nodes; So far, no new nodes have been added or removed. But then: delete any number of node types with their nodes; add any number of new node types, with nodes and features. The last two action lead to a shifting of nodes, and all features that map them, will be shifted accordingly. You can also pass meta data to be merged in. Finally, the resulting features will be written to disk. location You can pass just the location of the original dataset in the file system, i.e. the directory that contains the .tf files. targetLocation The directory into which the result dataset will be written. mergeFeatures You can merge several features into one. This is especially useful if there are many features each operating on different node types, and you want to unify them into one feature. The situation may occur that several of the features to be merged supply conflicting values for a node. Then the last feature in the merge list wins. The result feature may exist already. Also then there is a risk of conflict. Again, the merge result wins. 1 2 3 4 mergeFeatures = dict ( resultFeature1 = [ feat1 , feat2 ], resultFeature2 = [ feat3 , feat4 ], ), If the resulting feature is new, or needs a new description, you can provide metadata in the featureMeta argument. For new features you may want to set the valueType , although we try hard to deduce it from the data available. deleteFeatures This should be an iterable or space separated string of features that you want to delete from the result. addFeatures You can add as many features as you want, assigning values to all types, including new nodes of new types that have been generated in the steps before. You can also use this parameter to override existing features: if a feature that you are adding already exists, the new data will be merged in, overriding assignments of the existing feature if there is a conflict. The meta data of the old and new feature will also be merged. The parameter must have this shape: 1 2 3 4 5 6 7 8 9 addFeatures = dict ( nodeFeatures = dict ( feat1 = data1 , feat2 = data2 , ), edgeFeatures = dict ( feat3 = data3 , feat4 = data4 , ), If the resulting features are new, or need a new description, you can provide metadata in the featureMeta argument. For new features you may want to set the valueType , although we try hard to deduce it from the data available. mergeTypes You can merge several node types into one. The merged node type will have the union of nodes of the types that are merged. All relevant features will stay the same, except the otype feature of course. You can pass additional information to be added as features to nodes of the new node type. These features can be used to discriminate between the merged types. The parameter you pass takes this shape 1 2 3 4 5 6 7 8 9 10 mergeTypes = dict ( outTypeA = ( 'inType1' , 'inType2' , ), outTypeB = ( 'inType3' , 'inType4' , ), ) or 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 mergeTypes = dict ( outTypeA = dict ( inType1 = dict ( featureI = valueI , featureK = valueK , ), inType2 = dict ( featureL = valueL , featureM = valueM , ), ), outTypeB = dict ( inType3 = dict ( featureN = valueN , featureO = valueO , ), inType4 = dict ( featureP = valueP , featureQ = valueQ , ), ), ) It does not matter if these types and features already occur. The outTypes may be existing types of really new types. The new features may be existing or new features. Do not forget to provide meta data for new features in the featureMeta argument. This will migrate nodes of type inType1 or inType2 to nodes of outTypeA . In the extended form, when there are feature specifications associated with the old types, after merging the following assignments will be made: featureI = valueI to nodes coming from inType1 and featureK = valueK to nodes coming from inType2 . No nodes will be removed! slot types Merging is all about non-slot types. It is an error if a new type or an old type is a slot type. deleteTypes You can delete node types from the result altogether. You can specify a list of node types as an iterable or as a space separated string. If a node type has to be deleted, all nodes in that type will be removed, and features that assign values to these nodes will have those assignments removed. Example 1 deleteTypes = ( 'line' , 'sentence' ) 1 deleteTypes = ( ' line sentence ' ) slot types Deleting is all about non-slot types. It is an error to attempt to delete slot type. addTypes You may add as many node types as you want. Per node type that you add, you need to specify the current boundaries of that type and how all those nodes map to slots. You can also add features that assign values to those nodes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 dict ( nodeType1 = dict ( nodeFrom = from1 , nodeTo = to1 , nodeSlots = slots1 , nodeFeatures = nFeatures1 , edgeFeatures = eFeatures1 , ), nodeType2 = dict ( nodeFrom = from2 , nodeTo = to2 , nodeSlots = slots2 , nodeFeatures = nFeatures2 , edgeFeatures = eFeatures2 , ), ), The boundaries may be completely arbitrary, so if you get your nodes from another TF data source, you do not need to align their values. If you also add features about those nodes, the only thing that matters is that the features assign the right values to the nodes within the boundaries. Assignments to nodes outside the boundaries will be ignored. The slots that you link the new nodes to, must exist in the original. You cannot use this function to add slots to your data set. existing node types It is an error if a new node type already exists in the original. nodeFeatures, edgeFeatures You can add any number of features. Per feature you have to provide the mapping that defines the feature. These features may be new, or they may already be present in the original data. If these features have values to nodes that are not within the boundaries of the new node type, those values will not be assigned but silently discarded. Node features are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = val1 , n2 = val2 , ), feat2 = dict ( n1 = val1 , n2 = val2 , ), ), Edge features without values are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = { m1 , m2 }, n2 = { m3 , m4 }, ), feat2 = dict ( n1 = { m5 , m6 }, n2 = { m7 , m8 }, ), ), Edge features with values are specified like this: 1 2 3 4 5 6 7 8 9 10 dict ( feat1 = dict ( n1 = { m1 : v1 , m2 : v2 }, n2 = { m3 : v3 , m4 : v4 }, ), feat2 = dict ( n1 = { m5 : v5 , m6 : v6 }, n2 = { m7 : v7 , m8 : v8 }, ), ), featureMeta If the features you have specified in one of the paramers above are new, do not forget to pass metadata for them in this parameter It is especially important to state the value type: 1 2 3 4 5 6 7 8 9 10 featureMeta = dict ( featureI = dict ( valueType = 'int' , description = 'level of node' ), featureK = dict ( valueType = 'str' , description = 'subtype of node' ), ), You can also tweak the section/structure configuration and the text-formats that are specified in the otext feature. Just specify them as keys and values to the otext feature. The logic of tweaking meta data is this: what you provide in this parameter will be merged into existing meta data. If you want to remove a key from a feature, give it the value None. silent Suppress or enable informational messages.","title":"Modify"},{"location":"Api/Compose/#combine","text":"usage 1 2 3 4 5 6 7 8 from tf.compose import combine combine ( ( location1 , location2 , ), targetLocation , ) 1 2 3 4 5 6 7 8 9 10 11 combine( ( (name1, location1), (name2, location2), ), targetLocation, componentType=None, componentFeature=None, featureMeta=None, **otext, ) overview Creates a new TF data source out of a number of other ones. You may pass as many component data sources as you want. The combination will be the union of all nodes of the components, rearranged according to their types, where node types with the same names will be merged. The slots of the result are the concatenation of the slots of the components, which must all have the same slot type. The node and edge features will be remapped, so that they have the same values in the combined data as they had in the individual components. Optionally, nodes corresponding to the components themselves will be added to the combined result. Care will be taken of the metadata of the features and the contents of the otext.tf feature, which consists of metadata only. All details of the combination can be steered by means of parameters. locations You can either pass just the locations of the components, or you can give them a name. If you do not give names, the locations will be used as names. targetLocation The directory into which the feature files of the combined dataset will be written. componentType, componentFeature If a string value for componentType and/or componentFeature is passed, a new node type will be added, with nodes for each component. There will also be a new feature, that assigns the name of a component to the node of that component. The name of the new node type is the value of componentType if it is a non-empty string, else it is the value of componentFeature . The name of the new feature is componentFeature if it is a non-empty string, else it is the value of componentType . componentType must be fresh It is an error if the componentType is a node type that already occurs in one of the components. componentFeature may exist The componentFeature may already exist in one or more components. In that case the new feature values for nodes of componentType will just be added to it. Example 1 2 3 4 5 6 7 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , componentType = 'volume' , componentFeature = 'vol' , ) This results of a dataset with nodes and features from the components found at the indicated places on your file system. After combination, the components are visible in the data set as nodes of type volume , and the feature vol provides the names banks and river for those nodes. featureMeta The meta data of the components involved will be merged. If feature metadata of the same feature is encountered in different components, and if components specify different values for the same keys, the different values will be stored under a key with the name of the component appended to the key, separated by a ! . The special metadata field valueType will just be reduced to one single value str if some components have it as str and others as int . If the components assign the same value type to a feature, that value type will be assigned to the combined feature. If you want to assign other meta data to specific features, or pass meta data for new features that orginate from the merging process, you can pass them in the parameter featureMeta as in the following example, where we pass meta data for a feature called level with integer values. Example 1 2 3 4 5 6 7 8 9 10 11 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , featureMeta = dict ( level = dict ( valueType = 'int' , description = 'level of a section node' , ), ), ) The contents of the otext.tf features are also metadata, and their contents will be merged in exactly the same way. So if the section/structure specifications and the formats are not the same for all components, you will see them spread out in fields qualified by the component name with a ! sign between the key and the component. But you can add new specifications explicitly, as meta data of the otext feature. by passing them as keyword arguments. They will be passed directly to the combined otext.tf feature and will override anything with the same key that is already in one of the components. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 combine ( ( 'banks' , 'banks/tf/0.2' ), ( 'river' , 'banks/tf/0.4' ), 'riverbanks/tf/1.0' , featureMeta = dict ( otext = dict ( componentType = 'volume' , componentFeature = 'vol' , sectionTypes = 'volume,chapter,line' , sectionFeatures = 'title,number,number' , ), ), silent = False , ) will give rise to something like this (assuming that banks and rivers have some deviating material in their otext.tf : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @config @compiler=Dirk Roorda @dateWritten=2019-05-20T19:12:23Z @fmt:line-default={letters:XXX}{terminator} @fmt:line-term=line#{terminator} @fmt:text-orig-extra={letters}{punc}{gap} @fmt:text-orig-full={letters} @fmt:text-orig-full!banks={letters}{punc} @fmt:text-orig-full!rivers={letters}{gap} @name=Culture quotes from Iain Banks @purpose=exposition @sectionFeatures=title,number,number @sectionFeatures!banks=title,number,number @sectionFeatures!rivers=number,number,number @sectionTypes=volume,chapter,line @sectionTypes!banks=book,chapter,sentence @sectionTypes!rivers=chapter,sentence,line @source=Good Reads @status=with for similarities in a separate module @structureFeatures!banks=title,number,number,number @structureFeatures!rivers=title,number,number @structureTypes!banks=book,chapter,sentence,line @structureTypes!rivers=book,chapter,sentence @url=https://www.goodreads.com/work/quotes/14366-consider-phlebas @version=0.2 @writtenBy=Text-Fabric @writtenBy=Text-Fabric @dateWritten=2019-05-28T10:55:06Z silent Suppress or enable informational messages.","title":"Combine"},{"location":"Api/Computed/","text":"Computed data \u00b6 Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in quick-load format. Most of this data are the features, but there is some extra data needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable, so we have made it accessible in the C -api, which we document here. Pre-computed data storage Pre-computed data is stored in cache directories in a directory .tf inside the directory where the otype feature is encountered. After precomputation the result is pickled and gzipped and written to a .tfx file with the same name as the name of the feature. This is done for nromal features and pre-computed features likewise. After version 7.7.7 version the memory footprint of some precomputed features has been reduced. Because the precomputed features on disk are exact replicas of the precomputed features in RAM, older precomputed data does not work with versions of TF after 7.7.7. But from that version onwards, there is a parameter, PACK_VERSION that indicates the version of the packing algorithm. Precomputed data is not stored directly in the .tf cache directory, but in the .tf/{PACK_VERSION} directory. There is a utility, clean() that removes all outdated generated data from your ~/text-fabric-data directory, and optionally from your ~/github directory. Call() aka AllComputeds() 1 2 Call () AllComputeds () Description Returns a sorted list of all usable, loaded computed data names. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1 ( otype , averageSlots , minNode , maxNode ) where otype is the name of the node type, averageSlots the average size of objects in this type, measured in slots (usually words). minNode is the first node of this type, maxNode the last, and the nodes of this node type are exactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of that type usually occupy. The bigger the average object, the lower the levels. Books have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type phrase and above it a node type cluster , i.e. phrases are contained in clusters, but not vice versa. If all phrases are contained in clusters, and some clusters have more than one phrase, the automatic level ranking of node types works out well in this case. But if clusters only have very small phrases, and the big phrases do not occur in clusters, then the algorithm may assign a lower rank to clusters than to phrases. In general, it is too expensive to try to compute the levels in a sophisticated way. In order to remedy cases where the algorithm assigns wrong levels, you can add a @levels key to the otext config feature. See text . C.order.data Description An array of all nodes in the correct order. This is the order in which N() alias Node() traverses all nodes. Rationale To order all nodes in the canonical ordering is quite a bit of work, and we need this ordering all the time. C.rank.data Description An array of all indices of all nodes in the canonical order array. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know which position each node takes in the canonical order, in other words, at what index we find it in the C.order.data array. C.levUp.data and C.levDown.data Description These tables feed the L.d() and L.u() functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized. It is not advisable to use them directly, it is far better to use the L functions. Only when every bit of performance waste has to be squeezed out, this raw data might be a deal. C.boundary.data Description These tables feed the L.n() and L.p() functions. It is a tuple consisting of firstSlots and lastSlots . They are indexes for the first slot and last slot of nodes. Slot index For each slot, firstSlot gives all nodes (except slots) that start at that slot, and lastSlot gives all nodes (except slots) that end at that slot. Both firstSlot and lastSlot are tuples, and the information for node n can be found at position n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is chapter , and that of level 3 is verse . And suppose that we have features, named bookHeading , chapterHeading , and verseHeading that give the names or numbers of these. Custom names Note that the terms book , chapter , verse are not baked into Text-Fabric. It is the corpus data, especially the otext config feature that spells out the names of the sections. Then C.section.data is a tuple of two mappings , let us call them chapters and verses . chapters is a mapping, keyed by book nodes , and then by by chapter headings , giving the corresponding chapter **node**s as values. verses is a mapping, keyed by book nodes , and then by chapter headings , and then by verse headings , giving the corresponding verse **node**s as values. Supporting the T -Api The T -api is good in mapping nodes unto sections, such as books, chapters, verses and back. It knows how many chapters each book has, and how many verses each chapter. The T api is meant to make your life easier when you have to find passage labels by nodes or vice versa. That is why you probably never need to consult the underlying data. But you can! That data is stored in","title":"C - Computed"},{"location":"Api/Computed/#computed-data","text":"Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in quick-load format. Most of this data are the features, but there is some extra data needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable, so we have made it accessible in the C -api, which we document here. Pre-computed data storage Pre-computed data is stored in cache directories in a directory .tf inside the directory where the otype feature is encountered. After precomputation the result is pickled and gzipped and written to a .tfx file with the same name as the name of the feature. This is done for nromal features and pre-computed features likewise. After version 7.7.7 version the memory footprint of some precomputed features has been reduced. Because the precomputed features on disk are exact replicas of the precomputed features in RAM, older precomputed data does not work with versions of TF after 7.7.7. But from that version onwards, there is a parameter, PACK_VERSION that indicates the version of the packing algorithm. Precomputed data is not stored directly in the .tf cache directory, but in the .tf/{PACK_VERSION} directory. There is a utility, clean() that removes all outdated generated data from your ~/text-fabric-data directory, and optionally from your ~/github directory. Call() aka AllComputeds() 1 2 Call () AllComputeds () Description Returns a sorted list of all usable, loaded computed data names. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1 ( otype , averageSlots , minNode , maxNode ) where otype is the name of the node type, averageSlots the average size of objects in this type, measured in slots (usually words). minNode is the first node of this type, maxNode the last, and the nodes of this node type are exactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of that type usually occupy. The bigger the average object, the lower the levels. Books have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type phrase and above it a node type cluster , i.e. phrases are contained in clusters, but not vice versa. If all phrases are contained in clusters, and some clusters have more than one phrase, the automatic level ranking of node types works out well in this case. But if clusters only have very small phrases, and the big phrases do not occur in clusters, then the algorithm may assign a lower rank to clusters than to phrases. In general, it is too expensive to try to compute the levels in a sophisticated way. In order to remedy cases where the algorithm assigns wrong levels, you can add a @levels key to the otext config feature. See text . C.order.data Description An array of all nodes in the correct order. This is the order in which N() alias Node() traverses all nodes. Rationale To order all nodes in the canonical ordering is quite a bit of work, and we need this ordering all the time. C.rank.data Description An array of all indices of all nodes in the canonical order array. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know which position each node takes in the canonical order, in other words, at what index we find it in the C.order.data array. C.levUp.data and C.levDown.data Description These tables feed the L.d() and L.u() functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized. It is not advisable to use them directly, it is far better to use the L functions. Only when every bit of performance waste has to be squeezed out, this raw data might be a deal. C.boundary.data Description These tables feed the L.n() and L.p() functions. It is a tuple consisting of firstSlots and lastSlots . They are indexes for the first slot and last slot of nodes. Slot index For each slot, firstSlot gives all nodes (except slots) that start at that slot, and lastSlot gives all nodes (except slots) that end at that slot. Both firstSlot and lastSlot are tuples, and the information for node n can be found at position n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is chapter , and that of level 3 is verse . And suppose that we have features, named bookHeading , chapterHeading , and verseHeading that give the names or numbers of these. Custom names Note that the terms book , chapter , verse are not baked into Text-Fabric. It is the corpus data, especially the otext config feature that spells out the names of the sections. Then C.section.data is a tuple of two mappings , let us call them chapters and verses . chapters is a mapping, keyed by book nodes , and then by by chapter headings , giving the corresponding chapter **node**s as values. verses is a mapping, keyed by book nodes , and then by chapter headings , and then by verse headings , giving the corresponding verse **node**s as values. Supporting the T -Api The T -api is good in mapping nodes unto sections, such as books, chapters, verses and back. It knows how many chapters each book has, and how many verses each chapter. The T api is meant to make your life easier when you have to find passage labels by nodes or vice versa. That is why you probably never need to consult the underlying data. But you can! That data is stored in","title":"Computed data"},{"location":"Api/Data/","text":"Adding data modules \u00b6 Text-Fabric supports the flow of creating research data, packaging it, distributing it, and importing in apps. Assumptions \u00b6 The data sharing workflow is built around the following assumptions: main corpus app You work with a main corpus for which a text-fabric app is available. See Corpora versioned tf data The data you share consists of a set of TF features, tied to a specific version of the main corpus, preferably the most recent version. The new features must sit in a directory named after the version of the main corpus they correspond to. local github The data you share must reside in a directory on your hard drive. The convention is, that you have a directory github under your home directory. And inside github , you have directories for organizations or people first, and then repositories, exactly as the online GitHub is organized. Your own data should be in such a repo as well, e.g. ch-jensen/Semantic-mapping-of-participants/actor/tf or etcbc/lingo/heads/tf or annotation/banks/sim/tf synchronized with GitHub You must have your local repo contents synchronized with that on GitHub. Now other people can use your data. Whenever you push updates, users may fetch the updated data, depending on how they call up your data, see below. release on GitHub If your data is reasonably stable, consider making an official release on GitHub. Then you must attach your features as a zip file to that release. Text-Fabric has a command to produce a zip file with exactly the right structure and name. Consider to connect your repo with Zenodo . Then every time to make a release, your repo will be archived for the long term, and you get a DOI pointing to the released version. get data In order to get data, the only thing Text-Fabric needs to know, is a string with the organisation or person, the repository, and the path within the repository. Based on the string {org}/{repo}/{path} it will find the online repository, check the latest release, find the zip file, download and expand it to your local ~/text-fabric/data/{org}/{repo}/{path} If there are no releases, it will find the latest commit and use the data from there. But you may go back in the history, see below. everywhere The extra data is accessible whether you work in a Jupyter notebook, or in the Text-Fabric browser. The extra features are clearly listed after the incantation in a notebook, and they show up in the pretty displays in the TF browser. And when you export data from the TF browser, all data modules are reported in the provenance section. Step by step \u00b6 When you develop your own data features, you'll probably make many changes before you take the trouble of publishing them as a zip file attached to a release/ Here we describe the easiest workflow to work with your developing data with a view to share it much less often than you modify it. Produce in your local GitHub folder You probably have a program or notebook that synthesizes a bunch of new features. It is a good idea to have that program in a version control system, and publish it on GitHub, in a repository of your choice. Set up that program in such a way, that your features end up in the same repository, in a folder of your choosing, but directly under a folder that corresponds with the version of the main data source against which you are building your data. Currently, your features only live on your computer, in your local github folder. You may or may not commit your local changes to the online GitHub. But you do not want to create a new release and attach your zipped feature data to it yet. We use the existing annotation/banks/tf data as an example. We assume you have this data locally, in 1 ~/github/annotation/banks/tf under which there are versions such as 0.2 , which contain the actual .tf files. We are going to develop the sim feature, in 1 ~/github/annotation/banks/sim/tf Test the features When you want to load the new features, you can use the mod parameter: 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf' ) But TF then tries to download it from GitHub, or look it up from your ~/text-fabric-data . Both will fail, especially when you let TF manage your ~/text-fabric-data directory. You have to pass 'clone' as the checkout option: 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf:clone' ) The clone option means: use local data under ~/github . With this set, TF looks in the right place inside your ~/github directory. It will not go online, and not look into ~/text-fabric-data . Commit and push your features When the time comes to share your new feature data, everything is already in place to do that. Write access You can only do the following steps for repositories for which you have write access, so do not try to perform this on annotation/banks but use a repo of your own.** On the command line, go to the directory of your repository, and say 1 2 3 git add --all . git commit -m \"data update or whatever\" git push origin master and then your data is shared in the most basic way possible. But very effectively. From now on, other users (and you too) can use that data by passing just the switch 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf' ) without the clone option. If you do this, you get a freshly downloaded copy of your features in your ~/text-fabric-data directory. And every time you do this again, a check for updates will be performed. Make a release If you want to single out a certain commit as stable data and give it a version number, go to your repo on GitHub, click the releases link, and make a new release. Then click Draft a new release Fill in the details, especially the release version (something like 0.2 ), although nothing in the workflow depends on the exact form of the version number; you will see the release version in the provenance, though. Click the button Publish release . Now your data is available to others. Now the users of your data can refer to that copy by means of the version number. By default, the data from the latest release will be downloaded, and subsequent unreleased commits will be ignored. Package into zip files If you share many features, or a whole TF dataset, it is more economical to zip that data and attach it as a binary to the release. It is vitally important for the TF workflows that that zip file has the right name and the right structure. Text-Fabric comes with a command to create that zip file for you. Execute the following command in your terminal: 1 text-fabric-zip annotation/banks/tf You'll see 1 2 3 4 5 Create release data for annotation/banks/tf Found 2 versions zip files end up in /Users/dirk/Downloads/annotation-release/banks zipping annotation/banks 0.1 with 10 features ==> tf-0.1.zip zipping annotation/banks 0.2 with 10 features ==> tf-0.2.zip and as a result you have this in your Downloads folder 1 2 3 ~/Downloads/annotation-release/banks: tf-0.1.zip tf-0.2.zip Attach these versions, or just the newest version, to the release and publish the release. Zip your data with the text-fabric-zip command as explained above. It will look into your local github directory, pickup the features from there, zip them, and put the zip files in your Downloads folder. Then you can pick that zip file up and attach it manually to a new release of your repository on the online GitHub. Continue developing your features Probably you'll make changes to your features after having published them. Then you have the cutting edge version of your features in your local github directory, and the published version in your text-fabric-data directory. When you make new commits, users that call up your data in the standard way, will still get the latest stable release. But users that say 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf:hot' ) will get your cutting edge latest commit. Use data \u00b6 Now we are just reading data, so the following steps you can perform literally, without fear of overwriting data. Check it out! When calling up data and a TF-app, you can go back in history: to previous releases and previous commits, using a checkout parameter. You have already seen it, and here we spell them out in greater detail. You can specify the checkout parameter separately for the TF-app code (so you can go back to previous instantiations of the TF-app) the main data of the app plus its standard data modules every data-module that you include by means of the --mod= parameter. The values of the checkout parameters tell you to use data that is: clone : locally present under ~/github in the appropriate place local : locally present under ~/text-fabric-data in the appropriate place latest : from the latest online release hot : from the latest online commit '' : (default): from the latest online release, or if there are no releases, from the latest online commit 2387abc78f9de... : a concrete commit hash found on GitHub (under Commits) v1.3 : a release tag found on GitHub (under Releases) You pass the checkout values as follows: for the TF-app: after the app name : bhsa:clone , oldbabylonian:local , quran . If you leave it out, it defaults to the empty string: latest release or commit. for the main data and standard data modules of the app: in the checkout parameter : checkout='clone' , checkout='local' , checkout='' . If you omit checkout out, it defaults to the empty string: latest release or commit. for the data that you call up as a module: after the module name : annotation/banks/sim/tf:clone , annotation/banks/sim/tf:local , annotation/banks/sim/tf . If you leave it out, it defaults to the empty string: latest release or commit. The checkout parameter can be used in the use() function when you call up Text-Fabric in a program (or notebook) and when you call up the text-fabric browser on the command line: 1 use ( 'banks:clone' , checkout = 'latest' , mod = 'annotation/banks/sim/tf:hot' ) 1 text-fabric banks:clone --checkout = latest --mod = annotation/banks/sim/tf:hot Note that you can pass different specifiers for the distinct portions of data and code that you want to use. To see it in action, consult the repo notebook. More about using data modules Suppose you have called up a data module: 1 A = use ( 'banks' , mod = 'annotation/banks/sim/tf' ) or 1 text-fabric banks --mod = annotation/banks/sim/tf You can then use the features of the module in everywhere. Fill out this query: 1 2 word letters~th -sim=100> word and expand the first result in Genesis 1:1. The display looks like this: And if you export the data, the extra module is listed in the provenance. Feature display in the TF browser You can use new features in queries. Pretty displays will show those features automatically, because all features used in a query are displayed in the expanded view. If you want to see a feature that is not used in the query you can add it as a trivial search criterion. For example, if you want to see the sense feature when looking for phrases, add it like this 1 2 clause phrase sense* The * means: always true, so it will not influence the query result set, only its display; In fact, the feature sense is only present on nodes of type word . But mentioning a feature anywhere in the query will trigger the display wherever it occurs with a non-trivial values. The extra data modules are also shown in the provenance listings when you export data from the browser. Feature display in a Jupyter notebook After the incantation, you see an overview of all features per module where they come from, linked to their documentation or repository. You can use the new features exactly as you are used to, with F and E (for edge features). They will also automatically show up in pretty displays, provided you have run a query using them before. Alternatively, you can tell which features you want to add to the display. That can be done by displaySetup() and displayReset() , using the parameter extraFeatures . More modules at the same time Now that we get the hang of it, we would like to use multiple modules added to a main data source. We go to the bhsa (Hebrew Bible) and use the heads feature that Cody Kingham prepared in etcbc/lingo/heads as well as the actor feature that Christian H\u00f8ygaard-Jensen prepared in ch-jensen/Semantic-mapping-of-participants We'll include it next to the valence data, by calling the TF browser like this: 1 text-fabric bhsa --mod = etcbc/valence/tf,etcbc/lingo/heads/tf,ch-jensen/Semantic-mapping-of-participants/actor/tf Unsurprisingly: the heads and actor features and friends are downloaded and made ready for import. You can test it by means of this query 1 2 3 4 book book=Leviticus phrase sense* phrase_atom actor=KHN -heads> word Note that heads is an edge feature. In a Jupyter notebook, it goes like this: 1 2 3 4 5 6 7 8 9 10 from tf.app import use A = use ( 'bhsa' , mod = ( 'etcbc/valence/tf,' 'etcbc/lingo/heads/tf,' 'ch-jensen/Semantic-mapping-of-participants/actor/tf' ), hoist = globals (), ) Now you can run the same query as before: 1 2 3 4 5 6 results = A . search ( ''' book book=Leviticus phrase sense* phrase_atom actor=KHN -heads> word ''' ) And let's see results in pretty display. We have to manually declare that we want to see the sense and actor feature. 1 2 A.displaySetup(extraFeatures='sense actor') A.show(results, start=8, end=8, condensed=True, condenseType='verse') See the share tutorial. Exercise \u00b6 See whether you can find the quote in the Easter egg that is in etcbc/lingo/easter/tf !","title":"Data"},{"location":"Api/Data/#adding-data-modules","text":"Text-Fabric supports the flow of creating research data, packaging it, distributing it, and importing in apps.","title":"Adding data modules"},{"location":"Api/Data/#assumptions","text":"The data sharing workflow is built around the following assumptions: main corpus app You work with a main corpus for which a text-fabric app is available. See Corpora versioned tf data The data you share consists of a set of TF features, tied to a specific version of the main corpus, preferably the most recent version. The new features must sit in a directory named after the version of the main corpus they correspond to. local github The data you share must reside in a directory on your hard drive. The convention is, that you have a directory github under your home directory. And inside github , you have directories for organizations or people first, and then repositories, exactly as the online GitHub is organized. Your own data should be in such a repo as well, e.g. ch-jensen/Semantic-mapping-of-participants/actor/tf or etcbc/lingo/heads/tf or annotation/banks/sim/tf synchronized with GitHub You must have your local repo contents synchronized with that on GitHub. Now other people can use your data. Whenever you push updates, users may fetch the updated data, depending on how they call up your data, see below. release on GitHub If your data is reasonably stable, consider making an official release on GitHub. Then you must attach your features as a zip file to that release. Text-Fabric has a command to produce a zip file with exactly the right structure and name. Consider to connect your repo with Zenodo . Then every time to make a release, your repo will be archived for the long term, and you get a DOI pointing to the released version. get data In order to get data, the only thing Text-Fabric needs to know, is a string with the organisation or person, the repository, and the path within the repository. Based on the string {org}/{repo}/{path} it will find the online repository, check the latest release, find the zip file, download and expand it to your local ~/text-fabric/data/{org}/{repo}/{path} If there are no releases, it will find the latest commit and use the data from there. But you may go back in the history, see below. everywhere The extra data is accessible whether you work in a Jupyter notebook, or in the Text-Fabric browser. The extra features are clearly listed after the incantation in a notebook, and they show up in the pretty displays in the TF browser. And when you export data from the TF browser, all data modules are reported in the provenance section.","title":"Assumptions"},{"location":"Api/Data/#step-by-step","text":"When you develop your own data features, you'll probably make many changes before you take the trouble of publishing them as a zip file attached to a release/ Here we describe the easiest workflow to work with your developing data with a view to share it much less often than you modify it. Produce in your local GitHub folder You probably have a program or notebook that synthesizes a bunch of new features. It is a good idea to have that program in a version control system, and publish it on GitHub, in a repository of your choice. Set up that program in such a way, that your features end up in the same repository, in a folder of your choosing, but directly under a folder that corresponds with the version of the main data source against which you are building your data. Currently, your features only live on your computer, in your local github folder. You may or may not commit your local changes to the online GitHub. But you do not want to create a new release and attach your zipped feature data to it yet. We use the existing annotation/banks/tf data as an example. We assume you have this data locally, in 1 ~/github/annotation/banks/tf under which there are versions such as 0.2 , which contain the actual .tf files. We are going to develop the sim feature, in 1 ~/github/annotation/banks/sim/tf Test the features When you want to load the new features, you can use the mod parameter: 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf' ) But TF then tries to download it from GitHub, or look it up from your ~/text-fabric-data . Both will fail, especially when you let TF manage your ~/text-fabric-data directory. You have to pass 'clone' as the checkout option: 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf:clone' ) The clone option means: use local data under ~/github . With this set, TF looks in the right place inside your ~/github directory. It will not go online, and not look into ~/text-fabric-data . Commit and push your features When the time comes to share your new feature data, everything is already in place to do that. Write access You can only do the following steps for repositories for which you have write access, so do not try to perform this on annotation/banks but use a repo of your own.** On the command line, go to the directory of your repository, and say 1 2 3 git add --all . git commit -m \"data update or whatever\" git push origin master and then your data is shared in the most basic way possible. But very effectively. From now on, other users (and you too) can use that data by passing just the switch 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf' ) without the clone option. If you do this, you get a freshly downloaded copy of your features in your ~/text-fabric-data directory. And every time you do this again, a check for updates will be performed. Make a release If you want to single out a certain commit as stable data and give it a version number, go to your repo on GitHub, click the releases link, and make a new release. Then click Draft a new release Fill in the details, especially the release version (something like 0.2 ), although nothing in the workflow depends on the exact form of the version number; you will see the release version in the provenance, though. Click the button Publish release . Now your data is available to others. Now the users of your data can refer to that copy by means of the version number. By default, the data from the latest release will be downloaded, and subsequent unreleased commits will be ignored. Package into zip files If you share many features, or a whole TF dataset, it is more economical to zip that data and attach it as a binary to the release. It is vitally important for the TF workflows that that zip file has the right name and the right structure. Text-Fabric comes with a command to create that zip file for you. Execute the following command in your terminal: 1 text-fabric-zip annotation/banks/tf You'll see 1 2 3 4 5 Create release data for annotation/banks/tf Found 2 versions zip files end up in /Users/dirk/Downloads/annotation-release/banks zipping annotation/banks 0.1 with 10 features ==> tf-0.1.zip zipping annotation/banks 0.2 with 10 features ==> tf-0.2.zip and as a result you have this in your Downloads folder 1 2 3 ~/Downloads/annotation-release/banks: tf-0.1.zip tf-0.2.zip Attach these versions, or just the newest version, to the release and publish the release. Zip your data with the text-fabric-zip command as explained above. It will look into your local github directory, pickup the features from there, zip them, and put the zip files in your Downloads folder. Then you can pick that zip file up and attach it manually to a new release of your repository on the online GitHub. Continue developing your features Probably you'll make changes to your features after having published them. Then you have the cutting edge version of your features in your local github directory, and the published version in your text-fabric-data directory. When you make new commits, users that call up your data in the standard way, will still get the latest stable release. But users that say 1 use ( 'banks' , mod = f 'annotation/banks/sim/tf:hot' ) will get your cutting edge latest commit.","title":"Step by step"},{"location":"Api/Data/#use-data","text":"Now we are just reading data, so the following steps you can perform literally, without fear of overwriting data. Check it out! When calling up data and a TF-app, you can go back in history: to previous releases and previous commits, using a checkout parameter. You have already seen it, and here we spell them out in greater detail. You can specify the checkout parameter separately for the TF-app code (so you can go back to previous instantiations of the TF-app) the main data of the app plus its standard data modules every data-module that you include by means of the --mod= parameter. The values of the checkout parameters tell you to use data that is: clone : locally present under ~/github in the appropriate place local : locally present under ~/text-fabric-data in the appropriate place latest : from the latest online release hot : from the latest online commit '' : (default): from the latest online release, or if there are no releases, from the latest online commit 2387abc78f9de... : a concrete commit hash found on GitHub (under Commits) v1.3 : a release tag found on GitHub (under Releases) You pass the checkout values as follows: for the TF-app: after the app name : bhsa:clone , oldbabylonian:local , quran . If you leave it out, it defaults to the empty string: latest release or commit. for the main data and standard data modules of the app: in the checkout parameter : checkout='clone' , checkout='local' , checkout='' . If you omit checkout out, it defaults to the empty string: latest release or commit. for the data that you call up as a module: after the module name : annotation/banks/sim/tf:clone , annotation/banks/sim/tf:local , annotation/banks/sim/tf . If you leave it out, it defaults to the empty string: latest release or commit. The checkout parameter can be used in the use() function when you call up Text-Fabric in a program (or notebook) and when you call up the text-fabric browser on the command line: 1 use ( 'banks:clone' , checkout = 'latest' , mod = 'annotation/banks/sim/tf:hot' ) 1 text-fabric banks:clone --checkout = latest --mod = annotation/banks/sim/tf:hot Note that you can pass different specifiers for the distinct portions of data and code that you want to use. To see it in action, consult the repo notebook. More about using data modules Suppose you have called up a data module: 1 A = use ( 'banks' , mod = 'annotation/banks/sim/tf' ) or 1 text-fabric banks --mod = annotation/banks/sim/tf You can then use the features of the module in everywhere. Fill out this query: 1 2 word letters~th -sim=100> word and expand the first result in Genesis 1:1. The display looks like this: And if you export the data, the extra module is listed in the provenance. Feature display in the TF browser You can use new features in queries. Pretty displays will show those features automatically, because all features used in a query are displayed in the expanded view. If you want to see a feature that is not used in the query you can add it as a trivial search criterion. For example, if you want to see the sense feature when looking for phrases, add it like this 1 2 clause phrase sense* The * means: always true, so it will not influence the query result set, only its display; In fact, the feature sense is only present on nodes of type word . But mentioning a feature anywhere in the query will trigger the display wherever it occurs with a non-trivial values. The extra data modules are also shown in the provenance listings when you export data from the browser. Feature display in a Jupyter notebook After the incantation, you see an overview of all features per module where they come from, linked to their documentation or repository. You can use the new features exactly as you are used to, with F and E (for edge features). They will also automatically show up in pretty displays, provided you have run a query using them before. Alternatively, you can tell which features you want to add to the display. That can be done by displaySetup() and displayReset() , using the parameter extraFeatures . More modules at the same time Now that we get the hang of it, we would like to use multiple modules added to a main data source. We go to the bhsa (Hebrew Bible) and use the heads feature that Cody Kingham prepared in etcbc/lingo/heads as well as the actor feature that Christian H\u00f8ygaard-Jensen prepared in ch-jensen/Semantic-mapping-of-participants We'll include it next to the valence data, by calling the TF browser like this: 1 text-fabric bhsa --mod = etcbc/valence/tf,etcbc/lingo/heads/tf,ch-jensen/Semantic-mapping-of-participants/actor/tf Unsurprisingly: the heads and actor features and friends are downloaded and made ready for import. You can test it by means of this query 1 2 3 4 book book=Leviticus phrase sense* phrase_atom actor=KHN -heads> word Note that heads is an edge feature. In a Jupyter notebook, it goes like this: 1 2 3 4 5 6 7 8 9 10 from tf.app import use A = use ( 'bhsa' , mod = ( 'etcbc/valence/tf,' 'etcbc/lingo/heads/tf,' 'ch-jensen/Semantic-mapping-of-participants/actor/tf' ), hoist = globals (), ) Now you can run the same query as before: 1 2 3 4 5 6 results = A . search ( ''' book book=Leviticus phrase sense* phrase_atom actor=KHN -heads> word ''' ) And let's see results in pretty display. We have to manually declare that we want to see the sense and actor feature. 1 2 A.displaySetup(extraFeatures='sense actor') A.show(results, start=8, end=8, condensed=True, condenseType='verse') See the share tutorial.","title":"Use data"},{"location":"Api/Data/#exercise","text":"See whether you can find the quote in the Easter egg that is in etcbc/lingo/easter/tf !","title":"Exercise"},{"location":"Api/Fabric/","text":"Text-Fabric API \u00b6 Tutorial The tutorials for specific annotated corpora put the Text-Fabric API on show for vastly different corpora. Generic API versus apps This is the API of Text-Fabric in general. Text-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several apps that make working with specific corpora easier. Loading \u00b6 TF=Fabric() 1 2 from tf.fabric import Fabric TF = Fabric ( locations = directories , modules = subdirectories , silent = False ) Description Text-Fabric is initialized for a corpus. It will search a set of directories and catalog all .tf files it finds there. These are the features you can subsequently load. Here directories and subdirectories are strings with directory names separated by newlines, or iterables of directories. locations, modules The directories specified in locations will be searched for modules , which are paths that will be appended to the paths in locations . All .tf files (non-recursively) in any module will be added to the feature set to be loaded in this session. The order in modules is important, because if a feature occurs in multiple modules, the last one will be chosen. In this way you can easily override certain features in one module by features in an other module of your choice. otext@ in modules If modules contain features with a name starting with otext@ , then the format definitions in these features will be added to the format definitions in the regular otext feature (which is a WARP feature). In this way, modules that define new features for text representation, also can add new formats to the Text-API. Defaults The locations list has a few defaults: 1 2 3 ~/Downloads/text-fabric-data ~/text-fabric-data ~/github/text-fabric-data So if you have stored your main Text-Fabric dataset in text-fabric-data in one of these directories you do not have to pass a location to Fabric. The modules list defaults to [''] . So if you leave it out, Text-Fabric will just search the paths specified in locations . silent If silent=True is passed, banners and normal progress messages are suppressed. TF.explore() 1 2 features = TF . explore ( silent = False , show = True ) features or 1 2 TF . explore ( silent = False , show = False ) TF . featureSets Description This will give you a dictionary of all available features by kind. The kinds are: nodes , edges , configs , computeds . silent With silent=False a message containing the total numbers of features is issued. show The resulting dictionary is delivered in TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api = TF . load ( features , add = False , silent = False ) Description Reads the features indicated by features and loads them in memory ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. add If later on you want load more features, you can either: add the features to the original load() statement and just run it again make a new statement: TF.load(newfeatures, add=True) . The new features will be added to the same api, so you do not have to to call api.makeAvailableIn(globals()) again after this! silent The features will be loaded rather silently, most messages will be suppressed. Time consuming operations will always be announced, so that you know what Text-Fabric is doing. If silent=True is passed, all informational messages will be suppressed. This is handy I you want to load data as part of other methods, on-the-fly. api=TF.loadAll() 1 api = TF . loadAll ( silent = True ) Description Reads all available features and loads them in memory ready to be used in the rest of the program. silent As in TF.load(). ensureLoaded() 1 ensureLoaded ( features ) Description Reads the features indicated by features and checks if they are loaded. The unloaded ones will be loaded. Makes all of them ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ()) Description This method will export every member of the API (such as N , F , E , L , T , info ) to the global namespace. From now on, we will omit the api. in our documentation. Contents of the API After having loaded the features by api = TF.load(...) , the api harbours your Text-Fabric API. You can access node feature mydata by api.F.mydata.v(node) , edge feature mylink by api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit tedious to write the initial api. all the time. By this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital letters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored Description If you want to know which features were found but ignored (because the feature is also present in another, later, location), you can use this attribute to inspect the ignored features and their locations. loadLog() 1 api . loadlog () Description After loading you can view all messages using this method. It also shows the messages that have been suppressed due to silent=True . Saving features \u00b6 TF.save() 1 2 3 4 5 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, location = None , module = None , silent = None , ) Description If you have collected feature data in dictionaries, keyed by the names of the features, and valued by their feature data, then you can save that data to .tf feature files on disk. It is this easy to export new data as features: collect the data and metadata of the features and feed it in an orderly way to TF.save() and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and strings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or dictionaries as values. These sets should be sets of nodes (integers!), and these dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from metaData , which is a dictionary mapping a feature name to its metadata. value types The type of the values should conform to @valueType ( int or str ), which must be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have to say so, by specifying edgeValues = True in the metadata for that feature. generic metadata metaData may also contain fields under the empty name. These fields will be added to all features in nodeFeatures and edgeFeatures . config features If you need to write the config feature otext , which is a metadata-only feature, just add the metadata under key otext in metaData and make sure that otext is not a key in nodeFeatures nor in edgeFeatures . These fields will be written into the separate config feature otext , with no data associated. save location The (meta)data will be written to the very last directory that TF searched when looking for features (this is determined by the locations and modules parameters with which you called TF. If both locations and modules are empty, writing will take place in the current directory. But you can override it: If you pass module=something , TF will save in loc/something , where loc is the last member of the locations parameter of TF. If you pass location=something , TF will save in something/mod , where mod is the last meber of the modules parameter of TF. If you pass location=path1 and module=path2 , TF will save in path1/path2 . silent TF is silent if you specified silent=True in a preceding TF=Fabric() call. But if you did not, you can also pass silent=True to this call.","title":"TF"},{"location":"Api/Fabric/#text-fabric-api","text":"Tutorial The tutorials for specific annotated corpora put the Text-Fabric API on show for vastly different corpora. Generic API versus apps This is the API of Text-Fabric in general. Text-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several apps that make working with specific corpora easier.","title":"Text-Fabric API"},{"location":"Api/Fabric/#loading","text":"TF=Fabric() 1 2 from tf.fabric import Fabric TF = Fabric ( locations = directories , modules = subdirectories , silent = False ) Description Text-Fabric is initialized for a corpus. It will search a set of directories and catalog all .tf files it finds there. These are the features you can subsequently load. Here directories and subdirectories are strings with directory names separated by newlines, or iterables of directories. locations, modules The directories specified in locations will be searched for modules , which are paths that will be appended to the paths in locations . All .tf files (non-recursively) in any module will be added to the feature set to be loaded in this session. The order in modules is important, because if a feature occurs in multiple modules, the last one will be chosen. In this way you can easily override certain features in one module by features in an other module of your choice. otext@ in modules If modules contain features with a name starting with otext@ , then the format definitions in these features will be added to the format definitions in the regular otext feature (which is a WARP feature). In this way, modules that define new features for text representation, also can add new formats to the Text-API. Defaults The locations list has a few defaults: 1 2 3 ~/Downloads/text-fabric-data ~/text-fabric-data ~/github/text-fabric-data So if you have stored your main Text-Fabric dataset in text-fabric-data in one of these directories you do not have to pass a location to Fabric. The modules list defaults to [''] . So if you leave it out, Text-Fabric will just search the paths specified in locations . silent If silent=True is passed, banners and normal progress messages are suppressed. TF.explore() 1 2 features = TF . explore ( silent = False , show = True ) features or 1 2 TF . explore ( silent = False , show = False ) TF . featureSets Description This will give you a dictionary of all available features by kind. The kinds are: nodes , edges , configs , computeds . silent With silent=False a message containing the total numbers of features is issued. show The resulting dictionary is delivered in TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api = TF . load ( features , add = False , silent = False ) Description Reads the features indicated by features and loads them in memory ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. add If later on you want load more features, you can either: add the features to the original load() statement and just run it again make a new statement: TF.load(newfeatures, add=True) . The new features will be added to the same api, so you do not have to to call api.makeAvailableIn(globals()) again after this! silent The features will be loaded rather silently, most messages will be suppressed. Time consuming operations will always be announced, so that you know what Text-Fabric is doing. If silent=True is passed, all informational messages will be suppressed. This is handy I you want to load data as part of other methods, on-the-fly. api=TF.loadAll() 1 api = TF . loadAll ( silent = True ) Description Reads all available features and loads them in memory ready to be used in the rest of the program. silent As in TF.load(). ensureLoaded() 1 ensureLoaded ( features ) Description Reads the features indicated by features and checks if they are loaded. The unloaded ones will be loaded. Makes all of them ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ()) Description This method will export every member of the API (such as N , F , E , L , T , info ) to the global namespace. From now on, we will omit the api. in our documentation. Contents of the API After having loaded the features by api = TF.load(...) , the api harbours your Text-Fabric API. You can access node feature mydata by api.F.mydata.v(node) , edge feature mylink by api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit tedious to write the initial api. all the time. By this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital letters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored Description If you want to know which features were found but ignored (because the feature is also present in another, later, location), you can use this attribute to inspect the ignored features and their locations. loadLog() 1 api . loadlog () Description After loading you can view all messages using this method. It also shows the messages that have been suppressed due to silent=True .","title":"Loading"},{"location":"Api/Fabric/#saving-features","text":"TF.save() 1 2 3 4 5 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, location = None , module = None , silent = None , ) Description If you have collected feature data in dictionaries, keyed by the names of the features, and valued by their feature data, then you can save that data to .tf feature files on disk. It is this easy to export new data as features: collect the data and metadata of the features and feed it in an orderly way to TF.save() and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and strings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or dictionaries as values. These sets should be sets of nodes (integers!), and these dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from metaData , which is a dictionary mapping a feature name to its metadata. value types The type of the values should conform to @valueType ( int or str ), which must be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have to say so, by specifying edgeValues = True in the metadata for that feature. generic metadata metaData may also contain fields under the empty name. These fields will be added to all features in nodeFeatures and edgeFeatures . config features If you need to write the config feature otext , which is a metadata-only feature, just add the metadata under key otext in metaData and make sure that otext is not a key in nodeFeatures nor in edgeFeatures . These fields will be written into the separate config feature otext , with no data associated. save location The (meta)data will be written to the very last directory that TF searched when looking for features (this is determined by the locations and modules parameters with which you called TF. If both locations and modules are empty, writing will take place in the current directory. But you can override it: If you pass module=something , TF will save in loc/something , where loc is the last member of the locations parameter of TF. If you pass location=something , TF will save in something/mod , where mod is the last meber of the modules parameter of TF. If you pass location=path1 and module=path2 , TF will save in path1/path2 . silent TF is silent if you specified silent=True in a preceding TF=Fabric() call. But if you did not, you can also pass silent=True to this call.","title":"Saving features"},{"location":"Api/Features/","text":"Features \u00b6 Features TF can give you information of all features it has encountered. TF.featureSets 1 TF . featureSets Description Returns a dictionary with keys nodes , edges , configs , computeds . Under each key there is the set of feature names in that category. So you can easily test whether a node feature or edge feature is present in the dataset you are working with. configs These are config features, with metadata only, no data. E.g. otext . computeds These are blocks of precomputed data, available under the C. API, see below. May be unloaded The sets do not indicate whether a feature is loaded or not. There are other functions that give you the loaded node features ( Fall() ) and the loaded edge features ( Eall() ). TF.features A dictionary of all features that TF has found, whether loaded or not. Under each feature name is all info about that feature. Do not print! If a feature is loaded, its data is also in the feature info. This can be an enormous amount of information, and you can easily overwhelm your notebook if you print it. The best use of this is to get the metadata of features: 1 TF . features [ 'otype' ] . metaData This works for all features that have been found, not just otype , whether the feature is loaded or not. Use F If the feature is loaded, use 1 F . otype . meta or for any other loaded feature than otype . Generics for features \u00b6 Features are mappings Every feature is logically a mapping from nodes to values. A feature object gives you methods that you can pass a node and that returns its value for that node. It is easiest to think of all features as a dictionary keyed by nodes. However, some features have an optimized representation, and do not have a dictionary underneath. But you can still iterate over the data of a feature as if it were a dictionary. F. feature .items() 1 2 F . part_of_speech . items () E . similarity . items () A generator that yields the items of the feature, seen as a mapping. It does not yield entries for nodes without values, so this gives you a rather efficient way to iterate over just the feature data, instead of over all nodes. If you need this repeatedly, or you need the whole dictionary, you can store the result as follows: 1 data = dict ( F . part_of_speech . items ()) Node features \u00b6 F The node features API is exposed as F ( Fs ) or Feature ( FeatureString ). Fall() aka AllFeatures() 1 2 Fall () AllFeatures () Description Returns a sorted list of all usable, loaded node feature names. F. feature aka Feature. feature 1 2 F . part_of_speech Feature . part_of_speech Description Returns a sub-api for retrieving data that is stored in node features. In this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Fs instead. Fs(feature) aka FeatureString(feature) 1 2 3 4 Fs ( feature ) FeatureString ( feature ) Fs ( 'part-of-speech' ) FeatureString ( 'part-of-speech' ) Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called part-of-speech . Note that this is not a valid name in Python, yet we can work with features with such names. Both methods have identical results Suppose we have just issued feature = 'pos'. Then the result of Fs(feature) and F.pos` is identical. In most cases F works just fine, but Fs is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .meta 1 F . part_of_speech . meta The dictionary of meta data found at the start of the part_of_speech.tf file. F. feature .v(node) 1 F . part_of_speech . v ( node ) Description Get the value of a feature , such as part_of_speech for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1 2 F . part_of_speech . s ( value ) F . part_of_speech . s ( 'noun' ) Description Returns a generator of all nodes in the canonical order with a given value for a given feature. This is an other way to walk through nodes than using N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F. feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None ) Description Inspect the values of feature (in this example: part_of_speech ) and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those types will be counted. F.otype otype is a special node feature and has additional capabilities. Description F.otype.slotType is the node type that can fill the slots (usually: word ) F.otype.maxSlot is the largest slot number F.otype.maxNode is the largest node number F.otype.all is a list of all otypes from big to small (from books through clauses to words) F.otype.sInterval(otype) is like F.otype.s(otype) , but instead of returning you a range to iterate over, it will give you the starting and ending nodes of otype . This makes use of the fact that the data is so organized that all node types have single ranges of nodes as members. Edge features \u00b6 E The edge features API is exposed as E ( Es ) or Edge ( EdgeString ). Eall() aka AllEdges() 1 2 Eall () AllEdges () Description Returns a sorted list of all usable, loaded edge feature names. E. feature aka Edge. feature 1 2 E . head Feature . head Description Returns a sub-api for retrieving data that is stored in edge features. In this example, we assume there is a feature called head . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Es instead. Es(feature) aka EdgeString(feature) 1 2 3 4 Es ( feature ) EdgeString ( feature ) Es ( 'head' ) EdgeString ( 'head' ) Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called head . Both methods have identical results Suppose we have just issued feature = 'head'. Then the result of Es(feature) and E.pos` is identical. In most cases E works just fine, but Es is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .meta 1 E . head . meta The dictionary of meta data found at the start of the head.tf file. E. feature .f(node) 1 E . head . f ( node ) Description Get the nodes reached by feature -edges from a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than None . node The node from which the edges in question start. E. feature .t(node) 1 E . head . t ( node ) Description Get the nodes reached by feature -edges to a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if feature describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges to n , the empty tuple is returned, rather than None . node The node to which the edges in question go. E. feature .b(node) 1 E . head . b ( node ) Description Get the nodes from and to a certain node by a feature -edge. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from or to the node, the empty tuple is returned, rather than None . node The node from which the edges in question start or to which they go. Think of both , hence the b . symmetric closure The .b() methods gives the symmetric closure of a set of edges: if there is an edge between n and m , this method will produce it, no matter the direction of the edge. Some edge sets are semantically symmetric, for example similarity . If n is similar to m , then m is similar to n . But if you store such an edge feature completely, half of the data is redundant. So you do not have to do that, you only need to store one of the edges between n and m (it does not matter which one), and E.sim.b() will nevertheless produce the complete results. conflicting values If your set of edges is not symmetric, and edges carry values, it might very well be the case that edges between the same pair of nodes carry different values for the two directions. In that case, the .b() method gives precedence to the edges that depart from a node. Suppose we have 1 2 n == value=4 ==> m m == value=6 ==> n then 1 2 E.b(n) = (m, 4) E.b(m) = (n, 6) E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None , nodeTypesTo = None ) Description If the edge feature has no values, simply return the number of node pairs between an edge of this kind exists. If the edge feature does have values, we inspect them and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypesFrom If not None , only the values for edges that start from a node with type within nodeTypesFrom will be counted. nodeTypesTo If not None , only the values for edges that go to a node with type within nodeTypesTo will be counted. E.oslots oslots is a special edge feature and is mainly used to construct other parts of the API. It has less capabilities, and you will rarely need it. It does not have .f and .t methods, but an .s method instead. Description E.oslots.s(node) Gives the sorted list of slot numbers linked to a node, or put otherwise: the slots that support that node. node The node whose slots are being delivered.","title":"E/F - (Edge-)Features"},{"location":"Api/Features/#features","text":"Features TF can give you information of all features it has encountered. TF.featureSets 1 TF . featureSets Description Returns a dictionary with keys nodes , edges , configs , computeds . Under each key there is the set of feature names in that category. So you can easily test whether a node feature or edge feature is present in the dataset you are working with. configs These are config features, with metadata only, no data. E.g. otext . computeds These are blocks of precomputed data, available under the C. API, see below. May be unloaded The sets do not indicate whether a feature is loaded or not. There are other functions that give you the loaded node features ( Fall() ) and the loaded edge features ( Eall() ). TF.features A dictionary of all features that TF has found, whether loaded or not. Under each feature name is all info about that feature. Do not print! If a feature is loaded, its data is also in the feature info. This can be an enormous amount of information, and you can easily overwhelm your notebook if you print it. The best use of this is to get the metadata of features: 1 TF . features [ 'otype' ] . metaData This works for all features that have been found, not just otype , whether the feature is loaded or not. Use F If the feature is loaded, use 1 F . otype . meta or for any other loaded feature than otype .","title":"Features"},{"location":"Api/Features/#generics-for-features","text":"Features are mappings Every feature is logically a mapping from nodes to values. A feature object gives you methods that you can pass a node and that returns its value for that node. It is easiest to think of all features as a dictionary keyed by nodes. However, some features have an optimized representation, and do not have a dictionary underneath. But you can still iterate over the data of a feature as if it were a dictionary. F. feature .items() 1 2 F . part_of_speech . items () E . similarity . items () A generator that yields the items of the feature, seen as a mapping. It does not yield entries for nodes without values, so this gives you a rather efficient way to iterate over just the feature data, instead of over all nodes. If you need this repeatedly, or you need the whole dictionary, you can store the result as follows: 1 data = dict ( F . part_of_speech . items ())","title":"Generics for features"},{"location":"Api/Features/#node-features","text":"F The node features API is exposed as F ( Fs ) or Feature ( FeatureString ). Fall() aka AllFeatures() 1 2 Fall () AllFeatures () Description Returns a sorted list of all usable, loaded node feature names. F. feature aka Feature. feature 1 2 F . part_of_speech Feature . part_of_speech Description Returns a sub-api for retrieving data that is stored in node features. In this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Fs instead. Fs(feature) aka FeatureString(feature) 1 2 3 4 Fs ( feature ) FeatureString ( feature ) Fs ( 'part-of-speech' ) FeatureString ( 'part-of-speech' ) Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called part-of-speech . Note that this is not a valid name in Python, yet we can work with features with such names. Both methods have identical results Suppose we have just issued feature = 'pos'. Then the result of Fs(feature) and F.pos` is identical. In most cases F works just fine, but Fs is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .meta 1 F . part_of_speech . meta The dictionary of meta data found at the start of the part_of_speech.tf file. F. feature .v(node) 1 F . part_of_speech . v ( node ) Description Get the value of a feature , such as part_of_speech for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1 2 F . part_of_speech . s ( value ) F . part_of_speech . s ( 'noun' ) Description Returns a generator of all nodes in the canonical order with a given value for a given feature. This is an other way to walk through nodes than using N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F. feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None ) Description Inspect the values of feature (in this example: part_of_speech ) and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those types will be counted. F.otype otype is a special node feature and has additional capabilities. Description F.otype.slotType is the node type that can fill the slots (usually: word ) F.otype.maxSlot is the largest slot number F.otype.maxNode is the largest node number F.otype.all is a list of all otypes from big to small (from books through clauses to words) F.otype.sInterval(otype) is like F.otype.s(otype) , but instead of returning you a range to iterate over, it will give you the starting and ending nodes of otype . This makes use of the fact that the data is so organized that all node types have single ranges of nodes as members.","title":"Node features"},{"location":"Api/Features/#edge-features","text":"E The edge features API is exposed as E ( Es ) or Edge ( EdgeString ). Eall() aka AllEdges() 1 2 Eall () AllEdges () Description Returns a sorted list of all usable, loaded edge feature names. E. feature aka Edge. feature 1 2 E . head Feature . head Description Returns a sub-api for retrieving data that is stored in edge features. In this example, we assume there is a feature called head . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Es instead. Es(feature) aka EdgeString(feature) 1 2 3 4 Es ( feature ) EdgeString ( feature ) Es ( 'head' ) EdgeString ( 'head' ) Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called head . Both methods have identical results Suppose we have just issued feature = 'head'. Then the result of Es(feature) and E.pos` is identical. In most cases E works just fine, but Es is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .meta 1 E . head . meta The dictionary of meta data found at the start of the head.tf file. E. feature .f(node) 1 E . head . f ( node ) Description Get the nodes reached by feature -edges from a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than None . node The node from which the edges in question start. E. feature .t(node) 1 E . head . t ( node ) Description Get the nodes reached by feature -edges to a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if feature describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges to n , the empty tuple is returned, rather than None . node The node to which the edges in question go. E. feature .b(node) 1 E . head . b ( node ) Description Get the nodes from and to a certain node by a feature -edge. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from or to the node, the empty tuple is returned, rather than None . node The node from which the edges in question start or to which they go. Think of both , hence the b . symmetric closure The .b() methods gives the symmetric closure of a set of edges: if there is an edge between n and m , this method will produce it, no matter the direction of the edge. Some edge sets are semantically symmetric, for example similarity . If n is similar to m , then m is similar to n . But if you store such an edge feature completely, half of the data is redundant. So you do not have to do that, you only need to store one of the edges between n and m (it does not matter which one), and E.sim.b() will nevertheless produce the complete results. conflicting values If your set of edges is not symmetric, and edges carry values, it might very well be the case that edges between the same pair of nodes carry different values for the two directions. In that case, the .b() method gives precedence to the edges that depart from a node. Suppose we have 1 2 n == value=4 ==> m m == value=6 ==> n then 1 2 E.b(n) = (m, 4) E.b(m) = (n, 6) E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None , nodeTypesTo = None ) Description If the edge feature has no values, simply return the number of node pairs between an edge of this kind exists. If the edge feature does have values, we inspect them and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypesFrom If not None , only the values for edges that start from a node with type within nodeTypesFrom will be counted. nodeTypesTo If not None , only the values for edges that go to a node with type within nodeTypesTo will be counted. E.oslots oslots is a special edge feature and is mainly used to construct other parts of the API. It has less capabilities, and you will rarely need it. It does not have .f and .t methods, but an .s method instead. Description E.oslots.s(node) Gives the sorted list of slot numbers linked to a node, or put otherwise: the slots that support that node. node The node whose slots are being delivered.","title":"Edge features"},{"location":"Api/Helpers/","text":"Helpers \u00b6 Various functions that have a function that is not directly tied to a class. These functions are available in tf.core.helpers , so in order to use function fff , say 1 from tf.core.helpers import fff Messages \u00b6 shapeMessages Wraps error messages into HTML. The messages come from the TF API, through the TF kernel, in response to wrong search templates and other mistaken user input. HTML and Markdown \u00b6 _outLink(text, href, title=None, ...) Produce a formatted HTML link. mathEsc(val) Produce a representation of val where $ signs are escaped to <span>$</span> so that they are not interpreted as math in a Jupyter notebook. htmlEsc(val) Produce a representation of val that is safe for usage in a Markdown and HTML context. htmlEsc(val) Produce a representation of val that is safe for usage in a HTML context. mdEsc(val) Produce a representation of val that is safe for usage in a Markdown context.","title":"Helpers"},{"location":"Api/Helpers/#helpers","text":"Various functions that have a function that is not directly tied to a class. These functions are available in tf.core.helpers , so in order to use function fff , say 1 from tf.core.helpers import fff","title":"Helpers"},{"location":"Api/Helpers/#messages","text":"shapeMessages Wraps error messages into HTML. The messages come from the TF API, through the TF kernel, in response to wrong search templates and other mistaken user input.","title":"Messages"},{"location":"Api/Helpers/#html-and-markdown","text":"_outLink(text, href, title=None, ...) Produce a formatted HTML link. mathEsc(val) Produce a representation of val where $ signs are escaped to <span>$</span> so that they are not interpreted as math in a Jupyter notebook. htmlEsc(val) Produce a representation of val that is safe for usage in a Markdown and HTML context. htmlEsc(val) Produce a representation of val that is safe for usage in a HTML context. mdEsc(val) Produce a representation of val that is safe for usage in a Markdown context.","title":"HTML and Markdown"},{"location":"Api/Lib/","text":"Lib \u00b6 Various functions that have a function that is not directly tied to a class. These functions are available in tf.lib , so in order to use function fff , say 1 from tf.lib import fff Sets \u00b6 writeSets() 1 writeSets ( sets , dest ) Writes a dictionary of named sets to file. The dictionary will be written as a gzipped, pickled data structure. Intended use: if you have constructed custom node sets that you want to use in search templates that run in the TF browser. sets A dictionary in which the keys are names for the values, which are sets of nodes. dest A file path. You may use ~ to refer to your home directory. The result will be written from this file. Returns True if all went well, False otherwise. readSets(source) 1 sets = readSets ( source ) Reads a dictionary of named sets from a file specified by source . This is used by the TF browser, when the user has passed a --sets=fileName argument to it. source A file path. You may use ~ to refer to your home directory. This file must contain a gzipped, pickled data structure. Returns The data structure contained in the file if all went well, False otherwise.","title":"Lib"},{"location":"Api/Lib/#lib","text":"Various functions that have a function that is not directly tied to a class. These functions are available in tf.lib , so in order to use function fff , say 1 from tf.lib import fff","title":"Lib"},{"location":"Api/Lib/#sets","text":"writeSets() 1 writeSets ( sets , dest ) Writes a dictionary of named sets to file. The dictionary will be written as a gzipped, pickled data structure. Intended use: if you have constructed custom node sets that you want to use in search templates that run in the TF browser. sets A dictionary in which the keys are names for the values, which are sets of nodes. dest A file path. You may use ~ to refer to your home directory. The result will be written from this file. Returns True if all went well, False otherwise. readSets(source) 1 sets = readSets ( source ) Reads a dictionary of named sets from a file specified by source . This is used by the TF browser, when the user has passed a --sets=fileName argument to it. source A file path. You may use ~ to refer to your home directory. This file must contain a gzipped, pickled data structure. Returns The data structure contained in the file if all went well, False otherwise.","title":"Sets"},{"location":"Api/Locality/","text":"Locality \u00b6 Local navigation Here are the methods by which you can navigate easily from a node to its neighbours: parents and children, previous and next siblings. L The Locality API is exposed as L or Locality . otype parameter In all of the following L -functions, if the otype parameter is passed, the result is filtered and only nodes with otype=nodeType are retained. Results of the L. functions are tuples, not single nodes Even if an L -function returns a single node, it is packed in a tuple . So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ] L.u() 1 L . u ( node , otype = nodeType ) Description Produces an ordered tuple of nodes upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered. The result never includes node itself. But other nodes linked to the same set of slots as node count as embedder nodes. But slots are never embedders. L.d() 1 L . d ( node , otype = nodeType ) Description Produces an ordered tuple of nodes downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered. The result never includes node itself. But other nodes linked to the same set of slots as node count as embedded nodes. But nothing is embedded in slots. L.n() 1 L . n ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent next nodes. node The node whose right adjacent nodes will be delivered; i.e. the nodes whose first slot immediately follow the last slot of node . The result never includes node itself. L.p() 1 L . p ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent previous nodes from node , i.e. nodes whose last slot just precedes the first slot of node . Description Produces an ordered tuple of adjacent previous nodes. node The node whose lefy adjacent nodes will be delivered; i.e. the nodes whose last slot immediately precede the first slot of node . Locality and levels Here is something that is very important to be aware of when using sortNodes and the L.d(n) and L.u(n) methods. When we order nodes and report on which nodes embed which other nodes, we do not only take into account the sets of slots the nodes occupy, but also their level . See levels . Both the L.d(n) and L.u(n) work as follows: L.d(n) returns nodes such that embedding nodes come before embedded nodes words) L.u(n) returns nodes such that embedded nodes come before embedding nodes books) N.B.: Suppose you have node types verse and sentence , and usually a verse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode) will contain sentence nodes, L.d(sentenceNode) will not contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the same words, then that is a case where: L.d(verseNode) will contain one sentence node, L.d(sentenceNode) will contain one verse node.","title":"L - Locality"},{"location":"Api/Locality/#locality","text":"Local navigation Here are the methods by which you can navigate easily from a node to its neighbours: parents and children, previous and next siblings. L The Locality API is exposed as L or Locality . otype parameter In all of the following L -functions, if the otype parameter is passed, the result is filtered and only nodes with otype=nodeType are retained. Results of the L. functions are tuples, not single nodes Even if an L -function returns a single node, it is packed in a tuple . So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ] L.u() 1 L . u ( node , otype = nodeType ) Description Produces an ordered tuple of nodes upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered. The result never includes node itself. But other nodes linked to the same set of slots as node count as embedder nodes. But slots are never embedders. L.d() 1 L . d ( node , otype = nodeType ) Description Produces an ordered tuple of nodes downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered. The result never includes node itself. But other nodes linked to the same set of slots as node count as embedded nodes. But nothing is embedded in slots. L.n() 1 L . n ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent next nodes. node The node whose right adjacent nodes will be delivered; i.e. the nodes whose first slot immediately follow the last slot of node . The result never includes node itself. L.p() 1 L . p ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent previous nodes from node , i.e. nodes whose last slot just precedes the first slot of node . Description Produces an ordered tuple of adjacent previous nodes. node The node whose lefy adjacent nodes will be delivered; i.e. the nodes whose last slot immediately precede the first slot of node . Locality and levels Here is something that is very important to be aware of when using sortNodes and the L.d(n) and L.u(n) methods. When we order nodes and report on which nodes embed which other nodes, we do not only take into account the sets of slots the nodes occupy, but also their level . See levels . Both the L.d(n) and L.u(n) work as follows: L.d(n) returns nodes such that embedding nodes come before embedded nodes words) L.u(n) returns nodes such that embedded nodes come before embedding nodes books) N.B.: Suppose you have node types verse and sentence , and usually a verse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode) will contain sentence nodes, L.d(sentenceNode) will not contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the same words, then that is a case where: L.d(verseNode) will contain one sentence node, L.d(sentenceNode) will contain one verse node.","title":"Locality"},{"location":"Api/Misc/","text":"Miscellaneous \u00b6 Banner \u00b6 TF.version Description Contains the version number of the Text-Fabric library. TF.banner Description Contains the name and the version of the Text-Fabric library. Messaging \u00b6 Timed messages Error and informational messages can be issued, with a time indication. silentOn(), silentOff(), isSilent(), setSilent() Suppress or enable informational messages, i.e. messages issued through info() below. This is no influence on error messages, i.e. messages issued through error() below. 1 silentOn ( deep = False ) Suppress informational messages. If deep=True also suppresses warnings. 1 silentOff () Enable informational messages. 1 setSilent ( flag ) e.g. 1 2 3 setSilent(False) setSilent(True) setSilent('deep') Enable or suppress informational/warning messages dependent on flag . 1 isSilent () Whether informational messages are currently suppressed or not. info(), warning(), error() 1 2 3 info ( msg , tm = True , nl = True ) warning ( msg , tm = True , nl = True ) error ( msg , tm = True , nl = True ) Description Sends a message to standard output, possibly with time and newline. if info() is being used, the message is suppressed if the silent mode is currently on; if the silent mode is not True but 'deep' , warnings() will also be suppressed; error() messages always come through; if info() or warning is being used, the message is sent to stdout ; if error() is being used, the message is sent to stderr ; In a Jupyter notebook, the standard error is displayed with a reddish background colour. tm If True , an indicator of the elapsed time will be prepended to the message. nl If True a newline will be appended. indent() 1 indent ( level = None , reset = False ) Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer. Subsequent info() and error() will display their messages with this indent. reset If True , the elapsed time to will be reset to 0 at the given level. Timers at different levels are independent of each other. Clearing the cache \u00b6 clearCache() 1 TF . clearCache () Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the original data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, where you might want to clear the cache of precomputed results. Calling this function just does it, and it is equivalent with manually removing all .tfx files inside the hidden .tf directory inside your dataset. See also clean() below From version 7.7.7 onwards, Text-Fabric uses a parameter called PACK_VERSION to mark the stored pre-computed data. Whenever there are incompatible changes in the packed data format, this version number will be increased and Text-Fabric will not attempt to load the older pre-computed data. The older data will not be removed, however. Use the function clean() below to get rid of pre-computed data that you no longer need. No need to load It is not needed to execute a TF.load() first. clean() 1 2 from tf.clean import clean clean ( tfd = True , gh = False , dry = False , specific = None , current = False ) Description Removes all precomputed data resulting from other PACK_VERSION s than the one currently used by Text-Fabric. You find the current pack version in the parameters file . tfd By default, your ~/text-fabric-data is traversed and cleaned, but if you pass tfd=False it will be skipped. gh By default, your ~/github will be skipped, but if you pass gh=True it will be traversed and cleaned. specific You can pass a specific directory here. The standard directories ~/github and ~/text-fabric-data will not be used, only the directory you pass here. ~ will be expanded to your home directory. current If current=True, also the precomputed results of the current version will be removed. dry By default, nothing will be deleted, and you only get a list of what will be deleted if it were not a dry run. If you pass dry=False the delete actions will really be executed.","title":"Miscellaneous"},{"location":"Api/Misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"Api/Misc/#banner","text":"TF.version Description Contains the version number of the Text-Fabric library. TF.banner Description Contains the name and the version of the Text-Fabric library.","title":"Banner"},{"location":"Api/Misc/#messaging","text":"Timed messages Error and informational messages can be issued, with a time indication. silentOn(), silentOff(), isSilent(), setSilent() Suppress or enable informational messages, i.e. messages issued through info() below. This is no influence on error messages, i.e. messages issued through error() below. 1 silentOn ( deep = False ) Suppress informational messages. If deep=True also suppresses warnings. 1 silentOff () Enable informational messages. 1 setSilent ( flag ) e.g. 1 2 3 setSilent(False) setSilent(True) setSilent('deep') Enable or suppress informational/warning messages dependent on flag . 1 isSilent () Whether informational messages are currently suppressed or not. info(), warning(), error() 1 2 3 info ( msg , tm = True , nl = True ) warning ( msg , tm = True , nl = True ) error ( msg , tm = True , nl = True ) Description Sends a message to standard output, possibly with time and newline. if info() is being used, the message is suppressed if the silent mode is currently on; if the silent mode is not True but 'deep' , warnings() will also be suppressed; error() messages always come through; if info() or warning is being used, the message is sent to stdout ; if error() is being used, the message is sent to stderr ; In a Jupyter notebook, the standard error is displayed with a reddish background colour. tm If True , an indicator of the elapsed time will be prepended to the message. nl If True a newline will be appended. indent() 1 indent ( level = None , reset = False ) Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer. Subsequent info() and error() will display their messages with this indent. reset If True , the elapsed time to will be reset to 0 at the given level. Timers at different levels are independent of each other.","title":"Messaging"},{"location":"Api/Misc/#clearing-the-cache","text":"clearCache() 1 TF . clearCache () Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the original data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, where you might want to clear the cache of precomputed results. Calling this function just does it, and it is equivalent with manually removing all .tfx files inside the hidden .tf directory inside your dataset. See also clean() below From version 7.7.7 onwards, Text-Fabric uses a parameter called PACK_VERSION to mark the stored pre-computed data. Whenever there are incompatible changes in the packed data format, this version number will be increased and Text-Fabric will not attempt to load the older pre-computed data. The older data will not be removed, however. Use the function clean() below to get rid of pre-computed data that you no longer need. No need to load It is not needed to execute a TF.load() first. clean() 1 2 from tf.clean import clean clean ( tfd = True , gh = False , dry = False , specific = None , current = False ) Description Removes all precomputed data resulting from other PACK_VERSION s than the one currently used by Text-Fabric. You find the current pack version in the parameters file . tfd By default, your ~/text-fabric-data is traversed and cleaned, but if you pass tfd=False it will be skipped. gh By default, your ~/github will be skipped, but if you pass gh=True it will be traversed and cleaned. specific You can pass a specific directory here. The standard directories ~/github and ~/text-fabric-data will not be used, only the directory you pass here. ~ will be expanded to your home directory. current If current=True, also the precomputed results of the current version will be removed. dry By default, nothing will be deleted, and you only get a list of what will be deleted if it were not a dry run. If you pass dry=False the delete actions will really be executed.","title":"Clearing the cache"},{"location":"Api/Nodes/","text":"Navigating nodes \u00b6 N() 1 2 for n in N (): action Description The result of N() is a generator that walks through all nodes in the canonical order (see below). Iterating over N() delivers you all words and structural elements of your corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in a suitable order. Occasionally, during the walk you might want to visit embedding or embedded nodes to glean some feature information from them. More ways of walking Later, under Features there is another convenient way to walk through nodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way that you can enumerate all nodes in the order you encounter them if you walk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence, because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a book node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis 1:1, then a sentence node, then a clause node, a phrase node, and the first word node. Then follow all word nodes in the first phrase, then the phrase node of the second phrase, followed by the word nodes in that phrase. When ever you enter a higher structure, you will first get the node corresponding to that structure, and after that the nodes corresponding to the building blocks of that structure. This concept follows the intuition that slot sets with smaller elements come before slot set with bigger elements, and embedding slot sets come before embedded slot sets. Hence, if you enumerate a set of nodes that happens to constitute a tree hierarchy based on slot set embedding, and you enumerate those nodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994), Text Databases. One Database Model and Several Retrieval Languages , number 14 in Language and Computers, Editions Rodopi, Amsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, https://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by Doedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements available when working with an element. That is the advantage of pre-order over post-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet ) Description delivers an iterable of nodes as a tuple sorted by the canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList = sorted ( nodes , key = sortKey ) Description A function that provides for each node the key to be used to sort nodes in the canonical ordering. That means that the following two pieces of code do the same thing: sortNodes(nodeSet) and sorted(nodeSet, key=sortKey) . sortKeyTuple 1 tupleList = sorted ( tuples , key = sortKeyTuple ) Description Same as sortKey , but this one works on tuples instead of nodes. It appies sortKey to each member of the tuple. Handy to sort e.g. search results. We could sort them in canonical order like this: 1 sorted ( results , key = lambda tup : tuple ( sortKey ( n ) for n in tup )) This is exactly what sortKeyTuple does, but then a bit more efficient. otypeRank 1 otypeRank [ 'sentence' ] The node types are ordered in C.levels.data , and if you reverse that list, you get the rank of a type by looking at the position in which that type occurs. The slotType has otypeRank 0, and the more comprehensive a type is, the higher its rank.","title":"N - Nodes"},{"location":"Api/Nodes/#navigating-nodes","text":"N() 1 2 for n in N (): action Description The result of N() is a generator that walks through all nodes in the canonical order (see below). Iterating over N() delivers you all words and structural elements of your corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in a suitable order. Occasionally, during the walk you might want to visit embedding or embedded nodes to glean some feature information from them. More ways of walking Later, under Features there is another convenient way to walk through nodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way that you can enumerate all nodes in the order you encounter them if you walk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence, because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a book node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis 1:1, then a sentence node, then a clause node, a phrase node, and the first word node. Then follow all word nodes in the first phrase, then the phrase node of the second phrase, followed by the word nodes in that phrase. When ever you enter a higher structure, you will first get the node corresponding to that structure, and after that the nodes corresponding to the building blocks of that structure. This concept follows the intuition that slot sets with smaller elements come before slot set with bigger elements, and embedding slot sets come before embedded slot sets. Hence, if you enumerate a set of nodes that happens to constitute a tree hierarchy based on slot set embedding, and you enumerate those nodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994), Text Databases. One Database Model and Several Retrieval Languages , number 14 in Language and Computers, Editions Rodopi, Amsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, https://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by Doedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements available when working with an element. That is the advantage of pre-order over post-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet ) Description delivers an iterable of nodes as a tuple sorted by the canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList = sorted ( nodes , key = sortKey ) Description A function that provides for each node the key to be used to sort nodes in the canonical ordering. That means that the following two pieces of code do the same thing: sortNodes(nodeSet) and sorted(nodeSet, key=sortKey) . sortKeyTuple 1 tupleList = sorted ( tuples , key = sortKeyTuple ) Description Same as sortKey , but this one works on tuples instead of nodes. It appies sortKey to each member of the tuple. Handy to sort e.g. search results. We could sort them in canonical order like this: 1 sorted ( results , key = lambda tup : tuple ( sortKey ( n ) for n in tup )) This is exactly what sortKeyTuple does, but then a bit more efficient. otypeRank 1 otypeRank [ 'sentence' ] The node types are ordered in C.levels.data , and if you reverse that list, you get the rank of a type by looking at the position in which that type occurs. The slotType has otypeRank 0, and the more comprehensive a type is, the higher its rank.","title":"Navigating nodes"},{"location":"Api/Repo/","text":"Auto downloading from GitHub \u00b6 checkoutRepo() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from tf.applib.repo import checkoutRepo checkoutRepo ( org = 'annotation' repo = 'tutorials' , folder = 'text-fabric/examples/banks/tf' , version = '' , checkout = '' , source = None , dest = None , withPaths = True , keep = True , silent = False , label = 'data' , ) Description Maintain a local copy of a subfolder folder in GitHub repository repo of org . The copy may be taken from any point in the commit history of the online repo. If you call this function, it will check whether the requested data is already on your computer in the expected location. If not, it may check whether the data is online and if so, download it to the expected location. Result The result of a call to checkoutRepo() is a tuple: 1 ( commitOffline , releaseOffline , kindLocal , localBase , localDir ) Here is the meaning: commitOffline is the commit hash of the data you have offline afterwards releaseOffline is the release tag of the data you have offline afterwards kindLocal indicates whether an online check has been performed: it is None if there has been an online check. Otherwise it is clone if the data is in your ~/github directory else it is local . localBase where the data is under: ~/github or ~/text-fabric-data , or whatever you have passed as source and dest , see below. localDir releative path from localBase to your data. If your data has versions, localDir points to directory that has the versions, not to a specific version. Your local copy can be found under your ~/github or ~/text-fabric-data directory using a relative path org/repo/folder if there is a version , else org/repo/folder/version . checkout, source and dest The checkout parameter determines from which point in the history the copy will be taken and where it will be placed. That will be either your ~/github or your ~/text-fabric-data directories. You can override the hard-coded ~/github and ~/text-fabric-data directories by passing source and dest respectively. See the repo notebook for an exhaustive demo of all the checkout options. other parameters withPaths=False will loose the directory structure of files that are being downloaded. keep=False will destroy the destination directory before a download takes place. silent=True will suppress non-error messages. *label='something' will change the word \"data\" in log messages to what you choose. We use label='TF-app' when we use this function to checkout the code of a TF-app. Rate limiting The checkRepo() function uses the GitHub API. GitHub has a rate limiting policy for its API of max 60 calls per hour. If you use this function in an application of yours that uses it very often, you can increase the limits by making yourself known. Read more about rate limiting on Github register your app with GitHub Obtain your client-id and client-secret and put them in environment variables named GHCLIENT and GHSECRET on the system where your app runs. If checkRepo() finds these variables, it will add the credentials to every GitHub API call it makes, and that will increase the rate. Never pass your personal credentials on to your clients!","title":"Repo"},{"location":"Api/Repo/#auto-downloading-from-github","text":"checkoutRepo() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from tf.applib.repo import checkoutRepo checkoutRepo ( org = 'annotation' repo = 'tutorials' , folder = 'text-fabric/examples/banks/tf' , version = '' , checkout = '' , source = None , dest = None , withPaths = True , keep = True , silent = False , label = 'data' , ) Description Maintain a local copy of a subfolder folder in GitHub repository repo of org . The copy may be taken from any point in the commit history of the online repo. If you call this function, it will check whether the requested data is already on your computer in the expected location. If not, it may check whether the data is online and if so, download it to the expected location. Result The result of a call to checkoutRepo() is a tuple: 1 ( commitOffline , releaseOffline , kindLocal , localBase , localDir ) Here is the meaning: commitOffline is the commit hash of the data you have offline afterwards releaseOffline is the release tag of the data you have offline afterwards kindLocal indicates whether an online check has been performed: it is None if there has been an online check. Otherwise it is clone if the data is in your ~/github directory else it is local . localBase where the data is under: ~/github or ~/text-fabric-data , or whatever you have passed as source and dest , see below. localDir releative path from localBase to your data. If your data has versions, localDir points to directory that has the versions, not to a specific version. Your local copy can be found under your ~/github or ~/text-fabric-data directory using a relative path org/repo/folder if there is a version , else org/repo/folder/version . checkout, source and dest The checkout parameter determines from which point in the history the copy will be taken and where it will be placed. That will be either your ~/github or your ~/text-fabric-data directories. You can override the hard-coded ~/github and ~/text-fabric-data directories by passing source and dest respectively. See the repo notebook for an exhaustive demo of all the checkout options. other parameters withPaths=False will loose the directory structure of files that are being downloaded. keep=False will destroy the destination directory before a download takes place. silent=True will suppress non-error messages. *label='something' will change the word \"data\" in log messages to what you choose. We use label='TF-app' when we use this function to checkout the code of a TF-app. Rate limiting The checkRepo() function uses the GitHub API. GitHub has a rate limiting policy for its API of max 60 calls per hour. If you use this function in an application of yours that uses it very often, you can increase the limits by making yourself known. Read more about rate limiting on Github register your app with GitHub Obtain your client-id and client-secret and put them in environment variables named GHCLIENT and GHSECRET on the system where your app runs. If checkRepo() finds these variables, it will add the credentials to every GitHub API call it makes, and that will increase the rate. Never pass your personal credentials on to your clients!","title":"Auto downloading from GitHub"},{"location":"Api/Search/","text":"Search \u00b6 For a description of Text-Fabric search, go to Search S The Search API is exposed as S or Search . It's main method, search , takes a search template as argument. A template consists of elements that specify nodes with conditions and relations between them. The results of a search are instantiations of the search template. More precisely, each instantiation is a tuple of nodes that instantiate the elements in the template, in such a way that the relational pattern expressed in the template holds between the nodes of the result tuple. Search API \u00b6 The API for searching is a bit richer than the search() functions. Here is the whole interface. S.relationsLegend() 1 S . relationsLegend () Description Gives dynamic help about the basic relations that you can use in your search template. It includes the edge features that are available in your dataset. S.search() 1 S . search ( query , limit = None , shallow = False , sets = None , withContext = None ) Description Searches for combinations of nodes that together match a search template. This method returns a generator which yields the results one by one. One result is a tuple of nodes, where each node corresponds to an atom -line in your search template . query The query is a search template, i.e. a string that conforms to the rules described above. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. limit If limit is a number, it will fetch only that many results. TF as Database By means of S.search(query) you can use one TF instance as a database that multiple clients can use without the need for each client to call the costly load methods. You have to come up with a process that runs TF, has all features loaded, and that can respond to queries from other processes. We call such a process a TF kernel . Web servers can use such a daemonized TF to build efficient controllers. A TF kernel and web server are included in the Text-Fabric code base. See kernel and web . Generator versus tuple If limit is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and translated into a search plan. The following methods expose parts of the search process, and may provide you with useful information in case the search does not deliver what you expect. see the plan the method S.showPlan() below shows you at a glance the correspondence between the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate , strategy = None , silent = False , shallow = False , sets = None ) Description Your search template will be checked, studied, the search space will be narrowed down, and a plan for retrieving the results will be set up. If your query has quantifiers, the asscociated search templates will be constructed and executed. These searches will be reported clearly. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched matters a lot. The search strategy is an implementation detail, but we bring it to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x') and you will get a list of options reported to choose from. Feel free to experiment. To see what the strategies do, see the code . silent If you want to suppress most of the output, say silent=True . shallow, sets As in S.search() . S.showPlan() 1 S . showPlan ( details = False ) Description Search results are tuples of nodes and the plan shows which part of the tuple corresponds to which part of the search template. details If you say details=True , you also get an overview of the search space and a description of how the results will be retrieved. after S.study() This function is only meaningful after a call to S.study() . S.tweakPerformance() Theory Before the search engine retrieves result tuples of nodes, there is a process to narrow down the search space. See search design and remember that we use the term yarn for the sets of candidate nodes from which we stitch our results together. Edge spinning is the process of transferring constraints on one node via edges to constraints on another node. The one node has a yarn, i.e. a set of candidate nodes, and the node at the other side of the edge has a yarn. If the first yarn is small, and candidates in the first yarn must be related to candidates in the second yarn, then we might be able to reduce the second yarn by computing the relation for the nodes in the small yarn and seeing which nodes you get in the second yarn. The relation corresponds to the edge, hence the term edge spinning. The success of edge spinning depends mainly on two factors: Size difference Edge spinning works best if there is a big difference in size between the candidate sets for the nodes at both sides of an edge. Spread The spread of a relation is the number of different edges that can start from the same node or end at the same node. For example, the spread of the equality operator is just 1, but the spread of the inequality operator is virtually as big as the relevant yarn. If there are constraints on a node in one yarn, and if there is an edge between that yarn and another one, and if the spread is big, not much of the constraint can be transferred to the other yarn. Example Suppose both yarns are words, the first yarn has been constrained to verbs, and the equality relation holds must hold between the yarns. Then in all results the node from the second yarn is also a verb. So we can constrain the second yarn to verbs too. But if the relation is inequality, we cannot impose any additional restriction on nodes in the second yarn. All nodes in the second yarn are unequal to at least one verb. If there is a big size difference between the two yarns, and the spread is small, a the bigger yarn will be restricted considerably. Estimating the spread We estimate the spreads of edges over and over again, in a series of iterations where we reduce yarns. An exhaustive computation would be too expensive, so we take a sample of a limited amount of relation computations. 1 S . tweakPerformance ( yarnRatio = None , tryLimitFrom = None , tryLimitTo = None ) Tweaks parameters that influence the performance of the search engine. If you do not pass a parameter, its value will not be changed. If you pass None , its value will be reset to the default value. Or you can pass a value of your choosing. yarnRatio The yarnRatio is the minimal factor between the sizes of the smallest and the biggest set of candidates of the nodes at both ends of the edge. And that divided by the spread of the relation as estimated by a sample. Example Suppose we set the yarnRatio to 1.5. Then, if we have yarns of 100,000 and 10,000 members, with a relation between them with spread 5, then 100,000 / 10,000 / 5 = 2. This is higher than the yarnRatio of 1.5, so the search engine decides that edge spinning is worth it. The reasoning is that the 10,000 nodes in the smallest yarn are expected to reach only 10,000 * 5 nodes in the other yarn by the relation, and so we can achieve a significant reduction. If you have a very slow query, and you think that a bit more edge spinning helps, decrease the yarnRatio towards 0. If you find that a lot of queries spend too much time in edge spinning, increase the yarnRatio. tryLimitFrom, tryLimitTo In order to determine the spreads of the relations, TF takes random samples and extrapolates the results. It takes some nodes from the set at the from side of an edge, and some nodes at the to side of the same edge, and computes in how many cases the relation holds. That is a measure for the spread. The parameters tryLimitFrom and tryLimitTo dictate how big these samples are. The bigger, the better the estimation of the spread. But also the more work it is. If you find that your queries take consistently a tad too much time, consider lowering these parameters to 10. If you find that the times your queries take varies a lot, increase these values to 10000. Search results \u00b6 Preparation versus result fetching The method S.search() above combines the interpretation of a given template, the setting up of a plan, the constraining of the search space and the fetching of results. Here are a few methods that do actual result fetching. They must be called after a previous S.search() or S.study() . S.count() 1 S . count ( progress = None , limit = None ) Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message. The frequency is progress results, default every 100. limit Fetch results up to a given limit , default 1000. Setting limit to 0 or a negative value means no limit: all results will be counted. why needed You typically need this in cases where result fetching turns out to be (very) slow. generator versus list len(S.results()) does not work, because S.results() is a generator that delivers its results as they come. S.fetch() 1 S . fetch ( limit = None ) Description Finally, you can retrieve the results. The result of fetch() is not a list of all results, but a generator . It will retrieve results as long as they are requested and their are still results. limit Tries to get that many results and collects them in a tuple. So if limit is not None , the result is a tuple with a known length. Iterating over the fetch() generator You typically fetch results by saying: 1 2 3 4 i = 0 for r in S . results (): do_something ( r [ 0 ]) do_something_else ( r [ 1 ]) Alternatively, you can set the limit parameter, to ask for just so many results. They will be fetched, and when they are all collected, returned as a tuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 ) gives you the first bunch of results quickly. S.glean() 1 S . glean ( r ) Description A search result is just a tuple of nodes that correspond to your template, as indicated by showPlan() . Nodes give you access to all information that the corpus has about it. The glean() function is here to just give you a first impression quickly. r Pass a raw result tuple r , and you get a string indicating where it occurs, in terms of sections, and what text is associated with the results. Inspecting results 1 2 for result in S . fetch ( limit = 10 ): print ( S . glean ( result )) is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been obtained by search or not. More ways of showing results If you work in one of the corpora for which there is a TF app, you will be provided with more powerful methods show() and table() to display your results.","title":"S - Search"},{"location":"Api/Search/#search","text":"For a description of Text-Fabric search, go to Search S The Search API is exposed as S or Search . It's main method, search , takes a search template as argument. A template consists of elements that specify nodes with conditions and relations between them. The results of a search are instantiations of the search template. More precisely, each instantiation is a tuple of nodes that instantiate the elements in the template, in such a way that the relational pattern expressed in the template holds between the nodes of the result tuple.","title":"Search"},{"location":"Api/Search/#search-api","text":"The API for searching is a bit richer than the search() functions. Here is the whole interface. S.relationsLegend() 1 S . relationsLegend () Description Gives dynamic help about the basic relations that you can use in your search template. It includes the edge features that are available in your dataset. S.search() 1 S . search ( query , limit = None , shallow = False , sets = None , withContext = None ) Description Searches for combinations of nodes that together match a search template. This method returns a generator which yields the results one by one. One result is a tuple of nodes, where each node corresponds to an atom -line in your search template . query The query is a search template, i.e. a string that conforms to the rules described above. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. limit If limit is a number, it will fetch only that many results. TF as Database By means of S.search(query) you can use one TF instance as a database that multiple clients can use without the need for each client to call the costly load methods. You have to come up with a process that runs TF, has all features loaded, and that can respond to queries from other processes. We call such a process a TF kernel . Web servers can use such a daemonized TF to build efficient controllers. A TF kernel and web server are included in the Text-Fabric code base. See kernel and web . Generator versus tuple If limit is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and translated into a search plan. The following methods expose parts of the search process, and may provide you with useful information in case the search does not deliver what you expect. see the plan the method S.showPlan() below shows you at a glance the correspondence between the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate , strategy = None , silent = False , shallow = False , sets = None ) Description Your search template will be checked, studied, the search space will be narrowed down, and a plan for retrieving the results will be set up. If your query has quantifiers, the asscociated search templates will be constructed and executed. These searches will be reported clearly. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched matters a lot. The search strategy is an implementation detail, but we bring it to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x') and you will get a list of options reported to choose from. Feel free to experiment. To see what the strategies do, see the code . silent If you want to suppress most of the output, say silent=True . shallow, sets As in S.search() . S.showPlan() 1 S . showPlan ( details = False ) Description Search results are tuples of nodes and the plan shows which part of the tuple corresponds to which part of the search template. details If you say details=True , you also get an overview of the search space and a description of how the results will be retrieved. after S.study() This function is only meaningful after a call to S.study() . S.tweakPerformance() Theory Before the search engine retrieves result tuples of nodes, there is a process to narrow down the search space. See search design and remember that we use the term yarn for the sets of candidate nodes from which we stitch our results together. Edge spinning is the process of transferring constraints on one node via edges to constraints on another node. The one node has a yarn, i.e. a set of candidate nodes, and the node at the other side of the edge has a yarn. If the first yarn is small, and candidates in the first yarn must be related to candidates in the second yarn, then we might be able to reduce the second yarn by computing the relation for the nodes in the small yarn and seeing which nodes you get in the second yarn. The relation corresponds to the edge, hence the term edge spinning. The success of edge spinning depends mainly on two factors: Size difference Edge spinning works best if there is a big difference in size between the candidate sets for the nodes at both sides of an edge. Spread The spread of a relation is the number of different edges that can start from the same node or end at the same node. For example, the spread of the equality operator is just 1, but the spread of the inequality operator is virtually as big as the relevant yarn. If there are constraints on a node in one yarn, and if there is an edge between that yarn and another one, and if the spread is big, not much of the constraint can be transferred to the other yarn. Example Suppose both yarns are words, the first yarn has been constrained to verbs, and the equality relation holds must hold between the yarns. Then in all results the node from the second yarn is also a verb. So we can constrain the second yarn to verbs too. But if the relation is inequality, we cannot impose any additional restriction on nodes in the second yarn. All nodes in the second yarn are unequal to at least one verb. If there is a big size difference between the two yarns, and the spread is small, a the bigger yarn will be restricted considerably. Estimating the spread We estimate the spreads of edges over and over again, in a series of iterations where we reduce yarns. An exhaustive computation would be too expensive, so we take a sample of a limited amount of relation computations. 1 S . tweakPerformance ( yarnRatio = None , tryLimitFrom = None , tryLimitTo = None ) Tweaks parameters that influence the performance of the search engine. If you do not pass a parameter, its value will not be changed. If you pass None , its value will be reset to the default value. Or you can pass a value of your choosing. yarnRatio The yarnRatio is the minimal factor between the sizes of the smallest and the biggest set of candidates of the nodes at both ends of the edge. And that divided by the spread of the relation as estimated by a sample. Example Suppose we set the yarnRatio to 1.5. Then, if we have yarns of 100,000 and 10,000 members, with a relation between them with spread 5, then 100,000 / 10,000 / 5 = 2. This is higher than the yarnRatio of 1.5, so the search engine decides that edge spinning is worth it. The reasoning is that the 10,000 nodes in the smallest yarn are expected to reach only 10,000 * 5 nodes in the other yarn by the relation, and so we can achieve a significant reduction. If you have a very slow query, and you think that a bit more edge spinning helps, decrease the yarnRatio towards 0. If you find that a lot of queries spend too much time in edge spinning, increase the yarnRatio. tryLimitFrom, tryLimitTo In order to determine the spreads of the relations, TF takes random samples and extrapolates the results. It takes some nodes from the set at the from side of an edge, and some nodes at the to side of the same edge, and computes in how many cases the relation holds. That is a measure for the spread. The parameters tryLimitFrom and tryLimitTo dictate how big these samples are. The bigger, the better the estimation of the spread. But also the more work it is. If you find that your queries take consistently a tad too much time, consider lowering these parameters to 10. If you find that the times your queries take varies a lot, increase these values to 10000.","title":"Search API"},{"location":"Api/Search/#search-results","text":"Preparation versus result fetching The method S.search() above combines the interpretation of a given template, the setting up of a plan, the constraining of the search space and the fetching of results. Here are a few methods that do actual result fetching. They must be called after a previous S.search() or S.study() . S.count() 1 S . count ( progress = None , limit = None ) Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message. The frequency is progress results, default every 100. limit Fetch results up to a given limit , default 1000. Setting limit to 0 or a negative value means no limit: all results will be counted. why needed You typically need this in cases where result fetching turns out to be (very) slow. generator versus list len(S.results()) does not work, because S.results() is a generator that delivers its results as they come. S.fetch() 1 S . fetch ( limit = None ) Description Finally, you can retrieve the results. The result of fetch() is not a list of all results, but a generator . It will retrieve results as long as they are requested and their are still results. limit Tries to get that many results and collects them in a tuple. So if limit is not None , the result is a tuple with a known length. Iterating over the fetch() generator You typically fetch results by saying: 1 2 3 4 i = 0 for r in S . results (): do_something ( r [ 0 ]) do_something_else ( r [ 1 ]) Alternatively, you can set the limit parameter, to ask for just so many results. They will be fetched, and when they are all collected, returned as a tuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 ) gives you the first bunch of results quickly. S.glean() 1 S . glean ( r ) Description A search result is just a tuple of nodes that correspond to your template, as indicated by showPlan() . Nodes give you access to all information that the corpus has about it. The glean() function is here to just give you a first impression quickly. r Pass a raw result tuple r , and you get a string indicating where it occurs, in terms of sections, and what text is associated with the results. Inspecting results 1 2 for result in S . fetch ( limit = 10 ): print ( S . glean ( result )) is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been obtained by search or not. More ways of showing results If you work in one of the corpora for which there is a TF app, you will be provided with more powerful methods show() and table() to display your results.","title":"Search results"},{"location":"Api/Text/","text":"Text \u00b6 Overview Here are the functions that enable you to get the actual text in the dataset. There are several things to accomplish here, such as support the structure of the corpus support a rigid section system usable by the TF browser handle multilingual section labels; switch between various text representations. The details of the Text API are dependent on the warp feature otext , which is a config feature. T The Text API is exposed as T or Text . otext is optional If otext is missing, the Text API will not be build. If it exists, but does not specify structure or sections, those parts of the Text API will not be built. Likewise for text representations. Structure \u00b6 Structure types If a corpus has sectional elements, such as series, volume, book, part, document, chapter, fragment, verse, halfverse, line, etc, then you can configure those elements as structural types. If your TF dataset designer has done that, the T.api will provide a number of handy functions to navigate your corpus along its structure, programmatically. The banks example corpus shows a full example. Structure is defined in the otext feature, by means of the keys structureTypes and structureFeatures . These are comma-separated lists of equal length. structureTypes specifies the node types that act as structural types, in the order from biggest to smallest. structureFeatures specifies a feature that corresponds to each structural type. This feature must contain the label of the structural element, e.g. the title of a book, the number of a verse, etc. The order of the section types is significant. Suppose you have a book with a single chapter. Is the chapter part of the book, or is the book part of the chapter? The order decides. If book is mentioned in structureTypes before chapter then the chapter is part of the book, and not the other way around. However, it is allowed to have nesting of elements of the same kind. Proper embedding not required There are no assumptions on how exactly the structural elements lie embedded in each other, and whether they consist of uninterrupted stretches of material or not. Suppose a we have a book with two disjoint chapters and there is a verse that has material in both chapters. Then that verse is part of neither chapter, but it is still part of the book. If you go down from that book to its substructural elements, you find not only its chapters, but also that verse. So the great freedom with respect to structural elements also brings greater responsibility when using that structure. T.structureInfo() 1 T . structureInfo () Gives a summary of how structure has bee configured in the dataset. If there are headings that are the same for multiple structural nodes, you'll get a warning here, and you are told how you can get all of those. It also shows a short description of all structure-related methods of the T API. T.structure() 1 T . structure ( node = None ) Gives the structure of node and everything below it as a tuple. If node is None, it gives the complete structure of the whole dataset. T.structurePretty() 1 T . structure ( node = None , fullHeading = False ) Gives the structure of node and everything below it as a string with pretty indentation.. If node is None, it gives the complete structure of the whole dataset. For each structural element, only its own heading is added. But if you want to see the full heading, consisting of the headings of a node and all of its parents, you can get that if you pass fullHeading=True . T.top() 1 T . top () Gives all top-level structural nodes in the dataset. These are the nodes that are not embedded in a structural node of the same or a higher level. T.up() 1 T . up ( node ) Gives the parent of node , provided node is structural. The parent is that structural node that has the heading you get from node minus the last element. E.g., the parent of ((book, Genesis), (chapter, 3), (verse, 16)) is the node that has heading ((book, Genesis), (chapter, 3)) . T.down() 1 T . down ( node ) Gives the children of node , provided node is structural. The children are those structural nodes that have headings that are one longer than the one you get from node . E.g., the children of ((book, Genesis), (chapter, 3)) are the nodes with heading ((book, Genesis), (chapter, 3), (verse, 1)) , etc. T.headingFromNode() 1 T . headingFromNode ( node ) Gives the full heading of node , provided node is structural. The heading is defined of the tuple of pairs (node type, feature value) for all ancestors of that node. E.g., the heading of the verse node corresponding to Genesis 3:16 is ((book, Genesis), (chapter, 3), (verse, 16)) . If you are interested in the complete mapping: it is stored in the dictionary T.hdFromNd . T.nodeFromHeading() 1 T . nodeFromHeading ( heading ) Gives the full node corresponding to a heading, provided it exists. It is the inverse of T.headingFromNode() . If you are interested in the complete mapping: it is stored in the dictionary T.ndFromHd . If there is more than one node that corresponds to the heading, only the last one in the corpus will be returned. T.hdMult contains all such cases. Sections \u00b6 Section levels In otext the main section levels (usually book , chapter , verse ) can be defined. It loads the features it needs (so you do not have to specify those features, unless you want to use them via F ). And finally, it makes some functions available by which you can make handy use of that information. Section levels from a limited, rigid system There are up to three section levels, and this is a hard coded boundary. That makes this section system unsuitable to faithfully reflect the rich sectioning that may be present in a corpus. On the other hand, applications (such as TF apps) can access a predictable sectioning system by which they can chunk the material in practical portions. The rule of thumb is: Level 1 divides the corpus into top level units, of which there might be (very) many. The TF browser has a control that can deal with long lists. Level 2 divides a level 1 section into a chunk that can be loaded into a webpage, without overwhelming the browser. Even better, it should be just one or a few screenfuls of text, when represented in plain view. Level 3 divides a level 2 section into chunks that roughly corresponds to lines. Such lines typically take up one screenful if represented in pretty view. Section levels are generic In this documentation, we call the main section level book , the second level chapter , and the third level verse . Text-Fabric, however, is completely agnostic about how these levels are called. It is prepared to distinguish three section levels, but how they are called, must be configured in the dataset. The task of the otext feature is to declare which node type and feature correspond with which section level. Text-Fabric assumes that the first section level may have multilingual headings, but that section levels two and three have single language headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or integers, depending on whether your dataset has declared the corresponding feature valueType as str or as int . Conceivably, other works might have chapter and verse numbers like XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the sense that node types that act as containers have a lower level than node types that act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for your dataset, you can configure the right order in the otext feature, by means of a key levels with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign T.sectionTuple() 1 T . sectionTuple ( node , lastSlot = False , fillup = False ) Description Returns a tuple of nodes that correspond to the sections of level 0, 1 and 2 (e.g. book, chapter, verse). that include a node. More precisely, we retrieve the sections that contain a reference node, which is either the first slot or the last slot of the node in question. node The node whose containing section we want to get. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. fillup If False , and node is a level 0 section node, a 1-element tuple of this node is returned. And if node is a level 1 section node, a 2-element tuple is returned: the enclosing level 0 section node and node itself. If True , always a complete 3-tuple of a level 0, 1, and 2 section node is returned. T.sectionFromNode() 1 T . sectionFromNode ( node , lastSlot = False , lang = 'en' , fillup = False ) Description Returns the book/chapter/verse indications that correspond to the reference node, which is the first or last slot belonging node , dependent on lastSlot . The result is a tuple, consisting of the book name (in language lang ), the chapter number, and the verse number. But see the fillup parameter. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. fillup Same as for sectionTuple() crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse. In those cases, this function will return a different results for lastSlot=False and lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a None in the corresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section , lang = 'en' ) Description Given a section tuple, return the node of it. section section consists of a book name (in language lang ), and a chapter number and a verse number (both as strings or number depending on the value type of the corresponding feature). The verse number may be left out, the result is then a chapter node. Both verse and chapter numbers may be left out, the result is then a book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent. Book names and languages \u00b6 Book names and nodes The names of the books may be available in multiple languages. The book names are stored in node features with names of the form book@ la , where la is the ISO 639 two-letter code for that language. Text-Fabric will always load these features. T.languages 1 T . languages Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node , lang = 'en' ) Description gives the name of the book in which a node occurs. node The node in question. lang The lang parameter is a two letter language code. The default is en (English). If there is no feature data for the language chosen, the value of the ordinary book feature of the dataset will be returned. Works for all nodes n may or may not be a book node. If not, bookName() retrieves the embedding book node first. T.bookNode() 1 T . bookNode ( name , lang = 'en' ) Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the name parameter. If lang can not be found, the value of the ordinary book feature of the dataset will be used. If name cannot be found in the specified language, None will be returned. Function name follows configured section level If your dataset has configured section level one under an other name, say tablet , then these two methods follow that name. Instead of T.bookName() and T.bookNode() we have then T.tabletName() and T.tabletNode() . Text representation \u00b6 Text formats Text can be represented in multiple ways. We provide a number of formats with structured names. format names A format name is a string of keywords separated by - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is text-orig-full , we assume that every TF dataset defines this format. format content A format is a template string, with fixed text and variable text. The variable text comes from features. You specify the interpolation of features by surrounding the feature name by { } . For example, if letters and after are features, this is a text format: 1 {letters}{after} If you need tabs and newlines in a format, specify them by \\t and \\n. You can also conditionally choose between features, to substitute the value of another feature in case of empty values. For example, if you want to use the normal feature to represent a word, but if there is also a rare feature special that you want to use if it is defined for that word, you can make a format 1 {special/normal} This tries the feature special first, and if that is empty, it takes normal . You can also add a fixed default. If you want to display a . if neither special nor normal exist, you can say 1 {special/normal:.} formats specialized to a node type TF datasets may also define formats of the form nodetype -default where nodetype is a valid type of node in the dataset. These formats will be invoked in cases where no explicit format is specified as a fall back for some kind of nodes. See T.text() below. target node types for formats A node type may also be prepended to a format, with # as separator: nodetype # textformat In general, a format can be applied to any kind of node, and it will lookup the features defined in its template for that node. But some features have meaningful values for particular node types only. So formats may indicate that they should be applied to nodes of a specific type. See T.text() below. Remember that the formats are defined in the otext warp config feature of your set, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats. They do not have to complied with the above-mentioned scheme. layout in formats So far, text formats only result in plain text. A TF-app may define and implement extra text formats which may invoke all HTML+CSS styling that you can think of. T.formats 1 T . formats Description Show the text representation formats that have been defined in your dataset. T.text() 1 2 3 T . text ( nodes , fmt = None ) T . text ( node , fmt = None , descend = None ) T . text ( node , fmt = None , descend = None , explain = False ) Description Gives the text that corresponds to a bunch of nodes. The banks example corpus shows examples. nodes nodes can be a single node or an arbitrary iterable of nodes of arbitrary types. No attempt will be made to sort the nodes. If you need order, it is better to order the nodes before you feed them to T.text() . fmt The format of the text representation is given with fmt . If it is not specified or None, each node will be formatted with a node type specific format, defined as nodeType -default , if it exists. If there is no node specific format, the default format text-orig-full will be used. If text-orig-full is not defined, an error message will be issued, and the nodes will be represented by their types and numbers. If a value for fmt is passed, but it is not a format defined in the otext.tf feature, there will be an error message and None is returned. descend Whether to descend to constituent nodes. If True, nodes will be replaced by a sequence of their consituent nodes, which have a type specified by the format chosen, or, if the format does not specify such a type, the node will be replaced by the slots contained in it. If False, nodes will not be replaced. If descend is not specified or None, a node will be replaced by its constituent nodes, unless its type is associated with the given format or, if no format is given, by the default format of its type, or, if there is no such format, by its slots. no nodes to descend to If you call T.text(n, fmt=myfmt) and myfmt is targeted to a node type that is bigger than the node type of n , then the so-called descending leads to an empty sequence of nodes and hence to an empty string. explain The logic of this function is subtle. If you call it and the results baffles you, pass explain=True and it will explain what it is doing. The default is sensible Consider the simplest call to this function: T.text(node) . This will apply the default format to node . If node is non-slot, then in most cases the default format will be applied to the slots contained in node . But for special node types, where the best representation is not obtained by descending down to the contained slot nodes, the dataset may define special default types that use other features to furnish a decent representation. lexemes In some corpora case this happens for the type of lexemes: lex . Lexemes contain their occurrences as slots, but the representation of a lexeme is not the string of its occurrences, but resides in a feature such as voc_lex_utf8 (vocalized lexeme in Unicode). If the dataset defines the format lex-default={lex} , this is the only thing needed to regulate the representation of a lexeme. Hence, T.text(lx) results in the lexeme representation of lx . But if you really want to print out all occurrences of lexeme lx , you can say T.text(lx, descend=True) . words and signs In some corpora the characters or signs are the slot level, and there is a non slot level of words. Some text formats are best defined on signs, others best on words. For example, if words are associated with lexemes, stored in a word feature lex , we can define a text format lex-orig-full=word#{lex} When you call T.text(n) for a non-slot, non-word node, normally the node will be replaced by the slot nodes it contains, before applying the template in the format. But if you pass a format that specifies a different node type, nodes will be replaced by contained nodes of that type. So T.text(n, fmt='lex-orig-full') will lookup all word nodes under n and apply the template {lex} to them. same and different behaviours The consequences of the rules might be unexpected in some cases. Here are a few observations: formats like phrase-default can be implicitly invoked for phrase nodes, but descend=True prevents that; when a format targeted at phrases is invoked for phrase nodes, descend=True will not cause the expansion of those nodes to slot nodes, because the phrase node is already expanded to the target type of the format; memory aid If fmt is explicitly passed, it will be the format used no matter what, and it determines the level of the nodes to descend to; Descending is the norm, it can only be prevented by setting default formats for node types or by passing descend=False to T.text() ; descend=True is stronger than type-specific default formats, but weaker than explicitly passed formats; Pass explain=True for a dynamic explanation. Non slot nodes allowed In most cases, the nodes fed to T.text() are slots, and the formats are templates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features defined for a non-slot node type. If, for example, your slot type is glyph , and you want a format that renders lexemes, which are not defined for glyphs but for words, you can just define a format in terms of word features. It is your responsibility to take care to use the formats for node types for which they make sense. Escape whitespace in formats When defining formats in otext.tf , if you need a newline or tab in the format, specify it as \\n and \\t .","title":"T - Text"},{"location":"Api/Text/#text","text":"Overview Here are the functions that enable you to get the actual text in the dataset. There are several things to accomplish here, such as support the structure of the corpus support a rigid section system usable by the TF browser handle multilingual section labels; switch between various text representations. The details of the Text API are dependent on the warp feature otext , which is a config feature. T The Text API is exposed as T or Text . otext is optional If otext is missing, the Text API will not be build. If it exists, but does not specify structure or sections, those parts of the Text API will not be built. Likewise for text representations.","title":"Text"},{"location":"Api/Text/#structure","text":"Structure types If a corpus has sectional elements, such as series, volume, book, part, document, chapter, fragment, verse, halfverse, line, etc, then you can configure those elements as structural types. If your TF dataset designer has done that, the T.api will provide a number of handy functions to navigate your corpus along its structure, programmatically. The banks example corpus shows a full example. Structure is defined in the otext feature, by means of the keys structureTypes and structureFeatures . These are comma-separated lists of equal length. structureTypes specifies the node types that act as structural types, in the order from biggest to smallest. structureFeatures specifies a feature that corresponds to each structural type. This feature must contain the label of the structural element, e.g. the title of a book, the number of a verse, etc. The order of the section types is significant. Suppose you have a book with a single chapter. Is the chapter part of the book, or is the book part of the chapter? The order decides. If book is mentioned in structureTypes before chapter then the chapter is part of the book, and not the other way around. However, it is allowed to have nesting of elements of the same kind. Proper embedding not required There are no assumptions on how exactly the structural elements lie embedded in each other, and whether they consist of uninterrupted stretches of material or not. Suppose a we have a book with two disjoint chapters and there is a verse that has material in both chapters. Then that verse is part of neither chapter, but it is still part of the book. If you go down from that book to its substructural elements, you find not only its chapters, but also that verse. So the great freedom with respect to structural elements also brings greater responsibility when using that structure. T.structureInfo() 1 T . structureInfo () Gives a summary of how structure has bee configured in the dataset. If there are headings that are the same for multiple structural nodes, you'll get a warning here, and you are told how you can get all of those. It also shows a short description of all structure-related methods of the T API. T.structure() 1 T . structure ( node = None ) Gives the structure of node and everything below it as a tuple. If node is None, it gives the complete structure of the whole dataset. T.structurePretty() 1 T . structure ( node = None , fullHeading = False ) Gives the structure of node and everything below it as a string with pretty indentation.. If node is None, it gives the complete structure of the whole dataset. For each structural element, only its own heading is added. But if you want to see the full heading, consisting of the headings of a node and all of its parents, you can get that if you pass fullHeading=True . T.top() 1 T . top () Gives all top-level structural nodes in the dataset. These are the nodes that are not embedded in a structural node of the same or a higher level. T.up() 1 T . up ( node ) Gives the parent of node , provided node is structural. The parent is that structural node that has the heading you get from node minus the last element. E.g., the parent of ((book, Genesis), (chapter, 3), (verse, 16)) is the node that has heading ((book, Genesis), (chapter, 3)) . T.down() 1 T . down ( node ) Gives the children of node , provided node is structural. The children are those structural nodes that have headings that are one longer than the one you get from node . E.g., the children of ((book, Genesis), (chapter, 3)) are the nodes with heading ((book, Genesis), (chapter, 3), (verse, 1)) , etc. T.headingFromNode() 1 T . headingFromNode ( node ) Gives the full heading of node , provided node is structural. The heading is defined of the tuple of pairs (node type, feature value) for all ancestors of that node. E.g., the heading of the verse node corresponding to Genesis 3:16 is ((book, Genesis), (chapter, 3), (verse, 16)) . If you are interested in the complete mapping: it is stored in the dictionary T.hdFromNd . T.nodeFromHeading() 1 T . nodeFromHeading ( heading ) Gives the full node corresponding to a heading, provided it exists. It is the inverse of T.headingFromNode() . If you are interested in the complete mapping: it is stored in the dictionary T.ndFromHd . If there is more than one node that corresponds to the heading, only the last one in the corpus will be returned. T.hdMult contains all such cases.","title":"Structure"},{"location":"Api/Text/#sections","text":"Section levels In otext the main section levels (usually book , chapter , verse ) can be defined. It loads the features it needs (so you do not have to specify those features, unless you want to use them via F ). And finally, it makes some functions available by which you can make handy use of that information. Section levels from a limited, rigid system There are up to three section levels, and this is a hard coded boundary. That makes this section system unsuitable to faithfully reflect the rich sectioning that may be present in a corpus. On the other hand, applications (such as TF apps) can access a predictable sectioning system by which they can chunk the material in practical portions. The rule of thumb is: Level 1 divides the corpus into top level units, of which there might be (very) many. The TF browser has a control that can deal with long lists. Level 2 divides a level 1 section into a chunk that can be loaded into a webpage, without overwhelming the browser. Even better, it should be just one or a few screenfuls of text, when represented in plain view. Level 3 divides a level 2 section into chunks that roughly corresponds to lines. Such lines typically take up one screenful if represented in pretty view. Section levels are generic In this documentation, we call the main section level book , the second level chapter , and the third level verse . Text-Fabric, however, is completely agnostic about how these levels are called. It is prepared to distinguish three section levels, but how they are called, must be configured in the dataset. The task of the otext feature is to declare which node type and feature correspond with which section level. Text-Fabric assumes that the first section level may have multilingual headings, but that section levels two and three have single language headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or integers, depending on whether your dataset has declared the corresponding feature valueType as str or as int . Conceivably, other works might have chapter and verse numbers like XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the sense that node types that act as containers have a lower level than node types that act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for your dataset, you can configure the right order in the otext feature, by means of a key levels with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign T.sectionTuple() 1 T . sectionTuple ( node , lastSlot = False , fillup = False ) Description Returns a tuple of nodes that correspond to the sections of level 0, 1 and 2 (e.g. book, chapter, verse). that include a node. More precisely, we retrieve the sections that contain a reference node, which is either the first slot or the last slot of the node in question. node The node whose containing section we want to get. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. fillup If False , and node is a level 0 section node, a 1-element tuple of this node is returned. And if node is a level 1 section node, a 2-element tuple is returned: the enclosing level 0 section node and node itself. If True , always a complete 3-tuple of a level 0, 1, and 2 section node is returned. T.sectionFromNode() 1 T . sectionFromNode ( node , lastSlot = False , lang = 'en' , fillup = False ) Description Returns the book/chapter/verse indications that correspond to the reference node, which is the first or last slot belonging node , dependent on lastSlot . The result is a tuple, consisting of the book name (in language lang ), the chapter number, and the verse number. But see the fillup parameter. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. fillup Same as for sectionTuple() crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse. In those cases, this function will return a different results for lastSlot=False and lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a None in the corresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section , lang = 'en' ) Description Given a section tuple, return the node of it. section section consists of a book name (in language lang ), and a chapter number and a verse number (both as strings or number depending on the value type of the corresponding feature). The verse number may be left out, the result is then a chapter node. Both verse and chapter numbers may be left out, the result is then a book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent.","title":"Sections"},{"location":"Api/Text/#book-names-and-languages","text":"Book names and nodes The names of the books may be available in multiple languages. The book names are stored in node features with names of the form book@ la , where la is the ISO 639 two-letter code for that language. Text-Fabric will always load these features. T.languages 1 T . languages Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node , lang = 'en' ) Description gives the name of the book in which a node occurs. node The node in question. lang The lang parameter is a two letter language code. The default is en (English). If there is no feature data for the language chosen, the value of the ordinary book feature of the dataset will be returned. Works for all nodes n may or may not be a book node. If not, bookName() retrieves the embedding book node first. T.bookNode() 1 T . bookNode ( name , lang = 'en' ) Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the name parameter. If lang can not be found, the value of the ordinary book feature of the dataset will be used. If name cannot be found in the specified language, None will be returned. Function name follows configured section level If your dataset has configured section level one under an other name, say tablet , then these two methods follow that name. Instead of T.bookName() and T.bookNode() we have then T.tabletName() and T.tabletNode() .","title":"Book names and languages"},{"location":"Api/Text/#text-representation","text":"Text formats Text can be represented in multiple ways. We provide a number of formats with structured names. format names A format name is a string of keywords separated by - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is text-orig-full , we assume that every TF dataset defines this format. format content A format is a template string, with fixed text and variable text. The variable text comes from features. You specify the interpolation of features by surrounding the feature name by { } . For example, if letters and after are features, this is a text format: 1 {letters}{after} If you need tabs and newlines in a format, specify them by \\t and \\n. You can also conditionally choose between features, to substitute the value of another feature in case of empty values. For example, if you want to use the normal feature to represent a word, but if there is also a rare feature special that you want to use if it is defined for that word, you can make a format 1 {special/normal} This tries the feature special first, and if that is empty, it takes normal . You can also add a fixed default. If you want to display a . if neither special nor normal exist, you can say 1 {special/normal:.} formats specialized to a node type TF datasets may also define formats of the form nodetype -default where nodetype is a valid type of node in the dataset. These formats will be invoked in cases where no explicit format is specified as a fall back for some kind of nodes. See T.text() below. target node types for formats A node type may also be prepended to a format, with # as separator: nodetype # textformat In general, a format can be applied to any kind of node, and it will lookup the features defined in its template for that node. But some features have meaningful values for particular node types only. So formats may indicate that they should be applied to nodes of a specific type. See T.text() below. Remember that the formats are defined in the otext warp config feature of your set, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats. They do not have to complied with the above-mentioned scheme. layout in formats So far, text formats only result in plain text. A TF-app may define and implement extra text formats which may invoke all HTML+CSS styling that you can think of. T.formats 1 T . formats Description Show the text representation formats that have been defined in your dataset. T.text() 1 2 3 T . text ( nodes , fmt = None ) T . text ( node , fmt = None , descend = None ) T . text ( node , fmt = None , descend = None , explain = False ) Description Gives the text that corresponds to a bunch of nodes. The banks example corpus shows examples. nodes nodes can be a single node or an arbitrary iterable of nodes of arbitrary types. No attempt will be made to sort the nodes. If you need order, it is better to order the nodes before you feed them to T.text() . fmt The format of the text representation is given with fmt . If it is not specified or None, each node will be formatted with a node type specific format, defined as nodeType -default , if it exists. If there is no node specific format, the default format text-orig-full will be used. If text-orig-full is not defined, an error message will be issued, and the nodes will be represented by their types and numbers. If a value for fmt is passed, but it is not a format defined in the otext.tf feature, there will be an error message and None is returned. descend Whether to descend to constituent nodes. If True, nodes will be replaced by a sequence of their consituent nodes, which have a type specified by the format chosen, or, if the format does not specify such a type, the node will be replaced by the slots contained in it. If False, nodes will not be replaced. If descend is not specified or None, a node will be replaced by its constituent nodes, unless its type is associated with the given format or, if no format is given, by the default format of its type, or, if there is no such format, by its slots. no nodes to descend to If you call T.text(n, fmt=myfmt) and myfmt is targeted to a node type that is bigger than the node type of n , then the so-called descending leads to an empty sequence of nodes and hence to an empty string. explain The logic of this function is subtle. If you call it and the results baffles you, pass explain=True and it will explain what it is doing. The default is sensible Consider the simplest call to this function: T.text(node) . This will apply the default format to node . If node is non-slot, then in most cases the default format will be applied to the slots contained in node . But for special node types, where the best representation is not obtained by descending down to the contained slot nodes, the dataset may define special default types that use other features to furnish a decent representation. lexemes In some corpora case this happens for the type of lexemes: lex . Lexemes contain their occurrences as slots, but the representation of a lexeme is not the string of its occurrences, but resides in a feature such as voc_lex_utf8 (vocalized lexeme in Unicode). If the dataset defines the format lex-default={lex} , this is the only thing needed to regulate the representation of a lexeme. Hence, T.text(lx) results in the lexeme representation of lx . But if you really want to print out all occurrences of lexeme lx , you can say T.text(lx, descend=True) . words and signs In some corpora the characters or signs are the slot level, and there is a non slot level of words. Some text formats are best defined on signs, others best on words. For example, if words are associated with lexemes, stored in a word feature lex , we can define a text format lex-orig-full=word#{lex} When you call T.text(n) for a non-slot, non-word node, normally the node will be replaced by the slot nodes it contains, before applying the template in the format. But if you pass a format that specifies a different node type, nodes will be replaced by contained nodes of that type. So T.text(n, fmt='lex-orig-full') will lookup all word nodes under n and apply the template {lex} to them. same and different behaviours The consequences of the rules might be unexpected in some cases. Here are a few observations: formats like phrase-default can be implicitly invoked for phrase nodes, but descend=True prevents that; when a format targeted at phrases is invoked for phrase nodes, descend=True will not cause the expansion of those nodes to slot nodes, because the phrase node is already expanded to the target type of the format; memory aid If fmt is explicitly passed, it will be the format used no matter what, and it determines the level of the nodes to descend to; Descending is the norm, it can only be prevented by setting default formats for node types or by passing descend=False to T.text() ; descend=True is stronger than type-specific default formats, but weaker than explicitly passed formats; Pass explain=True for a dynamic explanation. Non slot nodes allowed In most cases, the nodes fed to T.text() are slots, and the formats are templates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features defined for a non-slot node type. If, for example, your slot type is glyph , and you want a format that renders lexemes, which are not defined for glyphs but for words, you can just define a format in terms of word features. It is your responsibility to take care to use the formats for node types for which they make sense. Escape whitespace in formats When defining formats in otext.tf , if you need a newline or tab in the format, specify it as \\n and \\t .","title":"Text representation"},{"location":"Code/Overview/","text":"Code organisation \u00b6 The code base of Text-Fabric is evolving to a considerable size . However, he code can be divided into a few major parts, each with their own, identifiable task. Some parts of the code are covered by unit tests . Base \u00b6 The generic API ( stats ) of Text-Fabric is responsible for: Data management Text-Fabric data consists of feature files . TF must be able to load them, save them, import/export from MQL. Provide an API TF must offer an API for handling its data in applications. That means: feature lookup, containment lookup, text serialization. Precomputation In order to make its API work efficiently, TF has to precompute certain compiled forms of the data. Search \u00b6 TF contains a search engine ( stats ) based on templates, which are little graphs of nodes and edges that must be instantiated against the corpus. Search vs MQL The template language is inspired by MQL, but has a different syntax. It is both weaker and stronger than MQL. Search vs hand coding Search templates are the most accessible way to get at the data, easier than hand-coding your own little programs. The underlying engine is quite complicated. Sometimes it is faster than hand coding, sometimes (much) slower. Apps \u00b6 TF contains corpus-dependent apps ( stats ). Display An app knows how to display a particular corpus. Download An app knows how to download a particular corpus from its online repository. Web interface An app can set up a web interface for a particular corpus. Web interface \u00b6 TF contains a web interface ( stats ) for interacting with your corpus without programming. This interface can be served by a local web server (part of TF), or you can set up an internet set served by a TF kernel and web server. Working with your corpus The web interface lets you fire queries (search templates) to TF and interact with the results: expanding rows to pretty displays; condensing results to verious container types; exporting results as PDF and CSV.","title":"Overview"},{"location":"Code/Overview/#code-organisation","text":"The code base of Text-Fabric is evolving to a considerable size . However, he code can be divided into a few major parts, each with their own, identifiable task. Some parts of the code are covered by unit tests .","title":"Code organisation"},{"location":"Code/Overview/#base","text":"The generic API ( stats ) of Text-Fabric is responsible for: Data management Text-Fabric data consists of feature files . TF must be able to load them, save them, import/export from MQL. Provide an API TF must offer an API for handling its data in applications. That means: feature lookup, containment lookup, text serialization. Precomputation In order to make its API work efficiently, TF has to precompute certain compiled forms of the data.","title":"Base"},{"location":"Code/Overview/#search","text":"TF contains a search engine ( stats ) based on templates, which are little graphs of nodes and edges that must be instantiated against the corpus. Search vs MQL The template language is inspired by MQL, but has a different syntax. It is both weaker and stronger than MQL. Search vs hand coding Search templates are the most accessible way to get at the data, easier than hand-coding your own little programs. The underlying engine is quite complicated. Sometimes it is faster than hand coding, sometimes (much) slower.","title":"Search"},{"location":"Code/Overview/#apps","text":"TF contains corpus-dependent apps ( stats ). Display An app knows how to display a particular corpus. Download An app knows how to download a particular corpus from its online repository. Web interface An app can set up a web interface for a particular corpus.","title":"Apps"},{"location":"Code/Overview/#web-interface","text":"TF contains a web interface ( stats ) for interacting with your corpus without programming. This interface can be served by a local web server (part of TF), or you can set up an internet set served by a TF kernel and web server. Working with your corpus The web interface lets you fire queries (search templates) to TF and interact with the results: expanding rows to pretty displays; condensing results to verious container types; exporting results as PDF and CSV.","title":"Web interface"},{"location":"Code/Stats/","text":"cloc github.com/AlDanial/cloc v 1.78 T=1.94 s (83.5 files/s, 21079.7 lines/s) Language files blank comment code Python 78 2871 1196 17066 Markdown 63 3565 0 11827 CSS 15 77 43 2618 JavaScript 2 72 40 719 HTML 2 5 0 492 YAML 1 8 0 276 Bourne Shell 1 0 0 1 -------- -------- -------- -------- -------- SUM: 162 6598 1279 32999","title":"Stats (overall)"},{"location":"Code/StatsApplib/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.05 s (339.3 files/s, 62538.2 lines/s) Language files blank comment code Python 16 405 118 2426 -------- -------- -------- -------- -------- SUM: 16 405 118 2426","title":"Stats (TF-app support)"},{"location":"Code/StatsApps/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.09 s (384.0 files/s, 75969.1 lines/s) Language files blank comment code Python 24 620 268 3867 CSS 10 14 15 1942 -------- -------- -------- -------- -------- SUM: 34 634 283 5809","title":"Stats (TF-apps)"},{"location":"Code/StatsCompose/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.02 s (128.5 files/s, 47469.8 lines/s) Language files blank comment code Python 3 186 16 906 -------- -------- -------- -------- -------- SUM: 3 186 16 906","title":"Stats (TF-compose)"},{"location":"Code/StatsConvert/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.03 s (129.3 files/s, 53946.8 lines/s) Language files blank comment code Python 4 256 66 1347 -------- -------- -------- -------- -------- SUM: 4 256 66 1347","title":"Stats (TF-convert)"},{"location":"Code/StatsCore/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.04 s (199.4 files/s, 60812.7 lines/s) Language files blank comment code Python 8 313 54 2073 -------- -------- -------- -------- -------- SUM: 8 313 54 2073","title":"Stats (TF-core)"},{"location":"Code/StatsSearch/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.06 s (145.9 files/s, 72570.2 lines/s) Language files blank comment code Python 9 595 379 3503 -------- -------- -------- -------- -------- SUM: 9 595 379 3503","title":"Stats (TF-search)"},{"location":"Code/StatsServer/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.05 s (357.4 files/s, 79011.2 lines/s) Language files blank comment code Python 9 311 235 1338 JavaScript 2 72 40 719 CSS 5 63 28 676 HTML 2 5 0 492 -------- -------- -------- -------- -------- SUM: 18 451 303 3225","title":"Stats (TF-server)"},{"location":"Code/StatsTest/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.02 s (81.6 files/s, 54551.5 lines/s) Language files blank comment code Python 2 63 46 1228 -------- -------- -------- -------- -------- SUM: 2 63 46 1228","title":"Stats (Tests)"},{"location":"Code/StatsToplevel/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.02 s (270.0 files/s, 34420.8 lines/s) Language files blank comment code Python 6 74 4 687 -------- -------- -------- -------- -------- SUM: 6 74 4 687","title":"Stats (TF-toplevel)"},{"location":"Code/StatsWriting/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.02 s (107.0 files/s, 29476.3 lines/s) Language files blank comment code Python 2 55 27 469 -------- -------- -------- -------- -------- SUM: 2 55 27 469","title":"Stats (TF-writing)"},{"location":"Code/Tests/","text":"Unit tests \u00b6 The Text-Fabric program code implements a combinatorical explosion. There are many environments in which the core engine operates, there are several apps that make use of it, and there is a built in query language. One way of mastering the complexity and safeguarding the correctness of it all, is by employing unit tests . To my embarrassment, most parts of Text-Fabric are not covered by unit tests. Here we describe the parts that are covered. But first a few words about the machinery of unit testing. Corpus \u00b6 We build a test corpus , wich contains only 10 slots, with node type sign and one other node type part . The code to build it is in makeTestTf.py . Probably the corpus will be enlarged when new tests are being implemented. Framework \u00b6 We use the unittest framework that comes with Python. It is just a few lines to setup a test class, hence we have not yet separated the code for setting up the tests and the data for the specific test suites. The fact that we currently have just one test suite does not give much incentive to factor the framework out right now. Relations \u00b6 The basic spatial relations that are being used in search deserve thorough unit testing. There are quite a few of them, they describe refined but abstract relations between nodes and the slots they are linked to, and they are optimized for performance, which make their implementation error-prone. For every relationship we test whether it holds or holds not between a bunch of particular nodes. The implementation of the relationships tries to determine on before hand whether its operands are guaranteed to be slots or guaranteed to be non-slots, or whether no guarantee can be given. For each particular test, all these cases will be triggered. All in all we defined 1000 pairs of nodes leading to 2500 queries, which will all be tested against expected answers.","title":"Tests"},{"location":"Code/Tests/#unit-tests","text":"The Text-Fabric program code implements a combinatorical explosion. There are many environments in which the core engine operates, there are several apps that make use of it, and there is a built in query language. One way of mastering the complexity and safeguarding the correctness of it all, is by employing unit tests . To my embarrassment, most parts of Text-Fabric are not covered by unit tests. Here we describe the parts that are covered. But first a few words about the machinery of unit testing.","title":"Unit tests"},{"location":"Code/Tests/#corpus","text":"We build a test corpus , wich contains only 10 slots, with node type sign and one other node type part . The code to build it is in makeTestTf.py . Probably the corpus will be enlarged when new tests are being implemented.","title":"Corpus"},{"location":"Code/Tests/#framework","text":"We use the unittest framework that comes with Python. It is just a few lines to setup a test class, hence we have not yet separated the code for setting up the tests and the data for the specific test suites. The fact that we currently have just one test suite does not give much incentive to factor the framework out right now.","title":"Framework"},{"location":"Code/Tests/#relations","text":"The basic spatial relations that are being used in search deserve thorough unit testing. There are quite a few of them, they describe refined but abstract relations between nodes and the slots they are linked to, and they are optimized for performance, which make their implementation error-prone. For every relationship we test whether it holds or holds not between a bunch of particular nodes. The implementation of the relationships tries to determine on before hand whether its operands are guaranteed to be slots or guaranteed to be non-slots, or whether no guarantee can be given. For each particular test, all these cases will be triggered. All in all we defined 1000 pairs of nodes leading to 2500 queries, which will all be tested against expected answers.","title":"Relations"},{"location":"Create/Convert/","text":"Convert \u00b6 Data conversion to TF by walking through the source You can convert a dataset to TF by writing a function that walks through it. That is a function that triggers a sequence of actions when reading the data. These actions drive Text-Fabric to build a valid Text-Fabric graph. Many checks will be performed. to and from MQL If your source is MQL, you are even better off: Text-Fabric has a module to import from MQL and to export to MQL. cv.walk() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from tf.fabric import Fabric from tf.convert.walker import CV TF = Fabric ( locations = OUT_DIR ) cv = CV ( TF ) def director ( cv ): # your code to unwrap your source data and trigger # the generation of TF nodes, edges and features slotType = 'word' # or whatever you choose otext = { # dictionary of config data for sections and text formats ... } generic = { # dictionary of metadata meant for all features ... } intFeatures = { # set of integer valued feature names ... } featureMeta = { # per feature dicts with metadata ... } good = cv . walk ( director , slotType , otext = otext , generic = generic , intFeatures = intFeatures , featureMeta = featureMeta , warn = True , force = False , silent = False , ) if good : ... load the new TF data ... Description cv.walk takes your director function which should unravel the source. You have to program the director , which takes one argument: cv . From the cv you can use a few standard actions that instruct Text-Fabric to build a graph. This function will check whether the metadata makes sense and is minimally complete. During node creation the section structure will be watched, and you will be warned if irregularities occur. After the creation of the feature data, some extra checks will be performed to see whether the metadata matches the data and vice versa. Destination directory The new feature data will be written to the output directory of the underlying TF object. In fact, the rules are exactly the same as for TF.save() . slotType The node type that acts as the type of the slots in the data set. oText The configuration information to be stored in the otext feature: * section types * section features * text formats Must be a dict. generic Metadata that will be written into the header of all generated TF features. Must be a dict. hint You can make changes to this later on, dynamically in your director. intFeatures The set of features that have integer values only. Must be an iterable. hint You can make changes to this later on, dynamically in your director. featureMeta For each node or edge feature descriptive metadata can be supplied. Must be a dict of dicts. hint You can make changes to this later on, dynamically in your director. warn=True This regulates the response to warnings: warn=True (default): stop after warnings (as if they are errors); warn=False continue after warnings but do show them; warn=None suppress all warnings. silent=False By this you can suppress informational messages: silent=True . force=False This forces the process to continue after errors. Your TF set might not be valid. Yet this can be useful during testing, when you know that not everything is OK, but you want to check some results. Especially when dealing with large datasets, you might want to test with little pieces. But then you get a kind of non-fatal errors that stand in the way of testing. For those cases: force=True . generateTf=True You can pass False here to suppress the actual writing of TF data. In that way you can dry-run the director to check for errors and warnings director An ordinary function that takes one argument, the cv object, and should not deliver anything. Writing this function is the main job to do when you want to convert a data source to TF. When you walk through the input data source, you'll encounter things that have to become slots, non-slot nodes, edges and features in the new data set. You issue these things by means of an action method from cv , such as cv.slot() or cv.node(nodeType) . When your action creates slots or non slot nodes, Text-Fabric will return you a reference to that node, that you can use later for more actions related to that node. 1 curPara = cv . node ( 'para' ) To add features to nodes, use a cv.feature() action. It will apply to the last node added, or you can pass it a node as argument. To add features to edges, issue a cv.edge() action. It will require two node arguments: the from node and the to node. There is always a set of current embedder nodes . When you create a slot node 1 curWord = cv . slot () then TF will link all current embedder nodes to the resulting slot. There are actions to add nodes to the set of embedder nodes, to remove them from it, and to add them again. ??? \"hint\" Metadata When the director runs, you have already specified all your feature metadata, including the value types. 1 2 3 4 5 6 7 8 But if some of that information is dependent on what you encounter in the data, you can do two things: (A) Run a preliminary pass over the data and gather the required information, before running the director. (B) Update the metadata later on by issuing `cv.meta()` actions from within your director, see below. cv action methods The cv class contains methods that are responsible for particular actions that steer the graph building. cv.slot() 1 n = cv . slot () Make a slot node and return the handle to it in n . No further information is needed. Remember that you can add features to the node by later cv.feature(n, key=value, ...) calls. cv.node() 1 n = cv . node ( nodeType ) Make a non-slot node and return the handle to it in n . You have to pass its node type , i.e. a string. Think of sentence , paragraph , phrase , word , sign , whatever. Non slot nodes will be automatically added to the set of embedders. Remember that you can add features to the node by later cv.feature(n, key=value, ...) calls. cv.terminate() 1 cv . terminate ( n ) terminate a node. The node n will be removed from the set of current embedders. This n must be the result of a previous cv.slot() or cv.node() action. cv.resume() 1 cv . resume ( n ) resume a node. If you resume a non-slot node, you add it again to the set of embedders. No new node will be created. If you resume a slot node, it will be added to the set of current embedders. No new slot will be created. cv.feature() 1 cv . feature ( n , name = value , ... , name = value ) Add node features . The features are passed as a list of keyword arguments. These features are added to node n . This n must be the result of a previous cv.slot() or cv.node() action. If a feature value is None it will not be added! The features are passed as a list of keyword arguments. cv.edge() 1 cv . edge ( nf , nt , name = value , ... , name = value ) Add edge features . You need to pass two nodes, nf ( from ) and nt ( to ). Together these specify an edge, and the features will be applied to this edge. You may pass values that are None , and a corresponding edge will be created. If for all pairs nf , nt the value is None , an edge without values will be created. For every nf , such a feature essentially specifies a set of nodes { nt } . The features are passed as a list of keyword arguments. cv.meta() 1 cv . meta ( feature , name = value , ... , name = value ) Add, modify, delete metadata fields of features. feature is the feature name. If a value is None , that name will be deleted from the metadata fields. A bare cv.meta(feature) will remove the feature from the metadata. If you modify the field valueType of a feature, that feature will be added or removed from the set of intFeatures . It will be checked whether you specify either int or str . cv.occurs() 1 occurs = cv . occurs ( featureName ) Returns True if the feature with name featureName occurs in the resulting data, else False. If you have assigned None values to a feature, that will count, i.e. that feature occurs in the data. If you add feature values conditionally, it might happen that some features will not be used at all. For example, if your conversion produces errors, you might add the error information to the result in the form of error features. Later on, when the errors have been weeded out, these features will not occur any more in the result, but then TF will complain that such is feature is declared but not used. At the end of your director you can remove unused features conditionally, using this function. cv.linked() 1 ss = cv . linked ( n ) Returns the slots ss to which a node is currently linked. If you construct non-slot nodes without linking them to slots, they will be removed when TF validates the collective result of the action methods. If you want to prevent that, you can insert an extra slot, but in order to do so, you have to detect that a node is still unlinked. This action is precisely meant for that. cv.active() 1 isActive = cv . active ( n ) Returns whether the node n is currently active, i.e. in the set of embedders. If you construct your nodes in a very dynamic way, it might be hard to keep track for each node whether it has been created, terminated, or resumed, in other words, whether it is active or not. This action is provides a direct and precise way to know whether a node is active. cv.activeTypes() 1 nTypes = cv . activeTypes () Returns the node types of the currently active nodes, i.e. the embedders. cv.get() 1 2 cv . get ( feature , n ) cv . get ( feature , nf , nt ) Retrieve feature values. feature is the name of the feature. For node features, n is the node which carries the value. For edge features, nf, nt is the pair of from-to nodes which carries the value. Example Follow the conversion tutorial Or study a more involved example for Old Babylonian","title":"Convert"},{"location":"Create/Convert/#convert","text":"Data conversion to TF by walking through the source You can convert a dataset to TF by writing a function that walks through it. That is a function that triggers a sequence of actions when reading the data. These actions drive Text-Fabric to build a valid Text-Fabric graph. Many checks will be performed. to and from MQL If your source is MQL, you are even better off: Text-Fabric has a module to import from MQL and to export to MQL. cv.walk() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from tf.fabric import Fabric from tf.convert.walker import CV TF = Fabric ( locations = OUT_DIR ) cv = CV ( TF ) def director ( cv ): # your code to unwrap your source data and trigger # the generation of TF nodes, edges and features slotType = 'word' # or whatever you choose otext = { # dictionary of config data for sections and text formats ... } generic = { # dictionary of metadata meant for all features ... } intFeatures = { # set of integer valued feature names ... } featureMeta = { # per feature dicts with metadata ... } good = cv . walk ( director , slotType , otext = otext , generic = generic , intFeatures = intFeatures , featureMeta = featureMeta , warn = True , force = False , silent = False , ) if good : ... load the new TF data ... Description cv.walk takes your director function which should unravel the source. You have to program the director , which takes one argument: cv . From the cv you can use a few standard actions that instruct Text-Fabric to build a graph. This function will check whether the metadata makes sense and is minimally complete. During node creation the section structure will be watched, and you will be warned if irregularities occur. After the creation of the feature data, some extra checks will be performed to see whether the metadata matches the data and vice versa. Destination directory The new feature data will be written to the output directory of the underlying TF object. In fact, the rules are exactly the same as for TF.save() . slotType The node type that acts as the type of the slots in the data set. oText The configuration information to be stored in the otext feature: * section types * section features * text formats Must be a dict. generic Metadata that will be written into the header of all generated TF features. Must be a dict. hint You can make changes to this later on, dynamically in your director. intFeatures The set of features that have integer values only. Must be an iterable. hint You can make changes to this later on, dynamically in your director. featureMeta For each node or edge feature descriptive metadata can be supplied. Must be a dict of dicts. hint You can make changes to this later on, dynamically in your director. warn=True This regulates the response to warnings: warn=True (default): stop after warnings (as if they are errors); warn=False continue after warnings but do show them; warn=None suppress all warnings. silent=False By this you can suppress informational messages: silent=True . force=False This forces the process to continue after errors. Your TF set might not be valid. Yet this can be useful during testing, when you know that not everything is OK, but you want to check some results. Especially when dealing with large datasets, you might want to test with little pieces. But then you get a kind of non-fatal errors that stand in the way of testing. For those cases: force=True . generateTf=True You can pass False here to suppress the actual writing of TF data. In that way you can dry-run the director to check for errors and warnings director An ordinary function that takes one argument, the cv object, and should not deliver anything. Writing this function is the main job to do when you want to convert a data source to TF. When you walk through the input data source, you'll encounter things that have to become slots, non-slot nodes, edges and features in the new data set. You issue these things by means of an action method from cv , such as cv.slot() or cv.node(nodeType) . When your action creates slots or non slot nodes, Text-Fabric will return you a reference to that node, that you can use later for more actions related to that node. 1 curPara = cv . node ( 'para' ) To add features to nodes, use a cv.feature() action. It will apply to the last node added, or you can pass it a node as argument. To add features to edges, issue a cv.edge() action. It will require two node arguments: the from node and the to node. There is always a set of current embedder nodes . When you create a slot node 1 curWord = cv . slot () then TF will link all current embedder nodes to the resulting slot. There are actions to add nodes to the set of embedder nodes, to remove them from it, and to add them again. ??? \"hint\" Metadata When the director runs, you have already specified all your feature metadata, including the value types. 1 2 3 4 5 6 7 8 But if some of that information is dependent on what you encounter in the data, you can do two things: (A) Run a preliminary pass over the data and gather the required information, before running the director. (B) Update the metadata later on by issuing `cv.meta()` actions from within your director, see below. cv action methods The cv class contains methods that are responsible for particular actions that steer the graph building. cv.slot() 1 n = cv . slot () Make a slot node and return the handle to it in n . No further information is needed. Remember that you can add features to the node by later cv.feature(n, key=value, ...) calls. cv.node() 1 n = cv . node ( nodeType ) Make a non-slot node and return the handle to it in n . You have to pass its node type , i.e. a string. Think of sentence , paragraph , phrase , word , sign , whatever. Non slot nodes will be automatically added to the set of embedders. Remember that you can add features to the node by later cv.feature(n, key=value, ...) calls. cv.terminate() 1 cv . terminate ( n ) terminate a node. The node n will be removed from the set of current embedders. This n must be the result of a previous cv.slot() or cv.node() action. cv.resume() 1 cv . resume ( n ) resume a node. If you resume a non-slot node, you add it again to the set of embedders. No new node will be created. If you resume a slot node, it will be added to the set of current embedders. No new slot will be created. cv.feature() 1 cv . feature ( n , name = value , ... , name = value ) Add node features . The features are passed as a list of keyword arguments. These features are added to node n . This n must be the result of a previous cv.slot() or cv.node() action. If a feature value is None it will not be added! The features are passed as a list of keyword arguments. cv.edge() 1 cv . edge ( nf , nt , name = value , ... , name = value ) Add edge features . You need to pass two nodes, nf ( from ) and nt ( to ). Together these specify an edge, and the features will be applied to this edge. You may pass values that are None , and a corresponding edge will be created. If for all pairs nf , nt the value is None , an edge without values will be created. For every nf , such a feature essentially specifies a set of nodes { nt } . The features are passed as a list of keyword arguments. cv.meta() 1 cv . meta ( feature , name = value , ... , name = value ) Add, modify, delete metadata fields of features. feature is the feature name. If a value is None , that name will be deleted from the metadata fields. A bare cv.meta(feature) will remove the feature from the metadata. If you modify the field valueType of a feature, that feature will be added or removed from the set of intFeatures . It will be checked whether you specify either int or str . cv.occurs() 1 occurs = cv . occurs ( featureName ) Returns True if the feature with name featureName occurs in the resulting data, else False. If you have assigned None values to a feature, that will count, i.e. that feature occurs in the data. If you add feature values conditionally, it might happen that some features will not be used at all. For example, if your conversion produces errors, you might add the error information to the result in the form of error features. Later on, when the errors have been weeded out, these features will not occur any more in the result, but then TF will complain that such is feature is declared but not used. At the end of your director you can remove unused features conditionally, using this function. cv.linked() 1 ss = cv . linked ( n ) Returns the slots ss to which a node is currently linked. If you construct non-slot nodes without linking them to slots, they will be removed when TF validates the collective result of the action methods. If you want to prevent that, you can insert an extra slot, but in order to do so, you have to detect that a node is still unlinked. This action is precisely meant for that. cv.active() 1 isActive = cv . active ( n ) Returns whether the node n is currently active, i.e. in the set of embedders. If you construct your nodes in a very dynamic way, it might be hard to keep track for each node whether it has been created, terminated, or resumed, in other words, whether it is active or not. This action is provides a direct and precise way to know whether a node is active. cv.activeTypes() 1 nTypes = cv . activeTypes () Returns the node types of the currently active nodes, i.e. the embedders. cv.get() 1 2 cv . get ( feature , n ) cv . get ( feature , nf , nt ) Retrieve feature values. feature is the name of the feature. For node features, n is the node which carries the value. For edge features, nf, nt is the pair of from-to nodes which carries the value. Example Follow the conversion tutorial Or study a more involved example for Old Babylonian","title":"Convert"},{"location":"Create/CreateTF/","text":"Patterns in creating TF datasets \u00b6 A detailed account of a conversion from arbitrary data to TF is given by the example of the 99-word mini-corpus Banks . Here we describe a such a process at a higher level of abstraction. We use a slightly bigger example text . This is not meant as a recipe, but as a description of the pieces of information that have to be assembled from the source text, and how to compose that into a Text-Fabric resource, which is a set of features. How you turn this insight into an executable program is dependent on how the source text is encoded and organized. You do not have to worry about the TF side of things, because TF itself will take care of that, by means of its source walker module. Analysis \u00b6 The example text is a string with a little bit of structure. Some of that structure gives us our node types, and other bits give us the features. Node types \u00b6 The text is divided into main sections, subsections, paragraphs, and sentences. The sentences are divided into words by white-space and/or punctuation. Step 1: define slots \u00b6 Perform the following operation mentally: * strip out all headings; * split the string on white-space; * number the resulting \"words\" from 1 up to as many as there are; * call the resulting numbers, let's say 1 .. S , the slots of the text. These words may contain punctuation or other non-alphabetical signs. We do not care for the moment. We just conceptualized the textual positions. They act as a skeleton without flesh. Everything in the text, the words themselves, but also additional annotations, will be added as features, which map positions to values. We start constructing a mapping otype from numbers to node types. We assign the string word to the numbers 1, ... , S . That means, we have now S nodes, all of type word . Step 2: add higher level nodes \u00b6 Continue the mental operation as follows: for each level of section , subsection and paragraph , make new nodes; nodes are numbers, start making new nodes directly after S . call all numbers so far, let's say 1 .. S .. N , the nodes of the text. We have added nodes to our skeleton, which now consists of N nodes. The first S of them are slots, i.e. textual positions. The rest of the nodes will be linked to slots. We have 4 main sections, so we extend the otype mapping as follows: S+1 ~ section S+2 ~ section S+3 ~ section S+4 ~ section Likewise, we have 11 subsections, so we continue extending: S+5 ~ subsection S+6 ~ subsection ... S+16 ~ subsection We do the same for paragraph . And after that, we break the paragraphs up into sentences (split on . ), and we add so many nodes of type sentence . The mapping otype is called a feature of nodes. Any mapping that assigns values to nodes, is called a (node-)feature. Containment \u00b6 We also have to record which words belong to which nodes. This information takes the shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we have to lists pairs of nodes (n, s) such that the slot s \"belongs\" to the node n . This is in fact a set of edges (pairs of nodes are edges), and a set of edges is an edge feature . In general it is possible to assign values to pairs of nodes, but for our containment information we just assign the empty value to every pair we have in our set. The edge feature that records the containment of words in nodes, is called oslots . Step 3: map nodes to sets of words \u00b6 For each of the higher level nodes n (the ones beyond S ) we have to lookup/remember/compute which slots w belong to it, and put that in the oslots mapping: S+1 ~ { 1, 2, 3, ... x, ..., y } S+2 ~ { y+1, y+2, ... } ... S+5 ~ { 1, 2, 3, ... x } S+6 ~ { x+1, x+2, ...} ... Features \u00b6 Now we have two features, a node feature otype and an edge feature oslots . This is merely the frame of our text, the warp so to speak. It contains the textual positions, and the information what the meaningful chunks are. Now it is time to weave the information in. Step 4: the actual text \u00b6 Remember the words with punctuation attached? We can split every word into three parts: text: the alphabetical characters in between prefix: the non-alphabetical leading characters suffix: the non-alphabetical trailing characters We can make three node features, prefix , text , and suffix . Remember that node features are mappings from numbers to values. Here we go: prefix[1] is the prefix of word 1 suffix[1] is the suffix of word 1 text[1] is the text of word 1 ... And so for all words. Step 5: more features \u00b6 For the sections and subsections we can make a feature heading , in which we store the headings of those sections. heading[S+1] is Introduction heading[S+5] is Basic concepts heading[S+16] is Identity ... For paragraphs we can figure out their sequence number within the subsection, and store that in a feature number : number[p] is 1 if p is the node corresponding to the first paragraph in a subsection. If you want absolute paragraph numbers, you can just add a feature for that: abs_number[p] is 23 if p is the node corresponding to the 23th paragraph in the corpus. Metadata \u00b6 You can supply metadata to all node features and edge features. Metadata must be given as a dictionary, where the keys are the names of the features in your dataset, and the values are themselves key-value pairs, where the values are just strings. You can mention where the source data comes from, who did the conversion, and you can give a description of the intention of this feature and the shape of its values. Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated key-value. You can also supply metadata for '' (the empty key). These key-values will be added to all other features. Here you can put stuff that pertains to the dataset as a whole, such as information about decisions that have been taken. You should also provide some special metadata to the key otext . This feature has no data, only metadata. It is not a node feature, not an edge feature, but a config feature. otext is responsible for sectioning and text representation. If you specify otext well, the T-API can make use of it, so that you have convenient, generic functions to get at your sections and to serialize your text in different formats. Step 6: sectioning metadata \u00b6 sectionTypes: 'section,subsection,paragraph' sectionFeatures: 'title,title,number' This tells Text-Fabric that node type section corresponds to section level 1, subsection to level 2, and paragraph to level 3. Moreover, Text-Fabric knows that the heading of sections at level 1 and 2 are in the feature title , and that the heading at level 3 is in the feature number . Step 7: text formats \u00b6 fmt:text-orig-plain: '{prefix}{text}{suffix}' fmt:text-orig-bare: '{text} ' fmt:text-orig-angle: ' <{text}> ' Here you have provided a bunch of text representation formats to Text-Fabric. The names of those formats are up to you, and the values as well. If you have a list of word nodes, say ws , then a user of your corpus can ask Text-Fabric: 1 T . text ( ws , fmt = 'text-orig-plain' ) This will spit out the full textual representation of those words, including the non-alphabetical stuff in their prefixes and suffixes. The second format, text-orig-bare , will leave prefix and suffix out. And if for whatever reason you need to wrap each word in angle brackets, you can achieve that with text-orig-angle . As an example of how text formats come in handy, have a look at the text formats that have been designed for Hebrew: 1 2 3 4 5 6 7 8 9 10 fmt:lex-orig-full: '{g_lex_utf8} ' fmt:lex-orig-plain: '{lex_utf8} ' fmt:lex-trans-full: '{g_lex} ' fmt:lex-trans-plain: '{lex0} ' fmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}' fmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}' fmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}' fmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}' fmt:text-trans-full-ketiv: '{g_word}{trailer}' fmt:text-trans-plain: '{g_cons}{trailer}' Note that the actual text-formats are not baked in into TF, but are supplied by you, the corpus designer. Writing out TF \u00b6 Once you have assembled your features and metadata as data structures in memory, you can use TF.save() to write out your data as a bunch of Text-Fabric files. Step 8: invoke TF.save() \u00b6 The call to make is 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Here you supply for nodeFeatures a dictionary keyed by your node feature names and valued by the feature data of those features. Likewise for the edge features. And the metadata you have composed goes into the metaData parameter. Finally, the module parameter dictates where on your system the TF-files will be written. If you use the walker module. module, TF will do this step automatically. First time usage \u00b6 When you start using your new dataset in Text-Fabric, you'll notice that there is some upfront computation going on. Text-Fabric computes derived data, especially about the relationships between nodes based on the slots they occupy. All that information comes from oslots . The oslots information is very terse, and using it directly would result in a hefty performance penalty. Likewise, all feature data will be read from the textual .tf files, represented in memory as a dictionary, and then that dictionary will be serialized and gzipped into a .tfx file in a hidden directory .tf . These .tfx files load an order of magnitude faster than the original .tf files. Text-Fabric uses the timestamps of the files to determine whether the .tfx files are outdated and need to be regenerated again. This whole machinery is invisible to you, the user, except for the delay at first time use. Enriching your corpus \u00b6 Maybe a linguistic friend of yours has a tool to determine the part of speech of each word in the text. Using TF itself it is not that hard to create a new feature pos , that maps each word node to the part of speech of that word. See for example how Cody Kingham adds the notion of linguistic head to the BHSA corpus of the Hebrew Bible. Step 9: add the new feature \u00b6 Once you have the feature pos , provide a bit of metadata, and call 1 2 3 4 5 TF . save ( nodeFeatures = { 'pos' : posData }, metaData = { 'pos' : posMetaData }, module = 'linguistics' , ) You get a TF module consisting of one feature pos.tf in the linguistics directory. Maybe you have more linguistic features to add. You do not have to create those features alongside the original corpus. It is perfectly possible to leave the corpus alone in its own GitHub repo, and write your new features in another repo. Users can just obtain the corpus and your linguistic module separately. When they call their Text-Fabric, they can point it to both locations, and Text-Fabric treats it as one dataset. Step 10: use the new feature \u00b6 The call to TF=Fabric() looks like this 1 TF = Fabric ( locations = [ corpusLocation , moduleLocation ]) All feature files found at these locations are loadable in your session.","title":"Pattern"},{"location":"Create/CreateTF/#patterns-in-creating-tf-datasets","text":"A detailed account of a conversion from arbitrary data to TF is given by the example of the 99-word mini-corpus Banks . Here we describe a such a process at a higher level of abstraction. We use a slightly bigger example text . This is not meant as a recipe, but as a description of the pieces of information that have to be assembled from the source text, and how to compose that into a Text-Fabric resource, which is a set of features. How you turn this insight into an executable program is dependent on how the source text is encoded and organized. You do not have to worry about the TF side of things, because TF itself will take care of that, by means of its source walker module.","title":"Patterns in creating TF datasets"},{"location":"Create/CreateTF/#analysis","text":"The example text is a string with a little bit of structure. Some of that structure gives us our node types, and other bits give us the features.","title":"Analysis"},{"location":"Create/CreateTF/#node-types","text":"The text is divided into main sections, subsections, paragraphs, and sentences. The sentences are divided into words by white-space and/or punctuation.","title":"Node types"},{"location":"Create/CreateTF/#step-1-define-slots","text":"Perform the following operation mentally: * strip out all headings; * split the string on white-space; * number the resulting \"words\" from 1 up to as many as there are; * call the resulting numbers, let's say 1 .. S , the slots of the text. These words may contain punctuation or other non-alphabetical signs. We do not care for the moment. We just conceptualized the textual positions. They act as a skeleton without flesh. Everything in the text, the words themselves, but also additional annotations, will be added as features, which map positions to values. We start constructing a mapping otype from numbers to node types. We assign the string word to the numbers 1, ... , S . That means, we have now S nodes, all of type word .","title":"Step 1: define slots"},{"location":"Create/CreateTF/#step-2-add-higher-level-nodes","text":"Continue the mental operation as follows: for each level of section , subsection and paragraph , make new nodes; nodes are numbers, start making new nodes directly after S . call all numbers so far, let's say 1 .. S .. N , the nodes of the text. We have added nodes to our skeleton, which now consists of N nodes. The first S of them are slots, i.e. textual positions. The rest of the nodes will be linked to slots. We have 4 main sections, so we extend the otype mapping as follows: S+1 ~ section S+2 ~ section S+3 ~ section S+4 ~ section Likewise, we have 11 subsections, so we continue extending: S+5 ~ subsection S+6 ~ subsection ... S+16 ~ subsection We do the same for paragraph . And after that, we break the paragraphs up into sentences (split on . ), and we add so many nodes of type sentence . The mapping otype is called a feature of nodes. Any mapping that assigns values to nodes, is called a (node-)feature.","title":"Step 2: add higher level nodes"},{"location":"Create/CreateTF/#containment","text":"We also have to record which words belong to which nodes. This information takes the shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we have to lists pairs of nodes (n, s) such that the slot s \"belongs\" to the node n . This is in fact a set of edges (pairs of nodes are edges), and a set of edges is an edge feature . In general it is possible to assign values to pairs of nodes, but for our containment information we just assign the empty value to every pair we have in our set. The edge feature that records the containment of words in nodes, is called oslots .","title":"Containment"},{"location":"Create/CreateTF/#step-3-map-nodes-to-sets-of-words","text":"For each of the higher level nodes n (the ones beyond S ) we have to lookup/remember/compute which slots w belong to it, and put that in the oslots mapping: S+1 ~ { 1, 2, 3, ... x, ..., y } S+2 ~ { y+1, y+2, ... } ... S+5 ~ { 1, 2, 3, ... x } S+6 ~ { x+1, x+2, ...} ...","title":"Step 3: map nodes to sets of words"},{"location":"Create/CreateTF/#features","text":"Now we have two features, a node feature otype and an edge feature oslots . This is merely the frame of our text, the warp so to speak. It contains the textual positions, and the information what the meaningful chunks are. Now it is time to weave the information in.","title":"Features"},{"location":"Create/CreateTF/#step-4-the-actual-text","text":"Remember the words with punctuation attached? We can split every word into three parts: text: the alphabetical characters in between prefix: the non-alphabetical leading characters suffix: the non-alphabetical trailing characters We can make three node features, prefix , text , and suffix . Remember that node features are mappings from numbers to values. Here we go: prefix[1] is the prefix of word 1 suffix[1] is the suffix of word 1 text[1] is the text of word 1 ... And so for all words.","title":"Step 4: the actual text"},{"location":"Create/CreateTF/#step-5-more-features","text":"For the sections and subsections we can make a feature heading , in which we store the headings of those sections. heading[S+1] is Introduction heading[S+5] is Basic concepts heading[S+16] is Identity ... For paragraphs we can figure out their sequence number within the subsection, and store that in a feature number : number[p] is 1 if p is the node corresponding to the first paragraph in a subsection. If you want absolute paragraph numbers, you can just add a feature for that: abs_number[p] is 23 if p is the node corresponding to the 23th paragraph in the corpus.","title":"Step 5: more features"},{"location":"Create/CreateTF/#metadata","text":"You can supply metadata to all node features and edge features. Metadata must be given as a dictionary, where the keys are the names of the features in your dataset, and the values are themselves key-value pairs, where the values are just strings. You can mention where the source data comes from, who did the conversion, and you can give a description of the intention of this feature and the shape of its values. Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated key-value. You can also supply metadata for '' (the empty key). These key-values will be added to all other features. Here you can put stuff that pertains to the dataset as a whole, such as information about decisions that have been taken. You should also provide some special metadata to the key otext . This feature has no data, only metadata. It is not a node feature, not an edge feature, but a config feature. otext is responsible for sectioning and text representation. If you specify otext well, the T-API can make use of it, so that you have convenient, generic functions to get at your sections and to serialize your text in different formats.","title":"Metadata"},{"location":"Create/CreateTF/#step-6-sectioning-metadata","text":"sectionTypes: 'section,subsection,paragraph' sectionFeatures: 'title,title,number' This tells Text-Fabric that node type section corresponds to section level 1, subsection to level 2, and paragraph to level 3. Moreover, Text-Fabric knows that the heading of sections at level 1 and 2 are in the feature title , and that the heading at level 3 is in the feature number .","title":"Step 6: sectioning metadata"},{"location":"Create/CreateTF/#step-7-text-formats","text":"fmt:text-orig-plain: '{prefix}{text}{suffix}' fmt:text-orig-bare: '{text} ' fmt:text-orig-angle: ' <{text}> ' Here you have provided a bunch of text representation formats to Text-Fabric. The names of those formats are up to you, and the values as well. If you have a list of word nodes, say ws , then a user of your corpus can ask Text-Fabric: 1 T . text ( ws , fmt = 'text-orig-plain' ) This will spit out the full textual representation of those words, including the non-alphabetical stuff in their prefixes and suffixes. The second format, text-orig-bare , will leave prefix and suffix out. And if for whatever reason you need to wrap each word in angle brackets, you can achieve that with text-orig-angle . As an example of how text formats come in handy, have a look at the text formats that have been designed for Hebrew: 1 2 3 4 5 6 7 8 9 10 fmt:lex-orig-full: '{g_lex_utf8} ' fmt:lex-orig-plain: '{lex_utf8} ' fmt:lex-trans-full: '{g_lex} ' fmt:lex-trans-plain: '{lex0} ' fmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}' fmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}' fmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}' fmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}' fmt:text-trans-full-ketiv: '{g_word}{trailer}' fmt:text-trans-plain: '{g_cons}{trailer}' Note that the actual text-formats are not baked in into TF, but are supplied by you, the corpus designer.","title":"Step 7: text formats"},{"location":"Create/CreateTF/#writing-out-tf","text":"Once you have assembled your features and metadata as data structures in memory, you can use TF.save() to write out your data as a bunch of Text-Fabric files.","title":"Writing out TF"},{"location":"Create/CreateTF/#step-8-invoke-tfsave","text":"The call to make is 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Here you supply for nodeFeatures a dictionary keyed by your node feature names and valued by the feature data of those features. Likewise for the edge features. And the metadata you have composed goes into the metaData parameter. Finally, the module parameter dictates where on your system the TF-files will be written. If you use the walker module. module, TF will do this step automatically.","title":"Step 8: invoke TF.save()"},{"location":"Create/CreateTF/#first-time-usage","text":"When you start using your new dataset in Text-Fabric, you'll notice that there is some upfront computation going on. Text-Fabric computes derived data, especially about the relationships between nodes based on the slots they occupy. All that information comes from oslots . The oslots information is very terse, and using it directly would result in a hefty performance penalty. Likewise, all feature data will be read from the textual .tf files, represented in memory as a dictionary, and then that dictionary will be serialized and gzipped into a .tfx file in a hidden directory .tf . These .tfx files load an order of magnitude faster than the original .tf files. Text-Fabric uses the timestamps of the files to determine whether the .tfx files are outdated and need to be regenerated again. This whole machinery is invisible to you, the user, except for the delay at first time use.","title":"First time usage"},{"location":"Create/CreateTF/#enriching-your-corpus","text":"Maybe a linguistic friend of yours has a tool to determine the part of speech of each word in the text. Using TF itself it is not that hard to create a new feature pos , that maps each word node to the part of speech of that word. See for example how Cody Kingham adds the notion of linguistic head to the BHSA corpus of the Hebrew Bible.","title":"Enriching your corpus"},{"location":"Create/CreateTF/#step-9-add-the-new-feature","text":"Once you have the feature pos , provide a bit of metadata, and call 1 2 3 4 5 TF . save ( nodeFeatures = { 'pos' : posData }, metaData = { 'pos' : posMetaData }, module = 'linguistics' , ) You get a TF module consisting of one feature pos.tf in the linguistics directory. Maybe you have more linguistic features to add. You do not have to create those features alongside the original corpus. It is perfectly possible to leave the corpus alone in its own GitHub repo, and write your new features in another repo. Users can just obtain the corpus and your linguistic module separately. When they call their Text-Fabric, they can point it to both locations, and Text-Fabric treats it as one dataset.","title":"Step 9: add the new feature"},{"location":"Create/CreateTF/#step-10-use-the-new-feature","text":"The call to TF=Fabric() looks like this 1 TF = Fabric ( locations = [ corpusLocation , moduleLocation ]) All feature files found at these locations are loadable in your session.","title":"Step 10: use the new feature"},{"location":"Create/ExampleText/","text":"Introduction \u00b6 The Social Construction of Reality is a 1966 book about the sociology of knowledge by the sociologists Peter L. Berger and Thomas Luckmann. Berger and Luckmann introduced the term \"social construction\" into the social sciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their central concept is that people and groups interacting in a social system create, over time, concepts or mental representations of each other's actions, and that these concepts eventually become habituated into reciprocal roles played by the actors in relation to each other. When these roles are made available to other members of society to enter into and play out, the reciprocal interactions are said to be institutionalized. In the process, meaning is embedded in society. Knowledge and people's conceptions (and beliefs) of what reality is become embedded in the institutional fabric of society. Reality is therefore said to be socially constructed. In 1998 the International Sociological Association listed The Social Construction of Reality as the fifth-most important sociological book of the 20 th century Basic concepts \u00b6 Social stock of knowledge \u00b6 Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max Weber, etc.) often focused too much on scientific and theoretical knowledge, but this is only a small part of social knowledge, concerning a very limited group. Customs, common interpretations, institutions, shared routines, habitualizations, the who-is-who and who-does-what in social processes and the division of labor, constitute a much larger part of knowledge in society. \u201c\u2026theoretical knowledge is only a small and by no means the most important part of what passed for knowledge in a society\u2026 the primary knowledge about the institutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019 about a social world, an assemblage of maxims, morals, proverbial nuggets of wisdom, values and beliefs, myths, and so forth\u201d (p.65) Semantic fields \u00b6 The general body of knowledge is socially distributed, and classified in semantic fields. The dynamic distribution and inter dependencies of these knowledge sectors provide structure to the social stock of knowledge: \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026 my knowledge of my own occupation and its world is very rich and specific, while I have only very sketchy knowledge of the occupational worlds of others\u201d (p.43) \u201cThe social distribution of knowledge thus begins with the simple fact that I do not know everything known to my fellowmen, and vice versa, and culminates in exceedingly complex and esoteric systems of expertise. Knowledge of how the socially available stock of knowledge is distributed, at least in outline, is an important element of that same stock of knowledge.\u201d (p.46) Language and signs \u00b6 Language also plays an important role in the analysis of integration of everyday reality. Language links up commonsense knowledge with finite provinces of meaning, thus enabling people, for example, to interpret dreams through understandings relevant in the daytime. \"Language is capable of transcending the reality of everyday life altogether. It can refer to experiences pertaining to finite provinces of meaning, it can span discrete spheres of reality...Language soars into regions that are not only de facto but also a priori unavailable to everyday experience.\"p. 40. Regarding the function of language and signs, Berger and Luckmann are indebted to George Herbert Mead and other figures in the field known as symbolic interactionism, as acknowledged in their Introduction, especially regarding the possibility of constructing objectivity. Signs and language provide interoperability for the construction of everyday reality: \u201cA sign [has the] explicit intention to serve as an index of subjective meanings \u2026 Language is capable of becoming the objective repository of vast accumulations of meaning and experience, which it can then preserve in time and transmit to following generations\u2026 Language also typifies experiences, allowing me to subsume them under broad categories in terms of which they have meaning not only to myself but also to my fellowmen\u201d (p.35-39) Social everyday reality \u00b6 Social everyday reality is characterized by Intersubjectivity (which refers to the coexistence of multiple realities in this context)(p. 23-25): \u201cCompared to the reality of everyday life, other realities appear as finite provinces of meaning, enclaves within the paramount reality marked by circumscribed meanings and modes of experience\u201d (p.25) This is in contrast to other realities, such as dreams, theoretical constructs, religious or mystic beliefs, artistic and imaginary worlds, etc. While individuals may visit other realities (such as watching a film), they are always brought back to everyday reality (once the film ends)(p. 25). Society as objective reality \u00b6 \u201c Social order is a human product, or more precisely, an ongoing human production \u201d Institutionalization[edit] Institutionalization of social processes grows out of the habitualization and customs, gained through mutual observation with subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces uncertainty and danger and allows our limited attention span to focus on more things at the same time, while institutionalized routines can be expected to continue \u201cas previously agreed\u201d: \u201cHabitualization carries with it the important psychological gain that choices are narrowed\u2026 the background of habitualized activity opens up a foreground for deliberation and innovation [which demand a higher level of attention]\u2026 The most important gain is that each [member of society] will be able to predict the other\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026 Many actions are possible on a low level of attention. Each action of one is no longer a source of astonishment and potential danger to the other\u201c (p.53-57). Social objective worlds \u00b6 Social (or institutional) objective worlds are one consequence of institutionalization, and are created when institutions are passed on to a new generation. This creates a reality that is vulnerable to the ideas of a minority which will then form the basis of social expectations in the future. The underlying reasoning is fully transparent to the creators of an institution, as they can reconstruct the circumstances under which they made agreements; while the second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and \u201cself-evident\u201d and they might not understand the underlying logic. \u201c\u2026a social world [is] a comprehensive and given reality confronting the individual in a manner analogous to the reality of the natural world\u2026 In early phases of socialization the child is quite incapable of distinguishing between the objectivity of natural phenomena and the objectivity of the social formations\u2026 The objective reality of institutions is not diminished if the individual does not understand their purpose or their mode of operation\u2026 He must \u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61) Division of labor \u00b6 Division of labor is another consequence of institutionalization. Institutions assign \u201croles\u201d to be performed by various actors, through typification of performances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As specialization increases in number as well as in size and sophistication, a civilization's culture contains more and more sections of knowledge specific to given roles or tasks, sections which become more and more esoteric to non-specialists. These areas of knowledge do not belong anymore to the common social world and culture. \u201cA society\u2019s stock of knowledge is structured in terms of what is generally relevant and what is relevant only to specific roles\u2026 the social distribution of knowledge entails a dichotomization in terms of general and role-specific relevance\u2026 because of the division of labor, role-specific knowledge will grow at a faster rate than generally relevant and accessible knowledge\u2026 The increasing number and complexity of [the resulting] sub universes [of specialized knowledge] make them increasingly inaccessible to outsiders (p.77-87) Symbolic universes \u00b6 Symbolic universes are created to provide legitimation to the created institutional structure. Symbolic universes are a set of beliefs \u201ceverybody knows\u201d that aim at making the institutionalized structure plausible and acceptable for the individual\u2014who might otherwise not understand or agree with the underlying logic of the institution. As an ideological system, the symbolic universe \u201cputs everything in its right place\u201d. It provides explanations for why we do things the way we do. Proverbs, moral maxims, wise sayings, mythology, religions and other theological thought, metaphysical traditions and other value systems are part of the symbolic universe. They are all (more or less sophisticated) ways to legitimize established institutions. \u201cThe function of legitimation is to make objectively available and subjectively plausible the \u2018first-order\u2019 objections that have been institutionalized\u2026 Proverbs, moral maxims and wise sayings are common on this level\u2026 [as well as] explicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a general theory of man\u2026 The symbolic universe also orders history. It locates all collective events in a cohesive unity that includes past, present and future.\u201d (p. 92-104) Universe-maintenance \u00b6 Universe-maintenance refers to specific procedures undertaken, often by an elite group, when the symbolic universe does not fulfill its purpose anymore, which is to legitimize the institutional structure in place. This happens, for example, in generational shifts, or when deviants create an internal movement against established institutions (e.g. against revolutions), or when a society is confronted with another society with a greatly different history and institutional structures. In primitive societies this happened through mythological systems, later on through theological thought. Today, an extremely complex set of science has secularized universe-maintenance. \u201cSpecific procedures of universe-maintenance become necessary when the symbolic universe has become a problem. As long as this is not the case, the symbolic universe is self-maintaining, that is self-legitimating. An intrinsic problem presents itself with the process of transmission of the symbolic universe from one generation to another\u2026 [additionally] two societies confronting each other with conflicting universes will both develop conceptual machinery designed to maintain their respective universes\u2026 mythology represents the most archaic form of universe-maintenance\u2026 theological thought may be distinguished from its mythological predecessor simply in terms of its greater degree of theoretical systematization\u2026 Modern science is an extreme step in this development. (p.104-116) Society as subjective reality \u00b6 Socialization \u00b6 Socialization is a two-step induction of the individual to participate in the social institutional structure, meaning in its objective reality. \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of society. In the life of every individual\u2026 there is a temporal sequence, in the course of which he is inducted into participation in the social dialectic\" (p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high degree of symmetry between objective and subjective reality\u201d (p. 163) Primary Socialization takes place as a child. It is highly charged emotionally and is not questioned. Secondary Socialization includes the acquisition of role-specific knowledge, thus taking one\u2019s place in the social division of labor. It is learned through training and specific rituals, and is not emotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s teacher\u201d. Training for secondary socialization can be very complex and depends on the complexity of division of labor in a society. Primary socialization is much less flexible than secondary socialization. E.g. shame for nudity comes from primary socialization, adequate dress code depends on secondary: A relatively minor shift in the subjective definition of reality would suffice for an individual to take for granted that one may go to the office without a tie. A much more drastic shift would be necessary to have him go, as a matter of course, without any clothes at all. \u201cThe child does not internalize the world of his significant others as one of many possible worlds\u2026 It is for this reason that the world internalized in primary socialization is so much more firmly entrenched in consciousness than worlds internalized in secondary socialization\u2026. Secondary socialization is the internalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of secondary socialization carry a high degree of anonymity\u2026 The same knowledge taught by one teacher could also be taught by another\u2026 The institutional distribution of tasks between primary and secondary socialization varies with the complexity of the social distribution of knowledge\u201d (p. 129-147) Conversation \u00b6 Conversation or verbal communication aims at reality-maintenance of the subjective reality. What seems to be a useless and unnecessary communication of redundant banalities is actually a constant mutual reconfirmation of each other's internal thoughts, in that it maintains subjective reality. \u201cOne may view the individual\u2019s everyday life in terms of the working away of a conversational apparatus that ongoingly maintains, modifies and reconstructs his subjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the station,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire world within which these apparently simple propositions make sense\u2026 the exchange confirms the subjective reality of this world\u2026 the great part, if not all, of everyday conversation maintains subjective reality\u2026 imagine the effect\u2026of an exchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine, darling, don\u2019t forget to take along your gun.\u2019 (p. 147-163) Identity \u00b6 Identity of an individual is subject to a struggle of affiliation to sometimes conflicting realities. For example, the reality from primary socialization (mother tells child not to steal) can be in contrast with second socialization (gang members teach teenager that stealing is cool). Our final social location in the institutional structure of society will ultimately also influence our body and organism. \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines how long and in what manner the individual organism shall live\u2026 Society also directly penetrates the organism in its functioning, most importantly in respect to sexuality and nutrition. While both sexuality and nutrition are grounded in biological drives\u2026 biological constitution does not tell him where he should seek sexual release and what he should eat.\u201d (p. 163-183)","title":"Example data"},{"location":"Create/ExampleText/#introduction","text":"The Social Construction of Reality is a 1966 book about the sociology of knowledge by the sociologists Peter L. Berger and Thomas Luckmann. Berger and Luckmann introduced the term \"social construction\" into the social sciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their central concept is that people and groups interacting in a social system create, over time, concepts or mental representations of each other's actions, and that these concepts eventually become habituated into reciprocal roles played by the actors in relation to each other. When these roles are made available to other members of society to enter into and play out, the reciprocal interactions are said to be institutionalized. In the process, meaning is embedded in society. Knowledge and people's conceptions (and beliefs) of what reality is become embedded in the institutional fabric of society. Reality is therefore said to be socially constructed. In 1998 the International Sociological Association listed The Social Construction of Reality as the fifth-most important sociological book of the 20 th century","title":"Introduction"},{"location":"Create/ExampleText/#basic-concepts","text":"","title":"Basic concepts"},{"location":"Create/ExampleText/#social-stock-of-knowledge","text":"Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max Weber, etc.) often focused too much on scientific and theoretical knowledge, but this is only a small part of social knowledge, concerning a very limited group. Customs, common interpretations, institutions, shared routines, habitualizations, the who-is-who and who-does-what in social processes and the division of labor, constitute a much larger part of knowledge in society. \u201c\u2026theoretical knowledge is only a small and by no means the most important part of what passed for knowledge in a society\u2026 the primary knowledge about the institutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019 about a social world, an assemblage of maxims, morals, proverbial nuggets of wisdom, values and beliefs, myths, and so forth\u201d (p.65)","title":"Social stock of knowledge"},{"location":"Create/ExampleText/#semantic-fields","text":"The general body of knowledge is socially distributed, and classified in semantic fields. The dynamic distribution and inter dependencies of these knowledge sectors provide structure to the social stock of knowledge: \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026 my knowledge of my own occupation and its world is very rich and specific, while I have only very sketchy knowledge of the occupational worlds of others\u201d (p.43) \u201cThe social distribution of knowledge thus begins with the simple fact that I do not know everything known to my fellowmen, and vice versa, and culminates in exceedingly complex and esoteric systems of expertise. Knowledge of how the socially available stock of knowledge is distributed, at least in outline, is an important element of that same stock of knowledge.\u201d (p.46)","title":"Semantic fields"},{"location":"Create/ExampleText/#language-and-signs","text":"Language also plays an important role in the analysis of integration of everyday reality. Language links up commonsense knowledge with finite provinces of meaning, thus enabling people, for example, to interpret dreams through understandings relevant in the daytime. \"Language is capable of transcending the reality of everyday life altogether. It can refer to experiences pertaining to finite provinces of meaning, it can span discrete spheres of reality...Language soars into regions that are not only de facto but also a priori unavailable to everyday experience.\"p. 40. Regarding the function of language and signs, Berger and Luckmann are indebted to George Herbert Mead and other figures in the field known as symbolic interactionism, as acknowledged in their Introduction, especially regarding the possibility of constructing objectivity. Signs and language provide interoperability for the construction of everyday reality: \u201cA sign [has the] explicit intention to serve as an index of subjective meanings \u2026 Language is capable of becoming the objective repository of vast accumulations of meaning and experience, which it can then preserve in time and transmit to following generations\u2026 Language also typifies experiences, allowing me to subsume them under broad categories in terms of which they have meaning not only to myself but also to my fellowmen\u201d (p.35-39)","title":"Language and signs"},{"location":"Create/ExampleText/#social-everyday-reality","text":"Social everyday reality is characterized by Intersubjectivity (which refers to the coexistence of multiple realities in this context)(p. 23-25): \u201cCompared to the reality of everyday life, other realities appear as finite provinces of meaning, enclaves within the paramount reality marked by circumscribed meanings and modes of experience\u201d (p.25) This is in contrast to other realities, such as dreams, theoretical constructs, religious or mystic beliefs, artistic and imaginary worlds, etc. While individuals may visit other realities (such as watching a film), they are always brought back to everyday reality (once the film ends)(p. 25).","title":"Social everyday reality"},{"location":"Create/ExampleText/#society-as-objective-reality","text":"\u201c Social order is a human product, or more precisely, an ongoing human production \u201d Institutionalization[edit] Institutionalization of social processes grows out of the habitualization and customs, gained through mutual observation with subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces uncertainty and danger and allows our limited attention span to focus on more things at the same time, while institutionalized routines can be expected to continue \u201cas previously agreed\u201d: \u201cHabitualization carries with it the important psychological gain that choices are narrowed\u2026 the background of habitualized activity opens up a foreground for deliberation and innovation [which demand a higher level of attention]\u2026 The most important gain is that each [member of society] will be able to predict the other\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026 Many actions are possible on a low level of attention. Each action of one is no longer a source of astonishment and potential danger to the other\u201c (p.53-57).","title":"Society as objective reality"},{"location":"Create/ExampleText/#social-objective-worlds","text":"Social (or institutional) objective worlds are one consequence of institutionalization, and are created when institutions are passed on to a new generation. This creates a reality that is vulnerable to the ideas of a minority which will then form the basis of social expectations in the future. The underlying reasoning is fully transparent to the creators of an institution, as they can reconstruct the circumstances under which they made agreements; while the second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and \u201cself-evident\u201d and they might not understand the underlying logic. \u201c\u2026a social world [is] a comprehensive and given reality confronting the individual in a manner analogous to the reality of the natural world\u2026 In early phases of socialization the child is quite incapable of distinguishing between the objectivity of natural phenomena and the objectivity of the social formations\u2026 The objective reality of institutions is not diminished if the individual does not understand their purpose or their mode of operation\u2026 He must \u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)","title":"Social objective worlds"},{"location":"Create/ExampleText/#division-of-labor","text":"Division of labor is another consequence of institutionalization. Institutions assign \u201croles\u201d to be performed by various actors, through typification of performances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As specialization increases in number as well as in size and sophistication, a civilization's culture contains more and more sections of knowledge specific to given roles or tasks, sections which become more and more esoteric to non-specialists. These areas of knowledge do not belong anymore to the common social world and culture. \u201cA society\u2019s stock of knowledge is structured in terms of what is generally relevant and what is relevant only to specific roles\u2026 the social distribution of knowledge entails a dichotomization in terms of general and role-specific relevance\u2026 because of the division of labor, role-specific knowledge will grow at a faster rate than generally relevant and accessible knowledge\u2026 The increasing number and complexity of [the resulting] sub universes [of specialized knowledge] make them increasingly inaccessible to outsiders (p.77-87)","title":"Division of labor"},{"location":"Create/ExampleText/#symbolic-universes","text":"Symbolic universes are created to provide legitimation to the created institutional structure. Symbolic universes are a set of beliefs \u201ceverybody knows\u201d that aim at making the institutionalized structure plausible and acceptable for the individual\u2014who might otherwise not understand or agree with the underlying logic of the institution. As an ideological system, the symbolic universe \u201cputs everything in its right place\u201d. It provides explanations for why we do things the way we do. Proverbs, moral maxims, wise sayings, mythology, religions and other theological thought, metaphysical traditions and other value systems are part of the symbolic universe. They are all (more or less sophisticated) ways to legitimize established institutions. \u201cThe function of legitimation is to make objectively available and subjectively plausible the \u2018first-order\u2019 objections that have been institutionalized\u2026 Proverbs, moral maxims and wise sayings are common on this level\u2026 [as well as] explicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a general theory of man\u2026 The symbolic universe also orders history. It locates all collective events in a cohesive unity that includes past, present and future.\u201d (p. 92-104)","title":"Symbolic universes"},{"location":"Create/ExampleText/#universe-maintenance","text":"Universe-maintenance refers to specific procedures undertaken, often by an elite group, when the symbolic universe does not fulfill its purpose anymore, which is to legitimize the institutional structure in place. This happens, for example, in generational shifts, or when deviants create an internal movement against established institutions (e.g. against revolutions), or when a society is confronted with another society with a greatly different history and institutional structures. In primitive societies this happened through mythological systems, later on through theological thought. Today, an extremely complex set of science has secularized universe-maintenance. \u201cSpecific procedures of universe-maintenance become necessary when the symbolic universe has become a problem. As long as this is not the case, the symbolic universe is self-maintaining, that is self-legitimating. An intrinsic problem presents itself with the process of transmission of the symbolic universe from one generation to another\u2026 [additionally] two societies confronting each other with conflicting universes will both develop conceptual machinery designed to maintain their respective universes\u2026 mythology represents the most archaic form of universe-maintenance\u2026 theological thought may be distinguished from its mythological predecessor simply in terms of its greater degree of theoretical systematization\u2026 Modern science is an extreme step in this development. (p.104-116)","title":"Universe-maintenance"},{"location":"Create/ExampleText/#society-as-subjective-reality","text":"","title":"Society as subjective reality"},{"location":"Create/ExampleText/#socialization","text":"Socialization is a two-step induction of the individual to participate in the social institutional structure, meaning in its objective reality. \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of society. In the life of every individual\u2026 there is a temporal sequence, in the course of which he is inducted into participation in the social dialectic\" (p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high degree of symmetry between objective and subjective reality\u201d (p. 163) Primary Socialization takes place as a child. It is highly charged emotionally and is not questioned. Secondary Socialization includes the acquisition of role-specific knowledge, thus taking one\u2019s place in the social division of labor. It is learned through training and specific rituals, and is not emotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s teacher\u201d. Training for secondary socialization can be very complex and depends on the complexity of division of labor in a society. Primary socialization is much less flexible than secondary socialization. E.g. shame for nudity comes from primary socialization, adequate dress code depends on secondary: A relatively minor shift in the subjective definition of reality would suffice for an individual to take for granted that one may go to the office without a tie. A much more drastic shift would be necessary to have him go, as a matter of course, without any clothes at all. \u201cThe child does not internalize the world of his significant others as one of many possible worlds\u2026 It is for this reason that the world internalized in primary socialization is so much more firmly entrenched in consciousness than worlds internalized in secondary socialization\u2026. Secondary socialization is the internalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of secondary socialization carry a high degree of anonymity\u2026 The same knowledge taught by one teacher could also be taught by another\u2026 The institutional distribution of tasks between primary and secondary socialization varies with the complexity of the social distribution of knowledge\u201d (p. 129-147)","title":"Socialization"},{"location":"Create/ExampleText/#conversation","text":"Conversation or verbal communication aims at reality-maintenance of the subjective reality. What seems to be a useless and unnecessary communication of redundant banalities is actually a constant mutual reconfirmation of each other's internal thoughts, in that it maintains subjective reality. \u201cOne may view the individual\u2019s everyday life in terms of the working away of a conversational apparatus that ongoingly maintains, modifies and reconstructs his subjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the station,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire world within which these apparently simple propositions make sense\u2026 the exchange confirms the subjective reality of this world\u2026 the great part, if not all, of everyday conversation maintains subjective reality\u2026 imagine the effect\u2026of an exchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine, darling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)","title":"Conversation"},{"location":"Create/ExampleText/#identity","text":"Identity of an individual is subject to a struggle of affiliation to sometimes conflicting realities. For example, the reality from primary socialization (mother tells child not to steal) can be in contrast with second socialization (gang members teach teenager that stealing is cool). Our final social location in the institutional structure of society will ultimately also influence our body and organism. \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines how long and in what manner the individual organism shall live\u2026 Society also directly penetrates the organism in its functioning, most importantly in respect to sexuality and nutrition. While both sexuality and nutrition are grounded in biological drives\u2026 biological constitution does not tell him where he should seek sexual release and what he should eat.\u201d (p. 163-183)","title":"Identity"},{"location":"Create/MQL/","text":"MQL \u00b6 Data interchange with MQL You can interchange with MQL data . Text-Fabric can read and write MQL dumps. An MQL dump is a text file, like an SQL dump. It contains the instructions to create and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName , dirName ) Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to dirName/dbName.mql . If dirName starts with ~ , the ~ will be expanded to your home directory. Likewise, .. will be expanded to the parent of the current directory, and . to the current directory, both only at the start of dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the Text-Fabric dataset it comes from: the TF slots correspond exactly with the MQL monads and have the same numbers; provided the monad numbers in the MQL dump are consecutive. In MQL this is not obligatory. Even if there gaps in the monads sequence, we will fill the holes during conversion, so the slots are tightly consecutive; the TF nodes correspond exactly with the MQL objects and have the same numbers Node features in MQL The values of TF features are of two types, int and str , and they translate to corresponding MQL types integer and string . The actual values do not undergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature. Only if the feature is a number feature, you may omit the quotes: 1 2 [word sp='verb'] [verse chapter=1 and verse=1] Enumeration types It is attractive to use eumeration types for the values of a feature, whereever possible, because then you can query those features in MQL with IN and without quotes: 1 [chapter book IN (Genesis, Exodus)] We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an enumeration. So we restrict ourselves to node features with string values. We put the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter, followed by letters, digits, or _ . The letters can only be plain ASCII letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type. Currently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features, we collect all enumerated values for all those features into one big enumeration type. The reason is that MQL considers equal values in different types as distinct values. If we had separate types, we could never compare values for different features. Values of edge features are ignored There is no place for edge values in MQL. There is only one concept of feature in MQL: object features, which are node features. But TF edges without values can be seen as node features: nodes are mapped onto sets of nodes to which the edges go. And that notion is supported by MQL: edge features are translated into MQL features of type LIST OF id_d , i.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers (yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do that by replacing illegal characters by _ , and, if the result does not start with a letter, we prepend an x . We do not check whether the name is a reserved C word. With these provisos: the given dbName correspond to the MQL database name the TF otypes correspond to the MQL objects the TF features correspond to the MQL features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible). It can be compressed greatly, especially by the program bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or directory with the same name as the MQL database, your import will fail spectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your text-fabric-data directory, e.g. to ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1 2 cd ~/Downloads rm dataset ; mql -b 3 < dataset.mql TF.importMQL() 1 TF . importMQL ( mqlFile , slotType = None , otext = None , meta = None ) Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this importMQL on a TF instance called with 1 TF = Fabric ( locations = targetDir ) Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type, because TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the otext.tf feature. Pass it as a dictionary of keys and values, like so: 1 2 3 4 otext = { 'fmt:text-trans-plain' : '{glyphs}{trailer}' , 'sectionFeatures' : 'book,chapter,verse' , } meta Likewise, you can add a dictionary of keys and values that will added to the metadata of all features. Handy to add provenance data here: 1 2 3 4 5 meta = dict ( dataset = 'DLC' , datasetName = 'Digital Language Corpus' , author = \"That 's me\" , )","title":"MQL"},{"location":"Create/MQL/#mql","text":"Data interchange with MQL You can interchange with MQL data . Text-Fabric can read and write MQL dumps. An MQL dump is a text file, like an SQL dump. It contains the instructions to create and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName , dirName ) Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to dirName/dbName.mql . If dirName starts with ~ , the ~ will be expanded to your home directory. Likewise, .. will be expanded to the parent of the current directory, and . to the current directory, both only at the start of dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the Text-Fabric dataset it comes from: the TF slots correspond exactly with the MQL monads and have the same numbers; provided the monad numbers in the MQL dump are consecutive. In MQL this is not obligatory. Even if there gaps in the monads sequence, we will fill the holes during conversion, so the slots are tightly consecutive; the TF nodes correspond exactly with the MQL objects and have the same numbers Node features in MQL The values of TF features are of two types, int and str , and they translate to corresponding MQL types integer and string . The actual values do not undergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature. Only if the feature is a number feature, you may omit the quotes: 1 2 [word sp='verb'] [verse chapter=1 and verse=1] Enumeration types It is attractive to use eumeration types for the values of a feature, whereever possible, because then you can query those features in MQL with IN and without quotes: 1 [chapter book IN (Genesis, Exodus)] We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an enumeration. So we restrict ourselves to node features with string values. We put the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter, followed by letters, digits, or _ . The letters can only be plain ASCII letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type. Currently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features, we collect all enumerated values for all those features into one big enumeration type. The reason is that MQL considers equal values in different types as distinct values. If we had separate types, we could never compare values for different features. Values of edge features are ignored There is no place for edge values in MQL. There is only one concept of feature in MQL: object features, which are node features. But TF edges without values can be seen as node features: nodes are mapped onto sets of nodes to which the edges go. And that notion is supported by MQL: edge features are translated into MQL features of type LIST OF id_d , i.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers (yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do that by replacing illegal characters by _ , and, if the result does not start with a letter, we prepend an x . We do not check whether the name is a reserved C word. With these provisos: the given dbName correspond to the MQL database name the TF otypes correspond to the MQL objects the TF features correspond to the MQL features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible). It can be compressed greatly, especially by the program bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or directory with the same name as the MQL database, your import will fail spectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your text-fabric-data directory, e.g. to ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1 2 cd ~/Downloads rm dataset ; mql -b 3 < dataset.mql TF.importMQL() 1 TF . importMQL ( mqlFile , slotType = None , otext = None , meta = None ) Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this importMQL on a TF instance called with 1 TF = Fabric ( locations = targetDir ) Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type, because TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the otext.tf feature. Pass it as a dictionary of keys and values, like so: 1 2 3 4 otext = { 'fmt:text-trans-plain' : '{glyphs}{trailer}' , 'sectionFeatures' : 'book,chapter,verse' , } meta Likewise, you can add a dictionary of keys and values that will added to the metadata of all features. Handy to add provenance data here: 1 2 3 4 5 meta = dict ( dataset = 'DLC' , datasetName = 'Digital Language Corpus' , author = \"That 's me\" , )","title":"MQL"},{"location":"Implementation/Apps/","text":"Apps \u00b6 About Text-Fabric is a generic engine to process text and annotations. When working with specific corpora, we want to have more power at our fingertips. We need extra power on top of the core TF engine. The way we have chosen to do it is via apps . An app is a bunch of extra functions that know the structure of a specific corpus. In particular, an app knows how to produce plain representations and pretty displays of nodes of each type in the corpus. For a list of current apps, see Corpora Components \u00b6 App components The apps themselves are those repos inside annotation whose names start with app- . The part after the app- is the name of the app. For each app , you find there a subfolder code with: static A folder with styles, fonts and logos, to be used by web servers such as the the text-fabric browser . In particular, display.css contains the styles used for pretty displays. These styles will be programmatically combined with other styles, to deliver them to the TF browser on the one hand, and to Jupyter notebooks on the other hand. config.py Settings to set up a browsing experience and to feed the specific AP for this app. The TF kernel, web server and browser need settings: setting example description PROTOCOL http:// protocol of website HOST localhost server address of the website PORT['kernel'] 18981 port through wich the TF kernel and the web server communicate PORT['web'] 8101 port at which the TF web server listens for requests OPTIONS tuple names of extra options for searching and displaying query results OPTIONS Each option is itself a tuple of 5 elements, like 1 ( 'showLines' , False , 'checkbox' , 'linen' , 'show line numbers' ) Such an option triggers the display of an HTML element in the TF browser by which the user can specify a value for an option: 1 2 < input type = \"checkbox\" id = \"linen\" name = \"showLines\" > < span > show line numbers </ span > So the members of the tuple are: name of the option, in HTML and in Python default value type of <input> , see input id string for the input element in HTML label by which the option is presented to the user The app itself is driven by the following settings setting type description ORG string GitHub organization name of the main data source of the corpus REPO string GitHub repository name of the main data source of the corpus RELATIVE string Path to the tf directory within the GitHub repository of the main data CORPUS string Descriptive name of the main corpus VERSION string Version of the main corpus that is used in the TF browser DOI_TEXT string Text of the Digital Object Identifier pointing to the main corpus DOI_URL url Digital Object Identifier that points to the main corpus DOC_URL url Base url for the online documentation of the main corpus DOC_INTRO string Relative url to the introduction page of the documentation of the main corpus CHAR_TEXT string Text of the link pointing to the character table of the relevant Unicode blocks CHAR_URL string Link to the character table of the relevant Unicode blocks FEATURE_URL url Url to feature documentation. Contains {feature} and {version} which can be filled in later with the actual feature name and version tag MODULE_SPECS tuple of dicts Provides information for standard data modules that will be loaded together with the main corpus: org , repo , relative , corpus , docUrl , doi (text and link) ZIP list data directories to be zipped for a release, given as either (org, repo, relative) or repo (with org and relative taken from main corpus); only used by text-fabric-zip when collecting data into zip files to be attached to a GitHub release CONDENSE_TYPE string the default node type to condense search results in, e.g. verse or tablet NONE_VALUES set feature values that are deemed uninformative, e.g. None , 'NA' STANDARD_FEATURES set features that are shown by default in all pretty displays EXCLUDED_FEATURES set features that are present in the data source but will not be loaded for the TF browser NO_DESCEND_TYPES set when representing nodes as text in exports from the TF browser, node of type in this set will not be expanded to their slot occurrences; e.g. lex : we do not want represent lexeme nodes by their list of occurrences EXAMPLE_SECTION html what a passage reference looks like in this corpus; may have additional information in the form of a link; used in the Help of the TF browser EXAMPLE_SECTION_TEXT string what a passage reference looks like in this corpus; just the plain text; used in the Help of the TF browser SECTION_SEP1 string separator between main and secondary sections in a passage reference; e.g. the space in Genesis 1:1 SECTION_SEP2 string separator between secondary and tertiary sections in a passage reference; e.g. the : in Genesis 1:1 FORMAT_CSS dict mapping between TF text formats and CSS classes; not all text formats need to be mapped DEFAULT_CLS string default CSS class for text in a specific TF text format, when no format-specific class is given in FORMAT_CSS DEFAULT_CLS_ORIG string default CSS class for text in a specific TF text format, when no format-specific class is given in FORMAT_CSS; and when the TF text format contains the -orig- string; used to specify classes for text in non-latin scripts CLASS_NAMES dict mapping between node types and CSS classes; used in pretty displays to format the representations of nodes of that type FONT_NAME string font family name to be used in CSS for representing text in original script FONT string file name of the offline font specified in FONT_NAME FONTW string file name of the webfont specified in FONT_NAME TEXT_FORMATS dict additional text formats that can use HTML styling. Keys: names of new text formats. Values: name of a method that implements that format. If the name is xxx , then app.py should implement a method fmt_xxx(node) to produce html for node node BROWSE_NAV_LEVEL int the section level up to which the browser shows a hierarchical tree. Either 1 or 2 BROWSE_CONTENT_PRETTY bool whether the content is shown as a list of subsectional items contained in the selected item or as a pretty display of the item itself app.py The functionality specific to the corpus in question, organized as an extended TF api. In the code you see this stored in variables with name app . In order to be an app that TF can use, app should provide the following attributes: attribute kind description webLink method given a node, produces a link to an online description of the corresponding object (to shebanq or cdli ) _plain method given a node, produce a plain representation of the corresponding object: not the full structure, but something that identifies it _pretty method given a node, produce elements of a pretty display of the corresponding object: the full structure other modules If you organize bits of the functionality of the app in modules to be imported by app.py , you can put them in this same directory. Do not import app-dependent modules If you import these other modules by means of the Python import system using import module or from module import name then everything works fine until you load two apps in the same program, that in turn load their other modules. As long as different apps load modules with different names, there is no problem/ But if two apps both have a module with the same name, then the first of them will be loaded, and both apps use the same code. In order to prevent this, you can use the function loadModule() to dynamically load these modules. They will be given an app-dependent internal name, so the Python importer will not conflate them. loadModule() Here is how you load auxiliary modules in your app.py . The example is taken from the uruk app, which loads two modules, atf and image . atf is a bunch of functions that enrich the api of the app. The atf module contains a function that adds all these functions to an object: atfApi 1 2 3 4 5 6 7 8 9 10 11 12 from tf.applib.app import loadModule class TfApp ( object ): def __init__ ( app , * args , _asApp = False , silent = False , ** kwargs ): atf = loadModule ( * args [ 0 : 2 ], 'atf' ) atf . atfApi ( app ) app . image = loadModule ( * args [ 0 : 2 ], 'image' ) setupApi ( app , * args , _asApp = _asApp , silent = silent , ** kwargs ) The place to put the loadModule() calls is in the __init()__ method of the TfApp object, before the call to setupApi() . Here the name of the app and the path to the code directory of the app are known. They are provided by the first two arguments by which the __init__() method is called. loadModule() needs the app name and the path to the code, to we pass it *args[0:2] , the first two arguments received by __init__() . The third argument for loadModule() is the file name of the module, without the .py . The result of loading the module is a code object, from which you can get all the names defined by the module and their semantics. In the atf case, we use the atfApi() function of the module to add a bunch of functions defined in that module as methods to the TfApp object. In the image case, we add the code object as an attribute to the TfApp object, so that all its methods can retrieve all names defined by the image module. Implementation \u00b6 App support Apps turn out to have several things in common that we want to deal with generically. These functions are collected in the api modules of TF. Two contexts Most functions with the app argument are meant to perform their duty in two contexts: when called in a Jupyter notebook they deliver output meant for a notebook output cell, using methods provided by the ipython package. when called by the web app they deliver output meant for the TF browser website, generating raw HTML. The app is the rich app specific API, and when we construct this API, we pass the information whether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app. We pass this information by setting the attribute _asApp on the app . If it is set, we use the app in the web app context. Most of the code in such functions is independent of _asApp . The main difference is how to output the result: by a call to an IPython display method, or by returning raw HTML. _asApp The app contains several display functions. By default they suppose that there is a Jupyter notebook context in which results can be rendered with IPython.display methods. But if we operate in the context of a web-interface, we need to generate straight HTML. We flag the web-interface case as _asApp == True .","title":"Apps"},{"location":"Implementation/Apps/#apps","text":"About Text-Fabric is a generic engine to process text and annotations. When working with specific corpora, we want to have more power at our fingertips. We need extra power on top of the core TF engine. The way we have chosen to do it is via apps . An app is a bunch of extra functions that know the structure of a specific corpus. In particular, an app knows how to produce plain representations and pretty displays of nodes of each type in the corpus. For a list of current apps, see Corpora","title":"Apps"},{"location":"Implementation/Apps/#components","text":"App components The apps themselves are those repos inside annotation whose names start with app- . The part after the app- is the name of the app. For each app , you find there a subfolder code with: static A folder with styles, fonts and logos, to be used by web servers such as the the text-fabric browser . In particular, display.css contains the styles used for pretty displays. These styles will be programmatically combined with other styles, to deliver them to the TF browser on the one hand, and to Jupyter notebooks on the other hand. config.py Settings to set up a browsing experience and to feed the specific AP for this app. The TF kernel, web server and browser need settings: setting example description PROTOCOL http:// protocol of website HOST localhost server address of the website PORT['kernel'] 18981 port through wich the TF kernel and the web server communicate PORT['web'] 8101 port at which the TF web server listens for requests OPTIONS tuple names of extra options for searching and displaying query results OPTIONS Each option is itself a tuple of 5 elements, like 1 ( 'showLines' , False , 'checkbox' , 'linen' , 'show line numbers' ) Such an option triggers the display of an HTML element in the TF browser by which the user can specify a value for an option: 1 2 < input type = \"checkbox\" id = \"linen\" name = \"showLines\" > < span > show line numbers </ span > So the members of the tuple are: name of the option, in HTML and in Python default value type of <input> , see input id string for the input element in HTML label by which the option is presented to the user The app itself is driven by the following settings setting type description ORG string GitHub organization name of the main data source of the corpus REPO string GitHub repository name of the main data source of the corpus RELATIVE string Path to the tf directory within the GitHub repository of the main data CORPUS string Descriptive name of the main corpus VERSION string Version of the main corpus that is used in the TF browser DOI_TEXT string Text of the Digital Object Identifier pointing to the main corpus DOI_URL url Digital Object Identifier that points to the main corpus DOC_URL url Base url for the online documentation of the main corpus DOC_INTRO string Relative url to the introduction page of the documentation of the main corpus CHAR_TEXT string Text of the link pointing to the character table of the relevant Unicode blocks CHAR_URL string Link to the character table of the relevant Unicode blocks FEATURE_URL url Url to feature documentation. Contains {feature} and {version} which can be filled in later with the actual feature name and version tag MODULE_SPECS tuple of dicts Provides information for standard data modules that will be loaded together with the main corpus: org , repo , relative , corpus , docUrl , doi (text and link) ZIP list data directories to be zipped for a release, given as either (org, repo, relative) or repo (with org and relative taken from main corpus); only used by text-fabric-zip when collecting data into zip files to be attached to a GitHub release CONDENSE_TYPE string the default node type to condense search results in, e.g. verse or tablet NONE_VALUES set feature values that are deemed uninformative, e.g. None , 'NA' STANDARD_FEATURES set features that are shown by default in all pretty displays EXCLUDED_FEATURES set features that are present in the data source but will not be loaded for the TF browser NO_DESCEND_TYPES set when representing nodes as text in exports from the TF browser, node of type in this set will not be expanded to their slot occurrences; e.g. lex : we do not want represent lexeme nodes by their list of occurrences EXAMPLE_SECTION html what a passage reference looks like in this corpus; may have additional information in the form of a link; used in the Help of the TF browser EXAMPLE_SECTION_TEXT string what a passage reference looks like in this corpus; just the plain text; used in the Help of the TF browser SECTION_SEP1 string separator between main and secondary sections in a passage reference; e.g. the space in Genesis 1:1 SECTION_SEP2 string separator between secondary and tertiary sections in a passage reference; e.g. the : in Genesis 1:1 FORMAT_CSS dict mapping between TF text formats and CSS classes; not all text formats need to be mapped DEFAULT_CLS string default CSS class for text in a specific TF text format, when no format-specific class is given in FORMAT_CSS DEFAULT_CLS_ORIG string default CSS class for text in a specific TF text format, when no format-specific class is given in FORMAT_CSS; and when the TF text format contains the -orig- string; used to specify classes for text in non-latin scripts CLASS_NAMES dict mapping between node types and CSS classes; used in pretty displays to format the representations of nodes of that type FONT_NAME string font family name to be used in CSS for representing text in original script FONT string file name of the offline font specified in FONT_NAME FONTW string file name of the webfont specified in FONT_NAME TEXT_FORMATS dict additional text formats that can use HTML styling. Keys: names of new text formats. Values: name of a method that implements that format. If the name is xxx , then app.py should implement a method fmt_xxx(node) to produce html for node node BROWSE_NAV_LEVEL int the section level up to which the browser shows a hierarchical tree. Either 1 or 2 BROWSE_CONTENT_PRETTY bool whether the content is shown as a list of subsectional items contained in the selected item or as a pretty display of the item itself app.py The functionality specific to the corpus in question, organized as an extended TF api. In the code you see this stored in variables with name app . In order to be an app that TF can use, app should provide the following attributes: attribute kind description webLink method given a node, produces a link to an online description of the corresponding object (to shebanq or cdli ) _plain method given a node, produce a plain representation of the corresponding object: not the full structure, but something that identifies it _pretty method given a node, produce elements of a pretty display of the corresponding object: the full structure other modules If you organize bits of the functionality of the app in modules to be imported by app.py , you can put them in this same directory. Do not import app-dependent modules If you import these other modules by means of the Python import system using import module or from module import name then everything works fine until you load two apps in the same program, that in turn load their other modules. As long as different apps load modules with different names, there is no problem/ But if two apps both have a module with the same name, then the first of them will be loaded, and both apps use the same code. In order to prevent this, you can use the function loadModule() to dynamically load these modules. They will be given an app-dependent internal name, so the Python importer will not conflate them. loadModule() Here is how you load auxiliary modules in your app.py . The example is taken from the uruk app, which loads two modules, atf and image . atf is a bunch of functions that enrich the api of the app. The atf module contains a function that adds all these functions to an object: atfApi 1 2 3 4 5 6 7 8 9 10 11 12 from tf.applib.app import loadModule class TfApp ( object ): def __init__ ( app , * args , _asApp = False , silent = False , ** kwargs ): atf = loadModule ( * args [ 0 : 2 ], 'atf' ) atf . atfApi ( app ) app . image = loadModule ( * args [ 0 : 2 ], 'image' ) setupApi ( app , * args , _asApp = _asApp , silent = silent , ** kwargs ) The place to put the loadModule() calls is in the __init()__ method of the TfApp object, before the call to setupApi() . Here the name of the app and the path to the code directory of the app are known. They are provided by the first two arguments by which the __init__() method is called. loadModule() needs the app name and the path to the code, to we pass it *args[0:2] , the first two arguments received by __init__() . The third argument for loadModule() is the file name of the module, without the .py . The result of loading the module is a code object, from which you can get all the names defined by the module and their semantics. In the atf case, we use the atfApi() function of the module to add a bunch of functions defined in that module as methods to the TfApp object. In the image case, we add the code object as an attribute to the TfApp object, so that all its methods can retrieve all names defined by the image module.","title":"Components"},{"location":"Implementation/Apps/#implementation","text":"App support Apps turn out to have several things in common that we want to deal with generically. These functions are collected in the api modules of TF. Two contexts Most functions with the app argument are meant to perform their duty in two contexts: when called in a Jupyter notebook they deliver output meant for a notebook output cell, using methods provided by the ipython package. when called by the web app they deliver output meant for the TF browser website, generating raw HTML. The app is the rich app specific API, and when we construct this API, we pass the information whether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app. We pass this information by setting the attribute _asApp on the app . If it is set, we use the app in the web app context. Most of the code in such functions is independent of _asApp . The main difference is how to output the result: by a call to an IPython display method, or by returning raw HTML. _asApp The app contains several display functions. By default they suppose that there is a Jupyter notebook context in which results can be rendered with IPython.display methods. But if we operate in the context of a web-interface, we need to generate straight HTML. We flag the web-interface case as _asApp == True .","title":"Implementation"},{"location":"Implementation/Data/","text":"Getting data \u00b6 Auto loading of TF data The specific apps call the functions getModulesData() and getData() to load and download data from github. When TF stores data in the text-fabric-data directory, it remembers from which release it came (in a file _release.txt ). All the data getters need to know is the organization, the repo, the path within the repo to the data, and the version of the (main) data source. The data should reside in directories that correspond to versions of the main data source. The path should point to the parent of these version directries. TF uses the GitHub API to discover which is the newest release of a repo.","title":"Data"},{"location":"Implementation/Data/#getting-data","text":"Auto loading of TF data The specific apps call the functions getModulesData() and getData() to load and download data from github. When TF stores data in the text-fabric-data directory, it remembers from which release it came (in a file _release.txt ). All the data getters need to know is the organization, the repo, the path within the repo to the data, and the version of the (main) data source. The data should reside in directories that correspond to versions of the main data source. The path should point to the parent of these version directries. TF uses the GitHub API to discover which is the newest release of a repo.","title":"Getting data"},{"location":"Implementation/Display/","text":"Display \u00b6 Generic/specific Displaying nodes is an intricate mix of functionality that is shared by all apps and that is specific to some app. Here is our logistics of pretty displays. Like a number of other methods pretty() is defined as a generic function and added as a method to each app : 1 app . pretty = types . MethodType ( pretty , app ) So although we define pretty(app, ...) as a generic function, through its argument app we can call app specific functionality. We follow this pattern for quite a bit of functions. They all have app as first argument. The case of pretty() is the most intricate one, since there is a lot of generic functionality and a lot of corpus specific functionality, as is evident from examples of the BHSA corpus and one from the Uruk corpus below. Here is the flow of information for pretty() : definition as a generic function pretty() ; this function fetches the relevant display parameters and gathers information about the node to display, e.g. its boundary slots; armed with this information, it calls the app-dependent _pretty() function, e.g. from uruk or bhsa ; _pretty() is a function that calls itself recursively for all other nodes that are involved in the display; for each node that _pretty() is going to display, it first computes a few standard things for that node by means of a generic function prettyPre() ; in particular, it will be computed whether the display of the node in question fits in the display of the node where it all began with, or whether parts of the display should be clipped; also, a header label for the current node will be comnposed, including relevant hyperlinks and optional extra information reuqired by the display options; finally, it is the turn of the app-dependent _pretty() to combine the header label with the displays it gets after recursively calling itself for subordinate nodes. Above: BHSA pretty display Below: Uruk pretty display","title":"Display"},{"location":"Implementation/Display/#display","text":"Generic/specific Displaying nodes is an intricate mix of functionality that is shared by all apps and that is specific to some app. Here is our logistics of pretty displays. Like a number of other methods pretty() is defined as a generic function and added as a method to each app : 1 app . pretty = types . MethodType ( pretty , app ) So although we define pretty(app, ...) as a generic function, through its argument app we can call app specific functionality. We follow this pattern for quite a bit of functions. They all have app as first argument. The case of pretty() is the most intricate one, since there is a lot of generic functionality and a lot of corpus specific functionality, as is evident from examples of the BHSA corpus and one from the Uruk corpus below. Here is the flow of information for pretty() : definition as a generic function pretty() ; this function fetches the relevant display parameters and gathers information about the node to display, e.g. its boundary slots; armed with this information, it calls the app-dependent _pretty() function, e.g. from uruk or bhsa ; _pretty() is a function that calls itself recursively for all other nodes that are involved in the display; for each node that _pretty() is going to display, it first computes a few standard things for that node by means of a generic function prettyPre() ; in particular, it will be computed whether the display of the node in question fits in the display of the node where it all began with, or whether parts of the display should be clipped; also, a header label for the current node will be comnposed, including relevant hyperlinks and optional extra information reuqired by the display options; finally, it is the turn of the app-dependent _pretty() to combine the header label with the displays it gets after recursively calling itself for subordinate nodes. Above: BHSA pretty display Below: Uruk pretty display","title":"Display"},{"location":"Implementation/Helpers/","text":"HTML and Markdown \u00b6 dm(markdown) Display a markdown string in a Jupyter notebook. dh(html) Display an HTML string in a Jupyter notebook.","title":"Helpers"},{"location":"Implementation/Helpers/#html-and-markdown","text":"dm(markdown) Display a markdown string in a Jupyter notebook. dh(html) Display an HTML string in a Jupyter notebook.","title":"HTML and Markdown"},{"location":"Implementation/Highlight/","text":"Highlight \u00b6 hlText() 1 A . hlText ( nodes , highlights , ** options ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ???+ info \"Description\" Outputs highlighted text. The function works much the same as ['T.text()'](Text.md#text-representation) except that you can pass an extra `highlights` parameter to direct the highlighting of portions of the output. ??? info \"nodes\" The sequence of nodes whose text you want to represent. ??? info \"highlights\" If `None`, no highlighting will occur. Otherwise, it should be a dict, mapping nodes to strings or a set of nodes. Such strings should be either the empty string, or a color name. The empty string leads to a default highlight color (yellow), color names lead to highlighting by that color. If the highlights are a set, then the nodes in it will be highlighted with the default colour. If a node maps to a color, then the portion of text that corresponds to that node will be highlighted.","title":"Highlight"},{"location":"Implementation/Highlight/#highlight","text":"hlText() 1 A . hlText ( nodes , highlights , ** options ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ???+ info \"Description\" Outputs highlighted text. The function works much the same as ['T.text()'](Text.md#text-representation) except that you can pass an extra `highlights` parameter to direct the highlighting of portions of the output. ??? info \"nodes\" The sequence of nodes whose text you want to represent. ??? info \"highlights\" If `None`, no highlighting will occur. Otherwise, it should be a dict, mapping nodes to strings or a set of nodes. Such strings should be either the empty string, or a color name. The empty string leads to a default highlight color (yellow), color names lead to highlighting by that color. If the highlights are a set, then the nodes in it will be highlighted with the default colour. If a node maps to a color, then the portion of text that corresponds to that node will be highlighted.","title":"Highlight"},{"location":"Implementation/Links/","text":"Links \u00b6 attributes of the app name type description tfsLink html link points to the documentation of the TF search engine tutLink html link points to the tutorial for TF search _sectionLink(node) Given a section node, produces a link that copies the section to the section pad (only in the TF browser)","title":"Links"},{"location":"Implementation/Links/#links","text":"attributes of the app name type description tfsLink html link points to the documentation of the TF search engine tutLink html link points to the tutorial for TF search _sectionLink(node) Given a section node, produces a link that copies the section to the section pad (only in the TF browser)","title":"Links"},{"location":"Implementation/Search/","text":"TF search performers \u00b6 search() 1 search ( app , query , silent = False , sets = None , shallow = False ) This is a thin wrapper around the generic search interface of TF: S.search The extra thing it does it collecting the results. S.search() may yield a generator, and this search() makes sure to iterate over that generator, collect the results, and return them as a sorted list. Context Jupyter The intended context of this function is: an ordinary Python program (including the Jupyter notebook). Web apps can better use runSearch below. runSearch() 1 runSearch ( app , query , cache ) A wrapper around the generic search interface of TF. Before running the TF search, the query will be looked up in the cache . If present, its cached results/error messages will be returned. If not, the query will be run, results/error messages collected, put in the cache , and returned. Context web app The intended context of this function is: web app. runSearchCondensed() 1 runSearchCondensed ( api , query , cache , condenseType ) When query results need to be condensed into a container, this function takes care of that. It first tries the cache for condensed query results. If that fails, it collects the bare query results from the cache or by running the query. Then it condenses the results, puts them in the cache , and returns them. Context web app The intended context of this function is: web app.","title":"Search"},{"location":"Implementation/Search/#tf-search-performers","text":"search() 1 search ( app , query , silent = False , sets = None , shallow = False ) This is a thin wrapper around the generic search interface of TF: S.search The extra thing it does it collecting the results. S.search() may yield a generator, and this search() makes sure to iterate over that generator, collect the results, and return them as a sorted list. Context Jupyter The intended context of this function is: an ordinary Python program (including the Jupyter notebook). Web apps can better use runSearch below. runSearch() 1 runSearch ( app , query , cache ) A wrapper around the generic search interface of TF. Before running the TF search, the query will be looked up in the cache . If present, its cached results/error messages will be returned. If not, the query will be run, results/error messages collected, put in the cache , and returned. Context web app The intended context of this function is: web app. runSearchCondensed() 1 runSearchCondensed ( api , query , cache , condenseType ) When query results need to be condensed into a container, this function takes care of that. It first tries the cache for condensed query results. If that fails, it collects the bare query results from the cache or by running the query. Then it condenses the results, puts them in the cache , and returns them. Context web app The intended context of this function is: web app.","title":"TF search performers"},{"location":"Implementation/Setup/","text":"Set up \u00b6 setupApi() This method is called by each specific app when it instantiates its associated class with a single object. A lot of things happen here: all data is looked up, downloaded, prepared, loaded, etc the underlying TF Fabric API is called custom links to documentation etc are set styling is set up several methods that are generically defined are added as instance methods Parameters This method is called with the parameters that the user passes to the incantation method use() Specific functions \u00b6 During setup several modules contribute functions that are included in the API. For example, the module display.py contains a function displayApi() that adds the methods plain() and pretty() and others to the API. The display module defines more functions, to be used by other parts of TF Fabric. These will not be advertised to the end user, although it is not impossible for end users access these methods. The same structure is used for the other modules that are part of the generic app support.","title":"Setup"},{"location":"Implementation/Setup/#set-up","text":"setupApi() This method is called by each specific app when it instantiates its associated class with a single object. A lot of things happen here: all data is looked up, downloaded, prepared, loaded, etc the underlying TF Fabric API is called custom links to documentation etc are set styling is set up several methods that are generically defined are added as instance methods Parameters This method is called with the parameters that the user passes to the incantation method use()","title":"Set up"},{"location":"Implementation/Setup/#specific-functions","text":"During setup several modules contribute functions that are included in the API. For example, the module display.py contains a function displayApi() that adds the methods plain() and pretty() and others to the API. The display module defines more functions, to be used by other parts of TF Fabric. These will not be advertised to the end user, although it is not impossible for end users access these methods. The same structure is used for the other modules that are part of the generic app support.","title":"Specific functions"},{"location":"Implementation/Tables/","text":"Tabular display for the TF-browser \u00b6 Constants \u00b6 Fixed values The following values are used by other parts of the program: name description RESULT string Composition \u00b6 compose() 1 compose ( app , tuples , features , position , opened , getx = None , ** displayParameters ) Takes a list of tuples and composes it into an HTML table. Some of the rows will be expandable, namely the rows specified by opened , for which extra data has been fetched. features is a list of names of features that will be shown in expanded pretty displays. Typically, it is the list of features used in the query that delivered the tuples. position The current position in the list. Will be highlighted in the display. getx=None If None , a portion of the tuples will be put in a table. otherwise, it is an index in the list for which a pretty display will be retrieved. Typically, this happens when a TF-browser user clicks on a table row in order to expand it. composeT() 1 composeT ( app , features , tuples , features , opened , getx = None , ** displayParameters ) \" Very much like compose() , but here the tuples come from a sections and/or tuples specification in the TF-browser. composeP(), getx=None, **displayParameters) 1 2 3 4 5 6 7 8 9 composeP ( app , sec0 , sec1 , features , query , sec2 = None , opened = set (), getx = None , ** displayParameters ) Like composeT() , but this is meant to compose the items at section level 2 (verses) within an item of section level 1 (chapter) within an item of section level 0 (a book). Typically invoked when a user of the TF-browser is browsing passages. The query is used to highlight its results in the passages that the user is browsing. plainTextS2() 1 plainTextS2 ( sNode , opened , sec2 , highlights , \\ * \\ * displayParameters ) Produces a single item corresponding to a section 2 level (verse) for display in the browser. It will rendered as plain text, but expandable to a pretty display. Highlighting The functions getPassageHighlights() , getHlNodes() , nodesFromTuples() are helpers to apply highlighting to query results in a passage. getBoundary(api, node) Utility function to ask from the TF API the first slot and the last slot contained in a node. getFeatures(app, node, ...) Helper for pretty() : wrap the requested features and their values for node in HTML for pretty display. header(app) Get the app-specific links to data and documentation and wrap it into HTML for display in the TF browser.","title":"Tables"},{"location":"Implementation/Tables/#tabular-display-for-the-tf-browser","text":"","title":"Tabular display for the TF-browser"},{"location":"Implementation/Tables/#constants","text":"Fixed values The following values are used by other parts of the program: name description RESULT string","title":"Constants"},{"location":"Implementation/Tables/#composition","text":"compose() 1 compose ( app , tuples , features , position , opened , getx = None , ** displayParameters ) Takes a list of tuples and composes it into an HTML table. Some of the rows will be expandable, namely the rows specified by opened , for which extra data has been fetched. features is a list of names of features that will be shown in expanded pretty displays. Typically, it is the list of features used in the query that delivered the tuples. position The current position in the list. Will be highlighted in the display. getx=None If None , a portion of the tuples will be put in a table. otherwise, it is an index in the list for which a pretty display will be retrieved. Typically, this happens when a TF-browser user clicks on a table row in order to expand it. composeT() 1 composeT ( app , features , tuples , features , opened , getx = None , ** displayParameters ) \" Very much like compose() , but here the tuples come from a sections and/or tuples specification in the TF-browser. composeP(), getx=None, **displayParameters) 1 2 3 4 5 6 7 8 9 composeP ( app , sec0 , sec1 , features , query , sec2 = None , opened = set (), getx = None , ** displayParameters ) Like composeT() , but this is meant to compose the items at section level 2 (verses) within an item of section level 1 (chapter) within an item of section level 0 (a book). Typically invoked when a user of the TF-browser is browsing passages. The query is used to highlight its results in the passages that the user is browsing. plainTextS2() 1 plainTextS2 ( sNode , opened , sec2 , highlights , \\ * \\ * displayParameters ) Produces a single item corresponding to a section 2 level (verse) for display in the browser. It will rendered as plain text, but expandable to a pretty display. Highlighting The functions getPassageHighlights() , getHlNodes() , nodesFromTuples() are helpers to apply highlighting to query results in a passage. getBoundary(api, node) Utility function to ask from the TF API the first slot and the last slot contained in a node. getFeatures(app, node, ...) Helper for pretty() : wrap the requested features and their values for node in HTML for pretty display. header(app) Get the app-specific links to data and documentation and wrap it into HTML for display in the TF browser.","title":"Composition"},{"location":"Model/Data-Model/","text":"Text-Fabric Data Model \u00b6 Everything about us, everything around us, everything we know and can know of is composed ultimately of patterns of nothing; that\u2019s the bottom line, the final truth. So where we find we have any control over those patterns, why not make the most elegant ones, the most enjoyable and good ones, in our own terms? -- Iain M. Banks . Consider Phlebas : A Culture Novel (Culture series) At a glance \u00b6 Take a text, put a grid around the words, and then leave out the words. What is left, are the textual positions, or slots . Pieces of text correspond to phrases, clauses, sentences, verses, chapters, books. Draw circles around those pieces, and then leave out their contents. What is left, are the textual objects, or nodes . Nodes can be connected to other nodes by edges . A basic function of edges is to indicate containment : this node corresponds to a set of slots that is contained in the slots of that node. But edges can also denote more abstract, linguistic relations between nodes. Nodes have types. Types are just a label that we use to make distinctions between word nodes, phrase nodes, ..., book nodes. The type assignment is an example of a feature of nodes: a mapping that assigns a piece of information to each node. This type assignment has a name: otype , and every Text-Fabric dataset has such a feature. Nodes may be linked to textual positions or slots . Some nodes are linked to a single slot, others to a set of slots, and yet others to no slots at all. Nodes of the first kind are identified with their slots, they have the same number as slot as they have as node. Nodes of the second kind have an edge to every slot (which is also a node) that they are linked to. The collection of these edges from nodes of the second kind to nodes of the first kind, is an example of a feature of edges: a mapping that assigns to each pair of nodes a boolean value: is this pair a link or not? This particular edge feature is called oslots , and every Text-Fabric dataset has such a feature. Nodes of the third kind represent information that is not part of the main body of text. We could represent the lexicon in this way. However, it is also possible to consider lexeme as a node type, where every lexeme node is linked to the set of slots that have an occurrence of that lexeme. Fabric metaphor \u00b6 AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen Before we go on, we invite you to look at a few basic terms in the craft of weaving . A weaver sets up a set of fixed, parallel threads, the warp . He then picks a thread, usually a colourful one, and sends it in a perpendicular way through the warp. This thread is called the weft . The instrument that carries the wefts through the warp is called the loom . The weaver continues operating the loom, back and forth, occasionally selecting new wefts, until he has completed a rectangular piece of fabric, the weave . source Now Text-Fabric, the tool, can be seen as the loom that sends features (the wefts) through a warp (the system of nodes and edges). The features otype and oslots are the ones that set up the system of nodes and edges. That's why we call them warp features. Every Text-Fabric dataset contains these two warp features. (Later on we'll see a third member of the warp, otext ). They provide the structure of a text and its annotations, without any content. Even the text itself is left out! All other information is added to the warp as features (the wefts): node features and edge features. A feature is a special aspect of the textual information, isolated as a kind of module. It is a collection of values which can be woven as a weft into the warp. One of the more basic things to add to the warp is the text itself. Ancient texts often have several text representations, like original (Unicode) characters or transliterated characters, with or without the complete set of diacritical marks. In Text-Fabric we do not have to choose between them: we can package each representation into a feature, and add it to the fabric. A Text-Fabric data set is a warp ( otype , oslots ) plus a collection of wefts (all other features). We may add other features to the same warp. Data sets with only wefts, but no warps, are called modules. When you use modules with a dataset, the modules must have been constructed around the warp of the dataset. Whenever you use Text-Fabric to generate new data, you are weaving a weave. The resulting dataset is a tight fabric of individual features (wefts), whose values are taken for a set of nodes (warp). Some features deserve a privileged place. After all, we are dealing with text , so we need a bit of information about which features carry textual representations and sectioning information (e.g. books, chapters, verses). This information is not hard-wired into Text-Fabric, but it is given in the form of a config feature. A config feature has no data, only metadata. Every Text-Fabric dataset may contain a config feature called otext , which specifies which node types and features correspond to sectional units such as books, chapters, and verses. It also contains templates for generating text representations for the slots. The otext feature is optional, because not all Text-Fabric datasets are expected to have extensive sectioning and text representation definitions. Especially when you are in the process of converting a data source (such as a treebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load the data without bothering about these matters. Model \u00b6 We summarize in brief statements our data model, including ways to represent the data, serialize it, and compute with it. Text objects: occupy arbitrary compositions of slots; carry a type (just a label); all slots carry the same type, the slot type ; e.g. word or character ; can be annotated by features (key-value pairs) can be connected by directed, labelled links to other text objects. The model knows which feature assigned values to nodes and edges. If two different features assign a value to an edge or node, both values can be read off later; one through the one feature, and one through the other. The data in Text-Fabric is organized as an annotated directed graph with a bit of additional structure. The correspondence is text positions => the first so many slot numbers text objects => nodes links between text objects => edges information associated with text objects => node features labels on links between text objects => edge features NB: since every link is specified by an edge feature, every link is implicitly labelled by the name of the edge feature. If the edge feature assigns values to edges, those values come on top of the implicit label. types of text objects => a special node feature called otype (read: object type) extent of text objects in terms of slots => a special edge feature called oslots (read: object slots) optional specifications for sectioning and representing text => a special config feature called otext (read: object text) Together, the otype , oslots , and the optional otext features are called the warp of a Text-Fabric dataset. Representation \u00b6 We represent the elements that make up such a graph as follows: nodes are integers, starting with 1, without gaps; the first maxSlot nodes correspond exactly with the slots, in the same order, where maxSlot is the number of slots; nodes greater than maxSlot correspond to general text objects; node features are mappings of integers to values; edge features are mappings of pairs of integers to values; i.e. edges are ordered pairs of integers; labelled edges are ordered tuples of two nodes and a value; values (for nodes and for edges) are strings (Unicode, utf8) or numbers; the otype feature maps the integers 1..maxSlot (including) to the slot type , where maxSlot is the last slot , the integers maxSlot+1..maxNode (including) to the relevant text object types; the oslots feature is an valueless edge feature, mapping all non-slot nodes to sets of slots; so there is an oslots edge between each non-slot node and each slot contained by that node; a Text-Fabric dataset is a collection of node features and edge features containing at least the warp features otype , oslots , and, optionally otext . More about the warp \u00b6 The warp/weft distinction is a handy way of separating textual organisation from textual content. Let us discuss the warp features a bit more. otype: node feature \u00b6 Maps each node to a label. The label typically is the kind of object that the node represents, with values such as 1 2 3 4 5 6 7 book chapter verse sentence clause phrase word There is a special kind of object type, the slot type , which is the atomic building block of the text objects. It is assumed that the complete text is built from a sequence of slots , from slot 1 till slot maxSlot (including), where the slots are numbered consecutively. There must be at least one slot. All other objects are defined with respect to the slots they contain. The slot type does not have to be called slot literally. If your basic entity is word , you may also call it word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with characters . Other choices may be equally viable. The only requirement is that all slots correspond exactly with the first so many nodes. The otype feature will map node 1 to a node type, and this node type is the type of all subsequent slots and also of the things that fill the slots. Note also the sectional features book chapter verse here. They will play a role in the third, optional, warp feature otext . oslots: edge feature \u00b6 Defines which slots are occupied by which objects. It does so by specifying edges from nodes to the slots they contain. From the information in oslots we can compute the embedding relationships between all nodes. It gives also rise to a canonical ordering of nodes. otext: config feature (optional) \u00b6 Declares which node types correspond to the first three levels of sectioning, usually book , chapter , verse . Also declares the corresponding features to get the names or numbers of the sections in those levels. Text-Fabric uses this information to construct the so-called Text-API, with functions to convert nodes to section labels and vice versa, represent section names in multiple languages, print formatted text for node sets. If information about sections or text representations are missing, Text-Fabric will build a reduced Text-API for you, but it will continue. Serializing and precomputing \u00b6 When Text-Fabric works with a dataset, it reads feature data files, and offers an API to process that data. The main task of Text-Fabric is to make processing efficient, so that it can be done in interactive ways, such as in a Jupyter notebook. To that end, Text-Fabric optimizes feature data after reading it for the first time and stores it in binary form for fast loading in next invocations; precomputes additional data from the warp features in order to provide convenient API functions. In Text-Fabric, we have various ways of encoding this model: as plain text in .tf feature files, as Python data structures in memory, as compressed serializations of the same data structures inside .tfx files in .tf cache directories.","title":"Data"},{"location":"Model/Data-Model/#text-fabric-data-model","text":"Everything about us, everything around us, everything we know and can know of is composed ultimately of patterns of nothing; that\u2019s the bottom line, the final truth. So where we find we have any control over those patterns, why not make the most elegant ones, the most enjoyable and good ones, in our own terms? -- Iain M. Banks . Consider Phlebas : A Culture Novel (Culture series)","title":"Text-Fabric Data Model"},{"location":"Model/Data-Model/#at-a-glance","text":"Take a text, put a grid around the words, and then leave out the words. What is left, are the textual positions, or slots . Pieces of text correspond to phrases, clauses, sentences, verses, chapters, books. Draw circles around those pieces, and then leave out their contents. What is left, are the textual objects, or nodes . Nodes can be connected to other nodes by edges . A basic function of edges is to indicate containment : this node corresponds to a set of slots that is contained in the slots of that node. But edges can also denote more abstract, linguistic relations between nodes. Nodes have types. Types are just a label that we use to make distinctions between word nodes, phrase nodes, ..., book nodes. The type assignment is an example of a feature of nodes: a mapping that assigns a piece of information to each node. This type assignment has a name: otype , and every Text-Fabric dataset has such a feature. Nodes may be linked to textual positions or slots . Some nodes are linked to a single slot, others to a set of slots, and yet others to no slots at all. Nodes of the first kind are identified with their slots, they have the same number as slot as they have as node. Nodes of the second kind have an edge to every slot (which is also a node) that they are linked to. The collection of these edges from nodes of the second kind to nodes of the first kind, is an example of a feature of edges: a mapping that assigns to each pair of nodes a boolean value: is this pair a link or not? This particular edge feature is called oslots , and every Text-Fabric dataset has such a feature. Nodes of the third kind represent information that is not part of the main body of text. We could represent the lexicon in this way. However, it is also possible to consider lexeme as a node type, where every lexeme node is linked to the set of slots that have an occurrence of that lexeme.","title":"At a glance"},{"location":"Model/Data-Model/#fabric-metaphor","text":"AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen Before we go on, we invite you to look at a few basic terms in the craft of weaving . A weaver sets up a set of fixed, parallel threads, the warp . He then picks a thread, usually a colourful one, and sends it in a perpendicular way through the warp. This thread is called the weft . The instrument that carries the wefts through the warp is called the loom . The weaver continues operating the loom, back and forth, occasionally selecting new wefts, until he has completed a rectangular piece of fabric, the weave . source Now Text-Fabric, the tool, can be seen as the loom that sends features (the wefts) through a warp (the system of nodes and edges). The features otype and oslots are the ones that set up the system of nodes and edges. That's why we call them warp features. Every Text-Fabric dataset contains these two warp features. (Later on we'll see a third member of the warp, otext ). They provide the structure of a text and its annotations, without any content. Even the text itself is left out! All other information is added to the warp as features (the wefts): node features and edge features. A feature is a special aspect of the textual information, isolated as a kind of module. It is a collection of values which can be woven as a weft into the warp. One of the more basic things to add to the warp is the text itself. Ancient texts often have several text representations, like original (Unicode) characters or transliterated characters, with or without the complete set of diacritical marks. In Text-Fabric we do not have to choose between them: we can package each representation into a feature, and add it to the fabric. A Text-Fabric data set is a warp ( otype , oslots ) plus a collection of wefts (all other features). We may add other features to the same warp. Data sets with only wefts, but no warps, are called modules. When you use modules with a dataset, the modules must have been constructed around the warp of the dataset. Whenever you use Text-Fabric to generate new data, you are weaving a weave. The resulting dataset is a tight fabric of individual features (wefts), whose values are taken for a set of nodes (warp). Some features deserve a privileged place. After all, we are dealing with text , so we need a bit of information about which features carry textual representations and sectioning information (e.g. books, chapters, verses). This information is not hard-wired into Text-Fabric, but it is given in the form of a config feature. A config feature has no data, only metadata. Every Text-Fabric dataset may contain a config feature called otext , which specifies which node types and features correspond to sectional units such as books, chapters, and verses. It also contains templates for generating text representations for the slots. The otext feature is optional, because not all Text-Fabric datasets are expected to have extensive sectioning and text representation definitions. Especially when you are in the process of converting a data source (such as a treebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load the data without bothering about these matters.","title":"Fabric metaphor"},{"location":"Model/Data-Model/#model","text":"We summarize in brief statements our data model, including ways to represent the data, serialize it, and compute with it. Text objects: occupy arbitrary compositions of slots; carry a type (just a label); all slots carry the same type, the slot type ; e.g. word or character ; can be annotated by features (key-value pairs) can be connected by directed, labelled links to other text objects. The model knows which feature assigned values to nodes and edges. If two different features assign a value to an edge or node, both values can be read off later; one through the one feature, and one through the other. The data in Text-Fabric is organized as an annotated directed graph with a bit of additional structure. The correspondence is text positions => the first so many slot numbers text objects => nodes links between text objects => edges information associated with text objects => node features labels on links between text objects => edge features NB: since every link is specified by an edge feature, every link is implicitly labelled by the name of the edge feature. If the edge feature assigns values to edges, those values come on top of the implicit label. types of text objects => a special node feature called otype (read: object type) extent of text objects in terms of slots => a special edge feature called oslots (read: object slots) optional specifications for sectioning and representing text => a special config feature called otext (read: object text) Together, the otype , oslots , and the optional otext features are called the warp of a Text-Fabric dataset.","title":"Model"},{"location":"Model/Data-Model/#representation","text":"We represent the elements that make up such a graph as follows: nodes are integers, starting with 1, without gaps; the first maxSlot nodes correspond exactly with the slots, in the same order, where maxSlot is the number of slots; nodes greater than maxSlot correspond to general text objects; node features are mappings of integers to values; edge features are mappings of pairs of integers to values; i.e. edges are ordered pairs of integers; labelled edges are ordered tuples of two nodes and a value; values (for nodes and for edges) are strings (Unicode, utf8) or numbers; the otype feature maps the integers 1..maxSlot (including) to the slot type , where maxSlot is the last slot , the integers maxSlot+1..maxNode (including) to the relevant text object types; the oslots feature is an valueless edge feature, mapping all non-slot nodes to sets of slots; so there is an oslots edge between each non-slot node and each slot contained by that node; a Text-Fabric dataset is a collection of node features and edge features containing at least the warp features otype , oslots , and, optionally otext .","title":"Representation"},{"location":"Model/Data-Model/#more-about-the-warp","text":"The warp/weft distinction is a handy way of separating textual organisation from textual content. Let us discuss the warp features a bit more.","title":"More about the warp"},{"location":"Model/Data-Model/#otype-node-feature","text":"Maps each node to a label. The label typically is the kind of object that the node represents, with values such as 1 2 3 4 5 6 7 book chapter verse sentence clause phrase word There is a special kind of object type, the slot type , which is the atomic building block of the text objects. It is assumed that the complete text is built from a sequence of slots , from slot 1 till slot maxSlot (including), where the slots are numbered consecutively. There must be at least one slot. All other objects are defined with respect to the slots they contain. The slot type does not have to be called slot literally. If your basic entity is word , you may also call it word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with characters . Other choices may be equally viable. The only requirement is that all slots correspond exactly with the first so many nodes. The otype feature will map node 1 to a node type, and this node type is the type of all subsequent slots and also of the things that fill the slots. Note also the sectional features book chapter verse here. They will play a role in the third, optional, warp feature otext .","title":"otype: node feature"},{"location":"Model/Data-Model/#oslots-edge-feature","text":"Defines which slots are occupied by which objects. It does so by specifying edges from nodes to the slots they contain. From the information in oslots we can compute the embedding relationships between all nodes. It gives also rise to a canonical ordering of nodes.","title":"oslots: edge feature"},{"location":"Model/Data-Model/#otext-config-feature-optional","text":"Declares which node types correspond to the first three levels of sectioning, usually book , chapter , verse . Also declares the corresponding features to get the names or numbers of the sections in those levels. Text-Fabric uses this information to construct the so-called Text-API, with functions to convert nodes to section labels and vice versa, represent section names in multiple languages, print formatted text for node sets. If information about sections or text representations are missing, Text-Fabric will build a reduced Text-API for you, but it will continue.","title":"otext: config feature (optional)"},{"location":"Model/Data-Model/#serializing-and-precomputing","text":"When Text-Fabric works with a dataset, it reads feature data files, and offers an API to process that data. The main task of Text-Fabric is to make processing efficient, so that it can be done in interactive ways, such as in a Jupyter notebook. To that end, Text-Fabric optimizes feature data after reading it for the first time and stores it in binary form for fast loading in next invocations; precomputes additional data from the warp features in order to provide convenient API functions. In Text-Fabric, we have various ways of encoding this model: as plain text in .tf feature files, as Python data structures in memory, as compressed serializations of the same data structures inside .tfx files in .tf cache directories.","title":"Serializing and precomputing"},{"location":"Model/File-formats/","text":"Text-Fabric File Format \u00b6 Overview \u00b6 A .tf feature file starts with a header , and is followed by the actual data. The whole file is a plain text in UNICODE-utf8. Header \u00b6 A .tf feature file always starts with one or more metadata lines of the form 1 @key or 1 @key=value The first line must be either 1 @node or 1 @edge or 1 @config This tells Text-Fabric whether the data in the feature file is a node feature or an edge feature. The value @config means that the file will be used as configuration info. It will only have metadata. There must also be a type declaration: 1 @valueType=type where type is str or int . @valueType declares the type of the values in this feature file. If it is anything other than str (= string ), Text-Fabric will convert it to that type when it reads the data from the file. Currently, the only other supported type is int for integers. In edge features, there may also be a declaration 1 @edgeValues indicating that the edge feature carries values. The default is that an edge does not carry values. The rest of the metadata is optional for now, but it is recommended to put a date stamp in it like this 1 @dateCreated=2016-11-20T13:26:59Z The time format should be ISO 8601 . Data \u00b6 After the metadata, there must be exactly one blank line, and every line after that is data. Data lines \u00b6 The form of a data line is 1 node_spec value for node features, and 1 node_spec node_spec value for edge features. These fields are separated by single tabs. NB : This is the default format. Under Optimizations below we shall describe the bits that can be left out, which will lead to significant improvement in space demands and processing speed. Node Specification \u00b6 Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of *node_spec*s. A node spec denotes a set of nodes. The simplest form of a node spec is just a single integer. Examples: 1 2 3 3 45 425000 Ranges are also allowed. Examples 1 2 3 1-10 5-13 28-57045 The nodes denoted by a range are all numbers between the endpoints of the range (including at both sides). So 1 2-4 denotes the nodes 2 , 3 , and 4 . You can also combine numbers and ranges arbitrarily by separating them with commas. Examples 1 1-3,5-10,15,23-37 Such a specification denotes the union of what is denoted by each comma-separated part. NB As node specs denote sets of nodes, the following node specs are in fact equivalent 1 2 3 1,1 and 1 2-3 and 3,2 1-5,2-7 and 1-7 We will be tolerant in that you may specify the end points of ranges in arbitrary order: 1 1-3 is the same as 3-1 Edges \u00b6 An edge is specified by an ordered pair of nodes. The edge is from the first node in the pair to the second one. An edge spec consists of two node specs. It denotes all edges that are from a node denoted by the first node spec to a node denoted by the second node spec. An edge might be labelled, in that case the label of the edge is specified by the value after the two node specs. Value \u00b6 The value is arbitrary text. The type of the value must conform to the @valueType declaration in the feature file. If it is missing, it is assumed to be str , which is the type of Unicode-utf8 strings. If it is int , it should be a valid representation of an integer number, There are a few escapes: \\\\ backslash \\t tab \\n newline These characters MUST always be escaped in a value string, otherwise the line as a whole might be ambiguous. NB: There is no representation for the absence of a value. The empty string as value means that there is a value and it is the empty string. If you want to describe the fact that node n does not have a value for the feature in question, the node must be left out of the feature. In order words, there should be no data line in the feature that targets this node. If the declared value type ( @valueType ) of a feature is int , then its empty values will be taken as absence of values, though. Consistency requirements \u00b6 There are a few additional requirements on feature data, having to do with the fact that features annotate nodes or edges of a graph. Single values \u00b6 It is assumed that a node feature assigns only one value to the same node. If the data contains multiple assignments to a node, only the last assignment will be honoured, the previous ones will be discarded. Likewise, it is assumed that an edge feature assigns only one value to the same edge. If the data contains multiple assignments to an edge, only the last assignment will be honoured. Violations maybe or may not be reported, and processing may continue without warnings.","title":"Format"},{"location":"Model/File-formats/#text-fabric-file-format","text":"","title":"Text-Fabric File Format"},{"location":"Model/File-formats/#overview","text":"A .tf feature file starts with a header , and is followed by the actual data. The whole file is a plain text in UNICODE-utf8.","title":"Overview"},{"location":"Model/File-formats/#header","text":"A .tf feature file always starts with one or more metadata lines of the form 1 @key or 1 @key=value The first line must be either 1 @node or 1 @edge or 1 @config This tells Text-Fabric whether the data in the feature file is a node feature or an edge feature. The value @config means that the file will be used as configuration info. It will only have metadata. There must also be a type declaration: 1 @valueType=type where type is str or int . @valueType declares the type of the values in this feature file. If it is anything other than str (= string ), Text-Fabric will convert it to that type when it reads the data from the file. Currently, the only other supported type is int for integers. In edge features, there may also be a declaration 1 @edgeValues indicating that the edge feature carries values. The default is that an edge does not carry values. The rest of the metadata is optional for now, but it is recommended to put a date stamp in it like this 1 @dateCreated=2016-11-20T13:26:59Z The time format should be ISO 8601 .","title":"Header"},{"location":"Model/File-formats/#data","text":"After the metadata, there must be exactly one blank line, and every line after that is data.","title":"Data"},{"location":"Model/File-formats/#data-lines","text":"The form of a data line is 1 node_spec value for node features, and 1 node_spec node_spec value for edge features. These fields are separated by single tabs. NB : This is the default format. Under Optimizations below we shall describe the bits that can be left out, which will lead to significant improvement in space demands and processing speed.","title":"Data lines"},{"location":"Model/File-formats/#node-specification","text":"Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of *node_spec*s. A node spec denotes a set of nodes. The simplest form of a node spec is just a single integer. Examples: 1 2 3 3 45 425000 Ranges are also allowed. Examples 1 2 3 1-10 5-13 28-57045 The nodes denoted by a range are all numbers between the endpoints of the range (including at both sides). So 1 2-4 denotes the nodes 2 , 3 , and 4 . You can also combine numbers and ranges arbitrarily by separating them with commas. Examples 1 1-3,5-10,15,23-37 Such a specification denotes the union of what is denoted by each comma-separated part. NB As node specs denote sets of nodes, the following node specs are in fact equivalent 1 2 3 1,1 and 1 2-3 and 3,2 1-5,2-7 and 1-7 We will be tolerant in that you may specify the end points of ranges in arbitrary order: 1 1-3 is the same as 3-1","title":"Node Specification"},{"location":"Model/File-formats/#edges","text":"An edge is specified by an ordered pair of nodes. The edge is from the first node in the pair to the second one. An edge spec consists of two node specs. It denotes all edges that are from a node denoted by the first node spec to a node denoted by the second node spec. An edge might be labelled, in that case the label of the edge is specified by the value after the two node specs.","title":"Edges"},{"location":"Model/File-formats/#value","text":"The value is arbitrary text. The type of the value must conform to the @valueType declaration in the feature file. If it is missing, it is assumed to be str , which is the type of Unicode-utf8 strings. If it is int , it should be a valid representation of an integer number, There are a few escapes: \\\\ backslash \\t tab \\n newline These characters MUST always be escaped in a value string, otherwise the line as a whole might be ambiguous. NB: There is no representation for the absence of a value. The empty string as value means that there is a value and it is the empty string. If you want to describe the fact that node n does not have a value for the feature in question, the node must be left out of the feature. In order words, there should be no data line in the feature that targets this node. If the declared value type ( @valueType ) of a feature is int , then its empty values will be taken as absence of values, though.","title":"Value"},{"location":"Model/File-formats/#consistency-requirements","text":"There are a few additional requirements on feature data, having to do with the fact that features annotate nodes or edges of a graph.","title":"Consistency requirements"},{"location":"Model/File-formats/#single-values","text":"It is assumed that a node feature assigns only one value to the same node. If the data contains multiple assignments to a node, only the last assignment will be honoured, the previous ones will be discarded. Likewise, it is assumed that an edge feature assigns only one value to the same edge. If the data contains multiple assignments to an edge, only the last assignment will be honoured. Violations maybe or may not be reported, and processing may continue without warnings.","title":"Single values"},{"location":"Model/Optimizations/","text":"File format Optimizations \u00b6 Rationale \u00b6 It is important to avoid an explosion of redundant data in .tf files. We want the .tf format to be suitable for archiving, transparent to the human eye, and easy (i.e. fast) to process. Using the implicit node \u00b6 You may leave out the node spec for node features, and the first node spec for edge features. When leaving out a node spec, you must also leave out the tab following the node spec. A line with the first node spec left out denotes the singleton node set consisting of the implicit node . Here are the rules for implicit nodes. On a line where there is an explicit node spec, the implicit node is equal to the highest node denoted by the explicit node spec; On a line without an explicit node spec, the implicit node is determined from the previous line as follows: if there is no previous line, take 1 ; else take the implicit node of the previous line and increment it by 1 . For edges, this optimization only happens for the first node spec. The second node spec must always be explicit. This optimizes some feature files greatly, e.g. the feature that contains the actual text of each word. Instead of 1 2 3 4 5 6 7 8 9 10 11 1 be 2 reshit 3 bara 4 elohim 5 et 6 ha 7 shamajim 8 we 9 et 10 ha 11 arets you can just say 1 2 3 4 5 6 7 8 9 10 11 be reshit bara elohim et ha shamajim we et ha arets This optimization is not obligatory. It is a device that may be used if you want to optimize the size of data files that you want to distribute. Omitting empty values \u00b6 If the value is the empty string, you may also leave out the preceding tab (if there is one). This is especially good for edge features, because most edges just consist of a node pair without any value. This optimization will cause a conceptual ambiguity if there is only one field present in a node feature, or if there are only two fields in an edge feature. It could mean that the (first) node spec has been left out, or that the value has been left out. In those cases we will assume that the node spec has been left out for node features. For edge features, it depends on whether the edge is declared to have values (with @edgeValues ). If the edge has values, then, as in the case of node features, we assume that the first node spec has been left out. But if the edge has no values, then we assume that both fields are node specs. So, in a node feature a line like this 1 42 means that the implicit node gets value 42 , and not that node 42 gets the empty value. Likewise, a line in an edge feature (without values) like this 1 42 43 means that there is an edge from 42 to 43 with empty value, and not that there is an edge from the implicit node to 42 with value 43. And, in the same edge, a line like this 1 42 means that there is an edge from the implicit node to 42 with the empty value. But, in an edge with values, the same lines are interpreted thus: 1 42 43 means that there is an edge from the implicit node to node 42 with value 43 . And 1 42 means that there is an edge from the implicit node to node 42 with empty value. The reason for these conventions is practical: edge features usually have empty labels, and there are many edges. In case of the Hebrew Text database, there are 1.5 million edges, so every extra character that is needed on a data line means that the file size increases with 1.5 MB. Nodes on the other hand, usually do not have empty values, and they are often specified in a consecutive way, especially slot (word) nodes. There are quite many distinct word features, and it would be a waste to have a column of half a million incremental integers in those files. Absence of values \u00b6 Say you have a node feature assigning a value to only 2000 of 400,000 nodes. (The Hebrew qere would be an example). It is better to make sure that the absent values are not coded as the empty string. So the feature data will look like 2000 lines, each with a node spec, rather than a sequence of 400,000 lines, most empty. If you want to leave out just a few isolated cases in a feature where most nodes get a value, you can do it like this: 1 2 3 4 5 6 7 8 9 @node x0000 ... x1000 1002 x1002 x1003 ... x9999 Here all 10,000 nodes get a value, except node 1001 . Note on redundancy \u00b6 Some features assign the same value to many nodes. It is tempting to make a value definition facility, so that values are coded by short codes, the most frequent values getting the shortest codes. After some experiments, it turned out that the overall gain was just 50%. I find this advantage too small to justify the increased code complexity, and above all, the reduced transparency of the .tf files. Examples \u00b6 Here are a few more and less contrived examples of legal feature data lines. Node features \u00b6 \\t\\n 2 2\\t3 foo\\nbar 1 Escape \\t as \\\\t meaning node 1 has value: tab newline node 2 has value: 2 tab 3 node 3 has value: foo newline bar node 1 gets a new value: Escape as \\t Edge features \u00b6 1 1 2 2 3 foo 1-2 2-3 bar meaning edge from 1 to 1 with no value edge from 1 to 2 with no value edge from 2 to 3 with value foo four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can go from a node to itself. Note also that this line reassigns a value to two edges: 1->2 and 2->3.","title":"Tweaks"},{"location":"Model/Optimizations/#file-format-optimizations","text":"","title":"File format Optimizations"},{"location":"Model/Optimizations/#rationale","text":"It is important to avoid an explosion of redundant data in .tf files. We want the .tf format to be suitable for archiving, transparent to the human eye, and easy (i.e. fast) to process.","title":"Rationale"},{"location":"Model/Optimizations/#using-the-implicit-node","text":"You may leave out the node spec for node features, and the first node spec for edge features. When leaving out a node spec, you must also leave out the tab following the node spec. A line with the first node spec left out denotes the singleton node set consisting of the implicit node . Here are the rules for implicit nodes. On a line where there is an explicit node spec, the implicit node is equal to the highest node denoted by the explicit node spec; On a line without an explicit node spec, the implicit node is determined from the previous line as follows: if there is no previous line, take 1 ; else take the implicit node of the previous line and increment it by 1 . For edges, this optimization only happens for the first node spec. The second node spec must always be explicit. This optimizes some feature files greatly, e.g. the feature that contains the actual text of each word. Instead of 1 2 3 4 5 6 7 8 9 10 11 1 be 2 reshit 3 bara 4 elohim 5 et 6 ha 7 shamajim 8 we 9 et 10 ha 11 arets you can just say 1 2 3 4 5 6 7 8 9 10 11 be reshit bara elohim et ha shamajim we et ha arets This optimization is not obligatory. It is a device that may be used if you want to optimize the size of data files that you want to distribute.","title":"Using the implicit node"},{"location":"Model/Optimizations/#omitting-empty-values","text":"If the value is the empty string, you may also leave out the preceding tab (if there is one). This is especially good for edge features, because most edges just consist of a node pair without any value. This optimization will cause a conceptual ambiguity if there is only one field present in a node feature, or if there are only two fields in an edge feature. It could mean that the (first) node spec has been left out, or that the value has been left out. In those cases we will assume that the node spec has been left out for node features. For edge features, it depends on whether the edge is declared to have values (with @edgeValues ). If the edge has values, then, as in the case of node features, we assume that the first node spec has been left out. But if the edge has no values, then we assume that both fields are node specs. So, in a node feature a line like this 1 42 means that the implicit node gets value 42 , and not that node 42 gets the empty value. Likewise, a line in an edge feature (without values) like this 1 42 43 means that there is an edge from 42 to 43 with empty value, and not that there is an edge from the implicit node to 42 with value 43. And, in the same edge, a line like this 1 42 means that there is an edge from the implicit node to 42 with the empty value. But, in an edge with values, the same lines are interpreted thus: 1 42 43 means that there is an edge from the implicit node to node 42 with value 43 . And 1 42 means that there is an edge from the implicit node to node 42 with empty value. The reason for these conventions is practical: edge features usually have empty labels, and there are many edges. In case of the Hebrew Text database, there are 1.5 million edges, so every extra character that is needed on a data line means that the file size increases with 1.5 MB. Nodes on the other hand, usually do not have empty values, and they are often specified in a consecutive way, especially slot (word) nodes. There are quite many distinct word features, and it would be a waste to have a column of half a million incremental integers in those files.","title":"Omitting empty values"},{"location":"Model/Optimizations/#absence-of-values","text":"Say you have a node feature assigning a value to only 2000 of 400,000 nodes. (The Hebrew qere would be an example). It is better to make sure that the absent values are not coded as the empty string. So the feature data will look like 2000 lines, each with a node spec, rather than a sequence of 400,000 lines, most empty. If you want to leave out just a few isolated cases in a feature where most nodes get a value, you can do it like this: 1 2 3 4 5 6 7 8 9 @node x0000 ... x1000 1002 x1002 x1003 ... x9999 Here all 10,000 nodes get a value, except node 1001 .","title":"Absence of values"},{"location":"Model/Optimizations/#note-on-redundancy","text":"Some features assign the same value to many nodes. It is tempting to make a value definition facility, so that values are coded by short codes, the most frequent values getting the shortest codes. After some experiments, it turned out that the overall gain was just 50%. I find this advantage too small to justify the increased code complexity, and above all, the reduced transparency of the .tf files.","title":"Note on redundancy"},{"location":"Model/Optimizations/#examples","text":"Here are a few more and less contrived examples of legal feature data lines.","title":"Examples"},{"location":"Model/Optimizations/#node-features","text":"\\t\\n 2 2\\t3 foo\\nbar 1 Escape \\t as \\\\t meaning node 1 has value: tab newline node 2 has value: 2 tab 3 node 3 has value: foo newline bar node 1 gets a new value: Escape as \\t","title":"Node features"},{"location":"Model/Optimizations/#edge-features","text":"1 1 2 2 3 foo 1-2 2-3 bar meaning edge from 1 to 1 with no value edge from 1 to 2 with no value edge from 2 to 3 with value foo four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can go from a node to itself. Note also that this line reassigns a value to two edges: 1->2 and 2->3.","title":"Edge features"},{"location":"Model/Search/","text":"Search Design \u00b6 Fabric metaphor \u00b6 The search space is a massive fabric of interconnected material. In it we discern the structures we are interested in: little pieces of fabric, also with interconnected material. When we search, we have a fabric in mind, woven from specific material, stitched together in a specific manner. Search in Text-Fabric works exactly like this: you give a sample patch, and Text-Fabric fetches all pieces of the big fabric that match your patch. The textile metaphor is particularly suited for grasping the search part of Text-Fabric, so I'm going to stick to it for a while. I have used it in the actual code as well, and even in the proofs that certain parts of the algorithm terminate and are correct. Yet it remains a metaphor, and the fit is not exact. The basic pattern of search is this: textile text example take several fleeces pick the nodes corresponding to a node type word s, phrase s, clause s, verse s spin thick yarns from them filter by feature conditions part-of-speech=verb gender= f book=Genesis vt spin the yarns further into thin yarns throw away nodes that do not have the right connections feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa stitch the yarns together with thread build results by selecting a member for every filtered node set word node 123456 in phrase node 657890 in clause node 490567 in verse node 1403456 We will explain the stages of the fabrication process in detail. Fleece \u00b6 A fleece corresponds with a very simple search template that asks for all objects of a given type: 1 word or 1 clause or, asking for multiple types: 1 2 3 4 5 verse clause phrase lex word Fleeces are the raw material from which we fabricate our search results. Every node type, such as word , sentence , book corresponds to a fleece. In Text-Fabric, every node has exactly one node type, so the whole space is neatly divided into a small set of fleeces. The most important characteristic of a fleece is it size: the number of nodes in a node type. Spinning thick yarn \u00b6 Consider search templates where we ask for specific members of a node type, by giving feature constraints: 1 2 3 4 5 6 verse book=Genesis clause type=rela phrase determined=yes lex id=jc/ word number=pl vt vt Every line in this search templates we call an atom : a node type plus a feature specification. The result of an atom is the set of all nodes in that node type that satisfy those feature conditions. Finding the results of an atom corresponds with first thing that we do with a fleece: spin a thick yarn from it. Yarns in general are obtained by spinning fleeces, i.e. by filtering node sets that correspond to a node type. A search template may contain multiple atoms. Text-Fabric collects all atoms of a template, grabs the corresponding fleeces, and spins thick yarns from them. For each atom it will spin a yarn, and if there are several atoms referring to the same node type, there will be several yarns spun from that fleece. This spinning of thick yarns out of fleeces happens in just one go. All fleeces together contain exactly all nodes, so Text-Fabric walks in one pass over all nodes, applies the feature conditions, and puts the nodes into the yarns depending on which conditions apply. By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical pass requires a fraction of a second. The most important characteristic of a yarn is its thickness , which we define as the number of nodes in the yarn divided by the number of nodes in the fleece. For example, a yarn consisting of the book node of Genesis only, has a thickness of 1/39, because there are 39 books in the fleece. A much thicker yarn is that of the verbs, which has a thickness of roughly \u2159. A very thin thread is that of the word <CQH (which occurs only once) with a thickness of only 1/400,000. Spinning thin yarns \u00b6 In order to find results, we have to further narrow down the search space. In other words, we are going to spin our thick yarns into thinner and thinner yarns. Before we can do that, we should make one thing clear. Connected by constraints \u00b6 If the template above were complete, it would lead to a monstrous number of results. Because a result of a template like this is any combination of verse-, clause-, phrase-, lex-, word nodes that individually satisfy their own atom condition. So the number of results is the product of the number of results of the individual atoms, which is pretty enormous. It is hard to imagine a situation where these results could be consumed. Usually, there are constraints active between the atoms. For example in a template like this: 1 2 3 4 5 6 7 8 1 verse book=Genesis 2 clause type=rela 3 phrase determined=yes 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l The meaning of this template is that we look for a verse that (1) claims to be in Genesis (2) has a clause whose type is rela (3) which in turn has a phrase of a determined character (4) which contains a word in the plural and that has a verbal tense (vt) There should also be a (6) lex object, identified by jc/ which is connected to the rest by the constraint that (8) the word of line 4 is contained in it. Note that all atoms are linked by constraints into one network. In graph theoretical terms: this template consists of exactly one connected component . If this were not so, we would have in fact two independent search tasks, where the result set would be the (cartesian) product of the result sets of the separate components. For example, if line 8 were missing, we would effectively search for things that match lines 1-4, and, independently, for things that match line 6. And every result of the first part, combined with any result of the second part, would be a valid result of the whole. Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the same time, let alone that it wants to waste time to generate all the results in the product. You will have to fire those search tasks one by one, and it is up to you how you combine the results. The upshot it: the atoms in the search template should form a network, connected by constraints . Text-Fabric will check this, and will only work with search templates that have only one connected component. Terminology \u00b6 By now we have arrived at the idea that our search template is a graph underneath: what we have called atoms are in fact the nodes, and what we have called constraints , are the edges. From now on, we will call the atoms qnodes and the constraints qedges . The q is to distinguish the nodes and the edges from the nodes and the edges of your dataset, the text nodes and text edges. When we use the term nodes and edges we will always refer to text nodes and edges. When we are searching, we maintain a yarn for every qnode . This yarn starts out to be the thick yarn as described above, but we are going to thin them. We can also see how our query templates are really topographic : a query template is a piece of local geography that we want to match against the data. Finding results is nothing else than instantiating qnodes of the search template by text nodes in such a way that the qedges hold between the text edges. Spinning a qedge \u00b6 So, where were we? We have spun thick threads based on the qnodes individually, but we have not done anything with the qedges . That is going to change now. Consider this piece of search template: 1 2 3 4 5 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l So our qnodes are w and l , and our qedge is w ]] l . Note that a lex object is the set of all occurrences of a lexeme. So w ]] l says that w is embedded in l , in other words, w is a slot contained in the slots of l . It is nothing else than the word w is an instance of the lexeme jc/ . We will now check the pairs of (lex, word)-nodes in the text, where the lex node is taken from the yarn of the qnode l , and the word from the yarn of the qnode w . We can throw away some words from the yarn of w , namely those words that do not lie in a lexeme that is in the yarn of l . In other words: the words that are not instances of lexeme jc/ are out! Conversely, if the lexeme jc/ does not have occurrences in the plural and with a verbal tense, we must kick it out of the yarn of l , leaving no members in it. If a yarn gets empty, we have an early detection that the search yields no results, and the whole process stops. In our case, however, this is not so, and we continue. This pass over the yarns at both sides of a qedge is a spin action. We spin this qedge, and the result is that the two yarns become spun more thinly, hopefully. With the yarn of words severely spun out, we are going to the next qedge, the one between words and phrases. 1 2 3 phrase determined=yes 4 w:word number=pl vt The indent is an implicit way of saying that the \"embeds\" relation [[ holds between the phrase and the word . An equivalent formulation of the template is 1 2 3 4 p:phrase determined=yes w:word number=pl vt p [[ w We race along the yarn w of remaining words and check for each word if it is contained in a phrase in the yarn of p , the determined phrases. If it is not, we throw the word out of the yarn of w . Similarly, we can throw out some phrases from the yarn of p , namely those phrases that do not contain words in the yarn of w . In other words: the phrases without plural words and verbal tense are also out. We continue spinning, now between phrases and clauses. 1 2 2 clause type=rela 3 phrase determined=yes Here we loose the phrases that are not contained in a clause of type=rela , and we loose all clauses that do not embed one of the few phrases left. The last spin action corresponds with 1 2 1 verse book=Genesis 2 clause type=rela So we throw away all our results if they are outside Genesis. We end up with a set of thin yarns, severely thinned out, even. This will be a good starting point for the last stage: picking members from each yarn to form results. We call this stitching and we'll get there in a moment. The spread of a qedge \u00b6 A very important property of a qedge is its spread . A qedge links every node n in its from -yarn to zero, one, or more nodes in its to -yarn. The number of nodes in the to -yarn is a key property. The average number of nodes m in the to -yarn per linked node n in the from -yarn is the spread of the edge. A few examples: An edge that corresponds to ]] , n embeds m . If this edge goes from books to words, then every book node n is linked to every one of its words. So very n has hundreds or thousands *m*s. The spread will roughly be 425,000 / 39 =~ 10,000 The opposite edge has a spread of exactly 1, because every word belongs to exactly one book. Edges with spread 1 are very pleasant for our stitching algorithm later on. An edge corresponding to = . These qedges are super efficient, because their relation = is a breeze to compute, and they have always a spread 1 in both directions. An edge corresponding to # , the node inequality relation. The relation # is still a breeze to compute, but the result is heavy: the set of all nodes not equal to a given node. The spread is nearly 100% of the yarn length, in both directions. These edges are not worth to spin, because if you have two yarns, no node will be excluded: if you have an n in the from -yarn, you will always be able to find a different n in the to -yarn (except when bot yarns are equal, and contain just one node). An edge corresponding to == , the relation between nodes that are linked to the same set of slots. The spread of this relation is not too big, but the cost of computing it adds up quickly when applied to many cases. Spinning all qedges \u00b6 Let us describe the spinning of yarns along edges in a bit more general way, and reflect on what it does for us. We spin all qedges of a template. But after spinning a qedge, the yarns involved may have changed. If that is the case, it makes sense to re-spin other qedges that are involved in the changed yarns. That is exactly what we do. We keep spinning, until the yarns have stabilized. A few key questions need to be addressed: Do the yarns stabilize? If they stabilize, what have we got? Is this an efficient process? Termination of spinning \u00b6 Yes, spinning qedges until nothing changes any more, terminates, provided you do not try to spin qedges that are up-to-date. If the yarns around an edge have not changed, it does not make sense to spin that qedge. See here for proof. What have we got? \u00b6 After spinning, it is guaranteed that we have not thrown away results. All nodes that are parts of valid results, are still in the yarns. But, conversely, can it be that there are still nodes in the yarns that are not part of a result? Yes, that is possible. Only when the graph of qnodes and qedges does not have a cycle, we know that all members of all yarns occur at least once in a result. See here for proof. Quite a few interesting queries, however, have cycles in in their graphs. So, in those cases, spinning qedges will not cause the maximal narrowing down of the search space. Efficiency \u00b6 And that raises the question: how effective is the process of spinning qedges? The answer is: it depends. If your qnodes have strong conditions on them, so that the first yarn is already very thin, then every yarn that is connected to this one by a qedge has also the chance to get very thin after spinning. In this case, the combined filtering effect of all edges can produce a rapid narrowing of the search space. Especially if we can implement edge spinning in an optimized way, this works like a charm. When we come to stitching results (which is potentially very expensive), we have already achieved a massive reduction of work. But if none of the yarns is thin at the outset, spinning qedges will not result in appreciable thinning of the yarns, while it might be an enormous amount of work, depending on the actual relations involved. The good news is that it is possible to detect those situations. Text-Fabric estimates whether it makes sense to spin a qedge, and if not, it will just skip spinning that edge. Which will make the final result gathering (stitching) more expensive. There is more to efficiency than this. It turns out that the strategy by which you select the next qedge to be spun, influences the efficiency. In general, it is best to always start with the thinnest yarns, and select edges that affect them. Also here there is complication: not every qedge is equally expensive when computed over a yarn. It might be better to compute a cheaper edge over a thicker yarn. Stitching \u00b6 The last step is actually getting results. A result is a bunch of nodes, one from each yarn, in such a way that result nodes on yarns fulfil the relationships that the qedges of the search template dictate. If we can find such a set of nodes, we have stitched the yarns together. We call such a result a stitch . A stitch is a tuple of text nodes, each corresponding to exactly one qnode. It is not completely trivial to find stitches, let alone to collect them efficiently. The general procedure is as follows: choose a yarn to start with; try a node in that yarn as starting point pick a qedge from the qnode associated with the yarn (the source yarn), to another qnode and consider that yarn (the target yarn), find a node in the target yarn that is in the right relationship with the node selected in the source yarn, and so on, until all qedges have been used, if all has gone well, deliver the nodes found as a result. Let us look to these steps in a bit more detail. There is an element of choice, and it is very important to study how big this element of choice is in the various stages. First we select a yarn, and in that yarn a node. Usually we have many choices and at least one, because result seeking only makes sense if all yarns are non-empty. The third choice is the related node in the target yarn. Here we may encounter anything from zero, one or many choices. If there are zero choices, then we know that our provisional stitching of yarns so far cannot be completed into a full stitching of all yarns. If we have made choices to get this far, then some of these choices have not been lucky. We have to back-track and try other alternatives. If there is just one choice, it is easy: we pick the one and only possible node in the target yarn, without introducing new points of choice. If there are many choices, we have to try them all, one by one. Some might lead to a full stitch, others not. An important situation to be aware of, is when a qedge leads the stitching process to a yarn, out of which a node has already been chosen by an earlier step. This is very well possible, since the search template might have cycles in the qedges, or multiple qedges arrive at the same qnode. When this happens, we do not have to select a target node, we only have to check whether the target node that has been selected before, stands in the right relationship to the current source node. The relationship, that is, which is dictated by the current qedge that we are at. If so, we can stitch on with other edges, without introducing choice points (very much like the one-choice above). If the relation fails to hold, this stitch is doomed, and we have to back-track (very much like the zero-choice above). Strategy of stitching \u00b6 The steps involved in stitching as described above are clear, but less clear is what yarn we shall select to start with, and in which order we shall follow the edges. We need a strategy, and multiple strategies might lead to the same results, albeit with varying efficiency. In Text-Fabric we employ a strategy, that makes the narrowest choices first. We call a choice narrow if there are few alternatives to choose from, and broad if there are many alternatives. By giving precedence to narrow choices, we prune larger parts of the search tree when we fail. If we are stitching, the more nodes we have gathered in our stitch, the greater the chance that a blocking relationship is encountered, i.e. a relationship that should hold between the nodes gathered so far, but which in fact does not hold. So we want to get as many nodes in our stitch as quickly as possible. If our search tree is narrowly branching near the root, and broadly branching near the leaves, the top n levels of the tree contain relatively few nodes. So we have relatively few possibilities to stitch n nodes together, and most reasons to fail will happen while visiting these n levels. If on the other hand our search tree is broadly branching near the root, and narrowly branching near the leaves, the top n levels of the tree contain many nodes. We will visit many nodes and try many stitchings of length n , of which a lot will fail. I have also tried a different, more complicated strategy, which is still implemented, and which can be used by means of an optional argument to S.study() , but results of this strategy were not particularly good. Small-first strategy \u00b6 Here is the small-first strategy in a bit more detail. we choose the smallest yarn to start with; for every qedge we estimate its current spread , i.e. how many targets it has per source on average, relative to the current source and target yarns; at every step there are three kinds of qedges: qedges that go between qnodes of which we have already stitched the yarns qedges that go from a yarn that is already part of the stitch to a yarn outside the stitch qedges that do not start at a yarn in the current stitch at every step, we first process all qedges of type (i), in arbitrary order; we select one edge with minimal spread out of type (ii) and process it; we postpone all edges of type (iii); we redetermine which edges are in all types. It cannot happen that at the end we have not visited all qnodes and yarns, because we have assumed that our search template consists of one connected component. Every qnode can be reached from every other through a series of qedges. So, as we perform step after step, as long as there are qnodes in type (iii), we can be sure that there are also qnodes in a path from the qnodes we have visited to the type (iii) qnodes. At least one of the qnodes in that path will be a type (ii) node. In the end there will no type (iii) nodes be left. We have added a few more things to optimize the process. A relationship between a source yarn and a target yarn can also be considered in the opposite direction. If its spread in the opposite direction is less than its spread in the normal direction, we use the opposite direction. Secondly, before we start stitching, we can compute the order of qedges that we will use for every stitch. We then sort the qnodes according to the order by which they will be encountered when we work through the qedges. When we are stitching, in the midst of a partial stitch, it is always the case that we have stitched qnodes 1 .. n for some n , and we still have to stitch all qnodes above n . That means that when we try to finish partial stitches of which an initial part has been fixed, the search process will not change that initial part of the stitch. Only when the algorithm has exhausted all possibilities based on that initial part, it will change the last node of the initial part, replace it by other options, and start searching further. This means that we just can maintain our partial stitch in a single list. We do not have to assemble many partial stitches as separate immutable tuples. We have implemented our deliver function as a generator, that walks over all stitch possibilities while maintaining just one current stitch. When the stitch has been completely filled in, a copy of it will be yielded, after which back-tracking occurs, by which the current stitch will get partly undefined, only to be filled up again by further searching. Read it all in the source code: def stitchOn(e) .","title":"Search"},{"location":"Model/Search/#search-design","text":"","title":"Search Design"},{"location":"Model/Search/#fabric-metaphor","text":"The search space is a massive fabric of interconnected material. In it we discern the structures we are interested in: little pieces of fabric, also with interconnected material. When we search, we have a fabric in mind, woven from specific material, stitched together in a specific manner. Search in Text-Fabric works exactly like this: you give a sample patch, and Text-Fabric fetches all pieces of the big fabric that match your patch. The textile metaphor is particularly suited for grasping the search part of Text-Fabric, so I'm going to stick to it for a while. I have used it in the actual code as well, and even in the proofs that certain parts of the algorithm terminate and are correct. Yet it remains a metaphor, and the fit is not exact. The basic pattern of search is this: textile text example take several fleeces pick the nodes corresponding to a node type word s, phrase s, clause s, verse s spin thick yarns from them filter by feature conditions part-of-speech=verb gender= f book=Genesis vt spin the yarns further into thin yarns throw away nodes that do not have the right connections feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa stitch the yarns together with thread build results by selecting a member for every filtered node set word node 123456 in phrase node 657890 in clause node 490567 in verse node 1403456 We will explain the stages of the fabrication process in detail.","title":"Fabric metaphor"},{"location":"Model/Search/#fleece","text":"A fleece corresponds with a very simple search template that asks for all objects of a given type: 1 word or 1 clause or, asking for multiple types: 1 2 3 4 5 verse clause phrase lex word Fleeces are the raw material from which we fabricate our search results. Every node type, such as word , sentence , book corresponds to a fleece. In Text-Fabric, every node has exactly one node type, so the whole space is neatly divided into a small set of fleeces. The most important characteristic of a fleece is it size: the number of nodes in a node type.","title":"Fleece"},{"location":"Model/Search/#spinning-thick-yarn","text":"Consider search templates where we ask for specific members of a node type, by giving feature constraints: 1 2 3 4 5 6 verse book=Genesis clause type=rela phrase determined=yes lex id=jc/ word number=pl vt vt Every line in this search templates we call an atom : a node type plus a feature specification. The result of an atom is the set of all nodes in that node type that satisfy those feature conditions. Finding the results of an atom corresponds with first thing that we do with a fleece: spin a thick yarn from it. Yarns in general are obtained by spinning fleeces, i.e. by filtering node sets that correspond to a node type. A search template may contain multiple atoms. Text-Fabric collects all atoms of a template, grabs the corresponding fleeces, and spins thick yarns from them. For each atom it will spin a yarn, and if there are several atoms referring to the same node type, there will be several yarns spun from that fleece. This spinning of thick yarns out of fleeces happens in just one go. All fleeces together contain exactly all nodes, so Text-Fabric walks in one pass over all nodes, applies the feature conditions, and puts the nodes into the yarns depending on which conditions apply. By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical pass requires a fraction of a second. The most important characteristic of a yarn is its thickness , which we define as the number of nodes in the yarn divided by the number of nodes in the fleece. For example, a yarn consisting of the book node of Genesis only, has a thickness of 1/39, because there are 39 books in the fleece. A much thicker yarn is that of the verbs, which has a thickness of roughly \u2159. A very thin thread is that of the word <CQH (which occurs only once) with a thickness of only 1/400,000.","title":"Spinning thick yarn"},{"location":"Model/Search/#spinning-thin-yarns","text":"In order to find results, we have to further narrow down the search space. In other words, we are going to spin our thick yarns into thinner and thinner yarns. Before we can do that, we should make one thing clear.","title":"Spinning thin yarns"},{"location":"Model/Search/#connected-by-constraints","text":"If the template above were complete, it would lead to a monstrous number of results. Because a result of a template like this is any combination of verse-, clause-, phrase-, lex-, word nodes that individually satisfy their own atom condition. So the number of results is the product of the number of results of the individual atoms, which is pretty enormous. It is hard to imagine a situation where these results could be consumed. Usually, there are constraints active between the atoms. For example in a template like this: 1 2 3 4 5 6 7 8 1 verse book=Genesis 2 clause type=rela 3 phrase determined=yes 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l The meaning of this template is that we look for a verse that (1) claims to be in Genesis (2) has a clause whose type is rela (3) which in turn has a phrase of a determined character (4) which contains a word in the plural and that has a verbal tense (vt) There should also be a (6) lex object, identified by jc/ which is connected to the rest by the constraint that (8) the word of line 4 is contained in it. Note that all atoms are linked by constraints into one network. In graph theoretical terms: this template consists of exactly one connected component . If this were not so, we would have in fact two independent search tasks, where the result set would be the (cartesian) product of the result sets of the separate components. For example, if line 8 were missing, we would effectively search for things that match lines 1-4, and, independently, for things that match line 6. And every result of the first part, combined with any result of the second part, would be a valid result of the whole. Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the same time, let alone that it wants to waste time to generate all the results in the product. You will have to fire those search tasks one by one, and it is up to you how you combine the results. The upshot it: the atoms in the search template should form a network, connected by constraints . Text-Fabric will check this, and will only work with search templates that have only one connected component.","title":"Connected by constraints"},{"location":"Model/Search/#terminology","text":"By now we have arrived at the idea that our search template is a graph underneath: what we have called atoms are in fact the nodes, and what we have called constraints , are the edges. From now on, we will call the atoms qnodes and the constraints qedges . The q is to distinguish the nodes and the edges from the nodes and the edges of your dataset, the text nodes and text edges. When we use the term nodes and edges we will always refer to text nodes and edges. When we are searching, we maintain a yarn for every qnode . This yarn starts out to be the thick yarn as described above, but we are going to thin them. We can also see how our query templates are really topographic : a query template is a piece of local geography that we want to match against the data. Finding results is nothing else than instantiating qnodes of the search template by text nodes in such a way that the qedges hold between the text edges.","title":"Terminology"},{"location":"Model/Search/#spinning-a-qedge","text":"So, where were we? We have spun thick threads based on the qnodes individually, but we have not done anything with the qedges . That is going to change now. Consider this piece of search template: 1 2 3 4 5 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l So our qnodes are w and l , and our qedge is w ]] l . Note that a lex object is the set of all occurrences of a lexeme. So w ]] l says that w is embedded in l , in other words, w is a slot contained in the slots of l . It is nothing else than the word w is an instance of the lexeme jc/ . We will now check the pairs of (lex, word)-nodes in the text, where the lex node is taken from the yarn of the qnode l , and the word from the yarn of the qnode w . We can throw away some words from the yarn of w , namely those words that do not lie in a lexeme that is in the yarn of l . In other words: the words that are not instances of lexeme jc/ are out! Conversely, if the lexeme jc/ does not have occurrences in the plural and with a verbal tense, we must kick it out of the yarn of l , leaving no members in it. If a yarn gets empty, we have an early detection that the search yields no results, and the whole process stops. In our case, however, this is not so, and we continue. This pass over the yarns at both sides of a qedge is a spin action. We spin this qedge, and the result is that the two yarns become spun more thinly, hopefully. With the yarn of words severely spun out, we are going to the next qedge, the one between words and phrases. 1 2 3 phrase determined=yes 4 w:word number=pl vt The indent is an implicit way of saying that the \"embeds\" relation [[ holds between the phrase and the word . An equivalent formulation of the template is 1 2 3 4 p:phrase determined=yes w:word number=pl vt p [[ w We race along the yarn w of remaining words and check for each word if it is contained in a phrase in the yarn of p , the determined phrases. If it is not, we throw the word out of the yarn of w . Similarly, we can throw out some phrases from the yarn of p , namely those phrases that do not contain words in the yarn of w . In other words: the phrases without plural words and verbal tense are also out. We continue spinning, now between phrases and clauses. 1 2 2 clause type=rela 3 phrase determined=yes Here we loose the phrases that are not contained in a clause of type=rela , and we loose all clauses that do not embed one of the few phrases left. The last spin action corresponds with 1 2 1 verse book=Genesis 2 clause type=rela So we throw away all our results if they are outside Genesis. We end up with a set of thin yarns, severely thinned out, even. This will be a good starting point for the last stage: picking members from each yarn to form results. We call this stitching and we'll get there in a moment.","title":"Spinning a qedge"},{"location":"Model/Search/#the-spread-of-a-qedge","text":"A very important property of a qedge is its spread . A qedge links every node n in its from -yarn to zero, one, or more nodes in its to -yarn. The number of nodes in the to -yarn is a key property. The average number of nodes m in the to -yarn per linked node n in the from -yarn is the spread of the edge. A few examples: An edge that corresponds to ]] , n embeds m . If this edge goes from books to words, then every book node n is linked to every one of its words. So very n has hundreds or thousands *m*s. The spread will roughly be 425,000 / 39 =~ 10,000 The opposite edge has a spread of exactly 1, because every word belongs to exactly one book. Edges with spread 1 are very pleasant for our stitching algorithm later on. An edge corresponding to = . These qedges are super efficient, because their relation = is a breeze to compute, and they have always a spread 1 in both directions. An edge corresponding to # , the node inequality relation. The relation # is still a breeze to compute, but the result is heavy: the set of all nodes not equal to a given node. The spread is nearly 100% of the yarn length, in both directions. These edges are not worth to spin, because if you have two yarns, no node will be excluded: if you have an n in the from -yarn, you will always be able to find a different n in the to -yarn (except when bot yarns are equal, and contain just one node). An edge corresponding to == , the relation between nodes that are linked to the same set of slots. The spread of this relation is not too big, but the cost of computing it adds up quickly when applied to many cases.","title":"The spread of a qedge"},{"location":"Model/Search/#spinning-all-qedges","text":"Let us describe the spinning of yarns along edges in a bit more general way, and reflect on what it does for us. We spin all qedges of a template. But after spinning a qedge, the yarns involved may have changed. If that is the case, it makes sense to re-spin other qedges that are involved in the changed yarns. That is exactly what we do. We keep spinning, until the yarns have stabilized. A few key questions need to be addressed: Do the yarns stabilize? If they stabilize, what have we got? Is this an efficient process?","title":"Spinning all qedges"},{"location":"Model/Search/#termination-of-spinning","text":"Yes, spinning qedges until nothing changes any more, terminates, provided you do not try to spin qedges that are up-to-date. If the yarns around an edge have not changed, it does not make sense to spin that qedge. See here for proof.","title":"Termination of spinning"},{"location":"Model/Search/#what-have-we-got","text":"After spinning, it is guaranteed that we have not thrown away results. All nodes that are parts of valid results, are still in the yarns. But, conversely, can it be that there are still nodes in the yarns that are not part of a result? Yes, that is possible. Only when the graph of qnodes and qedges does not have a cycle, we know that all members of all yarns occur at least once in a result. See here for proof. Quite a few interesting queries, however, have cycles in in their graphs. So, in those cases, spinning qedges will not cause the maximal narrowing down of the search space.","title":"What have we got?"},{"location":"Model/Search/#efficiency","text":"And that raises the question: how effective is the process of spinning qedges? The answer is: it depends. If your qnodes have strong conditions on them, so that the first yarn is already very thin, then every yarn that is connected to this one by a qedge has also the chance to get very thin after spinning. In this case, the combined filtering effect of all edges can produce a rapid narrowing of the search space. Especially if we can implement edge spinning in an optimized way, this works like a charm. When we come to stitching results (which is potentially very expensive), we have already achieved a massive reduction of work. But if none of the yarns is thin at the outset, spinning qedges will not result in appreciable thinning of the yarns, while it might be an enormous amount of work, depending on the actual relations involved. The good news is that it is possible to detect those situations. Text-Fabric estimates whether it makes sense to spin a qedge, and if not, it will just skip spinning that edge. Which will make the final result gathering (stitching) more expensive. There is more to efficiency than this. It turns out that the strategy by which you select the next qedge to be spun, influences the efficiency. In general, it is best to always start with the thinnest yarns, and select edges that affect them. Also here there is complication: not every qedge is equally expensive when computed over a yarn. It might be better to compute a cheaper edge over a thicker yarn.","title":"Efficiency"},{"location":"Model/Search/#stitching","text":"The last step is actually getting results. A result is a bunch of nodes, one from each yarn, in such a way that result nodes on yarns fulfil the relationships that the qedges of the search template dictate. If we can find such a set of nodes, we have stitched the yarns together. We call such a result a stitch . A stitch is a tuple of text nodes, each corresponding to exactly one qnode. It is not completely trivial to find stitches, let alone to collect them efficiently. The general procedure is as follows: choose a yarn to start with; try a node in that yarn as starting point pick a qedge from the qnode associated with the yarn (the source yarn), to another qnode and consider that yarn (the target yarn), find a node in the target yarn that is in the right relationship with the node selected in the source yarn, and so on, until all qedges have been used, if all has gone well, deliver the nodes found as a result. Let us look to these steps in a bit more detail. There is an element of choice, and it is very important to study how big this element of choice is in the various stages. First we select a yarn, and in that yarn a node. Usually we have many choices and at least one, because result seeking only makes sense if all yarns are non-empty. The third choice is the related node in the target yarn. Here we may encounter anything from zero, one or many choices. If there are zero choices, then we know that our provisional stitching of yarns so far cannot be completed into a full stitching of all yarns. If we have made choices to get this far, then some of these choices have not been lucky. We have to back-track and try other alternatives. If there is just one choice, it is easy: we pick the one and only possible node in the target yarn, without introducing new points of choice. If there are many choices, we have to try them all, one by one. Some might lead to a full stitch, others not. An important situation to be aware of, is when a qedge leads the stitching process to a yarn, out of which a node has already been chosen by an earlier step. This is very well possible, since the search template might have cycles in the qedges, or multiple qedges arrive at the same qnode. When this happens, we do not have to select a target node, we only have to check whether the target node that has been selected before, stands in the right relationship to the current source node. The relationship, that is, which is dictated by the current qedge that we are at. If so, we can stitch on with other edges, without introducing choice points (very much like the one-choice above). If the relation fails to hold, this stitch is doomed, and we have to back-track (very much like the zero-choice above).","title":"Stitching"},{"location":"Model/Search/#strategy-of-stitching","text":"The steps involved in stitching as described above are clear, but less clear is what yarn we shall select to start with, and in which order we shall follow the edges. We need a strategy, and multiple strategies might lead to the same results, albeit with varying efficiency. In Text-Fabric we employ a strategy, that makes the narrowest choices first. We call a choice narrow if there are few alternatives to choose from, and broad if there are many alternatives. By giving precedence to narrow choices, we prune larger parts of the search tree when we fail. If we are stitching, the more nodes we have gathered in our stitch, the greater the chance that a blocking relationship is encountered, i.e. a relationship that should hold between the nodes gathered so far, but which in fact does not hold. So we want to get as many nodes in our stitch as quickly as possible. If our search tree is narrowly branching near the root, and broadly branching near the leaves, the top n levels of the tree contain relatively few nodes. So we have relatively few possibilities to stitch n nodes together, and most reasons to fail will happen while visiting these n levels. If on the other hand our search tree is broadly branching near the root, and narrowly branching near the leaves, the top n levels of the tree contain many nodes. We will visit many nodes and try many stitchings of length n , of which a lot will fail. I have also tried a different, more complicated strategy, which is still implemented, and which can be used by means of an optional argument to S.study() , but results of this strategy were not particularly good.","title":"Strategy of stitching"},{"location":"Model/Search/#small-first-strategy","text":"Here is the small-first strategy in a bit more detail. we choose the smallest yarn to start with; for every qedge we estimate its current spread , i.e. how many targets it has per source on average, relative to the current source and target yarns; at every step there are three kinds of qedges: qedges that go between qnodes of which we have already stitched the yarns qedges that go from a yarn that is already part of the stitch to a yarn outside the stitch qedges that do not start at a yarn in the current stitch at every step, we first process all qedges of type (i), in arbitrary order; we select one edge with minimal spread out of type (ii) and process it; we postpone all edges of type (iii); we redetermine which edges are in all types. It cannot happen that at the end we have not visited all qnodes and yarns, because we have assumed that our search template consists of one connected component. Every qnode can be reached from every other through a series of qedges. So, as we perform step after step, as long as there are qnodes in type (iii), we can be sure that there are also qnodes in a path from the qnodes we have visited to the type (iii) qnodes. At least one of the qnodes in that path will be a type (ii) node. In the end there will no type (iii) nodes be left. We have added a few more things to optimize the process. A relationship between a source yarn and a target yarn can also be considered in the opposite direction. If its spread in the opposite direction is less than its spread in the normal direction, we use the opposite direction. Secondly, before we start stitching, we can compute the order of qedges that we will use for every stitch. We then sort the qnodes according to the order by which they will be encountered when we work through the qedges. When we are stitching, in the midst of a partial stitch, it is always the case that we have stitched qnodes 1 .. n for some n , and we still have to stitch all qnodes above n . That means that when we try to finish partial stitches of which an initial part has been fixed, the search process will not change that initial part of the stitch. Only when the algorithm has exhausted all possibilities based on that initial part, it will change the last node of the initial part, replace it by other options, and start searching further. This means that we just can maintain our partial stitch in a single list. We do not have to assemble many partial stitches as separate immutable tuples. We have implemented our deliver function as a generator, that walks over all stitch possibilities while maintaining just one current stitch. When the stitch has been completely filled in, a copy of it will be yielded, after which back-tracking occurs, by which the current stitch will get partly undefined, only to be filled up again by further searching. Read it all in the source code: def stitchOn(e) .","title":"Small-first strategy"},{"location":"Server/Common/","text":"Common Server Related Functions \u00b6 About \u00b6 About Here are functions that are being used by various parts of the TF browser infrastructure, such as kernel.py web.py start.py Argument parsing \u00b6 Apologies Really, we should replace the whole adhoc argument parsing by a decent use of the Python module argparse . Specific args The following table shows functions that are responsible for detecting a specific command line argument. There signature is 1 def getXxx ( cargs = sys . argv ) so they will take the script command line args by default, but you can also pass an other set of arguments instead. function argument getCheckout --checkout= getDebug -d getDocker -docker getModules --mod= ... getNoweb -noweb getSets --sets= ... getParam(interactive=False) Checks whether a dataSource parameter has been passed on the command line. If so, it checks whether it specifies an existing app. If no dataSource has been passed, and interactive is true, presents the user with a list of valid choices and asks for input. Locating the app \u00b6 The problem The data source specific apps are bundled inside the TF package. The web server of the TF browser needs the files in those apps, not as Python modules, but just as files on disk. So we have to tell the web server where they are, and we really do not know that in advance, because it is dependent on how the text-fabric package has been installed by pip3 on your machine. Yet we have found a way through the labyrinth! getAppdir(myDir, dataSource) The code in web.py will pass its file location as myDir . Form there this function computes the locstion of the file in which the webapp of the dataSource resides: the location of the dataSource package in apps . See also App structure Getting and setting form values \u00b6 Request and response The TF browser user interacts with the web app by clicking and typing, as a result of which a HTML form gets filled in. This form as regularly submitted to the web server with a request for a new incarnation of the page: a response. The values that come with a request, must be peeled out of the form, and stored as logical values. Most of the data has a known function to the web server, but there is also a list of webapp dependent options. The following functions deal with option values. getValues(options, form) Given a tuple of option specifications and form data from a web request, returns a dictionary of filled in values for those options. The options are specified in the config.py of an app. An option specification is a tuple of the following bits of information: name of the input element in the HTML form type of input (e.g. checkbox) value of html id attribute of the input element label for the input element Uruk options The options for the C, such as the phono transcriptions, you can sayunei app are: 1 2 3 4 options = ( ( 'lineart' , 'checkbox' , 'linea' , 'show lineart' ), ( 'lineNumbers' , 'checkbox' , 'linen' , 'show line numbers' ), ) This function isolates the option values from the rest of the form values, so that it can be passed as a whole ( **values ) to the app specific API. setValues(options, source, form) Fills in a form dictionary based on values in a source dictionary, but only insofar the keys to be filled out occur in the options specs, and with a cast of checkbox values to booleans. This function is used right after reading the form off a request. Raw form data is turned into logical data for further processing by the web server. HTML formatting \u00b6 HTML generation Here we generate the HTML for bigger chunks on the page. pageLinks(nResults, position, spread=10) Provide navigation links for results sets, big or small. It creates links around position in a set of nResults . The spread indicates how many links before and after position are generated in each column. There will be multiple columns. The right most column contains links to results position - spread to position + spread . Left of that there is a column for results position - spread*spread to position + spread*spread , stepping by spread . And so on, until the stepping factor becomes bigger than the result set. passageLinks(passages, sec0, sec1) Provide navigation links for passages, in the form of links to sections of level 0, 1 and 2 (books, chapters and verses). If sec0 is not given, only a list of sec0 links is produced. If sec0 is given, but sec1 not, a list of links for sec1s within the given sec0 is produced. If both sec0 and sec1 are given, de sec1 entry is focused. shapeOptions(options, values) Wraps the options, specified by the option specification in config.py into HTML. See also App structure shapeCondense(condenseTypes, value) Provides a radio-buttoned chooser for the condense types . value is the currently chosen option. shapeFormats(textFormats, value) Provides a radio-buttoned chooser for the text formats . value is the currently chosen option.","title":"Common"},{"location":"Server/Common/#common-server-related-functions","text":"","title":"Common Server Related Functions"},{"location":"Server/Common/#about","text":"About Here are functions that are being used by various parts of the TF browser infrastructure, such as kernel.py web.py start.py","title":"About"},{"location":"Server/Common/#argument-parsing","text":"Apologies Really, we should replace the whole adhoc argument parsing by a decent use of the Python module argparse . Specific args The following table shows functions that are responsible for detecting a specific command line argument. There signature is 1 def getXxx ( cargs = sys . argv ) so they will take the script command line args by default, but you can also pass an other set of arguments instead. function argument getCheckout --checkout= getDebug -d getDocker -docker getModules --mod= ... getNoweb -noweb getSets --sets= ... getParam(interactive=False) Checks whether a dataSource parameter has been passed on the command line. If so, it checks whether it specifies an existing app. If no dataSource has been passed, and interactive is true, presents the user with a list of valid choices and asks for input.","title":"Argument parsing"},{"location":"Server/Common/#locating-the-app","text":"The problem The data source specific apps are bundled inside the TF package. The web server of the TF browser needs the files in those apps, not as Python modules, but just as files on disk. So we have to tell the web server where they are, and we really do not know that in advance, because it is dependent on how the text-fabric package has been installed by pip3 on your machine. Yet we have found a way through the labyrinth! getAppdir(myDir, dataSource) The code in web.py will pass its file location as myDir . Form there this function computes the locstion of the file in which the webapp of the dataSource resides: the location of the dataSource package in apps . See also App structure","title":"Locating the app"},{"location":"Server/Common/#getting-and-setting-form-values","text":"Request and response The TF browser user interacts with the web app by clicking and typing, as a result of which a HTML form gets filled in. This form as regularly submitted to the web server with a request for a new incarnation of the page: a response. The values that come with a request, must be peeled out of the form, and stored as logical values. Most of the data has a known function to the web server, but there is also a list of webapp dependent options. The following functions deal with option values. getValues(options, form) Given a tuple of option specifications and form data from a web request, returns a dictionary of filled in values for those options. The options are specified in the config.py of an app. An option specification is a tuple of the following bits of information: name of the input element in the HTML form type of input (e.g. checkbox) value of html id attribute of the input element label for the input element Uruk options The options for the C, such as the phono transcriptions, you can sayunei app are: 1 2 3 4 options = ( ( 'lineart' , 'checkbox' , 'linea' , 'show lineart' ), ( 'lineNumbers' , 'checkbox' , 'linen' , 'show line numbers' ), ) This function isolates the option values from the rest of the form values, so that it can be passed as a whole ( **values ) to the app specific API. setValues(options, source, form) Fills in a form dictionary based on values in a source dictionary, but only insofar the keys to be filled out occur in the options specs, and with a cast of checkbox values to booleans. This function is used right after reading the form off a request. Raw form data is turned into logical data for further processing by the web server.","title":"Getting and setting form values"},{"location":"Server/Common/#html-formatting","text":"HTML generation Here we generate the HTML for bigger chunks on the page. pageLinks(nResults, position, spread=10) Provide navigation links for results sets, big or small. It creates links around position in a set of nResults . The spread indicates how many links before and after position are generated in each column. There will be multiple columns. The right most column contains links to results position - spread to position + spread . Left of that there is a column for results position - spread*spread to position + spread*spread , stepping by spread . And so on, until the stepping factor becomes bigger than the result set. passageLinks(passages, sec0, sec1) Provide navigation links for passages, in the form of links to sections of level 0, 1 and 2 (books, chapters and verses). If sec0 is not given, only a list of sec0 links is produced. If sec0 is given, but sec1 not, a list of links for sec1s within the given sec0 is produced. If both sec0 and sec1 are given, de sec1 entry is focused. shapeOptions(options, values) Wraps the options, specified by the option specification in config.py into HTML. See also App structure shapeCondense(condenseTypes, value) Provides a radio-buttoned chooser for the condense types . value is the currently chosen option. shapeFormats(textFormats, value) Provides a radio-buttoned chooser for the text formats . value is the currently chosen option.","title":"HTML formatting"},{"location":"Server/Kernel/","text":"Text-Fabric kernel \u00b6 About \u00b6 TF kernel Text-Fabric can be used as a service. The full API of Text-Fabric needs a lot of memory, which makes it unusably for rapid successions of loading and unloading, like when used in a web server context. However, you can start TF as a service process, after which many clients can connect to it, all looking at the same (read-only) data. We call this a TF kernel . The API that the TF kernel offers is limited, it is primarily template search that is offered. see Kernel API below. The code in kernel explains how it works. Start \u00b6 Run You can run the TF kernel as follows: 1 python3 -m tf.server.kernel ddd where ddd is one of the supported apps Example See the start-up script of the text-fabric browser. Connect \u00b6 Connect The TF kernel can be connected by an other Python program as follows: 1 2 3 from tf.server.kernel import makeTfConnection TF = makeTfConnection ( host , port ) api = TF . connect () After this, api can be used to obtain information from the TF kernel. Example See the web server of the text-fabric browser. Kernel API \u00b6 About The API of the TF kernel is created by the function makeTfKernel in the kernel module of the server subpackage. It returns a class TfKernel with a number of exposed methods that can be called by other programs. For the machinery of interprocess communication we rely on the rpyc module. See especially the docs on services . Shadow objects The way rpyc works in the case of data transmission has a pitfall. When a service returns a Python object to the client, it does not return the object itself, but only a shadow object so called netref objects. This strategy is called boxing . To the client the shadow object looks like the real thing, but when the client needs to access members, they will be fetched on the fly. This is a performance problem when the service sends a big list or dict, and the client iterates over all its items. Each item will be fetched in a separate interprocess call, which causes an enormous overhead. Boxing only happens for mutable objects. And here lies the work-around: The service must send big chunks of data as immutable objects, such as tuples. They are sent within a single interprocess call, and fly swiftly through the connecting pipe. monitor() A utility function that spits out some information from the kernel to the outside work. At this moment it is only used for debugging, but later it can be useful to monitor the kernel or manage it while it remains running. header() Calls the header() method of the app, which fetches all the stuff to create a header on the page with links to data and documentation of the data source. provenance() Calls the provenance() method of the app, which fetches provenance metadata to be shown on exported pages. setNames() Returns the names of sets that have been provided as custom sets to the kernel by means of the --sets= command line argument with which the kernel was started. A web server kan use this informatiomn to write out provernance info. css() Calls the loadCSS() method of the app, which delivers the CSS code to be inserted on the browser page. condenseTypes() Fetches several things from the app and the generic TF api: condenseType : the default node type that acts as a container for representing query results; for Bhsa it is verse , for Uruk it is tablet ; exampleSection : an example for the help text for this data source; levels : information about the node types in this data source. passage() Asks the kernel for passages (chunks of material corresponding to sections of level 1 (chapters). The material will be displayed as a sequence of plain representations of the sec2s, which can be expanded to pretty displays when the user chooses to do so. Parameters: sec0 The level 0 section (book) in which the passage occurs sec1 The level 1 section (chapter) to fetch sec2=None The level 2 section (verse) that should get focus opened The set of items that are currently expanded into pretty display features The features that should be displayed in pretty displays when expanding a plain representation of a sec2 into a pretty display query The query whose results should be highlighted in the passage display. getx If given, only a single sec2 (verse) will be fetched, but in pretty display. getx is the identifier (section label, verse number) of the item/ **options Additional, optional display options table() Fetches material corresponding to a list of sections or tuples of nodes. Parameters: kind Either sections or tuples : whether to find section material or tuple material; task The list of things (sections or tuples) to retrieve the material for; Typically coming from the section pad / node pad in the browser. opened=set() As in passage() features As in passage() getx As in passage() **options As in passage() search() The work horse of this API. Executes a TF search template, retrieves formatted results, retrieves formatted results for additional nodes and sections. Parameters: query Search template to be executed. Typically coming from the search pad in the browser. batch The number of table rows to show on one page in the browser. position=1 The position that is central in the browser. The navigation links take this position as the focus point, and enable the user to navigate to neighbouring results, in ever bigger strides. opened=set() Which results have been expanded and need extended results. Normally, only the information to provide a plain representation of a result is being fetched, but for the opened ones information is gathered for pretty displays. getx If given, only a single result will be fetched, but in pretty display. getx is the sequence number of the result. condensed Whether or not the results should be condensed . Normally, results come as tuples of nodes, and each tuple is shown in a corresponding table row in plain or pretty display. But you can also condense results in container objects. All tuples will be inspected, and the nodes of each tuple will be gathered in containers, and these containers will be displayed in table rows. What is lost is the notion of an individual result, and what is gained is a better overview of where the parts of the results are. condenseType When condensing results, you can choose the node type that acts as container. Nodes get suppressed Nodes in result tuples that have a type that is bigger than the condenseType, will be skipped. E.g. if you have chapter nodes in your results, but you condense to verses, the chapter nodes will not show up. But if you condense to books, they will show up. withNodes=False Whether to include the node numbers into the formatted results. linked=1 Which column in the results should be hyperlinked to online representations closest to the objects in that column. Counting columns starts at 1. options Additional keyword arguments are passed as options to the underlying API. For example, the Uruk API accepts linenumbers and lineart , which will ask to include line numbers and lineart in the formatted results. csvs() This is an other workhorse. It also asks for the things search() is asking for, but it does not want formatted results. It will get tabular data of result nodes, one for the sections , one for the node tuples , and one for the search results . For every node that occurs in this tabular data, features will be looked up. All loaded features will be looked up for those nodes. The result is a big table of nodes and feature values. The parameters are query , tuples , sections , condensed , condenseType and have the same meaning as in search() above.","title":"Kernel"},{"location":"Server/Kernel/#text-fabric-kernel","text":"","title":"Text-Fabric kernel"},{"location":"Server/Kernel/#about","text":"TF kernel Text-Fabric can be used as a service. The full API of Text-Fabric needs a lot of memory, which makes it unusably for rapid successions of loading and unloading, like when used in a web server context. However, you can start TF as a service process, after which many clients can connect to it, all looking at the same (read-only) data. We call this a TF kernel . The API that the TF kernel offers is limited, it is primarily template search that is offered. see Kernel API below. The code in kernel explains how it works.","title":"About"},{"location":"Server/Kernel/#start","text":"Run You can run the TF kernel as follows: 1 python3 -m tf.server.kernel ddd where ddd is one of the supported apps Example See the start-up script of the text-fabric browser.","title":"Start"},{"location":"Server/Kernel/#connect","text":"Connect The TF kernel can be connected by an other Python program as follows: 1 2 3 from tf.server.kernel import makeTfConnection TF = makeTfConnection ( host , port ) api = TF . connect () After this, api can be used to obtain information from the TF kernel. Example See the web server of the text-fabric browser.","title":"Connect"},{"location":"Server/Kernel/#kernel-api","text":"About The API of the TF kernel is created by the function makeTfKernel in the kernel module of the server subpackage. It returns a class TfKernel with a number of exposed methods that can be called by other programs. For the machinery of interprocess communication we rely on the rpyc module. See especially the docs on services . Shadow objects The way rpyc works in the case of data transmission has a pitfall. When a service returns a Python object to the client, it does not return the object itself, but only a shadow object so called netref objects. This strategy is called boxing . To the client the shadow object looks like the real thing, but when the client needs to access members, they will be fetched on the fly. This is a performance problem when the service sends a big list or dict, and the client iterates over all its items. Each item will be fetched in a separate interprocess call, which causes an enormous overhead. Boxing only happens for mutable objects. And here lies the work-around: The service must send big chunks of data as immutable objects, such as tuples. They are sent within a single interprocess call, and fly swiftly through the connecting pipe. monitor() A utility function that spits out some information from the kernel to the outside work. At this moment it is only used for debugging, but later it can be useful to monitor the kernel or manage it while it remains running. header() Calls the header() method of the app, which fetches all the stuff to create a header on the page with links to data and documentation of the data source. provenance() Calls the provenance() method of the app, which fetches provenance metadata to be shown on exported pages. setNames() Returns the names of sets that have been provided as custom sets to the kernel by means of the --sets= command line argument with which the kernel was started. A web server kan use this informatiomn to write out provernance info. css() Calls the loadCSS() method of the app, which delivers the CSS code to be inserted on the browser page. condenseTypes() Fetches several things from the app and the generic TF api: condenseType : the default node type that acts as a container for representing query results; for Bhsa it is verse , for Uruk it is tablet ; exampleSection : an example for the help text for this data source; levels : information about the node types in this data source. passage() Asks the kernel for passages (chunks of material corresponding to sections of level 1 (chapters). The material will be displayed as a sequence of plain representations of the sec2s, which can be expanded to pretty displays when the user chooses to do so. Parameters: sec0 The level 0 section (book) in which the passage occurs sec1 The level 1 section (chapter) to fetch sec2=None The level 2 section (verse) that should get focus opened The set of items that are currently expanded into pretty display features The features that should be displayed in pretty displays when expanding a plain representation of a sec2 into a pretty display query The query whose results should be highlighted in the passage display. getx If given, only a single sec2 (verse) will be fetched, but in pretty display. getx is the identifier (section label, verse number) of the item/ **options Additional, optional display options table() Fetches material corresponding to a list of sections or tuples of nodes. Parameters: kind Either sections or tuples : whether to find section material or tuple material; task The list of things (sections or tuples) to retrieve the material for; Typically coming from the section pad / node pad in the browser. opened=set() As in passage() features As in passage() getx As in passage() **options As in passage() search() The work horse of this API. Executes a TF search template, retrieves formatted results, retrieves formatted results for additional nodes and sections. Parameters: query Search template to be executed. Typically coming from the search pad in the browser. batch The number of table rows to show on one page in the browser. position=1 The position that is central in the browser. The navigation links take this position as the focus point, and enable the user to navigate to neighbouring results, in ever bigger strides. opened=set() Which results have been expanded and need extended results. Normally, only the information to provide a plain representation of a result is being fetched, but for the opened ones information is gathered for pretty displays. getx If given, only a single result will be fetched, but in pretty display. getx is the sequence number of the result. condensed Whether or not the results should be condensed . Normally, results come as tuples of nodes, and each tuple is shown in a corresponding table row in plain or pretty display. But you can also condense results in container objects. All tuples will be inspected, and the nodes of each tuple will be gathered in containers, and these containers will be displayed in table rows. What is lost is the notion of an individual result, and what is gained is a better overview of where the parts of the results are. condenseType When condensing results, you can choose the node type that acts as container. Nodes get suppressed Nodes in result tuples that have a type that is bigger than the condenseType, will be skipped. E.g. if you have chapter nodes in your results, but you condense to verses, the chapter nodes will not show up. But if you condense to books, they will show up. withNodes=False Whether to include the node numbers into the formatted results. linked=1 Which column in the results should be hyperlinked to online representations closest to the objects in that column. Counting columns starts at 1. options Additional keyword arguments are passed as options to the underlying API. For example, the Uruk API accepts linenumbers and lineart , which will ask to include line numbers and lineart in the formatted results. csvs() This is an other workhorse. It also asks for the things search() is asking for, but it does not want formatted results. It will get tabular data of result nodes, one for the sections , one for the node tuples , and one for the search results . For every node that occurs in this tabular data, features will be looked up. All loaded features will be looked up for those nodes. The result is a big table of nodes and feature values. The parameters are query , tuples , sections , condensed , condenseType and have the same meaning as in search() above.","title":"Kernel API"},{"location":"Server/Web/","text":"Web interface \u00b6 About \u00b6 Web interface TF contains a web interface in which you can enter a search template and view the results. This is realized by a web app based on Flask . This web app connects to the TF kernel and merges the retrieved data into a set of templates . See the code in web . Start up \u00b6 Start up TF kernel, web server and browser page are started up by means of a script called text-fabric , which will be installed in an executable directory by the pip installer. What the script does is the same as: 1 python3 -m tf.server.start Process management During start up the following happens: Kill previous processes The system is searched for non-terminated incarnations of the processes it wants to start up. If they are encountered, they will be killed, so that they cannot prevent a successful start up. TF kernel A TF kernel is started. This process loads the bulk of the TF data, so it can take a while. When it has loaded the data, it sends out a message that loading is done, which is picked up by the script. TF web server A short while after receiving the \"data loading done\" message, the TF web server is started. Debug mode If you have passed -d to the text-fabric script, the Flask web server will be started in debug and reload mode. That means that if you modify web.py or a module it imports, the web server will reload itself automatically. When you refresh the browser you see the changes. If you have changed templates, the css, or the javascript, you should do a \"refresh from origin\". Load web page After a short while, the default web browser will be started with a url and port at which the web server will listen. You see your browser being started up and the TF page being loaded. Waiting The script now waits till the web server is finished. You finish it by pressing Ctrl-C, and if you have used the -d flag, you have to press it twice. Terminate the TF kernel At this point, the text-fabric script will terminate the TF kernel. Clean up Now all processes that have started up have been killed. If something went wrong in this sequence, chances are that a process keeps running. It will be terminated next time you call the text-fabric . You can kill too If you run 1 text-fabric -k all tf-browser-related processes will be killed. 1 text-fabric -k ddd will kill all such processes as far as they are for data source ddd . Routes \u00b6 Routes There are 4 kinds of routes in the web app: url pattern effect /server/static/... serves a static file from the server-wide static folder /data/static/... serves a static file from the app specific static folder /local/static/... serves a static file from a local directory specified by the app anything else submits the form with user data and return the processed request Templates \u00b6 Templates There are two templates in views : index : the normal template for returning responses to user requests; export : the template used for exporting results; it has printer/PDF-friendly formatting: good page breaks. Pretty displays always occur on a page by their own. It has very few user interaction controls. When saved as PDF from the browser, it is a neat record of work done, with DOI links to the corpus and to Text-Fabric. CSS \u00b6 CSS We format the web pages with CSS, with extensive use of flexbox . There are several sources of CSS formatting: the CSS loaded from the app dependent extraApi, used for pretty displays; index.css : the formatting of the index web page with which the user interacts; export.css the formatting of the export page; base.css shared formatting between the index and export pages. Javascript \u00b6 Javascript We use a modest amount of Javascript on top of JQuery . For collapsing and expanding elements we use the details element. This is a convenient, Javascript-free way to manage collapsing. Unfortunately it is not supported by the Microsoft browsers, not even Edge. On Windows? Windows users should install Chrome of Firefox.","title":"Web"},{"location":"Server/Web/#web-interface","text":"","title":"Web interface"},{"location":"Server/Web/#about","text":"Web interface TF contains a web interface in which you can enter a search template and view the results. This is realized by a web app based on Flask . This web app connects to the TF kernel and merges the retrieved data into a set of templates . See the code in web .","title":"About"},{"location":"Server/Web/#start-up","text":"Start up TF kernel, web server and browser page are started up by means of a script called text-fabric , which will be installed in an executable directory by the pip installer. What the script does is the same as: 1 python3 -m tf.server.start Process management During start up the following happens: Kill previous processes The system is searched for non-terminated incarnations of the processes it wants to start up. If they are encountered, they will be killed, so that they cannot prevent a successful start up. TF kernel A TF kernel is started. This process loads the bulk of the TF data, so it can take a while. When it has loaded the data, it sends out a message that loading is done, which is picked up by the script. TF web server A short while after receiving the \"data loading done\" message, the TF web server is started. Debug mode If you have passed -d to the text-fabric script, the Flask web server will be started in debug and reload mode. That means that if you modify web.py or a module it imports, the web server will reload itself automatically. When you refresh the browser you see the changes. If you have changed templates, the css, or the javascript, you should do a \"refresh from origin\". Load web page After a short while, the default web browser will be started with a url and port at which the web server will listen. You see your browser being started up and the TF page being loaded. Waiting The script now waits till the web server is finished. You finish it by pressing Ctrl-C, and if you have used the -d flag, you have to press it twice. Terminate the TF kernel At this point, the text-fabric script will terminate the TF kernel. Clean up Now all processes that have started up have been killed. If something went wrong in this sequence, chances are that a process keeps running. It will be terminated next time you call the text-fabric . You can kill too If you run 1 text-fabric -k all tf-browser-related processes will be killed. 1 text-fabric -k ddd will kill all such processes as far as they are for data source ddd .","title":"Start up"},{"location":"Server/Web/#routes","text":"Routes There are 4 kinds of routes in the web app: url pattern effect /server/static/... serves a static file from the server-wide static folder /data/static/... serves a static file from the app specific static folder /local/static/... serves a static file from a local directory specified by the app anything else submits the form with user data and return the processed request","title":"Routes"},{"location":"Server/Web/#templates","text":"Templates There are two templates in views : index : the normal template for returning responses to user requests; export : the template used for exporting results; it has printer/PDF-friendly formatting: good page breaks. Pretty displays always occur on a page by their own. It has very few user interaction controls. When saved as PDF from the browser, it is a neat record of work done, with DOI links to the corpus and to Text-Fabric.","title":"Templates"},{"location":"Server/Web/#css","text":"CSS We format the web pages with CSS, with extensive use of flexbox . There are several sources of CSS formatting: the CSS loaded from the app dependent extraApi, used for pretty displays; index.css : the formatting of the index web page with which the user interacts; export.css the formatting of the export page; base.css shared formatting between the index and export pages.","title":"CSS"},{"location":"Server/Web/#javascript","text":"Javascript We use a modest amount of Javascript on top of JQuery . For collapsing and expanding elements we use the details element. This is a convenient, Javascript-free way to manage collapsing. Unfortunately it is not supported by the Microsoft browsers, not even Edge. On Windows? Windows users should install Chrome of Firefox.","title":"Javascript"},{"location":"Use/Browser/","text":"Browser \u00b6 TF in the browser The Text-Fabric package contains a command to work with your corpus in your browser. It sets up a local web server, which interacts with your web browser. Then you can view and search the corpus without programming and without internet connection. For some corpora an internet website has been set up. You can then work with that corpus without installing anything on your computer, but you do need an internet connection. In both cases the interface is the same. However, if your work locally, you can easily import extra data, made by yourself or by other people. Start up \u00b6 apps Below, when you see app , you have to substitute it by the name of an existing TF app. The Text-Fabric browser fetches the apps and corpora it needs from GitHub automatically. More about corpora On Windows? You can click the Start Menu, and type 1 text-fabric app in the search box, and then Enter. On Linux or Macos? You can open a terminal (command prompt), and just say 1 text-fabric app All platforms The corpus data will be downloaded automatically, and be loaded into text-fabric. Then your browser will open and load the search interface. There you'll find links to further help. More data \u00b6 About You can let TF use extra features: 1 2 3 text-fabric app --mod = org/repo/path text-fabric app --mod = org/repo/path -c text-fabric app --mod = org/repo/path,org/repo/path Here org , repo and path must be replaced with a github user or organization, a github repo, and a path within that repo. Read more about your data life-cycle in the Data guide. Custom sets \u00b6 About You can create custom sets of nodes, give them a name, and use those names in search templates. The TF browser can import those sets, so that you can use such queries in the browser too. Invoke 1 text-fabric app --sets = filePath Start a TF browser for app . Loads custom sets from filePath . filePath must specify a file on your local system (you may use ~ for your home directory). That file must have been written by calling tf.lib.writeSets . If so,it contains a dictionary of named node sets. These names can be used in search templates, and the TF browser will use this dictionary to resolve those names. See S.search() sets argument . Jobs \u00b6 Saving your session Your session (aka job ) will be saved in your browser, under the name app -default , or another name if you rename, duplicate, import or create new sessions. Multiple windows After you have issued the text-fabric command, a TF kernel is started for you. This is a process that holds all the data and can deliver it to other processes, such as your web browser. As long as you leave the TF kernel on, you have instant access to your corpus. You can open other browsers and windows and tabs with the same url, and they will load quickly, without the long wait you experienced when the TF kernel was loading. Shut down \u00b6 About You can close the TF kernel and web server by pressing Ctrl-C in the terminal or command prompt where you have started text-fabric . Clean up Before starting up, the TF browser will check if there are no running processes left from an earlier run. If so, it will kill them. You can also manually clean up yourself: text-fabric app -k Or, if you have also processes running for other apps: text-fabric -k Work with exported results \u00b6 About You can export your results to CSV files which you can process with various tools, including your own. You can use the \"Export\" tab to tell the story behind your query and then export your view. A new page will open, which you can save as a PDF. There is also a button to download all your results as data files. Exported materials job.json A json file with all information associated with your current session. You can import this in the Jobs section, and restore the session by which you created these results. about.md a Markdown file with your description and some provenance metadata. resultsx.tsv contains your precise search results, decorated with the features you have used in your searchTemplate. Not only the results on the current page, but all results. results.tsv contains your precise search results, as a list of tuples of nodes. Not only the results on the current page, but all results. sections.tsv contains the sections you have selected as a list of nodes. nodes.tsv contains the nodes you have selected as a list of tuples of nodes. Now, if you want to share your results for checking and replication, put all this in a research repository or in a GitHub repository, which you can then archive to ZENODO to obtain a DOI. Unicode in Excel CSVs The file resultsx.tsv is not in the usual utf8 encoding, but in utf_16 encoding. The reason is that this is the only encoding in which Excel handles CSV files properly. So if you work with this file in Python, specify the encoding utf_16 . 1 2 3 with open ( 'resultsx.tsv' , encoding = 'utf_16' ) as fh : for row in fh : # do something with row Conversely, if you want to write a CSV with Hebrew in it, to be opened in Excel, take care to: give the file name extension .tsv (not .csv ) make the file tab separated (do not use the comma or semicolon!) use the encoding utf_16_le (not merely utf_16 , nor utf8 !) start the file with a BOM mark. 1 2 3 4 5 with open ( 'mydata.tsv' , 'w' , encoding = 'utf_16_le' ) as fh : fh . write ( ' \\uFEFF ' ) for row in myData : fh . write ( ' \\t ' . join ( row )) fh . write ( ' \\n ' ) Gory details The file has been written with the utf_16_le encoding, and the first character is the unicode FEFF character. That is needed for machines so that they can see which byte in a 16 bits word is the least end ( le ) and which one is the big end ( be ). Knowing that the first character is FEFF, all machines can see whether this is in a least-endian (le) encoding or in a big-endian (be) . Hence this character is called the Byte Order Mark (BOM). See more on wikipedia . When reading a file with encoding utf_16 , Python reads the BOM, draws its conclusions, and strips the BOM. So when you iterate over its lines, you will not see the BOM, which is good. But when you read a file with encoding utf_16_le , Python passes the BOM through, and you have to skip it yourself. That is unpleasant. Hence, use utf_16 for reading.","title":"Browser"},{"location":"Use/Browser/#browser","text":"TF in the browser The Text-Fabric package contains a command to work with your corpus in your browser. It sets up a local web server, which interacts with your web browser. Then you can view and search the corpus without programming and without internet connection. For some corpora an internet website has been set up. You can then work with that corpus without installing anything on your computer, but you do need an internet connection. In both cases the interface is the same. However, if your work locally, you can easily import extra data, made by yourself or by other people.","title":"Browser"},{"location":"Use/Browser/#start-up","text":"apps Below, when you see app , you have to substitute it by the name of an existing TF app. The Text-Fabric browser fetches the apps and corpora it needs from GitHub automatically. More about corpora On Windows? You can click the Start Menu, and type 1 text-fabric app in the search box, and then Enter. On Linux or Macos? You can open a terminal (command prompt), and just say 1 text-fabric app All platforms The corpus data will be downloaded automatically, and be loaded into text-fabric. Then your browser will open and load the search interface. There you'll find links to further help.","title":"Start up"},{"location":"Use/Browser/#more-data","text":"About You can let TF use extra features: 1 2 3 text-fabric app --mod = org/repo/path text-fabric app --mod = org/repo/path -c text-fabric app --mod = org/repo/path,org/repo/path Here org , repo and path must be replaced with a github user or organization, a github repo, and a path within that repo. Read more about your data life-cycle in the Data guide.","title":"More data"},{"location":"Use/Browser/#custom-sets","text":"About You can create custom sets of nodes, give them a name, and use those names in search templates. The TF browser can import those sets, so that you can use such queries in the browser too. Invoke 1 text-fabric app --sets = filePath Start a TF browser for app . Loads custom sets from filePath . filePath must specify a file on your local system (you may use ~ for your home directory). That file must have been written by calling tf.lib.writeSets . If so,it contains a dictionary of named node sets. These names can be used in search templates, and the TF browser will use this dictionary to resolve those names. See S.search() sets argument .","title":"Custom sets"},{"location":"Use/Browser/#jobs","text":"Saving your session Your session (aka job ) will be saved in your browser, under the name app -default , or another name if you rename, duplicate, import or create new sessions. Multiple windows After you have issued the text-fabric command, a TF kernel is started for you. This is a process that holds all the data and can deliver it to other processes, such as your web browser. As long as you leave the TF kernel on, you have instant access to your corpus. You can open other browsers and windows and tabs with the same url, and they will load quickly, without the long wait you experienced when the TF kernel was loading.","title":"Jobs"},{"location":"Use/Browser/#shut-down","text":"About You can close the TF kernel and web server by pressing Ctrl-C in the terminal or command prompt where you have started text-fabric . Clean up Before starting up, the TF browser will check if there are no running processes left from an earlier run. If so, it will kill them. You can also manually clean up yourself: text-fabric app -k Or, if you have also processes running for other apps: text-fabric -k","title":"Shut down"},{"location":"Use/Browser/#work-with-exported-results","text":"About You can export your results to CSV files which you can process with various tools, including your own. You can use the \"Export\" tab to tell the story behind your query and then export your view. A new page will open, which you can save as a PDF. There is also a button to download all your results as data files. Exported materials job.json A json file with all information associated with your current session. You can import this in the Jobs section, and restore the session by which you created these results. about.md a Markdown file with your description and some provenance metadata. resultsx.tsv contains your precise search results, decorated with the features you have used in your searchTemplate. Not only the results on the current page, but all results. results.tsv contains your precise search results, as a list of tuples of nodes. Not only the results on the current page, but all results. sections.tsv contains the sections you have selected as a list of nodes. nodes.tsv contains the nodes you have selected as a list of tuples of nodes. Now, if you want to share your results for checking and replication, put all this in a research repository or in a GitHub repository, which you can then archive to ZENODO to obtain a DOI. Unicode in Excel CSVs The file resultsx.tsv is not in the usual utf8 encoding, but in utf_16 encoding. The reason is that this is the only encoding in which Excel handles CSV files properly. So if you work with this file in Python, specify the encoding utf_16 . 1 2 3 with open ( 'resultsx.tsv' , encoding = 'utf_16' ) as fh : for row in fh : # do something with row Conversely, if you want to write a CSV with Hebrew in it, to be opened in Excel, take care to: give the file name extension .tsv (not .csv ) make the file tab separated (do not use the comma or semicolon!) use the encoding utf_16_le (not merely utf_16 , nor utf8 !) start the file with a BOM mark. 1 2 3 4 5 with open ( 'mydata.tsv' , 'w' , encoding = 'utf_16_le' ) as fh : fh . write ( ' \\uFEFF ' ) for row in myData : fh . write ( ' \\t ' . join ( row )) fh . write ( ' \\n ' ) Gory details The file has been written with the utf_16_le encoding, and the first character is the unicode FEFF character. That is needed for machines so that they can see which byte in a 16 bits word is the least end ( le ) and which one is the big end ( be ). Knowing that the first character is FEFF, all machines can see whether this is in a least-endian (le) encoding or in a big-endian (be) . Hence this character is called the Byte Order Mark (BOM). See more on wikipedia . When reading a file with encoding utf_16 , Python reads the BOM, draws its conclusions, and strips the BOM. So when you iterate over its lines, you will not see the BOM, which is good. But when you read a file with encoding utf_16_le , Python passes the BOM through, and you have to skip it yourself. That is unpleasant. Hence, use utf_16 for reading.","title":"Work with exported results"},{"location":"Use/Search/","text":"Search \u00b6 What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are interested in has to be written as a search template . A search template expresses a pattern of nodes and edges with additional conditions also known as quantifiers . You can query a TF corpus in the TF browser, if your main corpus has been wrapped in a Text-Fabric app . You can also run queries on arbitrary TF corpora programmatically, e.g. in a Jupyter notebook, by using the S API . Search primer A search template consists of a bunch of lines, possibly indented, that specify objects to look for. Here is a simple example: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural word pos=noun gender=feminine number=singular This template looks for word combinations within a sentence within chapter 2 of either Genesis or Exodus, where one of the words is a verb and the other is a noun. Both have a feminine inflection, but the verb is plural and the noun is singular. The indentation signifies embedding, i.e. containment. The two words are contained in the same sentence, the sentence is contained in the chapter, the chapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for this. The order of the two words is immaterial. If there are cases where the verb follows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words intervening between the noun and the verb, they will be included in the results. Speaking of results: the S.search() function returns its results as tuples of nodes: 1 ( book , chapter , sentence , word1 , word2 ) With these nodes in hand, you can programmatically gather all information about the results that the corpus provides. If the order between the verb and the noun is important, you can specify that as an additional constraint. You can give the words a name, and state a relational condition. Here we state that the noun precedes the verb. 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 sentence vb:word pos=verb gender=feminine number=plural nn:word pos=noun gender=feminine number=singular nn < vb This can be written a bit more economically as: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural > word pos=noun gender=feminine number=singular If you insist that the noun immediately precedes the verb, you can use a different relational operator: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular There are more kinds of relational operators . If the noun must be the first word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence w:word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural s =: w or a bit more economically: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence =: word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural If the verb must be the last word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular <: w:word pos=verb gender=feminine number=plural s := w or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural := You can also use the edge features in the corpus as relational operators as well. Suppose we have an edge feature sub between clauses, such that if main clause m has subordinate clauses s1 , s2 and s3 , then 1 E.sub.f(m) = (s1, s2, s3) You can use this relation in search. Suppose we want to find the noun verb pair in subordinate clauses only. We can use this template: 1 2 3 4 5 6 7 book name=Genesis|Exodus chapter number=2 m:clause s:clause word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular m -sub> s or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 clause -sub> clause word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular Read m -sub> s as: there is a sub -arrow from m to s . Edge features may have values. For example, the crossref feature is a set of edges between parallel verses, with the levels of confidence as values. This number is an integer between 0 and 100. We can ask for parallel verses in an unqualified way: 1 2 verse <crossref> verse But we can also ask for the cases with a specific confidence: 1 2 verse <crossref=90> verse or cases with a high confidence: 1 2 verse <crossref>95> verse or cases with a low confidence: 1 2 verse <crossref<80> verse All feature conditions that you can assert on node features, you can also assert for edge features. If an edge feature is integer valued, such as crossref you can use comparisons; if it is string valued, you can use regular expressions. In both cases you can also use the other constructs, such as 1 2 verse <crossref=66|77> verse To get a more specific introduction to search, consult the search tutorials for app supported annotated corpora . Finally an example with quantifiers. We want all clauses where Pred-phrases consist of verbs only: 1 2 3 4 5 6 7 8 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ Search template reference \u00b6 Template lines We have these kinds of lines in a template: comment lines if the first non-white character on a line is % it is a comment line; you cannot comment out parts of lines after a non-white part; if a line is empty or has whitespace only, it is a comment line; comment lines are allowed everywhere; comment lines are ignored. atom lines (simple): indent name:otype-or-set features Examples word pos=verb gender=feminine vb:word pos=verb gender=feminine vb pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space characters, where tabs count for just 1. Avoid tabs! . The name: part is optional. If present, it defines a name for this atom that can be used in relational comparisons and other atoms. The otype-or-set part is optional. If it is absent, the name part must be present. The meaning of 1 2 p:phrase sp=verb p vs=qal is identical to the meaning of 1 2 3 p:phrase sp=verb pnew:phrase vs=qal p = pnew (with relop): indent op name:otype-or-set features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom and this atom. The preceding atom may be the parent, provided we are at its first child, or it may the preceding sibling. You can leave out the name:otype-or-set features bit. In that case, the relation holds between the preceding atom and its parent. The name: part is optional. Exactly as in the case without relop. The otype-or-set part is optional. Exactly as in the case without relop. The otype-or-set is either a node type that exists in your TF data set, or it is the name of a set that you have passed in the sets parameter alongside the query itself when you call S.search() or S.study() . See feature specifications below for all full variety of feature constraints on nodes and edges. feature lines: features Indent is not significant. Continuation of feature constraints after a preceding atom line or other feature line. This way you can divide lengthy feature constraints over multiple lines. See feature specifications below for the full variety of feature constraints on nodes and edges. relation lines: name operator name s := w m -sub> s m <sub- s m <sub> s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values. See relational operators below for the whole spectrum of relational constraints on nodes. quantifier sub-templates: Atom lines that contain an otype or set may be followed by Quantifiers consist of search templates themselves, demarcated by some special keywords: /without/ /where/ and /have/ /with/ and /or/ /-/ See quantifiers below for all the syntax and semantics. Feature specifications \u00b6 About The features above is a specification of what features with which values to search for. This specification must be written as a white-space separated list of feature specs . A feature spec has the form name valueSpec , with no space between the name and the valueSpec . Value specifications The valueSpec may have the following forms and meanings: form evaluates to True the feature name ... has any value except None # has value None * has arbitrary value = values has one of the values specified # values has none of the values specified > value is greater than value < value is less than value ~ regular expression has a value and it matches regular expression Why * ? The operator * after a feature name does not pose any restriction at all. It will not influence the search results. Why would you want to include such a \"criterion\"? Some applications, such as the Text-Fabric browser collect the features used in a query to retrieve result information to be presented to the user. So if you want to include the values of a particular feature, mention that feature with a * . All these forms are also valid as - name form > and < name form - and < name form > , in which case they specify value constraints on edge features. This is only meaningful if the edge feature is declared to have values (most edge features do not have values). Additional constraints There may be no space around the =#<>~ . name must be a feature name that exists in the dataset. If it references a feature that is not yet loaded, the feature will be loaded automatically. values must be a | separated list of feature values, no quotes. No spaces around the | . If you need a space or | or \\ in a value, escape it by a \\ . Escape tabs and newlines as \\t and \\n . When comparing values with < and > : value must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on string-valued features. If the feature in question is not defined for a node, or has the value None the outcome is always False . regular expression must be a string that conforms to the Python regular axpression syntax If you need a space in your regular expression, you have to escape it with a \\ . You can do regular expressions only on string-valued features, not on number-valued features. Relational operators \u00b6 Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the same slots are still unequal) # : is unequal (meaning a different node, a clause and a verse that occupy the same slots are still unequal) < > : before and after (in the canonical ordering ) Slot comparison == : occupy the same slots (identical slot sets) && : overlap (the intersection of both slot sets is not empty) ## : occupy different slots (but they may overlap, the set of slots of the two are different as sets) || : occupy disjoint slots (no slot occupied by the one is also occupied by the other) [[ ]] : embeds and contains (slot set inclusion, in both directions) Never holds between the same nodes. But it holds between different nodes with the same slots. But a slot can never embed an other node. << >> : before and after (with respect to the slots occupied: left ends before right starts and vice versa) <: :> : adjacent before and after (with respect to the slots occupied: left ends immediately before right starts and vice versa) =: left and right start at the same slot := left and right end at the same slot :: left and right start and end at the same slot Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring that one slot is equal to an other slot, you can require that they are k-near , i.e. they are at most k apart. Here are the relationships where you can do that. Instead of typing the letter k , provide the actual number you want. <k: :k> : k - adjacent before and after (with respect to the slots occupied: left ends k -near where right starts and vice versa) =k: left and right start at k -near slots :k= left and right end at k -near slots :k: left and right start and end at k -near slots Based on node features Nodes can be compared on the basis of the features that they have. For each pair of node features f , g there is a relation between nodes that holds precisely when feature f of the first node has the same value as feature g on the second node. This can be used in search templates. Not only equality is covered, also inequality, comparison, and matching. .f. and .f=g. feature equality: the f value of the left node is equal to the g value of the right node. .f. is an abbreviation for .f=f. . None values If one or both of the nodes does not have a value, the result is False . In particular, two nodes that have both None for a feature f , count as having unequal values for f . .f#g. feature inequality: the f value of the left node is unequal to the g value of the right node. None values If one or both of the nodes does not have a value, the result is True . In particular, two nodes that have both None for a feature f , count as having unequal values for f . .f<g. and .f>g. feature less than and greater than: the f value of the left node is less/greater than the g value of right node. This is only legal if both f and g are integer valued features. None values If one or both of the nodes does not have a value, the result is False . .f~regex~g. Features match: they are equal modulo the replacement of the parts that match the regex . This is only legal if both f and g are string valued features. Example If node n has feature lex with value donkey_1 and node m has feature lex with value donkey_2 , then the following holds: 1 n .lex~_[0-9]*$~lex. m The values are stripped of their final _1 and _2 strings before they are being compared, leaving the comparison donkey = donkey , which is True. None values If one or both of the nodes does not have a value, the result is False . Based on edge features Edge features are based on directed edges. An edge from n to m is not the same as an edge from m to n . For each direction there is a relation operator. And there is an operator corresponding to the symmetric closure of the edges. without values A - name > B: edge from A to B A < name - B: edge from B to A A < name > B: edge from A to B or from B to A or both These forms work for edges that do and do not have values; with values A - name valueSpec > B: edge with value from A to B A < name valueSpec - B: edge with value from B to A A < name valueSpec > B: edge with value from A to B or form B to A or both These forms work only for edges that do have values. Quantifiers \u00b6 What is a quantifier? Quantifiers are powerful expressions in templates. They state conditions on a given atom in your template. The atom in question is called the parent atom. The conditions may involve many nodes that are related to the parent, as in: all embedded words are a verb ; without a following predicate phrase ; with a mother clause or a mother phrase . That is where the term quantifier comes from. A quantifier quantifies its parent atom. /without/ Syntax: 1 2 3 4 atom /without/ templateN /-/ Meaning: node r is a result of this template if and only if r is a result of atom and there is no tuple RN such that ( r , RN ) is a result of 1 2 atom templateN /where/ Syntax: 1 2 3 4 5 6 atom /where/ templateA /have/ templateH /-/ Meaning: node r is a result of this template if and only if r is a result of atom and for all tuples ( RA ) such that ( r , RA ) is a result of 1 2 atom templateA there is a tuple RH such that ( r , RA , RH ) is a result of 1 2 3 atom templateA templateH /with/ Syntax: 1 2 3 4 5 6 7 8 atom /with/ templateO1 /or/ templateO2 /or/ templateO3 /-/ Meaning: node r is a result of this template if and only if: there is a tuple R1 such that ( r , R1 ) is a result of 1 2 atom templateO1 or there is a tuple R2 such that ( r , R2 ) is a result of 1 2 atom templateO2 or there is a tuple R3 such that ( r , R3 ) is a result of 1 2 atom templateO3 1 or more alternatives This quantifier can be used with any number of /or/ keywords, including none at all. If there is no /or/ , there is just one alternative. The only difference between 1 2 3 4 atom /with/ template /-/ and 1 2 atom template is that the results of the first query contain tuples with only one element, corresponding to the atom . The second query contains tuples of which the first element corresponds to the atom , and the remaining members correspond to the template . Parent The atom bit is an atom line, it acts as the parent of the quantifier. Inside a quantifier, you may refer to the parent by the special name .. . So you do not have to give a name to the parent. Multiple quantifiers You may have multiple quantifiers for one parent. Not in result tuples Whereas a the search for a normal template proceeds by finding a tuple that instantiates all its nodes in such a way that all relationships expressed in the template hold, a quantifier template is not instantiated. It asserts a condition that has to be tested for all nodes relative its parent. None of the atoms in a template of a quantifier corresponds to a node in a final result tuple. May be nested Templates within a quantifier may contain other quantifiers. The idea is, that whenever a search template is evaluated, quantifiers at the outer level of get interpreted. This interpretation gives rise to one or more templates to be constructed and run. Those new templates have been stripped of the outer layer of quantifiers, and when these templates are executed, the quantifiers at the next level have become outer. And so on. Restrictions Due to the implementation of quantifiers there are certain restrictions. Quantifiers must be put immediately below their parents or below preceding quantifiers of the same parent. The keywords of a quantifier must appear on lines with exactly the same indentation as the atom they quantify. The templates of a quantifier must have equal or greater indent than its keywords; The names accessible to the templates inside a quantifier are: the name .. , which is the name of the atom that is quantified; this name is automagically valid in quantifier templates; the name of the atom that is quantified (if that atom has a given name); names defined in the template itself; in /where/ , templateH may use names defined in templateA ; but only if these names are defined outside any quantifier of templateA . The following situations block the visibility of names: in /with/ , templateO i may not use names defined in templateO j for j other than i ; names defined outer quantifiers are not accessible in inner quantifiers; names defined inner quantifiers are not accessible in outer quantifiers. When you nest quantifiers, think of the way they will be recomposed into ordinary templates. This dictates whether your quantifier is syntactically valid or not. Indentation The indentation in quantifiers relative to their parent atom will be preserved. Nested quantifiers Consider 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj The auxiliary templates that will be run are: For the outer quantifier: 1 2 clause phrase function=Pred and 1 2 3 4 5 clause phrase function=Pred /without/ word sp#verb /-/ For the inner quantifier: 1 2 phrase function=Pred word sp#verb Note that the auxiliary template for the inner quantifier is shifted in its entirety to the left, but that the relative indentation is exactly as it shows in the original template. Implementation Here is a description of the implementation of the quantifiers. It is not the real implementation, but it makes clear what is going on, and why the quantifiers have certain limitations, and how indentation works. The basic idea is: a quantifier leads to the execution of one or more separate searche templates; the results of these searches are combined by means of set operations: difference , intersection , union , dependent on the nature of the quantifier; the end result of this combination will fed as a custom set to the original template after stripping the whole quantifier from that template. So we replace a quantifier by a custom set. Suppose we have 1 2 3 4 5 6 clause typ=Wx0 QUANTIFIER1 QUANTIFIER2 ... QUANTIFIERn rest-of-template We compute a set of clauses filteredClauses1 based on 1 2 clause typ=Wx0 QUANTIFIER1 and then compute a new set filteredClauses2 based on 1 2 3 4 5 S.search(''' fclause typ=Wx0 QUANTIFIER2 ''', customSets=dict(fclause=filteredClauses1) and so on until we have had QUANTIFIERn, leading to a set filteredClausesN of clauses that pass all filters set by the quantifiers. Finally, we deliver the results of 1 2 3 4 5 S.search(''' fclause rest-of-template ''', customSets=dict(fclause=filteredClausesN)","title":"Search"},{"location":"Use/Search/#search","text":"What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are interested in has to be written as a search template . A search template expresses a pattern of nodes and edges with additional conditions also known as quantifiers . You can query a TF corpus in the TF browser, if your main corpus has been wrapped in a Text-Fabric app . You can also run queries on arbitrary TF corpora programmatically, e.g. in a Jupyter notebook, by using the S API . Search primer A search template consists of a bunch of lines, possibly indented, that specify objects to look for. Here is a simple example: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural word pos=noun gender=feminine number=singular This template looks for word combinations within a sentence within chapter 2 of either Genesis or Exodus, where one of the words is a verb and the other is a noun. Both have a feminine inflection, but the verb is plural and the noun is singular. The indentation signifies embedding, i.e. containment. The two words are contained in the same sentence, the sentence is contained in the chapter, the chapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for this. The order of the two words is immaterial. If there are cases where the verb follows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words intervening between the noun and the verb, they will be included in the results. Speaking of results: the S.search() function returns its results as tuples of nodes: 1 ( book , chapter , sentence , word1 , word2 ) With these nodes in hand, you can programmatically gather all information about the results that the corpus provides. If the order between the verb and the noun is important, you can specify that as an additional constraint. You can give the words a name, and state a relational condition. Here we state that the noun precedes the verb. 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 sentence vb:word pos=verb gender=feminine number=plural nn:word pos=noun gender=feminine number=singular nn < vb This can be written a bit more economically as: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural > word pos=noun gender=feminine number=singular If you insist that the noun immediately precedes the verb, you can use a different relational operator: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular There are more kinds of relational operators . If the noun must be the first word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence w:word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural s =: w or a bit more economically: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence =: word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural If the verb must be the last word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular <: w:word pos=verb gender=feminine number=plural s := w or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular <: word pos=verb gender=feminine number=plural := You can also use the edge features in the corpus as relational operators as well. Suppose we have an edge feature sub between clauses, such that if main clause m has subordinate clauses s1 , s2 and s3 , then 1 E.sub.f(m) = (s1, s2, s3) You can use this relation in search. Suppose we want to find the noun verb pair in subordinate clauses only. We can use this template: 1 2 3 4 5 6 7 book name=Genesis|Exodus chapter number=2 m:clause s:clause word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular m -sub> s or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 clause -sub> clause word pos=verb gender=feminine number=plural :> word pos=noun gender=feminine number=singular Read m -sub> s as: there is a sub -arrow from m to s . Edge features may have values. For example, the crossref feature is a set of edges between parallel verses, with the levels of confidence as values. This number is an integer between 0 and 100. We can ask for parallel verses in an unqualified way: 1 2 verse <crossref> verse But we can also ask for the cases with a specific confidence: 1 2 verse <crossref=90> verse or cases with a high confidence: 1 2 verse <crossref>95> verse or cases with a low confidence: 1 2 verse <crossref<80> verse All feature conditions that you can assert on node features, you can also assert for edge features. If an edge feature is integer valued, such as crossref you can use comparisons; if it is string valued, you can use regular expressions. In both cases you can also use the other constructs, such as 1 2 verse <crossref=66|77> verse To get a more specific introduction to search, consult the search tutorials for app supported annotated corpora . Finally an example with quantifiers. We want all clauses where Pred-phrases consist of verbs only: 1 2 3 4 5 6 7 8 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/","title":"Search"},{"location":"Use/Search/#search-template-reference","text":"Template lines We have these kinds of lines in a template: comment lines if the first non-white character on a line is % it is a comment line; you cannot comment out parts of lines after a non-white part; if a line is empty or has whitespace only, it is a comment line; comment lines are allowed everywhere; comment lines are ignored. atom lines (simple): indent name:otype-or-set features Examples word pos=verb gender=feminine vb:word pos=verb gender=feminine vb pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space characters, where tabs count for just 1. Avoid tabs! . The name: part is optional. If present, it defines a name for this atom that can be used in relational comparisons and other atoms. The otype-or-set part is optional. If it is absent, the name part must be present. The meaning of 1 2 p:phrase sp=verb p vs=qal is identical to the meaning of 1 2 3 p:phrase sp=verb pnew:phrase vs=qal p = pnew (with relop): indent op name:otype-or-set features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom and this atom. The preceding atom may be the parent, provided we are at its first child, or it may the preceding sibling. You can leave out the name:otype-or-set features bit. In that case, the relation holds between the preceding atom and its parent. The name: part is optional. Exactly as in the case without relop. The otype-or-set part is optional. Exactly as in the case without relop. The otype-or-set is either a node type that exists in your TF data set, or it is the name of a set that you have passed in the sets parameter alongside the query itself when you call S.search() or S.study() . See feature specifications below for all full variety of feature constraints on nodes and edges. feature lines: features Indent is not significant. Continuation of feature constraints after a preceding atom line or other feature line. This way you can divide lengthy feature constraints over multiple lines. See feature specifications below for the full variety of feature constraints on nodes and edges. relation lines: name operator name s := w m -sub> s m <sub- s m <sub> s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values. See relational operators below for the whole spectrum of relational constraints on nodes. quantifier sub-templates: Atom lines that contain an otype or set may be followed by Quantifiers consist of search templates themselves, demarcated by some special keywords: /without/ /where/ and /have/ /with/ and /or/ /-/ See quantifiers below for all the syntax and semantics.","title":"Search template reference"},{"location":"Use/Search/#feature-specifications","text":"About The features above is a specification of what features with which values to search for. This specification must be written as a white-space separated list of feature specs . A feature spec has the form name valueSpec , with no space between the name and the valueSpec . Value specifications The valueSpec may have the following forms and meanings: form evaluates to True the feature name ... has any value except None # has value None * has arbitrary value = values has one of the values specified # values has none of the values specified > value is greater than value < value is less than value ~ regular expression has a value and it matches regular expression Why * ? The operator * after a feature name does not pose any restriction at all. It will not influence the search results. Why would you want to include such a \"criterion\"? Some applications, such as the Text-Fabric browser collect the features used in a query to retrieve result information to be presented to the user. So if you want to include the values of a particular feature, mention that feature with a * . All these forms are also valid as - name form > and < name form - and < name form > , in which case they specify value constraints on edge features. This is only meaningful if the edge feature is declared to have values (most edge features do not have values). Additional constraints There may be no space around the =#<>~ . name must be a feature name that exists in the dataset. If it references a feature that is not yet loaded, the feature will be loaded automatically. values must be a | separated list of feature values, no quotes. No spaces around the | . If you need a space or | or \\ in a value, escape it by a \\ . Escape tabs and newlines as \\t and \\n . When comparing values with < and > : value must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on string-valued features. If the feature in question is not defined for a node, or has the value None the outcome is always False . regular expression must be a string that conforms to the Python regular axpression syntax If you need a space in your regular expression, you have to escape it with a \\ . You can do regular expressions only on string-valued features, not on number-valued features.","title":"Feature specifications"},{"location":"Use/Search/#relational-operators","text":"Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the same slots are still unequal) # : is unequal (meaning a different node, a clause and a verse that occupy the same slots are still unequal) < > : before and after (in the canonical ordering ) Slot comparison == : occupy the same slots (identical slot sets) && : overlap (the intersection of both slot sets is not empty) ## : occupy different slots (but they may overlap, the set of slots of the two are different as sets) || : occupy disjoint slots (no slot occupied by the one is also occupied by the other) [[ ]] : embeds and contains (slot set inclusion, in both directions) Never holds between the same nodes. But it holds between different nodes with the same slots. But a slot can never embed an other node. << >> : before and after (with respect to the slots occupied: left ends before right starts and vice versa) <: :> : adjacent before and after (with respect to the slots occupied: left ends immediately before right starts and vice versa) =: left and right start at the same slot := left and right end at the same slot :: left and right start and end at the same slot Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring that one slot is equal to an other slot, you can require that they are k-near , i.e. they are at most k apart. Here are the relationships where you can do that. Instead of typing the letter k , provide the actual number you want. <k: :k> : k - adjacent before and after (with respect to the slots occupied: left ends k -near where right starts and vice versa) =k: left and right start at k -near slots :k= left and right end at k -near slots :k: left and right start and end at k -near slots Based on node features Nodes can be compared on the basis of the features that they have. For each pair of node features f , g there is a relation between nodes that holds precisely when feature f of the first node has the same value as feature g on the second node. This can be used in search templates. Not only equality is covered, also inequality, comparison, and matching. .f. and .f=g. feature equality: the f value of the left node is equal to the g value of the right node. .f. is an abbreviation for .f=f. . None values If one or both of the nodes does not have a value, the result is False . In particular, two nodes that have both None for a feature f , count as having unequal values for f . .f#g. feature inequality: the f value of the left node is unequal to the g value of the right node. None values If one or both of the nodes does not have a value, the result is True . In particular, two nodes that have both None for a feature f , count as having unequal values for f . .f<g. and .f>g. feature less than and greater than: the f value of the left node is less/greater than the g value of right node. This is only legal if both f and g are integer valued features. None values If one or both of the nodes does not have a value, the result is False . .f~regex~g. Features match: they are equal modulo the replacement of the parts that match the regex . This is only legal if both f and g are string valued features. Example If node n has feature lex with value donkey_1 and node m has feature lex with value donkey_2 , then the following holds: 1 n .lex~_[0-9]*$~lex. m The values are stripped of their final _1 and _2 strings before they are being compared, leaving the comparison donkey = donkey , which is True. None values If one or both of the nodes does not have a value, the result is False . Based on edge features Edge features are based on directed edges. An edge from n to m is not the same as an edge from m to n . For each direction there is a relation operator. And there is an operator corresponding to the symmetric closure of the edges. without values A - name > B: edge from A to B A < name - B: edge from B to A A < name > B: edge from A to B or from B to A or both These forms work for edges that do and do not have values; with values A - name valueSpec > B: edge with value from A to B A < name valueSpec - B: edge with value from B to A A < name valueSpec > B: edge with value from A to B or form B to A or both These forms work only for edges that do have values.","title":"Relational operators"},{"location":"Use/Search/#quantifiers","text":"What is a quantifier? Quantifiers are powerful expressions in templates. They state conditions on a given atom in your template. The atom in question is called the parent atom. The conditions may involve many nodes that are related to the parent, as in: all embedded words are a verb ; without a following predicate phrase ; with a mother clause or a mother phrase . That is where the term quantifier comes from. A quantifier quantifies its parent atom. /without/ Syntax: 1 2 3 4 atom /without/ templateN /-/ Meaning: node r is a result of this template if and only if r is a result of atom and there is no tuple RN such that ( r , RN ) is a result of 1 2 atom templateN /where/ Syntax: 1 2 3 4 5 6 atom /where/ templateA /have/ templateH /-/ Meaning: node r is a result of this template if and only if r is a result of atom and for all tuples ( RA ) such that ( r , RA ) is a result of 1 2 atom templateA there is a tuple RH such that ( r , RA , RH ) is a result of 1 2 3 atom templateA templateH /with/ Syntax: 1 2 3 4 5 6 7 8 atom /with/ templateO1 /or/ templateO2 /or/ templateO3 /-/ Meaning: node r is a result of this template if and only if: there is a tuple R1 such that ( r , R1 ) is a result of 1 2 atom templateO1 or there is a tuple R2 such that ( r , R2 ) is a result of 1 2 atom templateO2 or there is a tuple R3 such that ( r , R3 ) is a result of 1 2 atom templateO3 1 or more alternatives This quantifier can be used with any number of /or/ keywords, including none at all. If there is no /or/ , there is just one alternative. The only difference between 1 2 3 4 atom /with/ template /-/ and 1 2 atom template is that the results of the first query contain tuples with only one element, corresponding to the atom . The second query contains tuples of which the first element corresponds to the atom , and the remaining members correspond to the template . Parent The atom bit is an atom line, it acts as the parent of the quantifier. Inside a quantifier, you may refer to the parent by the special name .. . So you do not have to give a name to the parent. Multiple quantifiers You may have multiple quantifiers for one parent. Not in result tuples Whereas a the search for a normal template proceeds by finding a tuple that instantiates all its nodes in such a way that all relationships expressed in the template hold, a quantifier template is not instantiated. It asserts a condition that has to be tested for all nodes relative its parent. None of the atoms in a template of a quantifier corresponds to a node in a final result tuple. May be nested Templates within a quantifier may contain other quantifiers. The idea is, that whenever a search template is evaluated, quantifiers at the outer level of get interpreted. This interpretation gives rise to one or more templates to be constructed and run. Those new templates have been stripped of the outer layer of quantifiers, and when these templates are executed, the quantifiers at the next level have become outer. And so on. Restrictions Due to the implementation of quantifiers there are certain restrictions. Quantifiers must be put immediately below their parents or below preceding quantifiers of the same parent. The keywords of a quantifier must appear on lines with exactly the same indentation as the atom they quantify. The templates of a quantifier must have equal or greater indent than its keywords; The names accessible to the templates inside a quantifier are: the name .. , which is the name of the atom that is quantified; this name is automagically valid in quantifier templates; the name of the atom that is quantified (if that atom has a given name); names defined in the template itself; in /where/ , templateH may use names defined in templateA ; but only if these names are defined outside any quantifier of templateA . The following situations block the visibility of names: in /with/ , templateO i may not use names defined in templateO j for j other than i ; names defined outer quantifiers are not accessible in inner quantifiers; names defined inner quantifiers are not accessible in outer quantifiers. When you nest quantifiers, think of the way they will be recomposed into ordinary templates. This dictates whether your quantifier is syntactically valid or not. Indentation The indentation in quantifiers relative to their parent atom will be preserved. Nested quantifiers Consider 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj The auxiliary templates that will be run are: For the outer quantifier: 1 2 clause phrase function=Pred and 1 2 3 4 5 clause phrase function=Pred /without/ word sp#verb /-/ For the inner quantifier: 1 2 phrase function=Pred word sp#verb Note that the auxiliary template for the inner quantifier is shifted in its entirety to the left, but that the relative indentation is exactly as it shows in the original template. Implementation Here is a description of the implementation of the quantifiers. It is not the real implementation, but it makes clear what is going on, and why the quantifiers have certain limitations, and how indentation works. The basic idea is: a quantifier leads to the execution of one or more separate searche templates; the results of these searches are combined by means of set operations: difference , intersection , union , dependent on the nature of the quantifier; the end result of this combination will fed as a custom set to the original template after stripping the whole quantifier from that template. So we replace a quantifier by a custom set. Suppose we have 1 2 3 4 5 6 clause typ=Wx0 QUANTIFIER1 QUANTIFIER2 ... QUANTIFIERn rest-of-template We compute a set of clauses filteredClauses1 based on 1 2 clause typ=Wx0 QUANTIFIER1 and then compute a new set filteredClauses2 based on 1 2 3 4 5 S.search(''' fclause typ=Wx0 QUANTIFIER2 ''', customSets=dict(fclause=filteredClauses1) and so on until we have had QUANTIFIERn, leading to a set filteredClausesN of clauses that pass all filters set by the quantifiers. Finally, we deliver the results of 1 2 3 4 5 S.search(''' fclause rest-of-template ''', customSets=dict(fclause=filteredClausesN)","title":"Quantifiers"},{"location":"Use/Use/","text":"Usage \u00b6 Apps \u00b6 Text-Fabric is a generic engine to process text corpora enriched with sets of annotations. For a growing set of corpora additional functionality is available in the form of apps . Text-Fabric browser \u00b6 App-supported corpora can be explored in a browser interface that is usable without programming. Only familiarity with search templates is needed. Search templates \u00b6 Text-Fabric has a powerful yet intuitive search engine that works with templates , which follow closely the features of the annotations to the corpus. (Uruk) 1 2 3 4 5 tablet catalogId=P448702 line case terminal=1 number=2a sign type=ideograph :> sign type=numeral (Bhsa) 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj (Quran) 1 2 3 4 query = '''\\n\", aya\\n\", word pos=verb <: word pos=noun posx=proper root=Alh Text-Fabric API \u00b6 When mere search is not enough, analysis by means of programming is the next step. You can \"talk\" to your corpus through an API dedicated to your corpus which can download its data and display its material. You can use it together with the generic TF API to search your corpus programmatically by means of the same templates, prepare derived data for analysis in R, and create new data and distribute it to others. Advanced \u00b6 The advanced guide tells more about data usage and data sharing.","title":"Use"},{"location":"Use/Use/#usage","text":"","title":"Usage"},{"location":"Use/Use/#apps","text":"Text-Fabric is a generic engine to process text corpora enriched with sets of annotations. For a growing set of corpora additional functionality is available in the form of apps .","title":"Apps"},{"location":"Use/Use/#text-fabric-browser","text":"App-supported corpora can be explored in a browser interface that is usable without programming. Only familiarity with search templates is needed.","title":"Text-Fabric browser"},{"location":"Use/Use/#search-templates","text":"Text-Fabric has a powerful yet intuitive search engine that works with templates , which follow closely the features of the annotations to the corpus. (Uruk) 1 2 3 4 5 tablet catalogId=P448702 line case terminal=1 number=2a sign type=ideograph :> sign type=numeral (Bhsa) 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj (Quran) 1 2 3 4 query = '''\\n\", aya\\n\", word pos=verb <: word pos=noun posx=proper root=Alh","title":"Search templates"},{"location":"Use/Use/#text-fabric-api","text":"When mere search is not enough, analysis by means of programming is the next step. You can \"talk\" to your corpus through an API dedicated to your corpus which can download its data and display its material. You can use it together with the generic TF API to search your corpus programmatically by means of the same templates, prepare derived data for analysis in R, and create new data and distribute it to others.","title":"Text-Fabric API"},{"location":"Use/Use/#advanced","text":"The advanced guide tells more about data usage and data sharing.","title":"Advanced"},{"location":"Use/UseX/","text":"Advanced Guide \u00b6 Here are hints to help you to get the most out of Text-Fabric apps. For the full reference, start with Use . Incantation \u00b6 Start an app like this: 1 2 from tf.app import use A = use ( 'bhsa' , hoist = globals ()) 1 2 from tf.app import use A = use ( 'uruk' , hoist = globals ()) Read more in the App API Zipping your new data \u00b6 There is a command 1 text-fabric-zip to make a distribution of your own features. For a guide through all the steps of data sharing, see Data and for examples see the share tutorial. Using new data \u00b6 The text-fabric command has several optional command line arguments: --mod=... and -c By means of these arguments you can load extra features, either from your own system, or from GitHub. 1 text-fabric bhsa --mod = etcbc/valence/tf or if you want to check for new versions online: 1 text-fabric bhsa --mod = etcbc/valence/tf See the incantation . Using old data and apps \u00b6 It is even possible to go back to earlier versions of the data and apps, which might be needed if you want to reproduce results obtained with those versions. For app and data, you can add specifiers to point to a specific release or commit. Read more about your data life-cycle in the Data guide. Custom sets \u00b6 You can create custom sets of nodes, give them a name, and use those names in search templates. The TF browser can import those sets, so that you can use such queries in the browser too. 1 text-fabric appname --sets = filePath Read more in Browser . Display \u00b6 The way you control the display parameters for the functions pretty() and plain() and friends has changed. See Display .","title":"Advanced"},{"location":"Use/UseX/#advanced-guide","text":"Here are hints to help you to get the most out of Text-Fabric apps. For the full reference, start with Use .","title":"Advanced Guide"},{"location":"Use/UseX/#incantation","text":"Start an app like this: 1 2 from tf.app import use A = use ( 'bhsa' , hoist = globals ()) 1 2 from tf.app import use A = use ( 'uruk' , hoist = globals ()) Read more in the App API","title":"Incantation"},{"location":"Use/UseX/#zipping-your-new-data","text":"There is a command 1 text-fabric-zip to make a distribution of your own features. For a guide through all the steps of data sharing, see Data and for examples see the share tutorial.","title":"Zipping your new data"},{"location":"Use/UseX/#using-new-data","text":"The text-fabric command has several optional command line arguments: --mod=... and -c By means of these arguments you can load extra features, either from your own system, or from GitHub. 1 text-fabric bhsa --mod = etcbc/valence/tf or if you want to check for new versions online: 1 text-fabric bhsa --mod = etcbc/valence/tf See the incantation .","title":"Using new data"},{"location":"Use/UseX/#using-old-data-and-apps","text":"It is even possible to go back to earlier versions of the data and apps, which might be needed if you want to reproduce results obtained with those versions. For app and data, you can add specifiers to point to a specific release or commit. Read more about your data life-cycle in the Data guide.","title":"Using old data and apps"},{"location":"Use/UseX/#custom-sets","text":"You can create custom sets of nodes, give them a name, and use those names in search templates. The TF browser can import those sets, so that you can use such queries in the browser too. 1 text-fabric appname --sets = filePath Read more in Browser .","title":"Custom sets"},{"location":"Use/UseX/#display","text":"The way you control the display parameters for the functions pretty() and plain() and friends has changed. See Display .","title":"Display"},{"location":"Writing/Arabic/","text":"Arabic characters \u00b6 @font-face { font-family: AmiriQuran; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff2') format('woff2'), url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff') format('woff'), url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.ttf') format('truetype'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"AmiriQuran\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; } Letters \u00b6 transcription glyph phonetic remarks name unicode ' \u0621 ARABIC LETTER HAMZA 0621 A \u0627 ARABIC LETTER ALEF 0627 b \u0628 ARABIC LETTER BEH 0628 p \u0629 ARABIC LETTER TEH MARBUTA 0629 t \u062a ARABIC LETTER TEH 062a v \u062b ARABIC LETTER THEH 062b j \u062c ARABIC LETTER JEEM 062c H \u062d ARABIC LETTER HAH 062d x \u062e ARABIC LETTER KHAH 062e d \u062f ARABIC LETTER DAL 062f * \u0630 ARABIC LETTER THAL 0630 r \u0631 ARABIC LETTER REH 0631 z \u0632 ARABIC LETTER ZAIN 0632 s \u0633 ARABIC LETTER SEEN 0633 $ \u0634 ARABIC LETTER SHEEN 0634 S \u0635 ARABIC LETTER SAD 0635 D \u0636 ARABIC LETTER DAD 0636 T \u0637 ARABIC LETTER TAH 0637 Z \u0638 ARABIC LETTER ZAH 0638 E \u0639 ARABIC LETTER AIN 0639 g \u063a ARABIC LETTER GHAIN 063a f \u0641 ARABIC LETTER FEH 0641 q \u0642 ARABIC LETTER QAF 0642 k \u0643 ARABIC LETTER KAF 0643 l \u0644 ARABIC LETTER LAM 0644 m \u0645 ARABIC LETTER MEEM 0645 n \u0646 ARABIC LETTER NOON 0646 h \u0647 ARABIC LETTER HEH 0647 w \u0648 ARABIC LETTER WAW 0648 Y \u0649 ARABIC LETTER ALEF MAKSURA 0649 y \u064a ARABIC LETTER YEH 064a { \u0671 ARABIC LETTER ALEF WASLA 0671 Stops \u00b6 transcription glyph phonetic remarks name unicode - \u06ea ARABIC EMPTY CENTRE LOW STOP 06ea + \u06eb ARABIC EMPTY CENTRE HIGH STOP 06eb % \u06ec ARABIC ROUNDED HIGH STOP WITH FILLED CENTRE 06ec Letters (modified) \u00b6 transcription glyph phonetic remarks name unicode ` \u0670 ARABIC LETTER SUPERSCRIPT ALEF 0670 : \u06dc ARABIC SMALL HIGH SEEN 06dc [ \u06e2 ARABIC SMALL HIGH MEEM ISOLATED FORM 06e2 ; \u06e3 ARABIC SMALL LOW SEEN 06e3 , \u06e5 ARABIC SMALL WAW 06e5 . \u06e6 ARABIC SMALL YEH 06e6 ! \u06e8 ARABIC SMALL HIGH NOON 06e8 ] \u06ed ARABIC SMALL LOW MEEM 06ed Letters (combined) \u00b6 transcription glyph phonetic remarks name unicode > \u0623 ARABIC LETTER ALEF WITH HAMZA ABOVE 0623 & \u0624 ARABIC LETTER WAW WITH HAMZA ABOVE 0624 < \u0625 ARABIC LETTER ALEF WITH HAMZA BELOW 0625 } \u0626 ARABIC LETTER YEH WITH HAMZA ABOVE 0626 Modifiers \u00b6 transcription glyph phonetic remarks name unicode _ \u0640 ARABIC TATWEEL 0640 Vowel diacritics \u00b6 transcription glyph phonetic remarks name unicode F \u064b ARABIC FATHATAN 064b N \u064c ARABIC DAMMATAN 064c K \u064d ARABIC KASRATAN 064d a \u064e ARABIC FATHA 064e u \u064f ARABIC DAMMA 064f i \u0650 ARABIC KASRA 0650 Non-vocalic diacritics \u00b6 transcription glyph phonetic remarks name unicode ~ \u0651 ARABIC SHADDA 0651 o \u0652 ARABIC SUKUN 0652 ^ \u0653 ARABIC MADDAH ABOVE 0653 # \u0654 ARABIC HAMZA ABOVE 0654 @ \u06df ARABIC SMALL HIGH ROUNDED ZERO 06df \" \u06e0 ARABIC SMALL HIGH UPRIGHT RECTANGULAR ZERO 06e0 Separators \u00b6 transcription glyph phonetic remarks name unicode SPACE 0020 See also \u00b6 Arabic script in Unicode Arabic diacritics","title":"Arabic"},{"location":"Writing/Arabic/#arabic-characters","text":"@font-face { font-family: AmiriQuran; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff2') format('woff2'), url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff') format('woff'), url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.ttf') format('truetype'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"AmiriQuran\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; }","title":"Arabic characters"},{"location":"Writing/Arabic/#letters","text":"transcription glyph phonetic remarks name unicode ' \u0621 ARABIC LETTER HAMZA 0621 A \u0627 ARABIC LETTER ALEF 0627 b \u0628 ARABIC LETTER BEH 0628 p \u0629 ARABIC LETTER TEH MARBUTA 0629 t \u062a ARABIC LETTER TEH 062a v \u062b ARABIC LETTER THEH 062b j \u062c ARABIC LETTER JEEM 062c H \u062d ARABIC LETTER HAH 062d x \u062e ARABIC LETTER KHAH 062e d \u062f ARABIC LETTER DAL 062f * \u0630 ARABIC LETTER THAL 0630 r \u0631 ARABIC LETTER REH 0631 z \u0632 ARABIC LETTER ZAIN 0632 s \u0633 ARABIC LETTER SEEN 0633 $ \u0634 ARABIC LETTER SHEEN 0634 S \u0635 ARABIC LETTER SAD 0635 D \u0636 ARABIC LETTER DAD 0636 T \u0637 ARABIC LETTER TAH 0637 Z \u0638 ARABIC LETTER ZAH 0638 E \u0639 ARABIC LETTER AIN 0639 g \u063a ARABIC LETTER GHAIN 063a f \u0641 ARABIC LETTER FEH 0641 q \u0642 ARABIC LETTER QAF 0642 k \u0643 ARABIC LETTER KAF 0643 l \u0644 ARABIC LETTER LAM 0644 m \u0645 ARABIC LETTER MEEM 0645 n \u0646 ARABIC LETTER NOON 0646 h \u0647 ARABIC LETTER HEH 0647 w \u0648 ARABIC LETTER WAW 0648 Y \u0649 ARABIC LETTER ALEF MAKSURA 0649 y \u064a ARABIC LETTER YEH 064a { \u0671 ARABIC LETTER ALEF WASLA 0671","title":"Letters"},{"location":"Writing/Arabic/#stops","text":"transcription glyph phonetic remarks name unicode - \u06ea ARABIC EMPTY CENTRE LOW STOP 06ea + \u06eb ARABIC EMPTY CENTRE HIGH STOP 06eb % \u06ec ARABIC ROUNDED HIGH STOP WITH FILLED CENTRE 06ec","title":"Stops"},{"location":"Writing/Arabic/#letters-modified","text":"transcription glyph phonetic remarks name unicode ` \u0670 ARABIC LETTER SUPERSCRIPT ALEF 0670 : \u06dc ARABIC SMALL HIGH SEEN 06dc [ \u06e2 ARABIC SMALL HIGH MEEM ISOLATED FORM 06e2 ; \u06e3 ARABIC SMALL LOW SEEN 06e3 , \u06e5 ARABIC SMALL WAW 06e5 . \u06e6 ARABIC SMALL YEH 06e6 ! \u06e8 ARABIC SMALL HIGH NOON 06e8 ] \u06ed ARABIC SMALL LOW MEEM 06ed","title":"Letters (modified)"},{"location":"Writing/Arabic/#letters-combined","text":"transcription glyph phonetic remarks name unicode > \u0623 ARABIC LETTER ALEF WITH HAMZA ABOVE 0623 & \u0624 ARABIC LETTER WAW WITH HAMZA ABOVE 0624 < \u0625 ARABIC LETTER ALEF WITH HAMZA BELOW 0625 } \u0626 ARABIC LETTER YEH WITH HAMZA ABOVE 0626","title":"Letters (combined)"},{"location":"Writing/Arabic/#modifiers","text":"transcription glyph phonetic remarks name unicode _ \u0640 ARABIC TATWEEL 0640","title":"Modifiers"},{"location":"Writing/Arabic/#vowel-diacritics","text":"transcription glyph phonetic remarks name unicode F \u064b ARABIC FATHATAN 064b N \u064c ARABIC DAMMATAN 064c K \u064d ARABIC KASRATAN 064d a \u064e ARABIC FATHA 064e u \u064f ARABIC DAMMA 064f i \u0650 ARABIC KASRA 0650","title":"Vowel diacritics"},{"location":"Writing/Arabic/#non-vocalic-diacritics","text":"transcription glyph phonetic remarks name unicode ~ \u0651 ARABIC SHADDA 0651 o \u0652 ARABIC SUKUN 0652 ^ \u0653 ARABIC MADDAH ABOVE 0653 # \u0654 ARABIC HAMZA ABOVE 0654 @ \u06df ARABIC SMALL HIGH ROUNDED ZERO 06df \" \u06e0 ARABIC SMALL HIGH UPRIGHT RECTANGULAR ZERO 06e0","title":"Non-vocalic diacritics"},{"location":"Writing/Arabic/#separators","text":"transcription glyph phonetic remarks name unicode SPACE 0020","title":"Separators"},{"location":"Writing/Arabic/#see-also","text":"Arabic script in Unicode Arabic diacritics","title":"See also"},{"location":"Writing/Hebrew/","text":"Hebrew characters \u00b6 @font-face { font-family: \"Ezra SIL\"; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.ttf?raw=true'); src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"Ezra SIL\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; } Disclaimer This just a look-up table, not a full exposition of the organisation of the Masoretic system. Transcriptions The ETCBC transcription is used by the ETCBC. It has entries for all accents, but not for text-critical annotations such as uncertainty, and correction. The Abegg transcription is used in the Dead Sea scrolls. It has no entries for accents, but it has a repertoire of text-critical marks. We have back translated the latter to etcbc-compatible variants and entered them in the etcbc column, although they are not strictly etcbc marks. Phonetics The phonetic representation is meant as a tentative 1-1 correspondence with pronunciation, not with the script. See phono.ipynb , where the phonetic transcription is computed and thoroughly documented. Consonants \u00b6 Details For most consonants: an inner dot is a dagesh forte . For the \u05d1\u05d2\u05d3\u05db\u05e4\u05ea consonants: an inner dot is either a dagesh forte or a dagesh lene . When the \u05d4 contains a dot, it is called a mappiq . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode > a \u05d0 \u0294 when not *mater lectionis* *letter* alef 05D0 B b \u05d1 bb b v forte lene normal *letter* bet 05D1 G g \u05d2 gg g \u1e21 forte lene normal *letter* gimel 05D2 D d \u05d3 dd d \u1e0f forte lene normal *letter* dalet 05D3 H h \u05d4 h also with *mappiq*; when not *mater lectionis* *letter* he 05D4 W w \u05d5 ww w \u00fb forte when not part of a long vowel with dagesh as vowel *letter* vav 05D5 Z z \u05d6 zz z forte normal *letter* zayin 05D6 X j \u05d7 \u1e25 *letter* het 05D7 V f \u05d8 \u1e6d *letter* tet 05D8 J y \u05d9 yy y \u02b8 forte when not part of long vowel in front of final \u05d5 *letter* yod 05D9 K k \u05db kk k \u1e35 forte lene normal *letter* kaf 05DB k K \u05da k \u1e35 forte normal *letter* final kaf 05DA L l \u05dc ll l forte normal *letter* lamed 05DC M m \u05de mm m forte normal *letter* mem 05DE m M \u05dd m *letter* final mem 05DD N n \u05e0 nn n forte normal *letter* nun 05E0 n N \u05df n *letter* final nun 05DF S s \u05e1 ss s forte normal *letter* samekh 05E1 < o \u05e2 \u0295 *letter* ayin 05E2 P p \u05e4 pp p f forte lene normal *letter* pe 05E4 p P \u05e3 p f forte normal *letter* final pe 05E3 Y x \u05e6 \u1e63\u1e63 \u1e63 forte normal *letter* tsadi 05E6 y X \u05e5 \u1e63 *letter* final tsadi 05E5 Q q \u05e7 qq q forte normal *letter* qof 05E7 R r \u05e8 rr r forte normal *letter* resh 05E8 # C \u05e9 \u015d *letter* shin without dot 05E9 C v \u05e9\u05c1 \u0161\u0161 \u0161 forte normal *letter* shin with shin dot FB2A F c \u05e9\u05c2 \u015b\u015b \u015b forte normal *letter* shin with sin dot FB2B T t \u05ea tt t \u1e6f forte lene normal *letter* tav 05EA Vowels \u00b6 Qere Ketiv The phonetics follows the qere , not the ketiv , when they are different. In that case a * is added. Tetragrammaton The tetragrammaton \u05d9\u05d4\u05d5\u05d4 is (vowel)-pointed in different ways; the phonetics follows the pointing, but the tetragrammaton is put between [ ] . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode A A \u00c5 \u05b7 a \u2090 normal *furtive* *point* patah 05B7 :A S \u05b2 \u1d43 *point* hataf patah 05B2 @ D \u2202 \u00ce \u05b8 \u0101 o gadol qatan *point* qamats 05B8 :@ F \u0192 \u00cf \u05b3 \u1d52 *point* hataf qamats 05B3 E R \u00ae \u2030 \u05b6 e e\u02b8 normal with following \u05d9 *point* segol 05B6 :E T \u05b1 \u1d49 \u1d49\u02b8 normal with following \u05d9 *point* hataf segol 05B1 ; E \u00e9 \u00b4 \u05b5 \u00ea \u0113 with following \u05d9 alone *point* tsere 05B5 I I \u02c6 \u00ee \u00ca \u05b4 \u00ee i with following \u05d9 alone *point* hiriq 05B4 O O \u00f8 \u05b9 \u00f4 \u014d with following \u05d5 alone *point* holam 05B9 U U \u00fc \u00a8 \u05bb u *point* qubuts 05BB : V \u221a J \u25ca \u05b0 \u1d4a left out if silent *point* sheva 05B0 Other points and marks \u00b6 transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode . ; \u2026 \u00da \u00a5 \u03a9 \u05bc *point* dagesh or mapiq 05BC .c \u05c1 *point* shin dot 05C1 .f \u05c2 *point* sin dot 05C2 , \u05bf *point* rafe 05BF 35 \u05bd \u02c8 *point* meteg 05BD 75 \u05bd \u02c8 *point* meteg 05BD 95 \u05bd \u02c8 *point* meteg 05BD 52 \u05c4 \u02c8 *mark* upper dot 05C4 53 \u05c5 \u02c8 *mark* lower dot 05C5 * \u05af *mark* masora circle 05AF Punctuation \u00b6 Details Some specialties in the Masoretic system are not reflected in the phonetics: setumah \u05e1 ; petuhah \u05e3 ; nun-hafuka \u0307\u05c6 . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode 00 . \u05c3 . *punctuation* sof pasuq 05C3 n\u0303 \u05c6 *punctuation* nun hafukha 05C6 & - \u05be - *punctuation* maqaf 05BE _ (non breaking space) space 0020 0000 \u00b1 \u05c3\u05c3 Dead Sea scrolls. We use as Hebrew character a double sof pasuq. paleo-divider 05C3 05C3 ' / \u05f3 Dead Sea scrolls. We use as Hebrew character a geresh. morpheme-break 05F3 Hybrid \u00b6 Details There is a character that is mostly punctuation, but that can also influence the nature of some accents occurring in the word before. Such a character is a hybrid between punctuation and accent. See also the documentation of the BHSA about cantillation . transcription glyph phonetic remarks name unicode 05 \u05c0 *punctuation* paseq 05C0 Accents \u00b6 Details Some accents play a role in deciding whether a schwa is silent or mobile and whether a qamets is gadol or qatan. In the phonetics those accents appear as \u02c8 or \u02cc . Implied accents are also added. transcription glyph phonetic remarks name unicode 94 \u05a7 \u02c8 *accent* darga 05A7 13 \u05ad \u02c8 *accent* dehi 05AD 92 \u0591 \u02c8 *accent* etnahta 0591 61 \u059c \u02c8 *accent* geresh 059C 11 \u059d \u02c8 *accent* geresh muqdam 059D 62 \u059e \u02c8 *accent* gershayim 059E 64 \u05ac \u02c8 *accent* iluy 05AC 70 \u05a4 \u02c8 *accent* mahapakh 05A4 71 \u05a5 \u02cc *accent* merkha 05A5 72 \u05a6 \u02c8 *accent* merkha kefula 05A6 74 \u05a3 \u02c8 *accent* munah 05A3 60 \u05ab \u02c8 *accent* ole 05AB 03 \u0599 *accent* pashta 0599 83 \u05a1 \u02c8 *accent* pazer 05A1 33 \u05a8 \u02c8 *accent* qadma 05A8 63 \u05a8 \u02cc *accent* qadma 05A8 84 \u059f \u02c8 *accent* qarney para 059F 81 \u0597 \u02c8 *accent* revia 0597 01 \u0592 *accent* segol 0592 65 \u0593 \u02c8 *accent* shalshelet 0593 04 \u05a9 *accent* telisha qetana 05A9 24 \u05a9 *accent* telisha qetana 05A9 14 \u05a0 *accent* telisha gedola 05A0 44 \u05a0 *accent* telisha gedola 05A0 91 \u059b \u02c8 *accent* tevir 059B 73 \u0596 \u02cc *accent* tipeha 0596 93 \u05aa \u02c8 *accent* yerah ben yomo 05AA 10 \u059a \u02c8 *accent* yetiv 059A 80 \u0594 \u02c8 *accent* zaqef qatan 0594 85 \u0595 \u02c8 *accent* zaqef gadol 0595 82 \u0598 \u02c8 *accent* zarqa 0598 02 \u05ae \u02c8 *accent* zinor 05AE Numerals \u00b6 Details These signs occur in the Dead Sea scrolls. We represent them with conventional Hebrew characters for numbers and use the geresh accent or another accent to mark the letter as a numeral. The ETCBC codes are obtained by translating back from the unicode. transcription (ETCBC) transcription (Abegg) glyph remarks name >' A \u05d0\u059c *number* 1 >52 \u00e5 \u05d0\u05c4 alternative for 1, often at the end of a number, we use the upper dot to distinguish it from the other 1 *number* 1 >53 B \u05d0\u05c5 alternative for 1, often at the end of a number, we use the lower dot to distinguish it from the other 1 *number* 1 >35 \u222b \u05d0\u05bd alternative for 1, often at the end of a number, we use the meteg to distinguish it from the other 1 *number* 1 J' C \u05d9\u059c *number* 10 k' D \u05da\u059c *number* 20 Q' F \u05e7\u059c *number* 100 & + \u05be we use the maqaf to represent addition between numbers add Text-critical \u00b6 Details These signs occur in the Dead Sea scrolls. They are used to indicate uncertainty and editing acts by ancient scribes or modern editors. They do not have an associated glyph in Unicode. The ETCBC does not have codes for them, but we propose an ETCBC-compatible encoding for them. The ETCBC codes are surrounded by space, except for the brackets, where a space at the side of the ( or ) is not necessary. Codes that are marked as flag apply to the preceding character. Codes that are marked as brackets apply to the material within them. transcription (Abegg) transcription (etcbc) remarks name 0 \u03b5 token missing ? ? token uncertain (degree 1) \\ # token uncertain (degree 2) \ufffd #? token uncertain (degree 3) \u00d8 ? flag, applies to preceding character uncertain (degree 1) \u00ab # flag, applies to preceding character uncertain (degree 2) \u00bb #? flag, applies to preceding character uncertain (degree 3) | ## flag, applies to preceding character uncertain (degree 4) \u00ab \u00bb (# #) brackets uncertain (degree 2) \u2264 \u2265 (- -) brackets vacat (empty space) ( ) ( ) brackets alternative [ ] [ ] brackets reconstruction (modern) { } { } brackets removed (modern) {{ }} {{ }} brackets removed (ancient) < > (< >) brackets correction (modern) << >> (<< >>) brackets correction (ancient) ^ ^ (^ ^) brackets correction (supralinear, ancient)","title":"Hebrew"},{"location":"Writing/Hebrew/#hebrew-characters","text":"@font-face { font-family: \"Ezra SIL\"; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.ttf?raw=true'); src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"Ezra SIL\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; } Disclaimer This just a look-up table, not a full exposition of the organisation of the Masoretic system. Transcriptions The ETCBC transcription is used by the ETCBC. It has entries for all accents, but not for text-critical annotations such as uncertainty, and correction. The Abegg transcription is used in the Dead Sea scrolls. It has no entries for accents, but it has a repertoire of text-critical marks. We have back translated the latter to etcbc-compatible variants and entered them in the etcbc column, although they are not strictly etcbc marks. Phonetics The phonetic representation is meant as a tentative 1-1 correspondence with pronunciation, not with the script. See phono.ipynb , where the phonetic transcription is computed and thoroughly documented.","title":"Hebrew characters"},{"location":"Writing/Hebrew/#consonants","text":"Details For most consonants: an inner dot is a dagesh forte . For the \u05d1\u05d2\u05d3\u05db\u05e4\u05ea consonants: an inner dot is either a dagesh forte or a dagesh lene . When the \u05d4 contains a dot, it is called a mappiq . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode > a \u05d0 \u0294 when not *mater lectionis* *letter* alef 05D0 B b \u05d1 bb b v forte lene normal *letter* bet 05D1 G g \u05d2 gg g \u1e21 forte lene normal *letter* gimel 05D2 D d \u05d3 dd d \u1e0f forte lene normal *letter* dalet 05D3 H h \u05d4 h also with *mappiq*; when not *mater lectionis* *letter* he 05D4 W w \u05d5 ww w \u00fb forte when not part of a long vowel with dagesh as vowel *letter* vav 05D5 Z z \u05d6 zz z forte normal *letter* zayin 05D6 X j \u05d7 \u1e25 *letter* het 05D7 V f \u05d8 \u1e6d *letter* tet 05D8 J y \u05d9 yy y \u02b8 forte when not part of long vowel in front of final \u05d5 *letter* yod 05D9 K k \u05db kk k \u1e35 forte lene normal *letter* kaf 05DB k K \u05da k \u1e35 forte normal *letter* final kaf 05DA L l \u05dc ll l forte normal *letter* lamed 05DC M m \u05de mm m forte normal *letter* mem 05DE m M \u05dd m *letter* final mem 05DD N n \u05e0 nn n forte normal *letter* nun 05E0 n N \u05df n *letter* final nun 05DF S s \u05e1 ss s forte normal *letter* samekh 05E1 < o \u05e2 \u0295 *letter* ayin 05E2 P p \u05e4 pp p f forte lene normal *letter* pe 05E4 p P \u05e3 p f forte normal *letter* final pe 05E3 Y x \u05e6 \u1e63\u1e63 \u1e63 forte normal *letter* tsadi 05E6 y X \u05e5 \u1e63 *letter* final tsadi 05E5 Q q \u05e7 qq q forte normal *letter* qof 05E7 R r \u05e8 rr r forte normal *letter* resh 05E8 # C \u05e9 \u015d *letter* shin without dot 05E9 C v \u05e9\u05c1 \u0161\u0161 \u0161 forte normal *letter* shin with shin dot FB2A F c \u05e9\u05c2 \u015b\u015b \u015b forte normal *letter* shin with sin dot FB2B T t \u05ea tt t \u1e6f forte lene normal *letter* tav 05EA","title":"Consonants"},{"location":"Writing/Hebrew/#vowels","text":"Qere Ketiv The phonetics follows the qere , not the ketiv , when they are different. In that case a * is added. Tetragrammaton The tetragrammaton \u05d9\u05d4\u05d5\u05d4 is (vowel)-pointed in different ways; the phonetics follows the pointing, but the tetragrammaton is put between [ ] . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode A A \u00c5 \u05b7 a \u2090 normal *furtive* *point* patah 05B7 :A S \u05b2 \u1d43 *point* hataf patah 05B2 @ D \u2202 \u00ce \u05b8 \u0101 o gadol qatan *point* qamats 05B8 :@ F \u0192 \u00cf \u05b3 \u1d52 *point* hataf qamats 05B3 E R \u00ae \u2030 \u05b6 e e\u02b8 normal with following \u05d9 *point* segol 05B6 :E T \u05b1 \u1d49 \u1d49\u02b8 normal with following \u05d9 *point* hataf segol 05B1 ; E \u00e9 \u00b4 \u05b5 \u00ea \u0113 with following \u05d9 alone *point* tsere 05B5 I I \u02c6 \u00ee \u00ca \u05b4 \u00ee i with following \u05d9 alone *point* hiriq 05B4 O O \u00f8 \u05b9 \u00f4 \u014d with following \u05d5 alone *point* holam 05B9 U U \u00fc \u00a8 \u05bb u *point* qubuts 05BB : V \u221a J \u25ca \u05b0 \u1d4a left out if silent *point* sheva 05B0","title":"Vowels"},{"location":"Writing/Hebrew/#other-points-and-marks","text":"transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode . ; \u2026 \u00da \u00a5 \u03a9 \u05bc *point* dagesh or mapiq 05BC .c \u05c1 *point* shin dot 05C1 .f \u05c2 *point* sin dot 05C2 , \u05bf *point* rafe 05BF 35 \u05bd \u02c8 *point* meteg 05BD 75 \u05bd \u02c8 *point* meteg 05BD 95 \u05bd \u02c8 *point* meteg 05BD 52 \u05c4 \u02c8 *mark* upper dot 05C4 53 \u05c5 \u02c8 *mark* lower dot 05C5 * \u05af *mark* masora circle 05AF","title":"Other points and marks"},{"location":"Writing/Hebrew/#punctuation","text":"Details Some specialties in the Masoretic system are not reflected in the phonetics: setumah \u05e1 ; petuhah \u05e3 ; nun-hafuka \u0307\u05c6 . transcription (etcbc) transcription (Abegg) glyph phonetic remarks name unicode 00 . \u05c3 . *punctuation* sof pasuq 05C3 n\u0303 \u05c6 *punctuation* nun hafukha 05C6 & - \u05be - *punctuation* maqaf 05BE _ (non breaking space) space 0020 0000 \u00b1 \u05c3\u05c3 Dead Sea scrolls. We use as Hebrew character a double sof pasuq. paleo-divider 05C3 05C3 ' / \u05f3 Dead Sea scrolls. We use as Hebrew character a geresh. morpheme-break 05F3","title":"Punctuation"},{"location":"Writing/Hebrew/#hybrid","text":"Details There is a character that is mostly punctuation, but that can also influence the nature of some accents occurring in the word before. Such a character is a hybrid between punctuation and accent. See also the documentation of the BHSA about cantillation . transcription glyph phonetic remarks name unicode 05 \u05c0 *punctuation* paseq 05C0","title":"Hybrid"},{"location":"Writing/Hebrew/#accents","text":"Details Some accents play a role in deciding whether a schwa is silent or mobile and whether a qamets is gadol or qatan. In the phonetics those accents appear as \u02c8 or \u02cc . Implied accents are also added. transcription glyph phonetic remarks name unicode 94 \u05a7 \u02c8 *accent* darga 05A7 13 \u05ad \u02c8 *accent* dehi 05AD 92 \u0591 \u02c8 *accent* etnahta 0591 61 \u059c \u02c8 *accent* geresh 059C 11 \u059d \u02c8 *accent* geresh muqdam 059D 62 \u059e \u02c8 *accent* gershayim 059E 64 \u05ac \u02c8 *accent* iluy 05AC 70 \u05a4 \u02c8 *accent* mahapakh 05A4 71 \u05a5 \u02cc *accent* merkha 05A5 72 \u05a6 \u02c8 *accent* merkha kefula 05A6 74 \u05a3 \u02c8 *accent* munah 05A3 60 \u05ab \u02c8 *accent* ole 05AB 03 \u0599 *accent* pashta 0599 83 \u05a1 \u02c8 *accent* pazer 05A1 33 \u05a8 \u02c8 *accent* qadma 05A8 63 \u05a8 \u02cc *accent* qadma 05A8 84 \u059f \u02c8 *accent* qarney para 059F 81 \u0597 \u02c8 *accent* revia 0597 01 \u0592 *accent* segol 0592 65 \u0593 \u02c8 *accent* shalshelet 0593 04 \u05a9 *accent* telisha qetana 05A9 24 \u05a9 *accent* telisha qetana 05A9 14 \u05a0 *accent* telisha gedola 05A0 44 \u05a0 *accent* telisha gedola 05A0 91 \u059b \u02c8 *accent* tevir 059B 73 \u0596 \u02cc *accent* tipeha 0596 93 \u05aa \u02c8 *accent* yerah ben yomo 05AA 10 \u059a \u02c8 *accent* yetiv 059A 80 \u0594 \u02c8 *accent* zaqef qatan 0594 85 \u0595 \u02c8 *accent* zaqef gadol 0595 82 \u0598 \u02c8 *accent* zarqa 0598 02 \u05ae \u02c8 *accent* zinor 05AE","title":"Accents"},{"location":"Writing/Hebrew/#numerals","text":"Details These signs occur in the Dead Sea scrolls. We represent them with conventional Hebrew characters for numbers and use the geresh accent or another accent to mark the letter as a numeral. The ETCBC codes are obtained by translating back from the unicode. transcription (ETCBC) transcription (Abegg) glyph remarks name >' A \u05d0\u059c *number* 1 >52 \u00e5 \u05d0\u05c4 alternative for 1, often at the end of a number, we use the upper dot to distinguish it from the other 1 *number* 1 >53 B \u05d0\u05c5 alternative for 1, often at the end of a number, we use the lower dot to distinguish it from the other 1 *number* 1 >35 \u222b \u05d0\u05bd alternative for 1, often at the end of a number, we use the meteg to distinguish it from the other 1 *number* 1 J' C \u05d9\u059c *number* 10 k' D \u05da\u059c *number* 20 Q' F \u05e7\u059c *number* 100 & + \u05be we use the maqaf to represent addition between numbers add","title":"Numerals"},{"location":"Writing/Hebrew/#text-critical","text":"Details These signs occur in the Dead Sea scrolls. They are used to indicate uncertainty and editing acts by ancient scribes or modern editors. They do not have an associated glyph in Unicode. The ETCBC does not have codes for them, but we propose an ETCBC-compatible encoding for them. The ETCBC codes are surrounded by space, except for the brackets, where a space at the side of the ( or ) is not necessary. Codes that are marked as flag apply to the preceding character. Codes that are marked as brackets apply to the material within them. transcription (Abegg) transcription (etcbc) remarks name 0 \u03b5 token missing ? ? token uncertain (degree 1) \\ # token uncertain (degree 2) \ufffd #? token uncertain (degree 3) \u00d8 ? flag, applies to preceding character uncertain (degree 1) \u00ab # flag, applies to preceding character uncertain (degree 2) \u00bb #? flag, applies to preceding character uncertain (degree 3) | ## flag, applies to preceding character uncertain (degree 4) \u00ab \u00bb (# #) brackets uncertain (degree 2) \u2264 \u2265 (- -) brackets vacat (empty space) ( ) ( ) brackets alternative [ ] [ ] brackets reconstruction (modern) { } { } brackets removed (modern) {{ }} {{ }} brackets removed (ancient) < > (< >) brackets correction (modern) << >> (<< >>) brackets correction (ancient) ^ ^ (^ ^) brackets correction (supralinear, ancient)","title":"Text-critical"},{"location":"Writing/Syriac/","text":"Syriac Characters \u00b6 @font-face { font-family: \"Estrangelo Edessa\"; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.otf?raw=true'); src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"Estrangelo Edessa\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; } Letters \u00b6 transcription glyph phonetic remarks name unicode > \u0710 alaph 0710 B \u0712 beth 0712 G \u0713 gamal 0713 D \u0715 dalat 0715 H \u0717 he 0717 W \u0718 waw 0718 Z \u0719 zain 0719 X \u071a heth 071A V \u071b teth 071B J \u071d yod 071D K \u071f kaf 071F L \u0720 lamad 0720 M \u0721 mim 0721 N \u0722 nun 0722 S \u0723 semkath 0723 < \u0725 e 0725 P \u0726 pe 0726 Y \u0728 tsade 0728 Q \u0729 qof 0729 R \u072a resh 072A C \u072b shin 072B T \u072c taw 072C Word-bound diacritics \u00b6 transcription glyph phonetic remarks name unicode \" `\u0308 seyame 0308 # `\u0323 diacritical dot below 0323 ^ `\u0307 diacritical dot above 0307 Non-vocalic letter-bound diacritics \u00b6 transcription glyph phonetic remarks name unicode ^! `\u0743 unclear (syriac two vertical dots above) 0743 vocalic letter-bound diacritics \u00b6 transcription glyph phonetic remarks name unicode : shewa ?? A `\u0733 qamets 0733 A1 `\u0734 zeqapa 0734 A2 `\u0735 zeqofo 0735 O < td class=\"g\">`\u073f holem, rewaha 073F @ `\u0730 patah 0730 @1 `\u0731 petaha 0731 @2 `\u0732 petoho 0732 E `\u0736 segol 0736 E1 `\u0737 revasa arrika 0737 E2 `\u0738 revoso 0738 I `\u073a hireq 073A I1 `\u073b hevoso 073B U `\u073d qubbuts 073D U1 `\u073e esoso 073E Punctuation \u00b6 transcription glyph phonetic remarks name unicode #\\ \u0709 tahtaya, metkashpana (WS), meshalyana (WS) 0709 =. . pasuqa 002E =# \u0707 elaya 0707 =: : shewaya (WS), zauga (ES) 003A =^ \u0706 unclear (SYRIAC COLON SKEWED LEFT) 0706 =/ \u0707 elaya 0707 =\\ \u0706 unclear (SYRIAC COLON SKEWED LEFT) 0706 ^: \u0703 taksa (WS), zauga elaya (ES) 0703 ^\\ \u0708 unclear (SYRIAC SUPRALINEAR COLON SKEWED LEFT) 0708 Pericope markers \u00b6 transcription glyph phonetic remarks name unicode * \u0700 rosette 0700 . \u00b7 common dot in caesuras 00B7 _ \u2014 dash in caesuras 2014 o \u2022 large dot in caesuras 2022","title":"Syriac"},{"location":"Writing/Syriac/#syriac-characters","text":"@font-face { font-family: \"Estrangelo Edessa\"; src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.otf?raw=true'); src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff'); } body { font-family: sans-serif; } table.chars { border-collapse: collapse; } table.chars thead tr { color: #ffffff; background-color: #444444; } table.chars tbody td { border: 2px solid #bbbbbb; padding: 0.1em 0.5em; } h1.chars { margin-top: 1em; } .t { font-family: monospace; font-size: large; color: #0000ff; } .g { font-family: \"Estrangelo Edessa\", sans-serif; font-size: x-large; } .p { font-family: monospace; font-size: large; color: #666600; } .r { font-family: sans-serif; font-size: small; color: #555555; } .n { font-family: sans-serif; color: #990000; font-size: small; } .u { font-family: monospace; color: #990000; }","title":"Syriac Characters"},{"location":"Writing/Syriac/#letters","text":"transcription glyph phonetic remarks name unicode > \u0710 alaph 0710 B \u0712 beth 0712 G \u0713 gamal 0713 D \u0715 dalat 0715 H \u0717 he 0717 W \u0718 waw 0718 Z \u0719 zain 0719 X \u071a heth 071A V \u071b teth 071B J \u071d yod 071D K \u071f kaf 071F L \u0720 lamad 0720 M \u0721 mim 0721 N \u0722 nun 0722 S \u0723 semkath 0723 < \u0725 e 0725 P \u0726 pe 0726 Y \u0728 tsade 0728 Q \u0729 qof 0729 R \u072a resh 072A C \u072b shin 072B T \u072c taw 072C","title":"Letters"},{"location":"Writing/Syriac/#word-bound-diacritics","text":"transcription glyph phonetic remarks name unicode \" `\u0308 seyame 0308 # `\u0323 diacritical dot below 0323 ^ `\u0307 diacritical dot above 0307","title":"Word-bound diacritics"},{"location":"Writing/Syriac/#non-vocalic-letter-bound-diacritics","text":"transcription glyph phonetic remarks name unicode ^! `\u0743 unclear (syriac two vertical dots above) 0743","title":"Non-vocalic letter-bound diacritics"},{"location":"Writing/Syriac/#vocalic-letter-bound-diacritics","text":"transcription glyph phonetic remarks name unicode : shewa ?? A `\u0733 qamets 0733 A1 `\u0734 zeqapa 0734 A2 `\u0735 zeqofo 0735 O < td class=\"g\">`\u073f holem, rewaha 073F @ `\u0730 patah 0730 @1 `\u0731 petaha 0731 @2 `\u0732 petoho 0732 E `\u0736 segol 0736 E1 `\u0737 revasa arrika 0737 E2 `\u0738 revoso 0738 I `\u073a hireq 073A I1 `\u073b hevoso 073B U `\u073d qubbuts 073D U1 `\u073e esoso 073E","title":"vocalic letter-bound diacritics"},{"location":"Writing/Syriac/#punctuation","text":"transcription glyph phonetic remarks name unicode #\\ \u0709 tahtaya, metkashpana (WS), meshalyana (WS) 0709 =. . pasuqa 002E =# \u0707 elaya 0707 =: : shewaya (WS), zauga (ES) 003A =^ \u0706 unclear (SYRIAC COLON SKEWED LEFT) 0706 =/ \u0707 elaya 0707 =\\ \u0706 unclear (SYRIAC COLON SKEWED LEFT) 0706 ^: \u0703 taksa (WS), zauga elaya (ES) 0703 ^\\ \u0708 unclear (SYRIAC SUPRALINEAR COLON SKEWED LEFT) 0708","title":"Punctuation"},{"location":"Writing/Syriac/#pericope-markers","text":"transcription glyph phonetic remarks name unicode * \u0700 rosette 0700 . \u00b7 common dot in caesuras 00B7 _ \u2014 dash in caesuras 2014 o \u2022 large dot in caesuras 2022","title":"Pericope markers"},{"location":"Writing/Transcription/","text":"Transcription \u00b6 While Text-Fabric is a generic package to deal with text and annotations in a model of nodes, edges, and features, there is need for some additions. Transcription \u00b6 About transcription.py contains transliteration tables for Hebrew, Syriac and Arabic. It also calls functions to use these tables for converting Hebrew and Syriac text material to transliterated representations and back. There is also a phonetic transcription for Hebrew, designed in phono.ipynb Character tables Hebrew : full list of characters covered by the ETCBC and phonetic transcriptions Syriac : full list of characters covered by the ETCBC transcriptions Arabic : full list of characters covered by the transcription used for the Quran Usage Invoke the transcription functionality as follows: 1 from tf.writing.transcription import Transcription Some of the attributes and methods below are class attributes, others are instance attributes. A class attribute aaa can be retrieved by saying Transcription.aaa . To retrieve an instance attribute, you need an instance first, like 1 tr = Transcription () and then you can say tr.aaa . For each attribute we'll give a usage example. Transcription.hebrew mapping Maps all ETCBC transliteration character combinations for Hebrew to Unicode. Example: print the sof-pasuq: 1 print ( Transcription . hebrew_mapping [ '00' ]) Output: 1 \u05c3 Transcription.syriac mapping Maps all ETCBC transliteration character combinations for Syriac to Unicode. Example: print the semkath-final: 1 print ( Transcription . syriac_mapping [ 's' ]) Output: 1 \u0724 Transcription.arabic mapping Maps an Arabic transliteration character to Unicode. Example: print the beh 1 print ( Transcription . syriac_mapping [ 'b' ]) Output: 1 \u0628 Transcription.arabic mappingi Maps an Arabic letter in unicode to its transliteration Example: print the beh transliteration 1 print ( Transcription . syriac_mapping [ '\u0628' ]) Output: 1 b Transcription.suffix_and_finales(word) Given an ETCBC transliteration, split it into the word material and the interword material that follows it (space, punctuation). Replace the last consonant of the word material by its final form, if applicable. Output a tuple with the modified word material and the interword material. Example: 1 print ( Transcription . suffix_and_finales ( '71T_H@>@95REY00' )) Output: 1 ('71T_H@>@95REy', '00\\n') Note that the Y has been replaced by y . Transcription.suppress_space(word) Given an ETCBC transliteration of a word, match the end of the word for interpunction and spacing characters (sof pasuq, paseq, nun hafukha, setumah, petuhah, space, no-space) Example: 1 2 3 print ( Transcription . suppress_space ( 'B.:&' )) print ( Transcription . suppress_space ( 'B.@R@74>' )) print ( Transcription . suppress_space ( '71T_H@>@95REY00' )) Output: 1 2 3 <re.Match object; span=(3, 4), match='&'> None <re.Match object; span=(13, 15), match='00'> Transcription.to_etcbc_v(word) Given an ETCBC transliteration of a fully pointed word, strip all the non-vowel pointing (i.e. the accents). Example: 1 print ( Transcription . to_etcbc_v ( 'HAC.@MA73JIm' )) Output: 1 HAC.@MAJIm Transcription.to_etcbc_c(word) Given an ETCBC transliteration of a fully pointed word, strip everything except the consonants. Punctuation will also be stripped. Example: 1 print ( Transcription . to_etcbc_c ( 'HAC.@MA73JIm' )) Output: 1 H#MJM Note that the pointed shin ( C ) is replaced by an unpointed one ( # ). Transcription.to_hebrew(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Care will be taken that vowel pointing will be added to consonants before accent pointing. Example: 1 print ( Transcription . to_hebrew ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_x(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Vowel pointing and accent pointing will be applied in the order given by the input word. produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_x ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_v(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the accents. Example: 1 print ( Transcription . to_hebrew_v ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u05d9\u05b4\u05dd Transcription.to_hebrew_c(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_c ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05e9\u05de\u05d9\u05de Note that final consonant forms are not being used. Transcription.ph_simplify(pword) Given a phonological transliteration of a fully pointed word, produce a more coarse phonological transliteration. Example: 1 2 3 print ( Transcription . ph_simplify ( '\u0294\u1d49l\u014dh\u02c8\u00eem' )) print ( Transcription . ph_simplify ( 'm\u0101q\u02c8\u00f4m' )) print ( Transcription . ph_simplify ( 'kol' )) Output: 1 2 3 \u0294l\u014dh\u00eem m\u00e5q\u00f4m k\u00e5l Note that the simplified version transliterates the qamets gadol and qatan to the same character. tr.from_hebrew(word) Given a fully pointed word in Unicode Hebrew, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_hebrew ( '\u05d4\u05b8\u05d0\u05b8\u05bd\u05e8\u05b6\u05e5\u05c3' )) Output: 1 H@>@95REy00 tr.from_syriac(word) Given a word in Unicode Syriac, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_syriac ( '\u0721\u071f\u0723\u071d\u0722' )) Output: 1 MKSJN tr.to_syriac(word) Given a word in ETCBC transliteration, produce the word in Unicode Syriac. Example: 1 print ( tr . to_syriac ( 'MKSJN' )) Output: 1 \u0721\u071f\u0723\u071d\u0722 tr.from_arabic(word) Given a word in Unicode Arabic, produce the word in transliteration. Example: 1 print ( tr . from_arabic ( '\u0628\u0650\u0633\u0652\u0645\u0650' )) Output: 1 bisomi tr.to_arabic(word) Given a word in transliteration, produce the word in Unicode Arabic. Example: 1 print ( tr . to_arabic ( 'bisomi' )) Output: 1 \u0628\u0650\u0633\u0652\u0645\u0650","title":"Transcription"},{"location":"Writing/Transcription/#transcription","text":"While Text-Fabric is a generic package to deal with text and annotations in a model of nodes, edges, and features, there is need for some additions.","title":"Transcription"},{"location":"Writing/Transcription/#transcription_1","text":"About transcription.py contains transliteration tables for Hebrew, Syriac and Arabic. It also calls functions to use these tables for converting Hebrew and Syriac text material to transliterated representations and back. There is also a phonetic transcription for Hebrew, designed in phono.ipynb Character tables Hebrew : full list of characters covered by the ETCBC and phonetic transcriptions Syriac : full list of characters covered by the ETCBC transcriptions Arabic : full list of characters covered by the transcription used for the Quran Usage Invoke the transcription functionality as follows: 1 from tf.writing.transcription import Transcription Some of the attributes and methods below are class attributes, others are instance attributes. A class attribute aaa can be retrieved by saying Transcription.aaa . To retrieve an instance attribute, you need an instance first, like 1 tr = Transcription () and then you can say tr.aaa . For each attribute we'll give a usage example. Transcription.hebrew mapping Maps all ETCBC transliteration character combinations for Hebrew to Unicode. Example: print the sof-pasuq: 1 print ( Transcription . hebrew_mapping [ '00' ]) Output: 1 \u05c3 Transcription.syriac mapping Maps all ETCBC transliteration character combinations for Syriac to Unicode. Example: print the semkath-final: 1 print ( Transcription . syriac_mapping [ 's' ]) Output: 1 \u0724 Transcription.arabic mapping Maps an Arabic transliteration character to Unicode. Example: print the beh 1 print ( Transcription . syriac_mapping [ 'b' ]) Output: 1 \u0628 Transcription.arabic mappingi Maps an Arabic letter in unicode to its transliteration Example: print the beh transliteration 1 print ( Transcription . syriac_mapping [ '\u0628' ]) Output: 1 b Transcription.suffix_and_finales(word) Given an ETCBC transliteration, split it into the word material and the interword material that follows it (space, punctuation). Replace the last consonant of the word material by its final form, if applicable. Output a tuple with the modified word material and the interword material. Example: 1 print ( Transcription . suffix_and_finales ( '71T_H@>@95REY00' )) Output: 1 ('71T_H@>@95REy', '00\\n') Note that the Y has been replaced by y . Transcription.suppress_space(word) Given an ETCBC transliteration of a word, match the end of the word for interpunction and spacing characters (sof pasuq, paseq, nun hafukha, setumah, petuhah, space, no-space) Example: 1 2 3 print ( Transcription . suppress_space ( 'B.:&' )) print ( Transcription . suppress_space ( 'B.@R@74>' )) print ( Transcription . suppress_space ( '71T_H@>@95REY00' )) Output: 1 2 3 <re.Match object; span=(3, 4), match='&'> None <re.Match object; span=(13, 15), match='00'> Transcription.to_etcbc_v(word) Given an ETCBC transliteration of a fully pointed word, strip all the non-vowel pointing (i.e. the accents). Example: 1 print ( Transcription . to_etcbc_v ( 'HAC.@MA73JIm' )) Output: 1 HAC.@MAJIm Transcription.to_etcbc_c(word) Given an ETCBC transliteration of a fully pointed word, strip everything except the consonants. Punctuation will also be stripped. Example: 1 print ( Transcription . to_etcbc_c ( 'HAC.@MA73JIm' )) Output: 1 H#MJM Note that the pointed shin ( C ) is replaced by an unpointed one ( # ). Transcription.to_hebrew(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Care will be taken that vowel pointing will be added to consonants before accent pointing. Example: 1 print ( Transcription . to_hebrew ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_x(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Vowel pointing and accent pointing will be applied in the order given by the input word. produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_x ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_v(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the accents. Example: 1 print ( Transcription . to_hebrew_v ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u05d9\u05b4\u05dd Transcription.to_hebrew_c(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_c ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05e9\u05de\u05d9\u05de Note that final consonant forms are not being used. Transcription.ph_simplify(pword) Given a phonological transliteration of a fully pointed word, produce a more coarse phonological transliteration. Example: 1 2 3 print ( Transcription . ph_simplify ( '\u0294\u1d49l\u014dh\u02c8\u00eem' )) print ( Transcription . ph_simplify ( 'm\u0101q\u02c8\u00f4m' )) print ( Transcription . ph_simplify ( 'kol' )) Output: 1 2 3 \u0294l\u014dh\u00eem m\u00e5q\u00f4m k\u00e5l Note that the simplified version transliterates the qamets gadol and qatan to the same character. tr.from_hebrew(word) Given a fully pointed word in Unicode Hebrew, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_hebrew ( '\u05d4\u05b8\u05d0\u05b8\u05bd\u05e8\u05b6\u05e5\u05c3' )) Output: 1 H@>@95REy00 tr.from_syriac(word) Given a word in Unicode Syriac, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_syriac ( '\u0721\u071f\u0723\u071d\u0722' )) Output: 1 MKSJN tr.to_syriac(word) Given a word in ETCBC transliteration, produce the word in Unicode Syriac. Example: 1 print ( tr . to_syriac ( 'MKSJN' )) Output: 1 \u0721\u071f\u0723\u071d\u0722 tr.from_arabic(word) Given a word in Unicode Arabic, produce the word in transliteration. Example: 1 print ( tr . from_arabic ( '\u0628\u0650\u0633\u0652\u0645\u0650' )) Output: 1 bisomi tr.to_arabic(word) Given a word in transliteration, produce the word in Unicode Arabic. Example: 1 print ( tr . to_arabic ( 'bisomi' )) Output: 1 \u0628\u0650\u0633\u0652\u0645\u0650","title":"Transcription"}]}