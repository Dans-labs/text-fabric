{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Text-Fabric \u00b6 About \u00b6 Before diving head-on into Text-Fabric, you might want to read a bit more about what it is and where it came from. And after it, if you want to cite it, use this DOI: 10.5281/zenodo.592193 . Intro Text-Fabric is several things: a browser for ancient text corpora a Python3 package for processing ancient corpora A corpus of ancient texts and linguistic annotations represents a large body of knowledge. Text-Fabric makes that knowledge accessible to non-programmers by means of built-in a search interface that runs in your browser. From there the step to program your own analytics is not so big anymore. You can export your results to Excel and work with them from there. And if that is not enough, you can call the Text-Fabric API from your Python programs. This works really well in Jupyter notebooks. Factory Text-Fabric can be and has been used to construct websites, for example SHEBANQ . In the case of SHEBANQ, data has been converted to mysql databases. However, with the built-in TF kernel , it is also possible to have one process serve multiple connections and requests. Code statistics For a feel of the size of this project, in terms of lines of code, see Code lines Design principles There are a number of things that set Text-Fabric apart from most other ways to encode corpora. Minimalistic model Text-Fabric is based on a minimalistic data model for text plus annotations. A defining characteristic is that Text-Fabric does not make use of XML or JSON, but stores text as a bunch of features in plain text files. These features are interpreted against a graph of nodes and edges, which make up the abstract fabric of the text. Performance matters Based on this model, Text-Fabric offers a processing API to search, navigate and process text and its annotations. A lot of care has been taken to make this API work as fast as possible. Efficiency in data processing has been a design criterion from the start. See e.g. the comparisons between the Text-Fabric way of serializing (pickle + gzip) and avro , joblib , and marshal . Search for patterns The search API works with search templates that define relational patterns which will be instantiated by nodes and edges of the fabric. Pick and choose data Students can selectively load the feature data they need. When the time comes to share the fruits of their thought, they can do so in various ways: when using the TF browser, results can be exported as PDF and stored in a repository; when programming in a notebook, these notebooks can easily be shared online by using GitHub of NBViewer. Contributing data Researchers can easily produce new data modules of text-fabric data out of their findings. Author and co-creation Text-Fabric is not so much an original idea as well putting a few good ideas by others into practice. The idea for the Text-Fabric data model ultimately derives from ideas floating in the ETCBC-avant-la-lettre 30 years ago, culminating in Crist-Jan Doedens's Ph.D. thesis . The fact that Ulrik Sandborg Petersen has made these ideas practical in his Emdros database system 15 years ago was the next crucial step. But time moves on, and nowhere is that felt as keenly as in computing science. Programming has become easier, humanists become better programmers, and personal computers have become powerful enough to do a sizable amount of data science on them. That leads to exciting tipping points : In sociology, a tipping point is a point in time when a group - or a large number of group members \u2014 rapidly and dramatically changes its behavior by widely adopting a previously rare practice. WikiPedia Text-Fabric is an attempt to tip the scales by providing digital humanists with the functions they need now , based on technology that appeals now . Hence, my implementation of Text-Fabric search has been done from the ground up, and uses a strategy that is very different from Ulrik's MQL search engine. Dirk Roorda Acknowledgements While I wrote most of the code, a product like Text-Fabric is unthinkable without the contributions of avid users that take the trouble to give feedback and file issues, and have the zeal and stamina to hold on when things are frustrating and bugs overwhelming. In particular I thank Martijn Naaijer Cody Kingham Christiaan Erwich Cale Johnson Christian H\u00f8ygaard-Jensen Camil Staps Stephen Ku James Cu\u00e9nod Johan de Joode Gyusang Jin Kyoungsik Kim Ernst Boogert History The foundational ideas derive from work done in and around the ETCBC by Eep Talstra, Crist-Jan Doedens, Henk Harmsen, Ulrik Sandborg-Petersen and many others. The author entered in that world in 2007 as a DANS employee, doing a joint small data project, and a bigger project SHEBANQ in 2013/2014. In 2013 I developed LAF-Fabric in order to be able to construct the website SHEBANQ . I have taken out everything that makes LAF-Fabric complicated and all things that are not essential for the sake of raw data processing. Getting started \u00b6 Installation Use Documentation \u00b6 There is extensive documentation here. If you start using the Text-Fabric API in your programs, you'll definitely need it. If you are just starting with the Text-Fabric browser, it might help to look at the online tutorials for the BHSA, Peshitta, SyrNT, and Cunei corpora to see what Text-Fabric can reveal about the data. Tutorials There are tutorials and exercises to guide you into increasingly involved tasks on specific corpora (outside this repo): Biblia Hebraica Stuttgartensia Amstelodamensis Peshitta: Syriac Old Testament Syriac New Testament Proto-Cuneiform tablets from Uruk IV/III These links point to the static online versions. If you want to get these Jupyter notebooks on your system in order to execute them yourself, you can download them from a release: BHSA tutorial download Peshitta tutorial download SyrNT tutorial download Cunei tutorial download These are zip files, you can unpack them where you want. You have to have Jupyter installed. Concepts The conceptual model of Text-Fabric and how it is realized in a data model and an optimized file format. data model file format optimizations search design API Reference Text-Fabric offers lots of functionality that works for all corpora. Corpus designers can add apps to Text-Fabric that enhance its behaviours, especially in displaying the corpus in ways that make sense to people that study the corpus. TF api TF apps Bhsa app for the Hebrew Bible Peshitta app for the Syriac Old Testament Syrnt app for the Syriac New Testament Cunei app for the Proto-cuneiform tablets from Uruk Papers Papers (preprints on arxiv ), most of them published: Coding the Hebrew Bible Parallel Texts in the Hebrew Bible, New Methods and Visualizations The Hebrew Bible as Data: Laboratory - Sharing - Experiences (preprint: arxiv ) LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible Annotation as a New Paradigm in Research Archiving Presentation Here is a motivational presentation , given just before SBL 2016 in the Lutheran Church of San Antonio.","title":"Home"},{"location":"#text-fabric","text":"","title":"Text-Fabric"},{"location":"#about","text":"Before diving head-on into Text-Fabric, you might want to read a bit more about what it is and where it came from. And after it, if you want to cite it, use this DOI: 10.5281/zenodo.592193 . Intro Text-Fabric is several things: a browser for ancient text corpora a Python3 package for processing ancient corpora A corpus of ancient texts and linguistic annotations represents a large body of knowledge. Text-Fabric makes that knowledge accessible to non-programmers by means of built-in a search interface that runs in your browser. From there the step to program your own analytics is not so big anymore. You can export your results to Excel and work with them from there. And if that is not enough, you can call the Text-Fabric API from your Python programs. This works really well in Jupyter notebooks. Factory Text-Fabric can be and has been used to construct websites, for example SHEBANQ . In the case of SHEBANQ, data has been converted to mysql databases. However, with the built-in TF kernel , it is also possible to have one process serve multiple connections and requests. Code statistics For a feel of the size of this project, in terms of lines of code, see Code lines Design principles There are a number of things that set Text-Fabric apart from most other ways to encode corpora. Minimalistic model Text-Fabric is based on a minimalistic data model for text plus annotations. A defining characteristic is that Text-Fabric does not make use of XML or JSON, but stores text as a bunch of features in plain text files. These features are interpreted against a graph of nodes and edges, which make up the abstract fabric of the text. Performance matters Based on this model, Text-Fabric offers a processing API to search, navigate and process text and its annotations. A lot of care has been taken to make this API work as fast as possible. Efficiency in data processing has been a design criterion from the start. See e.g. the comparisons between the Text-Fabric way of serializing (pickle + gzip) and avro , joblib , and marshal . Search for patterns The search API works with search templates that define relational patterns which will be instantiated by nodes and edges of the fabric. Pick and choose data Students can selectively load the feature data they need. When the time comes to share the fruits of their thought, they can do so in various ways: when using the TF browser, results can be exported as PDF and stored in a repository; when programming in a notebook, these notebooks can easily be shared online by using GitHub of NBViewer. Contributing data Researchers can easily produce new data modules of text-fabric data out of their findings. Author and co-creation Text-Fabric is not so much an original idea as well putting a few good ideas by others into practice. The idea for the Text-Fabric data model ultimately derives from ideas floating in the ETCBC-avant-la-lettre 30 years ago, culminating in Crist-Jan Doedens's Ph.D. thesis . The fact that Ulrik Sandborg Petersen has made these ideas practical in his Emdros database system 15 years ago was the next crucial step. But time moves on, and nowhere is that felt as keenly as in computing science. Programming has become easier, humanists become better programmers, and personal computers have become powerful enough to do a sizable amount of data science on them. That leads to exciting tipping points : In sociology, a tipping point is a point in time when a group - or a large number of group members \u2014 rapidly and dramatically changes its behavior by widely adopting a previously rare practice. WikiPedia Text-Fabric is an attempt to tip the scales by providing digital humanists with the functions they need now , based on technology that appeals now . Hence, my implementation of Text-Fabric search has been done from the ground up, and uses a strategy that is very different from Ulrik's MQL search engine. Dirk Roorda Acknowledgements While I wrote most of the code, a product like Text-Fabric is unthinkable without the contributions of avid users that take the trouble to give feedback and file issues, and have the zeal and stamina to hold on when things are frustrating and bugs overwhelming. In particular I thank Martijn Naaijer Cody Kingham Christiaan Erwich Cale Johnson Christian H\u00f8ygaard-Jensen Camil Staps Stephen Ku James Cu\u00e9nod Johan de Joode Gyusang Jin Kyoungsik Kim Ernst Boogert History The foundational ideas derive from work done in and around the ETCBC by Eep Talstra, Crist-Jan Doedens, Henk Harmsen, Ulrik Sandborg-Petersen and many others. The author entered in that world in 2007 as a DANS employee, doing a joint small data project, and a bigger project SHEBANQ in 2013/2014. In 2013 I developed LAF-Fabric in order to be able to construct the website SHEBANQ . I have taken out everything that makes LAF-Fabric complicated and all things that are not essential for the sake of raw data processing.","title":"About"},{"location":"#getting-started","text":"Installation Use","title":"Getting started"},{"location":"#documentation","text":"There is extensive documentation here. If you start using the Text-Fabric API in your programs, you'll definitely need it. If you are just starting with the Text-Fabric browser, it might help to look at the online tutorials for the BHSA, Peshitta, SyrNT, and Cunei corpora to see what Text-Fabric can reveal about the data. Tutorials There are tutorials and exercises to guide you into increasingly involved tasks on specific corpora (outside this repo): Biblia Hebraica Stuttgartensia Amstelodamensis Peshitta: Syriac Old Testament Syriac New Testament Proto-Cuneiform tablets from Uruk IV/III These links point to the static online versions. If you want to get these Jupyter notebooks on your system in order to execute them yourself, you can download them from a release: BHSA tutorial download Peshitta tutorial download SyrNT tutorial download Cunei tutorial download These are zip files, you can unpack them where you want. You have to have Jupyter installed. Concepts The conceptual model of Text-Fabric and how it is realized in a data model and an optimized file format. data model file format optimizations search design API Reference Text-Fabric offers lots of functionality that works for all corpora. Corpus designers can add apps to Text-Fabric that enhance its behaviours, especially in displaying the corpus in ways that make sense to people that study the corpus. TF api TF apps Bhsa app for the Hebrew Bible Peshitta app for the Syriac Old Testament Syrnt app for the Syriac New Testament Cunei app for the Proto-cuneiform tablets from Uruk Papers Papers (preprints on arxiv ), most of them published: Coding the Hebrew Bible Parallel Texts in the Hebrew Bible, New Methods and Visualizations The Hebrew Bible as Data: Laboratory - Sharing - Experiences (preprint: arxiv ) LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible Annotation as a New Paradigm in Research Archiving Presentation Here is a motivational presentation , given just before SBL 2016 in the Lutheran Church of San Antonio.","title":"Documentation"},{"location":"Corpora/","text":"Corpora \u00b6 Corpora are usually stored in an online repository, such as GitHub or a research data archive such as DANS . Get corpora \u00b6 Automatically There are a few corpora in Text-Fabric that are being supported with extra modules. Text-Fabric will fetch them for you if you use the Text-Fabric browser. And if you work with them from within a Python program (e.g. in a Jupyter Notebook), you get the data when you just say B = Bhsa() . Manually You can also download the data you need up-front. There are basically two ways: from a release binary Go to the relevant GitHub repository, click on releases, and choose the relevant binary that has been attached to the release. Download it to your system. This is the most economical way to get data. clone the complete repository Clone the relevant GitHub repository to your system. This will get you lots of additional data that you might not directly need. On Windows? You have to install git in some way for this step. ETCBC \u00b6 ETCBC data Inspect the repositories of the etcbc organization on GitHub to see what is available. Per repository, click the Releases button so see the release version that holds the relevant binaries with TF-data. For example, try crossrefs . Hebrew Bible \u00b6 From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (25MB) 1 2 from tf.extra.bhsa import Bhsa B = Bhsa () ETCBC versions The data of the BHSA comes in major versions, ranging from 3 (2011) via 4 , 4b , 2016 , 2017 to c (2018). The latter is a continuous version, which will change over time. From GitHub If you want to be in complete control, you can get the complete data repository from GitHub (5GB): 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/bhsa and likewise you can get other ETCBC data modules such as phono . Peshitta \u00b6 From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (2MB) 1 2 from tf.extra.peshitta import Peshitta P = Peshitta () From GitHub If you want to be in complete control, you can get the complete data repository from GitHub: 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/peshitta Syriac New Testament \u00b6 From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (2MB) 1 2 from tf.extra.syrnt import Syrnt SY = Syrnt () From GitHub If you want to be in complete control, you can get the complete data repository from GitHub: 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/syrnt Cuneiform tablets from Uruk \u00b6 From within a program If you are in a Jupyter notebook or Python script, the Cunei API will fetch the data for you automatically: The TF-part is 1.6 MB the photos and lineart are 550MB! Caution So do this only when you have a good internet connection. From GitHub If you want to be in complete control, you can get the complete data repository from GitHub (1.5GB): 1 2 cd ~/github/Nino-cunei git clone https://github.com/Nino-cunei/uruk More corpora \u00b6 The Greek and Syriac New Testament have been converted to TF. We have example corpora in Sanskrit, and Babylonian. From GitHub 1 2 cd ~/github git clone https://github.com/etcbc/linksyr 1 2 cd ~/github git clone https://github.com/Dans-labs/text-fabric-data All these are not supported by extra interfaces.","title":"Corpora"},{"location":"Corpora/#corpora","text":"Corpora are usually stored in an online repository, such as GitHub or a research data archive such as DANS .","title":"Corpora"},{"location":"Corpora/#get-corpora","text":"Automatically There are a few corpora in Text-Fabric that are being supported with extra modules. Text-Fabric will fetch them for you if you use the Text-Fabric browser. And if you work with them from within a Python program (e.g. in a Jupyter Notebook), you get the data when you just say B = Bhsa() . Manually You can also download the data you need up-front. There are basically two ways: from a release binary Go to the relevant GitHub repository, click on releases, and choose the relevant binary that has been attached to the release. Download it to your system. This is the most economical way to get data. clone the complete repository Clone the relevant GitHub repository to your system. This will get you lots of additional data that you might not directly need. On Windows? You have to install git in some way for this step.","title":"Get corpora"},{"location":"Corpora/#etcbc","text":"ETCBC data Inspect the repositories of the etcbc organization on GitHub to see what is available. Per repository, click the Releases button so see the release version that holds the relevant binaries with TF-data. For example, try crossrefs .","title":"ETCBC"},{"location":"Corpora/#hebrew-bible","text":"From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (25MB) 1 2 from tf.extra.bhsa import Bhsa B = Bhsa () ETCBC versions The data of the BHSA comes in major versions, ranging from 3 (2011) via 4 , 4b , 2016 , 2017 to c (2018). The latter is a continuous version, which will change over time. From GitHub If you want to be in complete control, you can get the complete data repository from GitHub (5GB): 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/bhsa and likewise you can get other ETCBC data modules such as phono .","title":"Hebrew Bible"},{"location":"Corpora/#peshitta","text":"From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (2MB) 1 2 from tf.extra.peshitta import Peshitta P = Peshitta () From GitHub If you want to be in complete control, you can get the complete data repository from GitHub: 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/peshitta","title":"Peshitta"},{"location":"Corpora/#syriac-new-testament","text":"From within a program If you are in a Jupyter notebook or Python script, this will fetch the data (2MB) 1 2 from tf.extra.syrnt import Syrnt SY = Syrnt () From GitHub If you want to be in complete control, you can get the complete data repository from GitHub: 1 2 cd ~/github/etcbc git clone https://github.com/etcbc/syrnt","title":"Syriac New Testament"},{"location":"Corpora/#cuneiform-tablets-from-uruk","text":"From within a program If you are in a Jupyter notebook or Python script, the Cunei API will fetch the data for you automatically: The TF-part is 1.6 MB the photos and lineart are 550MB! Caution So do this only when you have a good internet connection. From GitHub If you want to be in complete control, you can get the complete data repository from GitHub (1.5GB): 1 2 cd ~/github/Nino-cunei git clone https://github.com/Nino-cunei/uruk","title":"Cuneiform tablets from Uruk"},{"location":"Corpora/#more-corpora","text":"The Greek and Syriac New Testament have been converted to TF. We have example corpora in Sanskrit, and Babylonian. From GitHub 1 2 cd ~/github git clone https://github.com/etcbc/linksyr 1 2 cd ~/github git clone https://github.com/Dans-labs/text-fabric-data All these are not supported by extra interfaces.","title":"More corpora"},{"location":"Faq/","text":"FAQ \u00b6 It does not work. Why? Latest Text-Fabric \u00b6 Always use the latest version of Text-Fabric, because there is still a lot of development going on. I have installed Text-Fabric, yet I get complaints that it cannot be found! Most likely, you installed Text-Fabric into another Python than you use when you run your Python programs. See Python Setup below. Why do I not get the latest version of Text-Fabric? When you get errors doing pip3 install text-fabric , there is probably an older version around. You have to say 1 pip3 install --upgrade text-fabric If this still does not download the most recent version of text-fabric , it may have been caused by caching. Then say: 1 pip3 install --upgrade --no-cache-dir text-fabric You can check what the newest distributed version of Text-Fabric is on PyPi . Why do I still not get the latest version of Text-Fabric!?!? Old versions on your system might get in the way. Sometimes pip3 uninstall text-fabric fails to remove all traces of Text-Fabric. Here is how you can remove them manually: locate the bin directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/bin (Windows Anaconda) C:\\Users\\You\\Anaconda3\\Scripts Remove the file text-fabric from this directory if it exists. locate the site-packages directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages Remove the subdirectory tf from this location, plus all files with text-fabric in the name. After this, you can make a fresh install of text-fabric : 1 pip3 install text-fabric Python setup \u00b6 If you are new to Python, it might be tricky to set up Python the right way. If you make unlucky choices, and work with trial and error, things might get messed up. Most of the times when text-fabric does not appear to work, it is because of this. Here are some hints to recover from that. Older versions do not get away! Older versions of Python may be in the way. The following hygeneic measures are known to be beneficial: When you have upgraded Python, remove PATH statements for older versions from your system startup files. For the Macos: look at .bashrc , .bash_profile in your home directory. For Windows: on the command prompt, say echo %path% to see what the content of your PATH variable is. If you see references to older versions of python than you actually work with, they need to be removed. Here is how Only for Python3 Do not remove references to Python 2.* , but only outdated Python 3.* versions. Text-Fabric browser \u00b6 I get an Internal Server Error ! When the TF browser opens with an Internal Server error, the most likely reason is that the TF kernel has not started up without errors. Look back at the terminal or command prompt where you started text-fabric . Error If somewhere down the road you see Error , I offer you my apologies! Copy and paste that error and send it to me , and I'll fix it as soon as I can, and I let you know on the issue list . Out of memory If TF has run out of memory, you might be able to do something about it. In this case, during loading TF did not have access too enough RAM memory. Maybe you had too many programs (or browser tabs) open at that time. Close as many programs as possible (even better, restart your machine) and try again. TF is know to work on Windows 10 machines with only 3GB RAM on board, but only in the best of circumstances. If your machine has 4GB of RAM, it should be possible to run TF, with care.","title":"Faq"},{"location":"Faq/#faq","text":"It does not work. Why?","title":"FAQ"},{"location":"Faq/#latest-text-fabric","text":"Always use the latest version of Text-Fabric, because there is still a lot of development going on. I have installed Text-Fabric, yet I get complaints that it cannot be found! Most likely, you installed Text-Fabric into another Python than you use when you run your Python programs. See Python Setup below. Why do I not get the latest version of Text-Fabric? When you get errors doing pip3 install text-fabric , there is probably an older version around. You have to say 1 pip3 install --upgrade text-fabric If this still does not download the most recent version of text-fabric , it may have been caused by caching. Then say: 1 pip3 install --upgrade --no-cache-dir text-fabric You can check what the newest distributed version of Text-Fabric is on PyPi . Why do I still not get the latest version of Text-Fabric!?!? Old versions on your system might get in the way. Sometimes pip3 uninstall text-fabric fails to remove all traces of Text-Fabric. Here is how you can remove them manually: locate the bin directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/bin (Windows Anaconda) C:\\Users\\You\\Anaconda3\\Scripts Remove the file text-fabric from this directory if it exists. locate the site-packages directory of the current Python, it is something like (Macos regular Python) /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages Remove the subdirectory tf from this location, plus all files with text-fabric in the name. After this, you can make a fresh install of text-fabric : 1 pip3 install text-fabric","title":"Latest Text-Fabric"},{"location":"Faq/#python-setup","text":"If you are new to Python, it might be tricky to set up Python the right way. If you make unlucky choices, and work with trial and error, things might get messed up. Most of the times when text-fabric does not appear to work, it is because of this. Here are some hints to recover from that. Older versions do not get away! Older versions of Python may be in the way. The following hygeneic measures are known to be beneficial: When you have upgraded Python, remove PATH statements for older versions from your system startup files. For the Macos: look at .bashrc , .bash_profile in your home directory. For Windows: on the command prompt, say echo %path% to see what the content of your PATH variable is. If you see references to older versions of python than you actually work with, they need to be removed. Here is how Only for Python3 Do not remove references to Python 2.* , but only outdated Python 3.* versions.","title":"Python setup"},{"location":"Faq/#text-fabric-browser","text":"I get an Internal Server Error ! When the TF browser opens with an Internal Server error, the most likely reason is that the TF kernel has not started up without errors. Look back at the terminal or command prompt where you started text-fabric . Error If somewhere down the road you see Error , I offer you my apologies! Copy and paste that error and send it to me , and I'll fix it as soon as I can, and I let you know on the issue list . Out of memory If TF has run out of memory, you might be able to do something about it. In this case, during loading TF did not have access too enough RAM memory. Maybe you had too many programs (or browser tabs) open at that time. Close as many programs as possible (even better, restart your machine) and try again. TF is know to work on Windows 10 machines with only 3GB RAM on board, but only in the best of circumstances. If your machine has 4GB of RAM, it should be possible to run TF, with care.","title":"Text-Fabric browser"},{"location":"Install/","text":"Install \u00b6 Text Fabric is a Python(3) package on the Python Package Index, so you can install it easily with pip3 or pip from the command line. Prerequisites \u00b6 Computer Your computer should be a 64-bit machine and it needs at least 3 GB RAM memory. It should run Linux, Macos, or Windows. close other programs When you run the Text-Fabric browser for the first time, make sure that most of that minimum of 3GB RAM is actually available, and not in use by other programs. Python \u00b6 3.6+ 64 bit Install or upgrade Python on your system to at least version 3.6.3. Go for the 64-bit version. Otherwise Python may not be able to address all the memory it needs. Distro The leanest install is provided by python.org . You can also install it from anaconda.com . on Windows? Choose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix, you need to have additional developers' tools installed. The Anaconda distribution has Jupyter out of the box. When installing Python, make sure that the installer adds the path to Python to your environment variables. Install Chrome of Firefox and set it as your default browser. The Text-Fabric browser does not display well in Microsoft Edge, for Edge does not support the details element. on Linux? On Ubuntu 18.04 Python 3.6 is already installed. But you have to install pip3 in order to add the Python package text-fabric. 1 sudo apt install python3-pip Text-Fabric \u00b6 Install Text-Fabric: \u00b6 1 pip3 install text-fabric On Windows: 1 pip install text-fabric to 3 or not to 3? From now on we always use python3 and pip3 in instructions. On Windows you have to say python and pip . There are more differences, when you go further with programming. Advice: if you are serious on programming, consider switching to a Unix -like platform, such as Linux or the Mac (macos). on Linux? On Ubuntu the text-fabric script ends up in your ~/.local/bin directory, but this is not on your PATH. You need to execute your .profile file first by: 1 source ~/.profile You need to do this every time when you open a new terminal and want to run Text-Fabric. If you get tired of this, you can add this to your .bashrc file: 1 2 PATH = \"~/.local/bin: ${ PATH } \" export PATH Upgrade Text-Fabric: \u00b6 1 pip3 install --upgrade text-fabric Jupyter notebook \u00b6 1 2 3 4 5 Optionally install [Jupyter](http://jupyter.org) as well: ```sh pip3 install jupyter ``` Jupyter Lab Jupyter lab is a nice context to work with Jupyter notebooks. Recommended for working with the the tutorials of Text-Fabric. Also when you want to copy and paste cells from one notebook to another. 1 2 pip3 install jupyterlab jupyter labextension install jupyterlab-toc The toc-extension is handy to get an overview when working with the lengthy tutorial. It will create an extra tab in the Jupyter Lab interface with a table of contents of the current notebook. Node version Jupyter lab is an interesting off-spring from the marriage between the Python world and the Javascript world. It is still in beta, and there are rough edges. In order to install lab extensions you need to have Node installed, but not the newest version. There is still a bug in the Jupyter lab that causes the installation to hang. With Node 8 you are fine.","title":"Install"},{"location":"Install/#install","text":"Text Fabric is a Python(3) package on the Python Package Index, so you can install it easily with pip3 or pip from the command line.","title":"Install"},{"location":"Install/#prerequisites","text":"Computer Your computer should be a 64-bit machine and it needs at least 3 GB RAM memory. It should run Linux, Macos, or Windows. close other programs When you run the Text-Fabric browser for the first time, make sure that most of that minimum of 3GB RAM is actually available, and not in use by other programs.","title":"Prerequisites"},{"location":"Install/#python","text":"3.6+ 64 bit Install or upgrade Python on your system to at least version 3.6.3. Go for the 64-bit version. Otherwise Python may not be able to address all the memory it needs. Distro The leanest install is provided by python.org . You can also install it from anaconda.com . on Windows? Choose Anaconda over standard Python. The reason is that when you want to add Jupyter to the mix, you need to have additional developers' tools installed. The Anaconda distribution has Jupyter out of the box. When installing Python, make sure that the installer adds the path to Python to your environment variables. Install Chrome of Firefox and set it as your default browser. The Text-Fabric browser does not display well in Microsoft Edge, for Edge does not support the details element. on Linux? On Ubuntu 18.04 Python 3.6 is already installed. But you have to install pip3 in order to add the Python package text-fabric. 1 sudo apt install python3-pip","title":"Python"},{"location":"Install/#text-fabric","text":"","title":"Text-Fabric"},{"location":"Install/#install-text-fabric","text":"1 pip3 install text-fabric On Windows: 1 pip install text-fabric to 3 or not to 3? From now on we always use python3 and pip3 in instructions. On Windows you have to say python and pip . There are more differences, when you go further with programming. Advice: if you are serious on programming, consider switching to a Unix -like platform, such as Linux or the Mac (macos). on Linux? On Ubuntu the text-fabric script ends up in your ~/.local/bin directory, but this is not on your PATH. You need to execute your .profile file first by: 1 source ~/.profile You need to do this every time when you open a new terminal and want to run Text-Fabric. If you get tired of this, you can add this to your .bashrc file: 1 2 PATH = \"~/.local/bin: ${ PATH } \" export PATH","title":"Install Text-Fabric:"},{"location":"Install/#upgrade-text-fabric","text":"1 pip3 install --upgrade text-fabric","title":"Upgrade Text-Fabric:"},{"location":"Install/#jupyter-notebook","text":"1 2 3 4 5 Optionally install [Jupyter](http://jupyter.org) as well: ```sh pip3 install jupyter ``` Jupyter Lab Jupyter lab is a nice context to work with Jupyter notebooks. Recommended for working with the the tutorials of Text-Fabric. Also when you want to copy and paste cells from one notebook to another. 1 2 pip3 install jupyterlab jupyter labextension install jupyterlab-toc The toc-extension is handy to get an overview when working with the lengthy tutorial. It will create an extra tab in the Jupyter Lab interface with a table of contents of the current notebook. Node version Jupyter lab is an interesting off-spring from the marriage between the Python world and the Javascript world. It is still in beta, and there are rough edges. In order to install lab extensions you need to have Node installed, but not the newest version. There is still a bug in the Jupyter lab that causes the installation to hang. With Node 8 you are fine.","title":"Jupyter notebook"},{"location":"News/","text":"Changes \u00b6 Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off all possibilities: bhsa peshitta syrnt cunei Queued for next release \u00b6 6.4.3-4 \u00b6 2018-11-06 Big bug fix in queries: basic relationships in combination with custom sets. The implementation of the basic relationships did not reckon with custom sets that contains both slot nodes and non-slot nodes. And it did not trigger the right code when a custom set has only slot nodes. That has been remedied. Some of the search tutorials have been expanded to include a number of these critical cases. A more complete test suite outside the tutorials is still on my to do list. Thanks to Cody Kingham for spotting and reporting this bug. 6.4, 6.4.1-2 \u00b6 2018-11-02 A passage browsing interface that interacts with the search results. The interface scrolls to the highlighted row. Minor things: More refined warnings when you run out of memory TF checks whether you are running 64 bit Python. If not, a warning is issued. 6.3.2 \u00b6 2018-10-27 Better documentation for installation of Text-Fabric on Ubuntu. Added new module requirements: ipykernel and notebook. 6.3.1 \u00b6 2018-10-24 An optional parameter silent=False has been added to the initialisation calls of the specific app APIs: you can say now 1 2 3 4 A = Bhsa ( silent = True ) A = Cunei ( silent = True ) A = Peshitta ( silent = True ) A = Syrnt ( silent = True ) and then all non-error messages will be suppressed. If the underlying TF API needs to precompute data, it will still be shown, because this may cause an otherwise unexpected delay. Since this is a releatively rare case, and since this can be remedied by running the call again, I leave this behaviour as it is. 6.3 \u00b6 2018-10-19 Character tables for Hebrew abd Syriac, with links to them from the TF browser Better font handling In the pretty and plain functions you can pass a fmt parameter, to control the text representation (original script, transcription, phonetics) You can also control the text representation in the Text-Fabric browser. 6.2.2 \u00b6 2018-10-18 Added ETCBC/WIT transcriptions to the SyrNT data source. Now both Peshitta and Syriac New Testament have ETCBC transcriptions. The older, rectangular logo makes place for the more crisp, circular one 6.2.1 \u00b6 2018-10-17 New app: Syrnt (Syriac New Testament. It works much like the Peshitta, but the SyrNT data has linguistic annotations at the word and lexeme levels. After this upgrade you can browse the SyrNT by saying text-fabric syrnt on the command line. 6.2 \u00b6 2018-10-16 New app: Peshitta. It works much like the BHSA, but there is one big difference: the current Peshitta data does not have linguistic annotations. There are just books, chapters, verses and words. We expect to add lemmatizations of words shortly. After this upgrade you can browse the peshitta by saying text-fabric peshitta on the command line. Fixed a bug in exportMQL: when there are no enumerated values, do not write out an empty CREATE ENUMERATION statement to the MQL file. 6.1 \u00b6 2018-10-12 More precise provenance data when you export results from the Text-Fabric data; Under the hood reorganization of configuration data of apps like Bhsa and Cunei; App-specific parts of the code have moved to more generic parts: a big cleanup has performed; This will make it easier to add new apps. 6.0.7-8-9 \u00b6 2018-10-11 Avoid computing the notebook name when the user passes a name for the notebook to Cunei() or Bhsa() . And when the computing needs to be done, all exceptions will be caught, because the code for determining the notebook name is brittle, and may crash if the Jupyter version does not match. Fixed the bug that the Bhsa and Cunei did not run properly outside a notebook or outside a github repo. In Bhsa and Cunei, the generated info after the incantation can be collapsed (features, API members). 6.0.6 \u00b6 2018-10-10 In the BHSA, the edge features are now shown too after the incantation. If you hoist the API members into your namespace, you will get a list of hoisted names, linked to the API documentation. 6.0.5 \u00b6 2018-10-09 When using BHSA and Cunei in a notebook, there is an even simpler incantation which auto downloads features. In the BHSA it is shown which features are loaded, with direct links to the feature docs. 6.0.4 \u00b6 2018-10-09 When using BHSA and Cunei in a notebook, there is a simpler incantation which auto downloads features. Some issues concerning paths in zipfiles of downloaded data have been solved. 6.0.3 \u00b6 Easier incantations for Bhsa() and Cunei() . It is no longer needed to pass the name of the notebook, but you can still do so: name='mynotebook' You can leave out the api argument in Bhsa() . Then you do not have to load features by means of TF.load() , Bhsa() will load a standard set of features, and if the BHSA data is missing, it will download them first. The former ways of calling Bhsa() and Cunei() are still valid. Note that all arguments have become optional. 2018-10-08 The Text-Fabric browser will always print a banner with its name and version. If you pass it the argument --help or -h or --version or -v it will show the relevant information and stop executing. 6.0.2 \u00b6 2018-10-07 The Text-Fabric browser takes it data by default from ~/text-fabric-data . It will not check local github clones for data. But if you pass the option -lgc , it will first check your local github clones. So it you do nothing special, the TF browser always works with the auto-downloaded data. 6.0.1 \u00b6 2018-10-06 Not only the core BHSA data will auto load, also the related PHONO and PARALLELS data. A new release has been made of the related data, and they are now in sync with the release of the core data. If you use auto load already, you do not have to do anything. But if you have the etcbc/phono and etcbc/parallels repos in your ~/github folder, you should do a git pull origin master on those repos. N.B. : I am contemplating to have the Text-Fabric browser always use data from ~/text-fabric-data and no longer from ~/github/etcbc . Then the TF browser always controls its own data, and it will not occur that the version of the TF browser is not compatible with the version of the TF data in your github repos, or that the main data and the related data are out of synch. The disadvantage is that if you have the github repos on your system, you get redundant data in ~/text-fabric-data . However, there is only one version kept in ~/text-fabric-data , so this is not much. 6.0 \u00b6 2018-10-05 A big update with several changes: API change: \u00b6 T.text() has got more behaviours. This change was needed for the Text-Fabric browser, in order to represent lexemes in exported files. Showcase: BHSA dictionary Here is how you can collect the BHSA lexemes in an Excel sheet. about.md RESULTSX.tsv It might also be handy for the programmers amongst you. See the updated API doc on T , expand the T.text() item. Auto update \u00b6 The Text-Fabric browser checks if you are using the most recent release of the data. Font rendering \u00b6 A font rendering issue in Safari 12 in macos Mojave prevented the use of Ezra SIL for Hebrew in notebooks. We now work around this by relying on the distribution of Ezra SIL as webfont in the font library . Additional small fixes. \u00b6 Not worth telling. update Text-Fabric To update Text-Fabric itself to version 6.0, consult Upgrade . Perform this step first, because the new TF may download the new data for you. Data update needed In order to work successfully with the new T.text() function, you need a newer release (1.4) of the BHSA data . (In fact, only one line in one feature has changed ( otext.tf ). Here is how you get the new data release: Automatically \u00b6 If previously your Text-Fabric browser has automatically downloaded the data for you, it will detect the new release and download it automatically. You do not have to do anything, except increase your patience. The download (24 MB) takes some time and after that Text-Fabric will precompute related data, which may take a few minutes. This is a one-time-step after a data update. Manually \u00b6 If you have a clone of the BHSA repository, then go to that directory and say git pull origin master . If you get error messages, then you have local changes in your local BHSA repository that conflict with the github version. Probably you have run the tutorials in place. Best thing to do is: copy your BHSA tutorial directory to somehwere else; remove your local BHSA repository entirely; decide whether you really want the whole repo back (nearly 4 GB). If not: you're done, and TF will download automatically the data it needs. If you still need it: move one directory up (into the etcbc directory) and do git clone https://github.com/ETCBC/bhsa . If you want to consult the tutorials, either: view them on nbviewer ; or run them in a directory outside the BHSA repo (where you have copied it a minute ago). 5.6.4 \u00b6 2018-10-04 Solved a font-rendering issue on Safari 12 (Macos Mojave): locally installed fonts, such as Ezra SIL are not being honored. So I linked to a stylesheet of the fontlibrary which has a webfont version of Ezra SIL. That worked. 5.6.3 \u00b6 2018-10-04 Exported tab-separated files get extension .tsv instead of .csv , because then they render better in GitHub. 5.6.2 \u00b6 2018-10-04 Small optimization. More docs about reading and writing Excel compatible CSV files with Hebrew characters in it. 5.6.1 \u00b6 2018-10-04 Better exporting from TF browser: a good RESULTSX.tsv with results, sensibly augmented with information, directly openable in Excel, even when non-latin unicode code characters are present . All features that occur in the searchTemplate are drawn in into the RESULTSX.tsv, onto the nodes they filter. An additonal feature filtering is now possible in searchTemplates: feature* . This acts as \"no additional constraint\", so it does not influence the result set. But it will be picked up and used to add information into the RESULTSX.tsv. 5.5.25 \u00b6 2018-10-03 The Text-Fabric browser exports results as node lists and produces also a CONTEXT.tsv with all feature values for all nodes in the results. However, it does not contain full text representations of the nodes and it is also not possible to see in what verses the nodes occur. That has changed. The last column of CONTEXT.tsv contains the full text of a node. And there are three columns at the beginning that contain the references to the sections the node is in. For the BHSA that is the book, chapter and verse. 5.5.24 \u00b6 2018-09-25 BHSA app in Text-Fabric Browser: the book names on the verse pad should be the English book names. That is now in the help texts, including a link to the list of English book names. 5.5.23 \u00b6 2018-09-21 Problem in use of msgCache in the search engine, which caused fetch() to fail in some cases. Fixed. 5.5.22 \u00b6 2018-09-13 Fix in left-to-right displays of extra features in pretty() displays in the BHSA. 5.5.21 \u00b6 2018-08-30 Bug fix in transcription.py w.r.t. to Syriac transcriptions. 5.5.20 \u00b6 2018-08-16 BHSA app: adjusted the color of the gloss attribute: darker. 5.5.19 \u00b6 2018-07-19 Fixed: when opening files for reading and writing for an export of a TF browser session: specify that the encoding is utf8 . This is needed on those windowses where the default encoding is something else, usually cp1252 . 5.5.18 \u00b6 2018-07-19 No change, only in the build script. This is a test whether after uploading to PyPi, users can upgrade without using the --no-cache-dir in their pip commands. 5.5.17 \u00b6 2018-07-19 The main functions in kernel and web can be passed arguments, instead that they always read from sys.argv. So that it can be used packaged apps. 5.5.16 \u00b6 2018-07-17 Extra option when starting up the text-fabric web interface: -docker to let the webserver listen at 0.0.0.0 instead of localhost . So that it can be used in a Docker container. 5.5.15 \u00b6 2018-07-16 Extra option when starting up the text-fabric web interface: -noweb to not start the web browser. So that it can be used in a Docker container. 5.5.13-14 \u00b6 2018-07-12 Better error reporting of quantified queries. 5.5.12 \u00b6 2018-07-11 Faster export of big csv lists. Tweaks in the web interface. Cleaner termination of processes. The concept TF data server is now called TF kernel 5.5.8-11 \u00b6 2018-07-10 Better in catching out-of-memory errors. Prevents creation of corrupt compiled binary TF data. Prevents starting the webserver if the TF kernel fails to load. 5.5.7 \u00b6 2018-07-09 Optimization is export from TF browser. 5.5.6 \u00b6 2018-07-09 Better help display. The opened-state of help sections is remembered. You can open help next to an other open section in the sidebar. 5.5.5 \u00b6 2018-07-08 Crisper icon. 5.5.4 \u00b6 2018-07-6 Docs updated. Little bit of refactoring. 5.5.1-3 \u00b6 2018-07-4 In the TF browser, use a selection of all the features when working with the BHSA. Otherwise in Windows you might run out of memory, even if you have 8GB RAM. 5.5 \u00b6 2018-07-4 Text-Fabric can download data for BHSA and Cunei. You do not have to clone github repositories for that. The data downloaded by Text-Fabric ends up in text-fabric-data under your home directory. 5.4.5-7 \u00b6 2018-07-03 Experimenting with setuptools to get the text-fabric script working on Windows. 5.4.4 \u00b6 2018-07-02 Added renaming/duplicating of jobs and change of directory. 5.4.3 \u00b6 2018-06-29 Small fixes in error reporting in search. 5.4.1-2 \u00b6 2018-06-28 Text-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata. Small fixes. 5.4 \u00b6 2018-06-26 Improved interface and functionality of the text-fabric browser: you can save your work you can enter verse references and tablet P numbers there is help there is a side bar Docs not up to date The API docs are not up-to-date: there are new functions in the Bhsa and Cunei APIs. The server/kernel/client apis are not completely spelled out. However, the help for the text-fabric browser is included in the interface itself. 5.3.3 \u00b6 2018-06-23 Small fix: command line args for text-fabric. 5.3.0-2 \u00b6 2018-06-22 Better process management When the TF web interface is started, it cleans up remnant process that might get in the way otherwise. You can also say 1 text-fabric -k to kill all remnant processes, or 1 text-fabric -k datasource to kill the processes for a specific datasource only. Manual node entry You can enter nodes manually in the TF browser. Handy for quick inspection. You can click on the sequence number to append the node tuple of that row to the tuple input field. That way you can collect interesting results. Name and Description You can enter a name which will be used as title and file name during export. You can enter a description in Markdown. When you export your query, the description appears formatted on top. Provenance If you export a query, provenance is added, using DOIs. Small fixes No more blank pages due to double page breaks. 5.2.1 \u00b6 2018-06-21 Added an expand all checkbox in the text-fabric browser, to expand all shown rows or to collapse them. Export function for search results in the text-fabric browser. What you see is what you get, 1 pretty display per page if you have the browser save it to pdf. Small tweaks 5.1 \u00b6 2018-06-21 When displaying results in condensed mode, you can now choose the level of the container in which results are highlighted. So far it was fixed to verse for the bhsa and tablet for cunei. The docs are lagging behind! But it is shown in the tutorials and you can observer it in the text-fabric browser. 5.0.1,2,3,4 \u00b6 2018-06-19 Addressed start-up problems. 5.0 \u00b6 2018-06-18 Built in webserver and client for local query running. It is implemented for Bhsa and Cunei. 4.4.2,3 \u00b6 2018-06-13 New distribution method with setuptools. Text-Fabric has now dependencies on modules rpyc and bottle, because it contains a built-in TF kernel and webserver. This website is still barely functional, though. 4.4.1 \u00b6 2018-06-10 Search API: Escapes in regular expression search was buggy and convoluted. If a feature value contains a | then in an RE you have to enter \\| to match it. But to have that work in a TF search, you needed to say \\\\\\| . On the other hand, in the same case for . instead of | , you could just sat \\. In the new situation you do not have to double escape in REs anymore. You can just say \\| and \\. . 4.4 \u00b6 2018-06-06 Search API: S.search() accepts a new optional parameter: withContext . It triggers the output of context information for nodes in the result tuples. 4.3.4, 4.3.5 \u00b6 2018-06-05 Search API: The /with/ /or/ /or/ /-/' quantifier is also allowed with zero /or/` s. Small fix in the /with/ quantifier if there are quantifiers between this one and its parent atom. 4.3.3 \u00b6 2018-06-01 Search API: Improved quantifiers in search: /where/ /have/ /without/ /with/ /or/ /-/ ; much clearer indentation rules (no caret anymore); better reporting by S.study() . 4.3.2 \u00b6 2018-05-31 Search API: quantifiers may use the name .. to refer to their parents you may use names in the place of atoms, which lessens the need for constructs with p = q stricter checks on the syntax and position of quantifiers 4.3.1 \u00b6 2018-05-30 Docs and metadata update 4.3.0 \u00b6 2018-05-30 API Change in Search. In search templates I recently added things like 1 word vt! which checks for words that do not have a value for feature vt . The syntax for this has now changed to 1 word vt# Unequal (#) in feature value conditions. Now you can say things like 1 word vt#infa|infc meaning that the value of feature is not one of infa , infc . So, in addition to = we have # for \"not equal\". * Quantifiers. You can now use quantifiers in search. One of them is like NOTEXIST in MQL. See the docs A number of minor fixes. 4.2.1 \u00b6 2018-05-25 Several improvements in the pretty display in Bhsa and Cunei APIs Under the hood changes in S.search() to prepare for quantifiers in search templates. Tokenisation of quantifiers already works Searches can now spawn auxiliary searches without polluting intermediate data This has been done by promoting the S API to a factory of search engines. By deafault, S creates and maintains a single factory, so to the user it is the same S . But when it needs to run a query in the middle of processing another query it can just spawn another search engine to do that, without interfering with the original search. NB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten. 4.2 \u00b6 2018-05-23 The Search API has been extended: you can use custom sets in your query templates you can search in shallow mode: instead of full result tuples, you just get a set of the top-level thing you mention in your template. This functionality is a precursor for quantifiers in search templates but is also a powerful addition to search in its own right. 4.1.2 \u00b6 2018-05-17 Bhsa and Cunei APIs: custom highlight colours also work for condensed results. you can pass the highlights parameter also to show and prettyTuple 4.1.1 \u00b6 2018-05-16 Bhsa API: you can customize the features that are shown in pretty displays. 4.1 \u00b6 2018-05-16 Bhsa and Cunei APIs: you can customize the highlighting of search results: different colours for different parts of the results you can choose your colours freely from all that CSS has to offer. See the updated search tutorials. 4.0.3 \u00b6 2018-05-11 No changes, just quirks in the update process to get a new version of TF out. 4.0.1 \u00b6 2018-05-11 Documentation updates. 4.0.0 \u00b6 2018-05-11 Additions to Search. You can now include the values of edges in your search templates. F. feature .freqList() accepts a new parameter: nodeTypes . It will restrict its results to nodes in one of the types in nodeTypes . You can now also do E. feature .freqList() . It will count the number of edges if the edge is declared to be without values, or it will give a frequency list of the edges by value if the edge has values. Like F.freqList , you can pass parameters to constrain the frequency list to certain node types. You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive ( nodeTypesTo ). New documentation system based on MkDocs . 3.4.12 \u00b6 2018-05-02 The Cunei and Bhsa APIs show the version of Text-Fabric that is being called. 3.4.11 \u00b6 2018-05-01 Cunei cases are divided horizontally and vertically, alternating with their nesting level; cases have a feature depth now, indicating at which level of nesting they are. 3.4.8-9-10 \u00b6 2018-04-30 Various small fixes, such as: Bhsa: Lexeme links in pretty displays. Cunei: Prevented spurious </div> in NbViewer. 3.4.7 \u00b6 Cunei: Modified local image names 3.4.6 \u00b6 Small tweaks in search. 3.4.5 \u00b6 2018-04-28 Bhsa API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() . 3.4.4 \u00b6 2018-04-27 Cunei API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() . 3.4.2 \u00b6 2018-04-26 Better search documentation. Cunei API: small fixes. 3.4.1 \u00b6 2018-04-25 Bhsa API: Search/show: you can now show results condensed: i.e. a list of passages with highlighted results is returned. This is how SHEBANQ represents the results of a query. If you have two results in the same verse, with condensed=True you get one verse display with two highlights, with condensed=False you get two verse displays with one highlight each. Cunei API: Search/show: the pretty , prettyTuple , show functions of the Bhsa API have bee translated to the Cunei API. You can now get very pretty displays of search results. 3.4 \u00b6 2018-04-23 Search : You can use regular expressions to specify feature values in queries. You could already search for nodes which have a non-None value for a certain feature. Now you can also search for the complement: nodes that do not have a certain feature. Bhsa API: The display of query results also works with lexeme nodes. 3.3.4 \u00b6 2018-04-20 Cunei API: Better height and width control for images. Leaner captions. 3.3.3 \u00b6 2018-04-19 Cunei API: casesByLevel() returns case nodes in corpus order. 3.3.2 \u00b6 2018-04-18 Change in the Cunei api reflecting that undivided lines have no cases now (was: they had a single case with the same material as the line). Also: the feature fullNumber on cases is now called number , and contains the full hierarchical part leading to a case. There is an extra feature terminal on lines and cases if they are not subdivided. Changes in Cunei and Bhsa api: fixed a bug that occurred when working outside a GitHub repository. 3.3.1 \u00b6 2018-04-18 Change in the Cunei api. casesByLevel() now takes an optional argument terminal instead of withChildren , with opposite values. withChildren=False is ambiguous: will it deliver only cases that have no children (intended), or will it deliver cases and their children (understood, but not intended). terminal=True : delivers only cases that are terminal. terminal=False : delivers all cases at that level. 3.3 \u00b6 2018-04-14 Small fix in the bhsa api. Bumped the version number because of the inclusion of corpus specific APIs. 3.2.6 \u00b6 2018-04-14 Text-Fabric now contains corpus specific extras: bhsa.py for the Hebrew Bible (BHSA) cunei.py for the Proto-Cuneiform corpus Uruk The Fabric(locations=locations, modules=modules) constructor now uses [''] as default value for modules. Now you can use the locations parameter on its own to specify the search paths for TF features, leaving the modules parameter undefined, if you wish. 3.2.5 \u00b6 2018-03-23 Enhancement in search templates: you can now test for the presence of features. Till now, you could only test for one or more concrete values of features. So, in addition to things like 1 word number=plural tense=yiqtol you can also say things like 1 word number=plural tense and it will give you words in the plural that have a tense. 3.2.4 \u00b6 2018-03-20 The short API names F , T , L etc. have been aliased to longer names: Feature , Text , Locality , etc. 3.2.2 \u00b6 2018-02-27 Removed the sub module cunei.py . It is better to keep corpus dependent modules in outside the TF package. 3.2.1 \u00b6 2018-02-26 Added a sub module cunei.py , which contains methods to produce ATF transcriptions for nodes of certain types. 3.2 \u00b6 2018-02-19 API change Previously, the functions L.d() and L.u() took rank into account. In the Hebrew Bible, that means that L.d(sentence) will not return a verse, even if the verse is contained in the sentence. This might be handy for sentences and verses, but in general this behaviour causes problems. It also disturbs the expectation that with these functions you get all embedders and embeddees. So we have lifted this restriction. Still, the results of the L functions have an ordering that takes into account the levels of the returned nodes. Enhancement Previously, Text-Fabric determined the levels of node types automatically, based on the average slot-size of nodes within the node types. So books get a lower level than chapters than verses than phrases, etc. However, working with cuneiform tablets revealed that containing node types may have a smaller average size than contained node types. This happens when a container type only contains small instances of the contained type and not the bigger ones. Now you can override the computation by text-fabric by means of a key-value in the otext feature. See the api . 3.1.5 \u00b6 2018-02-15 Fixed a small problem in sectionFromNode(n) when n is a node within a primary section but outside secondary/tertiary sections. 3.1.4 \u00b6 2018-02-15 Small fix in the Text API. If your data set does not have language dependent features, for section level 1 headings, such as book@en , book@sw , the Text API will not break, and the plain book feature will be taken always. We also reformatted all code with a PEP8 code formatter. 3.1.3 \u00b6 2018-01-29 Small adaptions in conversion from MQL to TF, it can now also convert the MQL coming from CALAP dataset (Syriac). 3.1.2 \u00b6 2018-01-27 Nothing changed, only the names of some variables and the text of some messages. The terminology has been made more consistent with the fabric metaphor, in particular, grid has been replaced by warp . 3.1.1 \u00b6 2017-10-21 The exportMQL() function now generates one single enumeration type that serves for all enumeration features. That makes it possible to compare values of different enumeration features with each other, such as ps and prs_ps . 3.1 \u00b6 2017-10-20 The exportMQL() function now generates enumeration types for features, if certain conditions are fulfilled. That makes it possible to query those features with the IN relationship of MQL, like [chapter book IN (Genesis, Exodus)] . 3.0.8 \u00b6 2017-10-07 When reading edges with values, also the edges without a value are taken in. 3.0.7 \u00b6 2017-10-07 Edges with edge values did not allow for the absence of values. Now they do. 3.0.6 \u00b6 2017-10-05 A major tweak in the importMQL() function so that it can handle gaps in the monad sequence. The issue arose when converting MQL for version 3 of the BHSA . In that version there are somewhat arbitrary gaps in the monad sequence between the books of the Hebrew Bible. I transform a gapped sequence of monads into a continuous sequence of slots. 3.0.5 \u00b6 2017-10-05 Another little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for version 3 of the BHSA . 3.0.4 \u00b6 2017-10-04 Little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical material. 3.0.2, 3.0.3 \u00b6 2017-10-03 No changes, only an update of the package metadata, to reflect that Text-Fabric has moved from ETCBC to Dans-labs . 3.0.1 \u00b6 2017-10-02 Bug fix in reading edge features with values. 3.0.0 \u00b6 2017-10-02 MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already exportMQL() . The consequence is that we can operate with much agility between the worlds of MQL and TF. We can start with source data in MQL, convert it to TF, combine it with other TF data sources, compute additional stuff and add it, and then finally export it as enriched MQL, so that the enriched data can be queried by MQL. 2.3.15 \u00b6 2017-09-29 Completion: TF defines the concept of edges that carry a value. But so far we have not used them. It turned out that it was impossible to let TF know that an edge carries values, when saving data as a new feature. Now it is possible. 2.3.14 \u00b6 2017-09-29 Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible. This is the consequence of a bug in precomputing the sections sections . The preparation step used 1 range ( firstVerse , lastVerse ) somewhere, which should of course have been 1 range ( firstVerse , lastVerse + 1 ) 2.3.13 \u00b6 2017-09-28 Loading TF was not completely silent if silent=True was passed. Better now. 2.3.12 \u00b6 2017-09-18 Small fix in TF.save() . The spec says that the metadata under the empty key will be inserted into all features, but in fact this did not happen. Instead it was used as a default when some feature did not have metadata specified. From now on, that metadata will spread through all features. New API function explore , to get a list of all known features in a dataset. 2.3.11 \u00b6 2017-09-18 Small fix in Search: the implementation of the relation operator || (disjoint slot sets) was faulty. Repaired. The search tutorial got an extra example: how to look for gaps. Gaps are not a primitive in the TF search language. Yet the language turns out to be powerful enough to look for gaps. This answers a question by Cody Kingham. 2.3.10 \u00b6 2017-08-24 When defining text formats in the otext.tf feature, you can now include newlines and tabs in the formats. Enter them as \\n and \\t . 2.3.9 \u00b6 2017-07-24 TF has a list of default locations to look for data sources: ~/Downloads , ~/github , etc. Now ~/Dropbox has been added to that list. 2.3.8 \u00b6 2017-07-24 The section levels (book, chapter, verse) were supposed to be customizable through the otext feature. But in fact, up till version 2.3.7 this did not work. From now on the names of the section types and the features that name/number them, are given in the otext feature. It is still the case that exactly three levels must be specified, otherwise it does not work. 2.3.7 \u00b6 2017-05-12 Fixes. Added an extra default location for looking for text-fabric-data sources, for the benefit of running text-fabric within a shared notebook service. 2.3.5, 2.3.6 \u00b6 2017-03-01 Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms in the template got discarded after an outdent. 2.3.4 \u00b6 2017-02-12 Also the Fabric() call can be made silent now. 2.3.3 \u00b6 2017-02-11 Improvements: you can load features more silently. See TF.load() ; you can search more silently. See S.study() ; you can search more concisely. See the new S.search() ; when fetching results, the amount parameter of S.fetch() has been renamed to limit ; the tutorial notebooks (see links on top) have been updated. 2.3.2 \u00b6 2017-02-03 Bug fix: the results of F.feature.s() , E.feature.f() , and E.features.t() are now all tuples. They were a mixture of tuples and lists. 2.3.1 \u00b6 2017-01-23 Bug fix: when searching simple queries with only one query node, the result nodes were delivered as integers, instead of 1-tuples of integers. 2.3 \u00b6 2017-01-13 We start archiving releases of Text-Fabric at Zenodo . 2.2.1 \u00b6 2017-01-09 Small fixes. 2.2.0 \u00b6 2017-01-06 New: sortKey \u00b6 The API has a new member: sortKey New relationships in templates: nearness . See for examples the end of the searchTutorial . Thanks to James Cu\u00e9nod for requesting nearness operators. Fixes \u00b6 in S.glean() word nodes were not printed; the check whether the search graph consists of a single connected component did not handle the case of one node without edges well; 2.1.3 \u00b6 2017-01-04 Various fixes. 2.1.0 \u00b6 2017-01-04 New: relations \u00b6 Some relations have been added to search templates: =: and := and :: : start at same slot , end at same slot , start at same slot and end at same slot <: and :> : adjacent before and adjacent next . The latter two can also be used through the L -api: L.p() and L.n() . The data that feeds them is precomputed and available as C.boundary . New: enhanced search templates \u00b6 You can now easily make extra constraints in search templates without naming atoms. See the searchTutorial for an updated exposition on searching. 2.0.0 \u00b6 2016-12-23 New: Search \u00b6 Want to feel cosy with Christmas? Put your laptop on your lap, update Text-Fabric, and start playing with search. Your laptop will spin itself warm with your queries! Text-Fabric just got a powerful search facility, based on (graph)-templates. It is still very fresh, and more experimentation will be needed. Feedback is welcome. Start with the tutorial . The implementation of this search engine can be nicely explained with a textile metaphor: spinning wool into yarn and then stitching the yarns together. That will be explained further in a document that I'll love to write during Xmas. 1.2.7 \u00b6 2016-12-14 New \u00b6 F.otype.sInterval() 1.2.6 \u00b6 2016-12-14 bug fix \u00b6 There was an error in computing the order of nodes. One of the consequences was that objects that occupy the same slots were not ordered properly. And that had as consequence that you could not go up from words in one-word phrases to their containing phrase. It has been remedied. Note Your computed data needs to be refreshed. This can be done by calling a new function TF.clearCache() . When you use TF after this, you will see it working quite hard to recompute a bunch of data. 1.2.5 \u00b6 2016-12-13 Documentation update 1.2.0 \u00b6 2016-12-08 Note Data update needed New \u00b6 Frequency lists \u00b6 F.feature.freqList() : get a sorted frequency list for any feature. Handy as a first step in exploring a feature. Export to MQL \u00b6 TF.exportMQL() : export a whole dataset as a MQL database. Including all modules that you have loaded with it. Changed \u00b6 The slot numbers start at 0, no longer at 1. Personally I prefer the zero starting point, but Emdros insists on positive monads and objects ids. Most important is that users do not have to add/subtract one from the numbers they see in TF if they want to use it in MQL and vice versa. Because of this you need to update your data too: 1 2 cd ~/github/text-fabric-data git pull origin master","title":"News"},{"location":"News/#changes","text":"Consult the tutorials after changes When we change the API, we make sure that the tutorials shows off all possibilities: bhsa peshitta syrnt cunei","title":"Changes"},{"location":"News/#queued-for-next-release","text":"","title":"Queued for next release"},{"location":"News/#643-4","text":"2018-11-06 Big bug fix in queries: basic relationships in combination with custom sets. The implementation of the basic relationships did not reckon with custom sets that contains both slot nodes and non-slot nodes. And it did not trigger the right code when a custom set has only slot nodes. That has been remedied. Some of the search tutorials have been expanded to include a number of these critical cases. A more complete test suite outside the tutorials is still on my to do list. Thanks to Cody Kingham for spotting and reporting this bug.","title":"6.4.3-4"},{"location":"News/#64-641-2","text":"2018-11-02 A passage browsing interface that interacts with the search results. The interface scrolls to the highlighted row. Minor things: More refined warnings when you run out of memory TF checks whether you are running 64 bit Python. If not, a warning is issued.","title":"6.4, 6.4.1-2"},{"location":"News/#632","text":"2018-10-27 Better documentation for installation of Text-Fabric on Ubuntu. Added new module requirements: ipykernel and notebook.","title":"6.3.2"},{"location":"News/#631","text":"2018-10-24 An optional parameter silent=False has been added to the initialisation calls of the specific app APIs: you can say now 1 2 3 4 A = Bhsa ( silent = True ) A = Cunei ( silent = True ) A = Peshitta ( silent = True ) A = Syrnt ( silent = True ) and then all non-error messages will be suppressed. If the underlying TF API needs to precompute data, it will still be shown, because this may cause an otherwise unexpected delay. Since this is a releatively rare case, and since this can be remedied by running the call again, I leave this behaviour as it is.","title":"6.3.1"},{"location":"News/#63","text":"2018-10-19 Character tables for Hebrew abd Syriac, with links to them from the TF browser Better font handling In the pretty and plain functions you can pass a fmt parameter, to control the text representation (original script, transcription, phonetics) You can also control the text representation in the Text-Fabric browser.","title":"6.3"},{"location":"News/#622","text":"2018-10-18 Added ETCBC/WIT transcriptions to the SyrNT data source. Now both Peshitta and Syriac New Testament have ETCBC transcriptions. The older, rectangular logo makes place for the more crisp, circular one","title":"6.2.2"},{"location":"News/#621","text":"2018-10-17 New app: Syrnt (Syriac New Testament. It works much like the Peshitta, but the SyrNT data has linguistic annotations at the word and lexeme levels. After this upgrade you can browse the SyrNT by saying text-fabric syrnt on the command line.","title":"6.2.1"},{"location":"News/#62","text":"2018-10-16 New app: Peshitta. It works much like the BHSA, but there is one big difference: the current Peshitta data does not have linguistic annotations. There are just books, chapters, verses and words. We expect to add lemmatizations of words shortly. After this upgrade you can browse the peshitta by saying text-fabric peshitta on the command line. Fixed a bug in exportMQL: when there are no enumerated values, do not write out an empty CREATE ENUMERATION statement to the MQL file.","title":"6.2"},{"location":"News/#61","text":"2018-10-12 More precise provenance data when you export results from the Text-Fabric data; Under the hood reorganization of configuration data of apps like Bhsa and Cunei; App-specific parts of the code have moved to more generic parts: a big cleanup has performed; This will make it easier to add new apps.","title":"6.1"},{"location":"News/#607-8-9","text":"2018-10-11 Avoid computing the notebook name when the user passes a name for the notebook to Cunei() or Bhsa() . And when the computing needs to be done, all exceptions will be caught, because the code for determining the notebook name is brittle, and may crash if the Jupyter version does not match. Fixed the bug that the Bhsa and Cunei did not run properly outside a notebook or outside a github repo. In Bhsa and Cunei, the generated info after the incantation can be collapsed (features, API members).","title":"6.0.7-8-9"},{"location":"News/#606","text":"2018-10-10 In the BHSA, the edge features are now shown too after the incantation. If you hoist the API members into your namespace, you will get a list of hoisted names, linked to the API documentation.","title":"6.0.6"},{"location":"News/#605","text":"2018-10-09 When using BHSA and Cunei in a notebook, there is an even simpler incantation which auto downloads features. In the BHSA it is shown which features are loaded, with direct links to the feature docs.","title":"6.0.5"},{"location":"News/#604","text":"2018-10-09 When using BHSA and Cunei in a notebook, there is a simpler incantation which auto downloads features. Some issues concerning paths in zipfiles of downloaded data have been solved.","title":"6.0.4"},{"location":"News/#603","text":"Easier incantations for Bhsa() and Cunei() . It is no longer needed to pass the name of the notebook, but you can still do so: name='mynotebook' You can leave out the api argument in Bhsa() . Then you do not have to load features by means of TF.load() , Bhsa() will load a standard set of features, and if the BHSA data is missing, it will download them first. The former ways of calling Bhsa() and Cunei() are still valid. Note that all arguments have become optional. 2018-10-08 The Text-Fabric browser will always print a banner with its name and version. If you pass it the argument --help or -h or --version or -v it will show the relevant information and stop executing.","title":"6.0.3"},{"location":"News/#602","text":"2018-10-07 The Text-Fabric browser takes it data by default from ~/text-fabric-data . It will not check local github clones for data. But if you pass the option -lgc , it will first check your local github clones. So it you do nothing special, the TF browser always works with the auto-downloaded data.","title":"6.0.2"},{"location":"News/#601","text":"2018-10-06 Not only the core BHSA data will auto load, also the related PHONO and PARALLELS data. A new release has been made of the related data, and they are now in sync with the release of the core data. If you use auto load already, you do not have to do anything. But if you have the etcbc/phono and etcbc/parallels repos in your ~/github folder, you should do a git pull origin master on those repos. N.B. : I am contemplating to have the Text-Fabric browser always use data from ~/text-fabric-data and no longer from ~/github/etcbc . Then the TF browser always controls its own data, and it will not occur that the version of the TF browser is not compatible with the version of the TF data in your github repos, or that the main data and the related data are out of synch. The disadvantage is that if you have the github repos on your system, you get redundant data in ~/text-fabric-data . However, there is only one version kept in ~/text-fabric-data , so this is not much.","title":"6.0.1"},{"location":"News/#60","text":"2018-10-05 A big update with several changes:","title":"6.0"},{"location":"News/#api-change","text":"T.text() has got more behaviours. This change was needed for the Text-Fabric browser, in order to represent lexemes in exported files. Showcase: BHSA dictionary Here is how you can collect the BHSA lexemes in an Excel sheet. about.md RESULTSX.tsv It might also be handy for the programmers amongst you. See the updated API doc on T , expand the T.text() item.","title":"API change:"},{"location":"News/#auto-update","text":"The Text-Fabric browser checks if you are using the most recent release of the data.","title":"Auto update"},{"location":"News/#font-rendering","text":"A font rendering issue in Safari 12 in macos Mojave prevented the use of Ezra SIL for Hebrew in notebooks. We now work around this by relying on the distribution of Ezra SIL as webfont in the font library .","title":"Font rendering"},{"location":"News/#additional-small-fixes","text":"Not worth telling. update Text-Fabric To update Text-Fabric itself to version 6.0, consult Upgrade . Perform this step first, because the new TF may download the new data for you. Data update needed In order to work successfully with the new T.text() function, you need a newer release (1.4) of the BHSA data . (In fact, only one line in one feature has changed ( otext.tf ). Here is how you get the new data release:","title":"Additional small fixes."},{"location":"News/#automatically","text":"If previously your Text-Fabric browser has automatically downloaded the data for you, it will detect the new release and download it automatically. You do not have to do anything, except increase your patience. The download (24 MB) takes some time and after that Text-Fabric will precompute related data, which may take a few minutes. This is a one-time-step after a data update.","title":"Automatically"},{"location":"News/#manually","text":"If you have a clone of the BHSA repository, then go to that directory and say git pull origin master . If you get error messages, then you have local changes in your local BHSA repository that conflict with the github version. Probably you have run the tutorials in place. Best thing to do is: copy your BHSA tutorial directory to somehwere else; remove your local BHSA repository entirely; decide whether you really want the whole repo back (nearly 4 GB). If not: you're done, and TF will download automatically the data it needs. If you still need it: move one directory up (into the etcbc directory) and do git clone https://github.com/ETCBC/bhsa . If you want to consult the tutorials, either: view them on nbviewer ; or run them in a directory outside the BHSA repo (where you have copied it a minute ago).","title":"Manually"},{"location":"News/#564","text":"2018-10-04 Solved a font-rendering issue on Safari 12 (Macos Mojave): locally installed fonts, such as Ezra SIL are not being honored. So I linked to a stylesheet of the fontlibrary which has a webfont version of Ezra SIL. That worked.","title":"5.6.4"},{"location":"News/#563","text":"2018-10-04 Exported tab-separated files get extension .tsv instead of .csv , because then they render better in GitHub.","title":"5.6.3"},{"location":"News/#562","text":"2018-10-04 Small optimization. More docs about reading and writing Excel compatible CSV files with Hebrew characters in it.","title":"5.6.2"},{"location":"News/#561","text":"2018-10-04 Better exporting from TF browser: a good RESULTSX.tsv with results, sensibly augmented with information, directly openable in Excel, even when non-latin unicode code characters are present . All features that occur in the searchTemplate are drawn in into the RESULTSX.tsv, onto the nodes they filter. An additonal feature filtering is now possible in searchTemplates: feature* . This acts as \"no additional constraint\", so it does not influence the result set. But it will be picked up and used to add information into the RESULTSX.tsv.","title":"5.6.1"},{"location":"News/#5525","text":"2018-10-03 The Text-Fabric browser exports results as node lists and produces also a CONTEXT.tsv with all feature values for all nodes in the results. However, it does not contain full text representations of the nodes and it is also not possible to see in what verses the nodes occur. That has changed. The last column of CONTEXT.tsv contains the full text of a node. And there are three columns at the beginning that contain the references to the sections the node is in. For the BHSA that is the book, chapter and verse.","title":"5.5.25"},{"location":"News/#5524","text":"2018-09-25 BHSA app in Text-Fabric Browser: the book names on the verse pad should be the English book names. That is now in the help texts, including a link to the list of English book names.","title":"5.5.24"},{"location":"News/#5523","text":"2018-09-21 Problem in use of msgCache in the search engine, which caused fetch() to fail in some cases. Fixed.","title":"5.5.23"},{"location":"News/#5522","text":"2018-09-13 Fix in left-to-right displays of extra features in pretty() displays in the BHSA.","title":"5.5.22"},{"location":"News/#5521","text":"2018-08-30 Bug fix in transcription.py w.r.t. to Syriac transcriptions.","title":"5.5.21"},{"location":"News/#5520","text":"2018-08-16 BHSA app: adjusted the color of the gloss attribute: darker.","title":"5.5.20"},{"location":"News/#5519","text":"2018-07-19 Fixed: when opening files for reading and writing for an export of a TF browser session: specify that the encoding is utf8 . This is needed on those windowses where the default encoding is something else, usually cp1252 .","title":"5.5.19"},{"location":"News/#5518","text":"2018-07-19 No change, only in the build script. This is a test whether after uploading to PyPi, users can upgrade without using the --no-cache-dir in their pip commands.","title":"5.5.18"},{"location":"News/#5517","text":"2018-07-19 The main functions in kernel and web can be passed arguments, instead that they always read from sys.argv. So that it can be used packaged apps.","title":"5.5.17"},{"location":"News/#5516","text":"2018-07-17 Extra option when starting up the text-fabric web interface: -docker to let the webserver listen at 0.0.0.0 instead of localhost . So that it can be used in a Docker container.","title":"5.5.16"},{"location":"News/#5515","text":"2018-07-16 Extra option when starting up the text-fabric web interface: -noweb to not start the web browser. So that it can be used in a Docker container.","title":"5.5.15"},{"location":"News/#5513-14","text":"2018-07-12 Better error reporting of quantified queries.","title":"5.5.13-14"},{"location":"News/#5512","text":"2018-07-11 Faster export of big csv lists. Tweaks in the web interface. Cleaner termination of processes. The concept TF data server is now called TF kernel","title":"5.5.12"},{"location":"News/#558-11","text":"2018-07-10 Better in catching out-of-memory errors. Prevents creation of corrupt compiled binary TF data. Prevents starting the webserver if the TF kernel fails to load.","title":"5.5.8-11"},{"location":"News/#557","text":"2018-07-09 Optimization is export from TF browser.","title":"5.5.7"},{"location":"News/#556","text":"2018-07-09 Better help display. The opened-state of help sections is remembered. You can open help next to an other open section in the sidebar.","title":"5.5.6"},{"location":"News/#555","text":"2018-07-08 Crisper icon.","title":"5.5.5"},{"location":"News/#554","text":"2018-07-6 Docs updated. Little bit of refactoring.","title":"5.5.4"},{"location":"News/#551-3","text":"2018-07-4 In the TF browser, use a selection of all the features when working with the BHSA. Otherwise in Windows you might run out of memory, even if you have 8GB RAM.","title":"5.5.1-3"},{"location":"News/#55","text":"2018-07-4 Text-Fabric can download data for BHSA and Cunei. You do not have to clone github repositories for that. The data downloaded by Text-Fabric ends up in text-fabric-data under your home directory.","title":"5.5"},{"location":"News/#545-7","text":"2018-07-03 Experimenting with setuptools to get the text-fabric script working on Windows.","title":"5.4.5-7"},{"location":"News/#544","text":"2018-07-02 Added renaming/duplicating of jobs and change of directory.","title":"5.4.4"},{"location":"News/#543","text":"2018-06-29 Small fixes in error reporting in search.","title":"5.4.3"},{"location":"News/#541-2","text":"2018-06-28 Text-Fabric browser: at export a csv file with all results is created, and also a markdown file with metadata. Small fixes.","title":"5.4.1-2"},{"location":"News/#54","text":"2018-06-26 Improved interface and functionality of the text-fabric browser: you can save your work you can enter verse references and tablet P numbers there is help there is a side bar Docs not up to date The API docs are not up-to-date: there are new functions in the Bhsa and Cunei APIs. The server/kernel/client apis are not completely spelled out. However, the help for the text-fabric browser is included in the interface itself.","title":"5.4"},{"location":"News/#533","text":"2018-06-23 Small fix: command line args for text-fabric.","title":"5.3.3"},{"location":"News/#530-2","text":"2018-06-22 Better process management When the TF web interface is started, it cleans up remnant process that might get in the way otherwise. You can also say 1 text-fabric -k to kill all remnant processes, or 1 text-fabric -k datasource to kill the processes for a specific datasource only. Manual node entry You can enter nodes manually in the TF browser. Handy for quick inspection. You can click on the sequence number to append the node tuple of that row to the tuple input field. That way you can collect interesting results. Name and Description You can enter a name which will be used as title and file name during export. You can enter a description in Markdown. When you export your query, the description appears formatted on top. Provenance If you export a query, provenance is added, using DOIs. Small fixes No more blank pages due to double page breaks.","title":"5.3.0-2"},{"location":"News/#521","text":"2018-06-21 Added an expand all checkbox in the text-fabric browser, to expand all shown rows or to collapse them. Export function for search results in the text-fabric browser. What you see is what you get, 1 pretty display per page if you have the browser save it to pdf. Small tweaks","title":"5.2.1"},{"location":"News/#51","text":"2018-06-21 When displaying results in condensed mode, you can now choose the level of the container in which results are highlighted. So far it was fixed to verse for the bhsa and tablet for cunei. The docs are lagging behind! But it is shown in the tutorials and you can observer it in the text-fabric browser.","title":"5.1"},{"location":"News/#501234","text":"2018-06-19 Addressed start-up problems.","title":"5.0.1,2,3,4"},{"location":"News/#50","text":"2018-06-18 Built in webserver and client for local query running. It is implemented for Bhsa and Cunei.","title":"5.0"},{"location":"News/#4423","text":"2018-06-13 New distribution method with setuptools. Text-Fabric has now dependencies on modules rpyc and bottle, because it contains a built-in TF kernel and webserver. This website is still barely functional, though.","title":"4.4.2,3"},{"location":"News/#441","text":"2018-06-10 Search API: Escapes in regular expression search was buggy and convoluted. If a feature value contains a | then in an RE you have to enter \\| to match it. But to have that work in a TF search, you needed to say \\\\\\| . On the other hand, in the same case for . instead of | , you could just sat \\. In the new situation you do not have to double escape in REs anymore. You can just say \\| and \\. .","title":"4.4.1"},{"location":"News/#44","text":"2018-06-06 Search API: S.search() accepts a new optional parameter: withContext . It triggers the output of context information for nodes in the result tuples.","title":"4.4"},{"location":"News/#434-435","text":"2018-06-05 Search API: The /with/ /or/ /or/ /-/' quantifier is also allowed with zero /or/` s. Small fix in the /with/ quantifier if there are quantifiers between this one and its parent atom.","title":"4.3.4, 4.3.5"},{"location":"News/#433","text":"2018-06-01 Search API: Improved quantifiers in search: /where/ /have/ /without/ /with/ /or/ /-/ ; much clearer indentation rules (no caret anymore); better reporting by S.study() .","title":"4.3.3"},{"location":"News/#432","text":"2018-05-31 Search API: quantifiers may use the name .. to refer to their parents you may use names in the place of atoms, which lessens the need for constructs with p = q stricter checks on the syntax and position of quantifiers","title":"4.3.2"},{"location":"News/#431","text":"2018-05-30 Docs and metadata update","title":"4.3.1"},{"location":"News/#430","text":"2018-05-30 API Change in Search. In search templates I recently added things like 1 word vt! which checks for words that do not have a value for feature vt . The syntax for this has now changed to 1 word vt# Unequal (#) in feature value conditions. Now you can say things like 1 word vt#infa|infc meaning that the value of feature is not one of infa , infc . So, in addition to = we have # for \"not equal\". * Quantifiers. You can now use quantifiers in search. One of them is like NOTEXIST in MQL. See the docs A number of minor fixes.","title":"4.3.0"},{"location":"News/#421","text":"2018-05-25 Several improvements in the pretty display in Bhsa and Cunei APIs Under the hood changes in S.search() to prepare for quantifiers in search templates. Tokenisation of quantifiers already works Searches can now spawn auxiliary searches without polluting intermediate data This has been done by promoting the S API to a factory of search engines. By deafault, S creates and maintains a single factory, so to the user it is the same S . But when it needs to run a query in the middle of processing another query it can just spawn another search engine to do that, without interfering with the original search. NB: the search tutorial for the Bhsa got too big. It has thoroughly been rewritten.","title":"4.2.1"},{"location":"News/#42","text":"2018-05-23 The Search API has been extended: you can use custom sets in your query templates you can search in shallow mode: instead of full result tuples, you just get a set of the top-level thing you mention in your template. This functionality is a precursor for quantifiers in search templates but is also a powerful addition to search in its own right.","title":"4.2"},{"location":"News/#412","text":"2018-05-17 Bhsa and Cunei APIs: custom highlight colours also work for condensed results. you can pass the highlights parameter also to show and prettyTuple","title":"4.1.2"},{"location":"News/#411","text":"2018-05-16 Bhsa API: you can customize the features that are shown in pretty displays.","title":"4.1.1"},{"location":"News/#41","text":"2018-05-16 Bhsa and Cunei APIs: you can customize the highlighting of search results: different colours for different parts of the results you can choose your colours freely from all that CSS has to offer. See the updated search tutorials.","title":"4.1"},{"location":"News/#403","text":"2018-05-11 No changes, just quirks in the update process to get a new version of TF out.","title":"4.0.3"},{"location":"News/#401","text":"2018-05-11 Documentation updates.","title":"4.0.1"},{"location":"News/#400","text":"2018-05-11 Additions to Search. You can now include the values of edges in your search templates. F. feature .freqList() accepts a new parameter: nodeTypes . It will restrict its results to nodes in one of the types in nodeTypes . You can now also do E. feature .freqList() . It will count the number of edges if the edge is declared to be without values, or it will give a frequency list of the edges by value if the edge has values. Like F.freqList , you can pass parameters to constrain the frequency list to certain node types. You can constrain the node types from which the edges start ( nodeTypesFrom ) and where they arrive ( nodeTypesTo ). New documentation system based on MkDocs .","title":"4.0.0"},{"location":"News/#3412","text":"2018-05-02 The Cunei and Bhsa APIs show the version of Text-Fabric that is being called.","title":"3.4.12"},{"location":"News/#3411","text":"2018-05-01 Cunei cases are divided horizontally and vertically, alternating with their nesting level; cases have a feature depth now, indicating at which level of nesting they are.","title":"3.4.11"},{"location":"News/#348-9-10","text":"2018-04-30 Various small fixes, such as: Bhsa: Lexeme links in pretty displays. Cunei: Prevented spurious </div> in NbViewer.","title":"3.4.8-9-10"},{"location":"News/#347","text":"Cunei: Modified local image names","title":"3.4.7"},{"location":"News/#346","text":"Small tweaks in search.","title":"3.4.6"},{"location":"News/#345","text":"2018-04-28 Bhsa API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() .","title":"3.4.5"},{"location":"News/#344","text":"2018-04-27 Cunei API: new functions plain() and table() for plainly representing nodes, tuples and result lists, as opposed to the abundant representations by pretty() and show() .","title":"3.4.4"},{"location":"News/#342","text":"2018-04-26 Better search documentation. Cunei API: small fixes.","title":"3.4.2"},{"location":"News/#341","text":"2018-04-25 Bhsa API: Search/show: you can now show results condensed: i.e. a list of passages with highlighted results is returned. This is how SHEBANQ represents the results of a query. If you have two results in the same verse, with condensed=True you get one verse display with two highlights, with condensed=False you get two verse displays with one highlight each. Cunei API: Search/show: the pretty , prettyTuple , show functions of the Bhsa API have bee translated to the Cunei API. You can now get very pretty displays of search results.","title":"3.4.1"},{"location":"News/#34","text":"2018-04-23 Search : You can use regular expressions to specify feature values in queries. You could already search for nodes which have a non-None value for a certain feature. Now you can also search for the complement: nodes that do not have a certain feature. Bhsa API: The display of query results also works with lexeme nodes.","title":"3.4"},{"location":"News/#334","text":"2018-04-20 Cunei API: Better height and width control for images. Leaner captions.","title":"3.3.4"},{"location":"News/#333","text":"2018-04-19 Cunei API: casesByLevel() returns case nodes in corpus order.","title":"3.3.3"},{"location":"News/#332","text":"2018-04-18 Change in the Cunei api reflecting that undivided lines have no cases now (was: they had a single case with the same material as the line). Also: the feature fullNumber on cases is now called number , and contains the full hierarchical part leading to a case. There is an extra feature terminal on lines and cases if they are not subdivided. Changes in Cunei and Bhsa api: fixed a bug that occurred when working outside a GitHub repository.","title":"3.3.2"},{"location":"News/#331","text":"2018-04-18 Change in the Cunei api. casesByLevel() now takes an optional argument terminal instead of withChildren , with opposite values. withChildren=False is ambiguous: will it deliver only cases that have no children (intended), or will it deliver cases and their children (understood, but not intended). terminal=True : delivers only cases that are terminal. terminal=False : delivers all cases at that level.","title":"3.3.1"},{"location":"News/#33","text":"2018-04-14 Small fix in the bhsa api. Bumped the version number because of the inclusion of corpus specific APIs.","title":"3.3"},{"location":"News/#326","text":"2018-04-14 Text-Fabric now contains corpus specific extras: bhsa.py for the Hebrew Bible (BHSA) cunei.py for the Proto-Cuneiform corpus Uruk The Fabric(locations=locations, modules=modules) constructor now uses [''] as default value for modules. Now you can use the locations parameter on its own to specify the search paths for TF features, leaving the modules parameter undefined, if you wish.","title":"3.2.6"},{"location":"News/#325","text":"2018-03-23 Enhancement in search templates: you can now test for the presence of features. Till now, you could only test for one or more concrete values of features. So, in addition to things like 1 word number=plural tense=yiqtol you can also say things like 1 word number=plural tense and it will give you words in the plural that have a tense.","title":"3.2.5"},{"location":"News/#324","text":"2018-03-20 The short API names F , T , L etc. have been aliased to longer names: Feature , Text , Locality , etc.","title":"3.2.4"},{"location":"News/#322","text":"2018-02-27 Removed the sub module cunei.py . It is better to keep corpus dependent modules in outside the TF package.","title":"3.2.2"},{"location":"News/#321","text":"2018-02-26 Added a sub module cunei.py , which contains methods to produce ATF transcriptions for nodes of certain types.","title":"3.2.1"},{"location":"News/#32","text":"2018-02-19 API change Previously, the functions L.d() and L.u() took rank into account. In the Hebrew Bible, that means that L.d(sentence) will not return a verse, even if the verse is contained in the sentence. This might be handy for sentences and verses, but in general this behaviour causes problems. It also disturbs the expectation that with these functions you get all embedders and embeddees. So we have lifted this restriction. Still, the results of the L functions have an ordering that takes into account the levels of the returned nodes. Enhancement Previously, Text-Fabric determined the levels of node types automatically, based on the average slot-size of nodes within the node types. So books get a lower level than chapters than verses than phrases, etc. However, working with cuneiform tablets revealed that containing node types may have a smaller average size than contained node types. This happens when a container type only contains small instances of the contained type and not the bigger ones. Now you can override the computation by text-fabric by means of a key-value in the otext feature. See the api .","title":"3.2"},{"location":"News/#315","text":"2018-02-15 Fixed a small problem in sectionFromNode(n) when n is a node within a primary section but outside secondary/tertiary sections.","title":"3.1.5"},{"location":"News/#314","text":"2018-02-15 Small fix in the Text API. If your data set does not have language dependent features, for section level 1 headings, such as book@en , book@sw , the Text API will not break, and the plain book feature will be taken always. We also reformatted all code with a PEP8 code formatter.","title":"3.1.4"},{"location":"News/#313","text":"2018-01-29 Small adaptions in conversion from MQL to TF, it can now also convert the MQL coming from CALAP dataset (Syriac).","title":"3.1.3"},{"location":"News/#312","text":"2018-01-27 Nothing changed, only the names of some variables and the text of some messages. The terminology has been made more consistent with the fabric metaphor, in particular, grid has been replaced by warp .","title":"3.1.2"},{"location":"News/#311","text":"2017-10-21 The exportMQL() function now generates one single enumeration type that serves for all enumeration features. That makes it possible to compare values of different enumeration features with each other, such as ps and prs_ps .","title":"3.1.1"},{"location":"News/#31","text":"2017-10-20 The exportMQL() function now generates enumeration types for features, if certain conditions are fulfilled. That makes it possible to query those features with the IN relationship of MQL, like [chapter book IN (Genesis, Exodus)] .","title":"3.1"},{"location":"News/#308","text":"2017-10-07 When reading edges with values, also the edges without a value are taken in.","title":"3.0.8"},{"location":"News/#307","text":"2017-10-07 Edges with edge values did not allow for the absence of values. Now they do.","title":"3.0.7"},{"location":"News/#306","text":"2017-10-05 A major tweak in the importMQL() function so that it can handle gaps in the monad sequence. The issue arose when converting MQL for version 3 of the BHSA . In that version there are somewhat arbitrary gaps in the monad sequence between the books of the Hebrew Bible. I transform a gapped sequence of monads into a continuous sequence of slots.","title":"3.0.6"},{"location":"News/#305","text":"2017-10-05 Another little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for version 3 of the BHSA .","title":"3.0.5"},{"location":"News/#304","text":"2017-10-04 Little tweak in the importMQL() function so that it can handle more patterns in the MQL dump file. The issue arose when converting MQL for extrabiblical material.","title":"3.0.4"},{"location":"News/#302-303","text":"2017-10-03 No changes, only an update of the package metadata, to reflect that Text-Fabric has moved from ETCBC to Dans-labs .","title":"3.0.2, 3.0.3"},{"location":"News/#301","text":"2017-10-02 Bug fix in reading edge features with values.","title":"3.0.1"},{"location":"News/#300","text":"2017-10-02 MQL! You can now convert MQL data into a TF dataset: importMQL() . We had already exportMQL() . The consequence is that we can operate with much agility between the worlds of MQL and TF. We can start with source data in MQL, convert it to TF, combine it with other TF data sources, compute additional stuff and add it, and then finally export it as enriched MQL, so that the enriched data can be queried by MQL.","title":"3.0.0"},{"location":"News/#2315","text":"2017-09-29 Completion: TF defines the concept of edges that carry a value. But so far we have not used them. It turned out that it was impossible to let TF know that an edge carries values, when saving data as a new feature. Now it is possible.","title":"2.3.15"},{"location":"News/#2314","text":"2017-09-29 Bug fix: it was not possible to get T.nodeFromSection(('2_Chronicles', 36, 23)) , the last verse in the Bible. This is the consequence of a bug in precomputing the sections sections . The preparation step used 1 range ( firstVerse , lastVerse ) somewhere, which should of course have been 1 range ( firstVerse , lastVerse + 1 )","title":"2.3.14"},{"location":"News/#2313","text":"2017-09-28 Loading TF was not completely silent if silent=True was passed. Better now.","title":"2.3.13"},{"location":"News/#2312","text":"2017-09-18 Small fix in TF.save() . The spec says that the metadata under the empty key will be inserted into all features, but in fact this did not happen. Instead it was used as a default when some feature did not have metadata specified. From now on, that metadata will spread through all features. New API function explore , to get a list of all known features in a dataset.","title":"2.3.12"},{"location":"News/#2311","text":"2017-09-18 Small fix in Search: the implementation of the relation operator || (disjoint slot sets) was faulty. Repaired. The search tutorial got an extra example: how to look for gaps. Gaps are not a primitive in the TF search language. Yet the language turns out to be powerful enough to look for gaps. This answers a question by Cody Kingham.","title":"2.3.11"},{"location":"News/#2310","text":"2017-08-24 When defining text formats in the otext.tf feature, you can now include newlines and tabs in the formats. Enter them as \\n and \\t .","title":"2.3.10"},{"location":"News/#239","text":"2017-07-24 TF has a list of default locations to look for data sources: ~/Downloads , ~/github , etc. Now ~/Dropbox has been added to that list.","title":"2.3.9"},{"location":"News/#238","text":"2017-07-24 The section levels (book, chapter, verse) were supposed to be customizable through the otext feature. But in fact, up till version 2.3.7 this did not work. From now on the names of the section types and the features that name/number them, are given in the otext feature. It is still the case that exactly three levels must be specified, otherwise it does not work.","title":"2.3.8"},{"location":"News/#237","text":"2017-05-12 Fixes. Added an extra default location for looking for text-fabric-data sources, for the benefit of running text-fabric within a shared notebook service.","title":"2.3.7"},{"location":"News/#235-236","text":"2017-03-01 Bug fix in Search. Spotted by Cody Kingham. Relational operators between atoms in the template got discarded after an outdent.","title":"2.3.5, 2.3.6"},{"location":"News/#234","text":"2017-02-12 Also the Fabric() call can be made silent now.","title":"2.3.4"},{"location":"News/#233","text":"2017-02-11 Improvements: you can load features more silently. See TF.load() ; you can search more silently. See S.study() ; you can search more concisely. See the new S.search() ; when fetching results, the amount parameter of S.fetch() has been renamed to limit ; the tutorial notebooks (see links on top) have been updated.","title":"2.3.3"},{"location":"News/#232","text":"2017-02-03 Bug fix: the results of F.feature.s() , E.feature.f() , and E.features.t() are now all tuples. They were a mixture of tuples and lists.","title":"2.3.2"},{"location":"News/#231","text":"2017-01-23 Bug fix: when searching simple queries with only one query node, the result nodes were delivered as integers, instead of 1-tuples of integers.","title":"2.3.1"},{"location":"News/#23","text":"2017-01-13 We start archiving releases of Text-Fabric at Zenodo .","title":"2.3"},{"location":"News/#221","text":"2017-01-09 Small fixes.","title":"2.2.1"},{"location":"News/#220","text":"2017-01-06","title":"2.2.0"},{"location":"News/#new-sortkey","text":"The API has a new member: sortKey New relationships in templates: nearness . See for examples the end of the searchTutorial . Thanks to James Cu\u00e9nod for requesting nearness operators.","title":"New: sortKey"},{"location":"News/#fixes","text":"in S.glean() word nodes were not printed; the check whether the search graph consists of a single connected component did not handle the case of one node without edges well;","title":"Fixes"},{"location":"News/#213","text":"2017-01-04 Various fixes.","title":"2.1.3"},{"location":"News/#210","text":"2017-01-04","title":"2.1.0"},{"location":"News/#new-relations","text":"Some relations have been added to search templates: =: and := and :: : start at same slot , end at same slot , start at same slot and end at same slot <: and :> : adjacent before and adjacent next . The latter two can also be used through the L -api: L.p() and L.n() . The data that feeds them is precomputed and available as C.boundary .","title":"New: relations"},{"location":"News/#new-enhanced-search-templates","text":"You can now easily make extra constraints in search templates without naming atoms. See the searchTutorial for an updated exposition on searching.","title":"New: enhanced search templates"},{"location":"News/#200","text":"2016-12-23","title":"2.0.0"},{"location":"News/#new-search","text":"Want to feel cosy with Christmas? Put your laptop on your lap, update Text-Fabric, and start playing with search. Your laptop will spin itself warm with your queries! Text-Fabric just got a powerful search facility, based on (graph)-templates. It is still very fresh, and more experimentation will be needed. Feedback is welcome. Start with the tutorial . The implementation of this search engine can be nicely explained with a textile metaphor: spinning wool into yarn and then stitching the yarns together. That will be explained further in a document that I'll love to write during Xmas.","title":"New: Search"},{"location":"News/#127","text":"2016-12-14","title":"1.2.7"},{"location":"News/#new","text":"F.otype.sInterval()","title":"New"},{"location":"News/#126","text":"2016-12-14","title":"1.2.6"},{"location":"News/#bug-fix","text":"There was an error in computing the order of nodes. One of the consequences was that objects that occupy the same slots were not ordered properly. And that had as consequence that you could not go up from words in one-word phrases to their containing phrase. It has been remedied. Note Your computed data needs to be refreshed. This can be done by calling a new function TF.clearCache() . When you use TF after this, you will see it working quite hard to recompute a bunch of data.","title":"bug fix"},{"location":"News/#125","text":"2016-12-13 Documentation update","title":"1.2.5"},{"location":"News/#120","text":"2016-12-08 Note Data update needed","title":"1.2.0"},{"location":"News/#new_1","text":"","title":"New"},{"location":"News/#frequency-lists","text":"F.feature.freqList() : get a sorted frequency list for any feature. Handy as a first step in exploring a feature.","title":"Frequency lists"},{"location":"News/#export-to-mql","text":"TF.exportMQL() : export a whole dataset as a MQL database. Including all modules that you have loaded with it.","title":"Export to MQL"},{"location":"News/#changed","text":"The slot numbers start at 0, no longer at 1. Personally I prefer the zero starting point, but Emdros insists on positive monads and objects ids. Most important is that users do not have to add/subtract one from the numbers they see in TF if they want to use it in MQL and vice versa. Because of this you need to update your data too: 1 2 cd ~/github/text-fabric-data git pull origin master","title":"Changed"},{"location":"Use/","text":"Usage \u00b6 Use Text-Fabric browser \u00b6 Explore your corpus without programming. Here is how to start up text-fabric. On Windows? You can click the Start Menu, and type text-fabric bhsa or text-fabric peshitta or text-fabric syrnt or text-fabric cunei in the search box, and then Enter. On Linux or Macos? You can open a terminal (command prompt), and just say 1 text-fabric bhsa or 1 text-fabric peshitta or 1 text-fabric syrnt or 1 text-fabric cunei All platforms The corpus data will be downloaded automatically, and be loaded into text-fabric. Then your browser will open and load the search interface. There you'll find links to further help. Local Github Clones If you pass the option -lgc to text-fabric , than Text-Fabric will check whether you have data in your local github clones under ~/github . If you have relevant data there, Text-Fabric will use it. This is useful if your local data is better/newer than the data on the last published github release. Above: Querying the BHSA data Below: Querying the Cunei data Fetching corpora The Text-Fabric browser fetches the corpora it needs from GitHub automatically. The TF data is fairly compact (25 MB for the Hebrew Bible, 2,1 MB for the Peshitta, 2,1 MB for the Syriac New Testament, 1.6 MB for the Cunei corpus). Size of data There might be sizable additional data (550 MB images for the Cunei corpus). In that case, take care to have a good internet connection when you use the Text-Fabric browser for the first time. More about corpora Saving your session Your session will be saved in a file with extension .tfjob in the directory from where you have issued the text-fabric command. From within the browser you can rename and duplicate sessions and move to other directories. You can also load other sessions in other tabs. Multiple windows After you have issued the text-fabric command, a TF kernel is started for you. This is a process that holds all the data and can deliver it to other processes, such as your web browser. As long as you leave the TF kernel on, you have instant access to your corpus. You can open other browsers and windows and tabs with the same url, and they will load quickly, without the long wait you experienced when the TF kernel was loading. Close You can close the TF kernel by pressing Ctrl-C in the terminal or command prompt where you have started text-fabric up. Work with exported results \u00b6 You can export your results to CSV files which you can process with various tools, including your own. Exporting your results You can use the \"Export\" tab to tell the story behind your query and then export all your results. A new page will open, which you can save as a PDF. There is also a markdown file about.md with your description and some provenance metadata. Moreover, a file RESULTSX.tsv is written into a local directory corresponding to the job you are in, which contains your precise search results, decorated with the features you have used in your searchTemplate. In addition, some extra data files will be written along side. Your results as tuples of nodes, your condensed results (if you have opted for them), and a CONTEXT.tsv that contains all feature values for every node in the results. Now, if you want to share your results for checking and replication, put all this in a GitHub repository: Example Cunei : about.md RESULTSX.tsv BHSA : about.md RESULTSX.tsv Example with lexemes BHSA : about.md RESULTSX.tsv This example shows how you can get a complete dictionary in your pocket by issuing a simple TF query. If you want to be able to cite those results in a journal article, archive the GitHub repo in question to ZENODO and obtain a DOI. Encoding The file RESULTS.tsv is not in the usual utf8 encoding, but in utf_16 encoding. The reason is that this is the only encoding in which Excel handles CSV files properly. So if you work with this file in Python, specify the encoding utf_16 . 1 2 3 with open ( 'RESULTSX.tsv' , encoding = 'utf_16' ) as fh : for row in fh : # do something with row Conversely, if you want to write a CSV with Hebrew in it, to be opened in Excel, take care to: give the file name extension .tsv (not .csv ) make the file tab separated (do not use the comma or semicolon!) use the encoding utf_16_le (not merely utf_16 , nor utf8 !) start the file with a BOM mark. 1 2 3 4 5 with open ( 'mydata.tsv' , 'w' , encoding = 'utf_16_le' ) as fh : fh . write ( ' \\uFEFF ' ) for row in myData : fh . write ( ' \\t ' . join ( row )) fh . write ( ' \\n ' ) Gory details The file has been written with the utf_16_le encoding, and the first character is the unicode FEFF character. That is needed for machines so that they can see which byte in a 16 bits word is the least end ( le ) and which one is the big end ( be ). Knowing that the first character is FEFF, all machines can see whether this is in a least-endian (le) encoding or in a big-endian (be) . Hence this character is called the Byte Order Mark (BOM). See more on wikipedia . When reading a file with encoding utf_16 , Python reads the BOM, draws its conclusions, and strips the BOM. So when you iterate over its lines, you will not see the BOM, which is good. But when you read a file with encoding utf_16_le , Python passes the BOM through, and you have to skip it yourself. That is unpleasant. Hence, use utf_16 for reading. Use the Text-Fabric API \u00b6 Explore your corpus by means of programming. Into the notebook Start programming: write a python script or code in the Jupyter notebook 1 2 cd somewhere-else jupyter notebook Enter the following text in a code cell BHSA : 1 2 from tf.extra.bhsa import Bhsa B = Bhsa ( hoist = globals ()) Peshitta : 1 2 from tf.extra.peshitta import Peshitta P = Peshitta ( hoist = globals ()) SyrNT : 1 2 from tf.extra.syrnt import Syrnt SY = Syrnt ( hoist = globals ()) Cunei : 1 2 from tf.extra.cunei import Cunei CN = Cunei ( hoist = globals ()) data With these incantations, Text-Fabric will download the data automatically and store it in a directory text-fabric-data directly in your home directory. If you have data in other places, you can also use that by means of extra arguments supplied to Bhsa() , Peshitta() , Syrnt() and Cunei() . Using Hebrew data To get started with the Hebrew corpus, use its tutorial in the BHSA repo: start . Or go straight to the bhsa-api-docs . Using Syriac data To get started with the Peshitta corpus, use its tutorial in the Peshitta repo: start . Or go straight to the peshitta-api-docs . To get started with the Syriac New Testament corpus, use its tutorial in the Syrnt repo: start . Or go straight to the syrnt-api-docs . Using Cuneiform data To get started with the Uruk corpus, use its tutorial in the Nino-cunei repo: start . Or go straight to the cunei-api-docs .","title":"Use"},{"location":"Use/#usage","text":"","title":"Usage"},{"location":"Use/#use-text-fabric-browser","text":"Explore your corpus without programming. Here is how to start up text-fabric. On Windows? You can click the Start Menu, and type text-fabric bhsa or text-fabric peshitta or text-fabric syrnt or text-fabric cunei in the search box, and then Enter. On Linux or Macos? You can open a terminal (command prompt), and just say 1 text-fabric bhsa or 1 text-fabric peshitta or 1 text-fabric syrnt or 1 text-fabric cunei All platforms The corpus data will be downloaded automatically, and be loaded into text-fabric. Then your browser will open and load the search interface. There you'll find links to further help. Local Github Clones If you pass the option -lgc to text-fabric , than Text-Fabric will check whether you have data in your local github clones under ~/github . If you have relevant data there, Text-Fabric will use it. This is useful if your local data is better/newer than the data on the last published github release. Above: Querying the BHSA data Below: Querying the Cunei data Fetching corpora The Text-Fabric browser fetches the corpora it needs from GitHub automatically. The TF data is fairly compact (25 MB for the Hebrew Bible, 2,1 MB for the Peshitta, 2,1 MB for the Syriac New Testament, 1.6 MB for the Cunei corpus). Size of data There might be sizable additional data (550 MB images for the Cunei corpus). In that case, take care to have a good internet connection when you use the Text-Fabric browser for the first time. More about corpora Saving your session Your session will be saved in a file with extension .tfjob in the directory from where you have issued the text-fabric command. From within the browser you can rename and duplicate sessions and move to other directories. You can also load other sessions in other tabs. Multiple windows After you have issued the text-fabric command, a TF kernel is started for you. This is a process that holds all the data and can deliver it to other processes, such as your web browser. As long as you leave the TF kernel on, you have instant access to your corpus. You can open other browsers and windows and tabs with the same url, and they will load quickly, without the long wait you experienced when the TF kernel was loading. Close You can close the TF kernel by pressing Ctrl-C in the terminal or command prompt where you have started text-fabric up.","title":"Use Text-Fabric browser"},{"location":"Use/#work-with-exported-results","text":"You can export your results to CSV files which you can process with various tools, including your own. Exporting your results You can use the \"Export\" tab to tell the story behind your query and then export all your results. A new page will open, which you can save as a PDF. There is also a markdown file about.md with your description and some provenance metadata. Moreover, a file RESULTSX.tsv is written into a local directory corresponding to the job you are in, which contains your precise search results, decorated with the features you have used in your searchTemplate. In addition, some extra data files will be written along side. Your results as tuples of nodes, your condensed results (if you have opted for them), and a CONTEXT.tsv that contains all feature values for every node in the results. Now, if you want to share your results for checking and replication, put all this in a GitHub repository: Example Cunei : about.md RESULTSX.tsv BHSA : about.md RESULTSX.tsv Example with lexemes BHSA : about.md RESULTSX.tsv This example shows how you can get a complete dictionary in your pocket by issuing a simple TF query. If you want to be able to cite those results in a journal article, archive the GitHub repo in question to ZENODO and obtain a DOI. Encoding The file RESULTS.tsv is not in the usual utf8 encoding, but in utf_16 encoding. The reason is that this is the only encoding in which Excel handles CSV files properly. So if you work with this file in Python, specify the encoding utf_16 . 1 2 3 with open ( 'RESULTSX.tsv' , encoding = 'utf_16' ) as fh : for row in fh : # do something with row Conversely, if you want to write a CSV with Hebrew in it, to be opened in Excel, take care to: give the file name extension .tsv (not .csv ) make the file tab separated (do not use the comma or semicolon!) use the encoding utf_16_le (not merely utf_16 , nor utf8 !) start the file with a BOM mark. 1 2 3 4 5 with open ( 'mydata.tsv' , 'w' , encoding = 'utf_16_le' ) as fh : fh . write ( ' \\uFEFF ' ) for row in myData : fh . write ( ' \\t ' . join ( row )) fh . write ( ' \\n ' ) Gory details The file has been written with the utf_16_le encoding, and the first character is the unicode FEFF character. That is needed for machines so that they can see which byte in a 16 bits word is the least end ( le ) and which one is the big end ( be ). Knowing that the first character is FEFF, all machines can see whether this is in a least-endian (le) encoding or in a big-endian (be) . Hence this character is called the Byte Order Mark (BOM). See more on wikipedia . When reading a file with encoding utf_16 , Python reads the BOM, draws its conclusions, and strips the BOM. So when you iterate over its lines, you will not see the BOM, which is good. But when you read a file with encoding utf_16_le , Python passes the BOM through, and you have to skip it yourself. That is unpleasant. Hence, use utf_16 for reading.","title":"Work with exported results"},{"location":"Use/#use-the-text-fabric-api","text":"Explore your corpus by means of programming. Into the notebook Start programming: write a python script or code in the Jupyter notebook 1 2 cd somewhere-else jupyter notebook Enter the following text in a code cell BHSA : 1 2 from tf.extra.bhsa import Bhsa B = Bhsa ( hoist = globals ()) Peshitta : 1 2 from tf.extra.peshitta import Peshitta P = Peshitta ( hoist = globals ()) SyrNT : 1 2 from tf.extra.syrnt import Syrnt SY = Syrnt ( hoist = globals ()) Cunei : 1 2 from tf.extra.cunei import Cunei CN = Cunei ( hoist = globals ()) data With these incantations, Text-Fabric will download the data automatically and store it in a directory text-fabric-data directly in your home directory. If you have data in other places, you can also use that by means of extra arguments supplied to Bhsa() , Peshitta() , Syrnt() and Cunei() . Using Hebrew data To get started with the Hebrew corpus, use its tutorial in the BHSA repo: start . Or go straight to the bhsa-api-docs . Using Syriac data To get started with the Peshitta corpus, use its tutorial in the Peshitta repo: start . Or go straight to the peshitta-api-docs . To get started with the Syriac New Testament corpus, use its tutorial in the Syrnt repo: start . Or go straight to the syrnt-api-docs . Using Cuneiform data To get started with the Uruk corpus, use its tutorial in the Nino-cunei repo: start . Or go straight to the cunei-api-docs .","title":"Use the Text-Fabric API"},{"location":"Api/Apps/","text":"Apps \u00b6 About Text-Fabric is a generic engine to process text and annotations. When working with specific corpora, we want to have more power at our fingertips. We need extra power on top of the TF engine. The way we have chosen to do it is via apps . An app is a bunch of extra functions that know the structure of a specific corpus. In particular, an app knows how to produce plain representations and pretty displays of nodes of each type in the corpus. Current apps \u00b6 Current apps At the moment we have these apps name description bhsa Biblia Hebraica Stuttgartensia (Amstelodamensis) cunei Proto-cuneiform Uruk Corpus peshitta Syriac Old Testament syrnt Syriac New Testament The structure of apps \u00b6 App components The apps themselves are modules inside tf.extra For each app , you find there: module app .py contains all the functionality specific to the corpus in question, organized as an extended TF api. In the code this is referred to as the extraApi . In order to be an app that TF can use, extraApi should provide the following attributes: attribute kind description api object the generic TF api obtained by loading the features: `api = TF.load(...) asApi boolean True if working for the web-interface, False otherwise classNames dict mapping from node types to CSS class names to be used in plain and pretty displays of nodes of that type condenseType string the default node type to which results will be condensed, e.g. verse , tablet dataLink html link points to the repository where the TF data of the corpus is stored exampleSection text a concrete section indicator that points to an object of the condenseType (e.g. Genesis 1:1 , P005381 ) featureLink html link points to the documentation of the TF features of the corpus loadCSS method deliver CSS styling to notebook or web interface (depending on asApi ) nodeFromDefaultSection method given a section string pointing to an object of condenseType , return the corresponding node (or an error message) noneValues set feature values that are deemed uninteresting; features with those values will be suppressed in pretty displays plain method given a node, produce a plain representation of the corresponding object: not the full structure, but something that identifies it pretty method given a node, produce a pretty display of the corresponding object: the full structure prettyFeaturesLoaded set initial set of features that should be loaded for pretty displays tfsLink html link points to the documentation of the TF search engine tutLink html link points to the tutorial for TF search webLink method given a node, produces a link to an online description of the corresponding object (to shebanq or cdli asApi The extraApi contain several display functions. By default they suppose that there is a Jupyter notebook context in which results can be rendered with IPython.display methods. But if we operate in the context of a web-interface, we need to generate straight HTML. We flag the web-interface case as asApi == True . pretty Not all of the pretty method needs to be defined by the app. In fact, the function itself is defined generically in apphelpers . But it calls the method _pretty , which must be defined in the app. In turn, parth of _pretty is taking care of by the generic prettyPre in apphelpers.py . What remains in _pretty is the pure, app-dependent code for displaying nodes. webapp the package app -app is used by the text-fabric browser, and contains settings and assets to set up a browsing experience. config.py : settings a static folder with fonts and logos. config.py Contains values for parameters and an API calling function. extraApi(locations, modules) Responsible for calling the extra Api for the corpus with the desired locations and modules. This extraApi will be active as a TF kernel, interacting with a local webserver that serves local web page in the browser. web browsing settings The TF kernel, webserver and browser need settings: setting example description protocol http:// protocol of local website host localhost server address of local website webport 8001 port for the local website port 18981 port through wich the TF kernel and the webserver communicate data settings The TF kernel needs context information: setting type description locations list where to look for tf features modules list combines with locations to search paths for tf features localDir directory name temporary directory for writing and reading options tuple names of extra options for seaerching and displaying query results PROVENANCE dict corpus specific provenance metadata: name and DOI The generic part of apps \u00b6 App helpers Apps turn out to have several things in common that we want to deal with generically. These functions are collected in the apphelpers module of TF. Generic/specific Sometimes there is an intricate mix of functionality that is shared by all apps and that is specific to some app. Here is our logistics of functionality. There are a number of methods that are offered as a generic function and just added as a method to the extraApi of the app, e.g. pretty() For example, the Bhsa app imports pretty first: 1 from tf.apphelpers import pretty and in the Bhsa __init__() function it says: 1 self . pretty = types . MethodType ( pretty , self ) which adds the function pretty as an instance method to the class Bhsa. The first argument extraApi of the function pretty acts as the self when pretty() is used as a method of Bhsa. So although we define pretty(extraApi, ...) as a generic function, through its argument extraApi we can call app specific functionality. We follow this pattern for quite a bit of functions. They all have extraApi as first argument. Two contexts Most functions with the extraApi argument are meant to perform their duty in two contexts: when called in a Jupyter notebook they deliver output meant for a notebook output cell, using methods provided by the ipython package. when called by the web app they deliver output meant for the TF browser website, generating raw HTML. The extraApi is the rich app specific API, and when we construct this API, we pass the information whether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app. We pass this information by setting the attribute asApi on the extraApi . If it is set, we use the extraApi in the web app context. Most of the code in such functions is independent of asApi . The main difference is how to output the result: by a call to an IPython display method, or by returning raw HTML. TF Data getters \u00b6 Auto loading of TF data The specific apps have functions to load and download data from github. They check first whether there is local data in a github repository, and if not, they check a local text-fabric-data directory, and if not, they download data from a know online GitHub repo into the local text-fabric-data directory. The data functions provided take parameters with these meanings: dataUrl The complete url from which to download data. ghBase The location of the local github directory, usually ~/github . This directory is expected to be subdivided by org and then by repo, just as the online GitHub. dataRel The relative path within the local github/text-fabric-data directory to the directory that holds versions of TF data. version The version of the TF data of interest. hasData(dataRel, ghBase, version) Checks whether there is TF data in standard locations. Returns the full path of the local github directory if the data is found in the expected place below it. Returns the full path of the local text-fabric-data directory if the data is found in the expected place below it. Returns False if there no offline copy of the data has been found in these locations. getData(dataUrl, dataRel, ghBase, version) Checks whether there is TF data in standard locations. If not, downloads data from dataUrl and places it in ~/text-fabric-data/dataRel/version getDataCustom(dataUrl, dest) Retrieves a zip file from dataUrl , and unpacks it at directory dest locally. TF search performers \u00b6 search(extraApi, query, silent=False, sets=None, shallow=False) This is a thin wrapper around the generic search interface of TF: S.search The extra thing it does it collecting the results. S.search() may yield a generator, and this search() makes sure to iterate over that generator, collect the results, and return them as a sorted list. Context Jupyter The intended context of this function is: an ordinary Python program (including the Jupyter notebook). Web apps can better use runSearch below. runSearch(api, query, cache) A wrapper around the generic search interface of TF. Before running the TF search, the query will be looked up in the cache . If present, its cached results/error messages will be returned. If not, the query will be run, results/error messages collected, put in the cache , and returned. Context web app The intended context of this function is: web app. runSearchCondensed(api, query, cache, condenseType) When query results need to be condensed into a container, this function takes care of that. It first tries the cache for condensed query results. If that fails, it collects the bare query results from the cache or by running the query. Then it condenses the results, puts them in the cache , and returns them. Context web app The intended context of this function is: web app. Tabular display \u00b6 table(extraApi, tuples, ...) Takes a list of tuples and composes it into a Markdown table. Context Jupyter The intended context of this function is: the Jupyter notebook. compose(extraApi, tuples, start. position, opened, ...) Takes a list of tuples and composes it into an HTML table. Some of the rows will be expandable, namely the rows specified by opened , for which extra data has been fetched. Context web app The intended context of this function is: web app. plainTuple(extraApi, tuple) Displays a tuple of nodes as a table row: a markdown row in the context Jupyter; an HTML row in the context web app. Pretty display \u00b6 What is pretty? Nodes are just numbers, but they stand for all the information that the corpus has about a certain item. pretty(node) makes a lot of that information visible in an app dependent way. For the Bhsa it means showing nested and intruding sentences, clauses and phrases. For the Cunei tablets it means showing alternating vertical and horizontal subdivisions of faces into columns, lines and cases. For the Peshitta it currently means showing the words of verses in unicode and ETCBC/WIT transcription. For the SyrNT it currently means showing the words of verses in unicode and SEDRA transcription. When you show a pretty representation of a node, usually pretty representations of \"contained\" nodes will also be drawn. You can selectively highlight those nodes with custom colors. When pretty-displaying a tuple of nodes, container nodes that contain those nodes will be looked up and displayed, and the actual tuple nodes will be highlighted. You can customize the highlight colors by selecting colors on the basis of the postions of nodes in their tuples, or you can explicitly pass a micro-managed colormap of nodes to colors. In pretty displays you can opt for showing/hiding the node numbers, for suppressing certain standard features, and there are app dependent options. In the case of Cunei tablets, you can opt to show the lineart of signs and quads, and to show the line numbers of the source transcriptions. show(extraApi, tuples, ...) Takes a list of tuples and composes it into a sequence of pretty displays per tuple. Context Jupyter The intended context of this function is: the Jupyter notebook. prettyTuple(extraApi, tuple) Displays a tuple of nodes as an expanded display, both in the context Jupyter and in the context web app. pretty(extrApi, node, ...) Displays a single node as an expanded display, both in the context Jupyter and in the context web app. prettyPre(extraApi, node, ...) Helper for pretty . Pretty display is pretty complicated. There are large portions of functionality that are generic, and large portions that are app specific. This function computes a lot of generic things, based on which a pretty display can be constructed. prettySetup(extraApi, features=None, noneValues=None) Pretty displays show a chosen set of standard features for nodes. By means of the parameter suppress you can leave out certain features. But what if you want to add features to the display? That is what prettySetup() does. It adds a list of features to the display, provided they are loadable. If they are not yet loaded, they will be loaded. Features with values that represent no information, will be suppressed. But you can customise what counts as no information, by passing a set of such values as noneValues . HTML and Markdown \u00b6 getBoundary(api, node) Utility function to ask from the TF API the first slot and the last slot contained in a node. getFeatures(extraApi, node, ...) Helper for pretty() : wrap the requested features and their values for node in HTML for pretty display. getContext(api, nodes) Get the features and values for a set of nodes . All loaded features will be retrieved. header(extraApi) Get the app-specific links to data and documentation and wrap it into HTML for display in the TF browser. outLink(text, href, title=None, ...) Produce a formatted HTML link. htmlEsc(val) Produce a representation of val that is safe for usage in a HTML context. mdEsc(val) Produce a representation of val that is safe for usage in a Markdown context. dm(markdown) Display a markdown string in a Jupyter notebook. Constants \u00b6 Fixed values The following values are used by other parts of the program: name description RESULT the label of a query result: result GH_BASE the location of the local github directory URL_GH the url to the GitHub site URL_NB the url to the NBViewer site","title":"Apps"},{"location":"Api/Apps/#apps","text":"About Text-Fabric is a generic engine to process text and annotations. When working with specific corpora, we want to have more power at our fingertips. We need extra power on top of the TF engine. The way we have chosen to do it is via apps . An app is a bunch of extra functions that know the structure of a specific corpus. In particular, an app knows how to produce plain representations and pretty displays of nodes of each type in the corpus.","title":"Apps"},{"location":"Api/Apps/#current-apps","text":"Current apps At the moment we have these apps name description bhsa Biblia Hebraica Stuttgartensia (Amstelodamensis) cunei Proto-cuneiform Uruk Corpus peshitta Syriac Old Testament syrnt Syriac New Testament","title":"Current apps"},{"location":"Api/Apps/#the-structure-of-apps","text":"App components The apps themselves are modules inside tf.extra For each app , you find there: module app .py contains all the functionality specific to the corpus in question, organized as an extended TF api. In the code this is referred to as the extraApi . In order to be an app that TF can use, extraApi should provide the following attributes: attribute kind description api object the generic TF api obtained by loading the features: `api = TF.load(...) asApi boolean True if working for the web-interface, False otherwise classNames dict mapping from node types to CSS class names to be used in plain and pretty displays of nodes of that type condenseType string the default node type to which results will be condensed, e.g. verse , tablet dataLink html link points to the repository where the TF data of the corpus is stored exampleSection text a concrete section indicator that points to an object of the condenseType (e.g. Genesis 1:1 , P005381 ) featureLink html link points to the documentation of the TF features of the corpus loadCSS method deliver CSS styling to notebook or web interface (depending on asApi ) nodeFromDefaultSection method given a section string pointing to an object of condenseType , return the corresponding node (or an error message) noneValues set feature values that are deemed uninteresting; features with those values will be suppressed in pretty displays plain method given a node, produce a plain representation of the corresponding object: not the full structure, but something that identifies it pretty method given a node, produce a pretty display of the corresponding object: the full structure prettyFeaturesLoaded set initial set of features that should be loaded for pretty displays tfsLink html link points to the documentation of the TF search engine tutLink html link points to the tutorial for TF search webLink method given a node, produces a link to an online description of the corresponding object (to shebanq or cdli asApi The extraApi contain several display functions. By default they suppose that there is a Jupyter notebook context in which results can be rendered with IPython.display methods. But if we operate in the context of a web-interface, we need to generate straight HTML. We flag the web-interface case as asApi == True . pretty Not all of the pretty method needs to be defined by the app. In fact, the function itself is defined generically in apphelpers . But it calls the method _pretty , which must be defined in the app. In turn, parth of _pretty is taking care of by the generic prettyPre in apphelpers.py . What remains in _pretty is the pure, app-dependent code for displaying nodes. webapp the package app -app is used by the text-fabric browser, and contains settings and assets to set up a browsing experience. config.py : settings a static folder with fonts and logos. config.py Contains values for parameters and an API calling function. extraApi(locations, modules) Responsible for calling the extra Api for the corpus with the desired locations and modules. This extraApi will be active as a TF kernel, interacting with a local webserver that serves local web page in the browser. web browsing settings The TF kernel, webserver and browser need settings: setting example description protocol http:// protocol of local website host localhost server address of local website webport 8001 port for the local website port 18981 port through wich the TF kernel and the webserver communicate data settings The TF kernel needs context information: setting type description locations list where to look for tf features modules list combines with locations to search paths for tf features localDir directory name temporary directory for writing and reading options tuple names of extra options for seaerching and displaying query results PROVENANCE dict corpus specific provenance metadata: name and DOI","title":"The structure of apps"},{"location":"Api/Apps/#the-generic-part-of-apps","text":"App helpers Apps turn out to have several things in common that we want to deal with generically. These functions are collected in the apphelpers module of TF. Generic/specific Sometimes there is an intricate mix of functionality that is shared by all apps and that is specific to some app. Here is our logistics of functionality. There are a number of methods that are offered as a generic function and just added as a method to the extraApi of the app, e.g. pretty() For example, the Bhsa app imports pretty first: 1 from tf.apphelpers import pretty and in the Bhsa __init__() function it says: 1 self . pretty = types . MethodType ( pretty , self ) which adds the function pretty as an instance method to the class Bhsa. The first argument extraApi of the function pretty acts as the self when pretty() is used as a method of Bhsa. So although we define pretty(extraApi, ...) as a generic function, through its argument extraApi we can call app specific functionality. We follow this pattern for quite a bit of functions. They all have extraApi as first argument. Two contexts Most functions with the extraApi argument are meant to perform their duty in two contexts: when called in a Jupyter notebook they deliver output meant for a notebook output cell, using methods provided by the ipython package. when called by the web app they deliver output meant for the TF browser website, generating raw HTML. The extraApi is the rich app specific API, and when we construct this API, we pass the information whether it is constructed for the purposes of the Jupyter notebook, or for the purposes of the web app. We pass this information by setting the attribute asApi on the extraApi . If it is set, we use the extraApi in the web app context. Most of the code in such functions is independent of asApi . The main difference is how to output the result: by a call to an IPython display method, or by returning raw HTML.","title":"The generic part of apps"},{"location":"Api/Apps/#tf-data-getters","text":"Auto loading of TF data The specific apps have functions to load and download data from github. They check first whether there is local data in a github repository, and if not, they check a local text-fabric-data directory, and if not, they download data from a know online GitHub repo into the local text-fabric-data directory. The data functions provided take parameters with these meanings: dataUrl The complete url from which to download data. ghBase The location of the local github directory, usually ~/github . This directory is expected to be subdivided by org and then by repo, just as the online GitHub. dataRel The relative path within the local github/text-fabric-data directory to the directory that holds versions of TF data. version The version of the TF data of interest. hasData(dataRel, ghBase, version) Checks whether there is TF data in standard locations. Returns the full path of the local github directory if the data is found in the expected place below it. Returns the full path of the local text-fabric-data directory if the data is found in the expected place below it. Returns False if there no offline copy of the data has been found in these locations. getData(dataUrl, dataRel, ghBase, version) Checks whether there is TF data in standard locations. If not, downloads data from dataUrl and places it in ~/text-fabric-data/dataRel/version getDataCustom(dataUrl, dest) Retrieves a zip file from dataUrl , and unpacks it at directory dest locally.","title":"TF Data getters"},{"location":"Api/Apps/#tf-search-performers","text":"search(extraApi, query, silent=False, sets=None, shallow=False) This is a thin wrapper around the generic search interface of TF: S.search The extra thing it does it collecting the results. S.search() may yield a generator, and this search() makes sure to iterate over that generator, collect the results, and return them as a sorted list. Context Jupyter The intended context of this function is: an ordinary Python program (including the Jupyter notebook). Web apps can better use runSearch below. runSearch(api, query, cache) A wrapper around the generic search interface of TF. Before running the TF search, the query will be looked up in the cache . If present, its cached results/error messages will be returned. If not, the query will be run, results/error messages collected, put in the cache , and returned. Context web app The intended context of this function is: web app. runSearchCondensed(api, query, cache, condenseType) When query results need to be condensed into a container, this function takes care of that. It first tries the cache for condensed query results. If that fails, it collects the bare query results from the cache or by running the query. Then it condenses the results, puts them in the cache , and returns them. Context web app The intended context of this function is: web app.","title":"TF search performers"},{"location":"Api/Apps/#tabular-display","text":"table(extraApi, tuples, ...) Takes a list of tuples and composes it into a Markdown table. Context Jupyter The intended context of this function is: the Jupyter notebook. compose(extraApi, tuples, start. position, opened, ...) Takes a list of tuples and composes it into an HTML table. Some of the rows will be expandable, namely the rows specified by opened , for which extra data has been fetched. Context web app The intended context of this function is: web app. plainTuple(extraApi, tuple) Displays a tuple of nodes as a table row: a markdown row in the context Jupyter; an HTML row in the context web app.","title":"Tabular display"},{"location":"Api/Apps/#pretty-display","text":"What is pretty? Nodes are just numbers, but they stand for all the information that the corpus has about a certain item. pretty(node) makes a lot of that information visible in an app dependent way. For the Bhsa it means showing nested and intruding sentences, clauses and phrases. For the Cunei tablets it means showing alternating vertical and horizontal subdivisions of faces into columns, lines and cases. For the Peshitta it currently means showing the words of verses in unicode and ETCBC/WIT transcription. For the SyrNT it currently means showing the words of verses in unicode and SEDRA transcription. When you show a pretty representation of a node, usually pretty representations of \"contained\" nodes will also be drawn. You can selectively highlight those nodes with custom colors. When pretty-displaying a tuple of nodes, container nodes that contain those nodes will be looked up and displayed, and the actual tuple nodes will be highlighted. You can customize the highlight colors by selecting colors on the basis of the postions of nodes in their tuples, or you can explicitly pass a micro-managed colormap of nodes to colors. In pretty displays you can opt for showing/hiding the node numbers, for suppressing certain standard features, and there are app dependent options. In the case of Cunei tablets, you can opt to show the lineart of signs and quads, and to show the line numbers of the source transcriptions. show(extraApi, tuples, ...) Takes a list of tuples and composes it into a sequence of pretty displays per tuple. Context Jupyter The intended context of this function is: the Jupyter notebook. prettyTuple(extraApi, tuple) Displays a tuple of nodes as an expanded display, both in the context Jupyter and in the context web app. pretty(extrApi, node, ...) Displays a single node as an expanded display, both in the context Jupyter and in the context web app. prettyPre(extraApi, node, ...) Helper for pretty . Pretty display is pretty complicated. There are large portions of functionality that are generic, and large portions that are app specific. This function computes a lot of generic things, based on which a pretty display can be constructed. prettySetup(extraApi, features=None, noneValues=None) Pretty displays show a chosen set of standard features for nodes. By means of the parameter suppress you can leave out certain features. But what if you want to add features to the display? That is what prettySetup() does. It adds a list of features to the display, provided they are loadable. If they are not yet loaded, they will be loaded. Features with values that represent no information, will be suppressed. But you can customise what counts as no information, by passing a set of such values as noneValues .","title":"Pretty display"},{"location":"Api/Apps/#html-and-markdown","text":"getBoundary(api, node) Utility function to ask from the TF API the first slot and the last slot contained in a node. getFeatures(extraApi, node, ...) Helper for pretty() : wrap the requested features and their values for node in HTML for pretty display. getContext(api, nodes) Get the features and values for a set of nodes . All loaded features will be retrieved. header(extraApi) Get the app-specific links to data and documentation and wrap it into HTML for display in the TF browser. outLink(text, href, title=None, ...) Produce a formatted HTML link. htmlEsc(val) Produce a representation of val that is safe for usage in a HTML context. mdEsc(val) Produce a representation of val that is safe for usage in a Markdown context. dm(markdown) Display a markdown string in a Jupyter notebook.","title":"HTML and Markdown"},{"location":"Api/Apps/#constants","text":"Fixed values The following values are used by other parts of the program: name description RESULT the label of a query result: result GH_BASE the location of the local github directory URL_GH the url to the GitHub site URL_NB the url to the NBViewer site","title":"Constants"},{"location":"Api/Bhsa/","text":"BHSA \u00b6 About \u00b6 The module bhsa.py contains a number of handy functions on top of Text-Fabric and especially its Search part. Minimal incantation \u00b6 1 2 from tf.extra.bhsa import Bhsa A = Bhsa ( hoist = globals ()) Explanation The first line makes the Bhsa API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Bhsa API and gives it the name A . During start-up the following happens: (1) the Bhsa data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) most optimized features of the Bhsa dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation . Set up \u00b6 import Bhsa The Bhsa API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.bhsa import Bhsa Initialisation \u00b6 Bhsa() 1 A = Bhsa ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific BHSA version The easiest way to load a specific version of the BHSA is like so: 1 A = Bhsa ( version = '2017' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '2017' TF = Fabric ( locations = f '~/github/etcbc/bhsa/tf/{VERSION}' ) api = TF . load ( ''' function sp gn nu ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the bhsa module by a call to Bhsa() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Bhsa API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output of this call will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through. Linking \u00b6 A.shbLink() 1 A . shbLink ( node , text = None ) Description Produces a link to SHEBANQ node node can be an arbitrary node. The link targets the verse that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 on SHEBANQ 1 A . shbLink ( 100000 ) Plain display \u00b6 Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , fmt = None , linked = True , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. linked linked indicates whether the result should be a link to SHEBANQ to the appropriate book/chapter/verse. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , fmt = None , linked = 1 , withNodes = False , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in SHEBANQ; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , fmt = None , start = 1 , end = len ( results ), linked = 1 , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() . Pretty display \u00b6 Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values, e.g. 1 noneValues = { None , 'NA' , 'unknown' , 'm' , 'sg' , 'p3' } None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. This is how SHEBANQ displays its query results. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() . Search \u00b6 A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set gappedPhrases of all phrase nodes that have a gap, you can pass sets=dict(gphrase=gappedPhrases) , and then in your query you can say 1 2 gphrase function=Pred word sp=verb etc. search template reference See the search template reference","title":"Hebrew Bible"},{"location":"Api/Bhsa/#bhsa","text":"","title":"BHSA"},{"location":"Api/Bhsa/#about","text":"The module bhsa.py contains a number of handy functions on top of Text-Fabric and especially its Search part.","title":"About"},{"location":"Api/Bhsa/#minimal-incantation","text":"1 2 from tf.extra.bhsa import Bhsa A = Bhsa ( hoist = globals ()) Explanation The first line makes the Bhsa API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Bhsa API and gives it the name A . During start-up the following happens: (1) the Bhsa data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) most optimized features of the Bhsa dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation .","title":"Minimal incantation"},{"location":"Api/Bhsa/#set-up","text":"import Bhsa The Bhsa API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.bhsa import Bhsa","title":"Set up"},{"location":"Api/Bhsa/#initialisation","text":"Bhsa() 1 A = Bhsa ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific BHSA version The easiest way to load a specific version of the BHSA is like so: 1 A = Bhsa ( version = '2017' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '2017' TF = Fabric ( locations = f '~/github/etcbc/bhsa/tf/{VERSION}' ) api = TF . load ( ''' function sp gn nu ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the bhsa module by a call to Bhsa() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Bhsa API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output of this call will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through.","title":"Initialisation"},{"location":"Api/Bhsa/#linking","text":"A.shbLink() 1 A . shbLink ( node , text = None ) Description Produces a link to SHEBANQ node node can be an arbitrary node. The link targets the verse that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 on SHEBANQ 1 A . shbLink ( 100000 )","title":"Linking"},{"location":"Api/Bhsa/#plain-display","text":"Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , fmt = None , linked = True , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. linked linked indicates whether the result should be a link to SHEBANQ to the appropriate book/chapter/verse. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , fmt = None , linked = 1 , withNodes = False , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in SHEBANQ; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , fmt = None , start = 1 , end = len ( results ), linked = 1 , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() .","title":"Plain display"},{"location":"Api/Bhsa/#pretty-display","text":"Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values, e.g. 1 noneValues = { None , 'NA' , 'unknown' , 'm' , 'sg' , 'p3' } None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. This is how SHEBANQ displays its query results. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() .","title":"Pretty display"},{"location":"Api/Bhsa/#search","text":"A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set gappedPhrases of all phrase nodes that have a gap, you can pass sets=dict(gphrase=gappedPhrases) , and then in your query you can say 1 2 gphrase function=Pred word sp=verb etc. search template reference See the search template reference","title":"Search"},{"location":"Api/Cunei/","text":"Cunei \u00b6 About \u00b6 The module cunei.py contains a number of handy functions to deal with TF nodes for cuneiform tablets and ATF transcriptions of them and CDLI photos and lineart. See also about , images , transcription . Minimal incantation \u00b6 1 2 from tf.extra.bhsa import Cunei A = Cunei ( hoist = globals ()) Explanation The first line makes the Cunei API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Cunei API and gives it the name A . During start-up the following happens: (1) the Cunei data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) most optimized features of the Cunei dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation . Set up \u00b6 from tf.extra.cunei import Cunei import Cunei The Cunei API is distributed with Text-Fabric. You have to import it into your program. Initialisation \u00b6 Cunei() 1 A = Cunei ( name = None , hoist = globals (), silent = False ) Description Text-Fabric will be started for you and load all features as explained above in the minimal incantation.. When Cunei is initializing, it scans the image directory of the data and reports how many photos and lineart images it sees. name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Cunei API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through. Linking \u00b6 A.cdli() 1 A . cdli ( tablet , linkText = None , asString = False ) Description Produces a link to a tablet page on CDLI, to be placed in an output cell. tablet tablet is either a node of type tablet or a P-number of a tablet. linkText You may provide the text to be displayed as the link. If you do not provide any, the P-number of the tablet will be used. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the HTML as string, just say asString=True . A.tabletLink() 1 A . tabletLink ( node , text = None , asString = False ) Description Produces a link to CDLI node node can be an arbitrary node. The link targets the tablet that contains the material contained by the node. text You may provide the text to be displayed as the link. If you do not provide a link text, the P-number of the tablet will be chosen. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the HTML as string, just say asString=True . Sign 10000 on CDLI 1 A . tabletLink ( 100000 ) Plain display \u00b6 Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , linked = True , withNodes = False , lineart = True , lineNumbers = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. linked linked indicates whether the result should be a link to CDLI to the tablet on which the node occurs. withNodes withNodes indicates whether node numbers should be displayed. lineart lineart indicates whether to display a lineart image in addition (only relevant for signs and quads) lineNumbers lineNumbers indicates whether corresponding line numbers in the ATF source should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 2 3 4 5 6 A . plainTuple ( nodes , seqNumber , linked = 1 , withNodes = False , lineart = True , lineNumbers = False , asString = False , ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the CDLI page of the containing tablet; (the first data column is column 1) withNodes, lineart, lineNumbers, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 9 A . table ( results , start = 1 , end = len ( results ), linked = 1 , withNodes = False , lineart = True , lineNumbers = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, withNodes, lineart, lineNumbers, asString Same as in A.plainTuple() . Pretty display \u00b6 Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.pretty() 1 A . pretty ( node , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. withNodes withNodes indicates whether node numbers should be displayed. lineart, lineNumbers Same as in A.plain() . suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as tablets and cases are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 9 A . prettyTuple ( nodes , seqNumber , withNodes = False , lineart = True , lineNumbers = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By tablet We examine all nodes in the tuple. We collect and show all tablets in which they occur and highlight the material corresponding to the all nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, withNodes, lineart, lineNumbers Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color: If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 11 A . show ( results , condensed = True , start = 1 , end = len ( results ), withNodes = False , lineart = True , lineNumbers = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all tablets with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. withNodes, lineart, lineNumbers, suppress, colorMap, highlights Same as in A.prettyTuple() . Search \u00b6 A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set specialCases of all case nodes that are special in some way, you can pass sets=dict(scase=specialCases) , and then in your query you can say 1 2 scase number=1a sign grapheme=UKKIN etc. search template reference See the search template reference ATF representation \u00b6 Generate ATF Signs and quads and clusters can be represented by an ascii string, in the so-called Ascii Text Format, ATF . We provide a bunch of function that, given a node, generate the appropriate ATF representation. A.atfFromSign() 1 A . atfFromSign ( node , flags = False ) Description Reproduces the ATF representation of a sign. node node must have node type sign . flags flags whether the flags associated with the sign will be included in the ATF. A.atfFromQuad() 1 A . atfFromQuad ( node , flags = False ) Description Reproduces the ATF representation of a quad. node node must have node type quad . flags flags whether the flags associated with the quad will be included in the ATF. A.atfFromOuterQuad() 1 A . atfFromOuterQuad ( node , flags = False ) Description Reproduces the ATF representation of a quad or sign. node node must have node type quad or sign . flags flags whether the flags associated with the quad will be included in the ATF. outer quads If you take an ATF transcription line with linguistic material on it, and you split it on white space, and you forget the brackets that cluster quads and signs, then you get a sequence of outer quads and signs. If you need to get the ATF representation for these items, this function does conveniently produce them. You do not have to worry yourself about the sign/quad distinction here. A.atfFromCluster() 1 A . atfFromCluster ( node , flags = False ) Description Reproduces the ATF representation of a cluster. node node must have node type quad . clusters Clusters are bracketings of quads that indicate proper names, uncertainty, or supplied material. In ATF they look like ( )a or [ ] or < > Sub-clusters Sub-clusters will also be represented. Signs belonging to multiple nested clusters will only be represented once. A.getSource() 1 A . getSource ( node , nodeType = None , lineNumbers = False ) Description Delivers the transcription source of nodes that correspond to the ATF source line level. This in contrast with the A.atfFromXxx() functions that work for nodes that correspond to parts of the ATF source lines. node node must have a type in tablet , face , column , comment , line , case . nodeType If nodeType is passed, only source lines of this type are returned. lineNumbers lineNumbers : if True , add line numbers to the result, these numbers say where the source line occurs in the source file. TF from ATF conversion The conversion of ATF to Text-Fabric has saved the original source lines and their line numbers in the features srcLn and srcLnNum respectively. This function makes use of those features. Sections \u00b6 Sections in tablets Text-Fabric supports 3 section levels in general. The Uruk corpus uses them for tablets , columns and lines . But lines may be divided in cases and subcases, which are also numbered. We need to mimick some functions of the Text-Fabric T Api for sections, so that we can retrieve cases more easily. Consider search Text-Fabric Search is a generic and powerful mechanism for information retrieval. In most cases it is easier to extract nodes by search than by hand-written code using the functions here. A.nodeFromCase() 1 A . nodeFromCase (( P - number , face : columnNumber , hLineNumber )) Description Gives you a node, if you specify a terminal case, i.e. a numbered transcription line. Compare T.nodeFromSection() This function is analogous to T.nodeFromSection() of Text-Fabric. case specification This function takes a single argument which must be a tuple ( tabletNumber , face : columnNumber , hierarchical-line-number ). dots The hierarchical number may contain the original . that they often have in the transcriptions, but you may also leave them out. Not found If no such node exists, you get None back. A.caseFromNode() 1 A . caseFromNode ( node ) Description Gives you a terminal case specification, if you give a node of a case or something inside a case or line. Compare T.sectionFromNode() This function is analogous to T.sectionFromNode() of Text-Fabric. case specification A case specification is a tuple ( tabletNumber , face : columnNumber , hierarchical-line-number ). The hierarchical line number will not contain dots. node node must be of a terminal case (these are the cases that have a full hierarchical number; these cases correspond to the individual numbered lines in the transcription sources). other nodes If node corresponds to something inside a transcription line, the node of the terminal case or line in which it is contained will be used. A.lineFromNode() 1 A . lineFromNode ( node ) Description If called on a node corresponding to something inside a transcription line, it will navigate to up to the terminal case or line in which it is contained, and return that node. node node must correspond to something inside a transcription line: sign , quad , cluster . A.casesByLevel() 1 A . casesByLevel ( k , terminal = True ) Description Grabs all (sub)cases of a specified level. You can choose to filter the result to those (sub)cases that are terminal , i.e. those which do not contain subcases anymore. Such cases correspond to individual lines in the ATF. k k is an integer, indicating the level of (sub)cases you want. 0 is lines, 1 is top-level cases, 2 is subcases, 3 is subsubcases, and so on. terminal terminal : if True , only lines and cases that have the feature terminal are delivered. Otherwise, all lines/cases of that level will be delivered. A.getOuterQuads() 1 A . getOuterQuads ( node ) Description Collects the outer quads and isolated signs under a node. node node is typically a tablet, face, column, line, or case. This is the container of the outer quads. Outer quads Outer quads and isolated signs is what you get if you split line material by white space and remove cluster brackets. Images \u00b6 A.photo() and A.lineart() 1 2 A . photo ( nodes , key = None , asLink = True , withCaption = 'bottom' , ** options ) A . lineart ( nodes , key = None , asLink = True , withCaption = 'bottom' , ** options ) Description Fetches photos or linearts for tablets, signs or quads, and returns it in a way that it can be embedded in an output cell. The images that show up are clickable and link through to an online, higher resolution version on CDLI. Images will have, by default, a caption that links to the relevant page on CDLI. Placement The result will be returned as a row of images. Subsequent calls to photo() and lineart() will result in vertically stacked rows. nodes nodes is one or more nodes . As far as they are of type tablet , quad or sign , a photo or lineart will be looked up for them. by name Instead of a node you may also supply the P-number of a tablet or the name of the sign or quad. key key is an optional string specifying which of the available images for this node you want to use. look up if you want to know which keys are available for a node, supply key='xxx' , or any non-existing key. asLink asLink=True : no image will be placed, only a link to the online image at CDLI. In this case the caption will be suppressed, unless explicitly given. withCaption withCaption='bottom' controls whether a CDLI link to the tablet page must be put under the image. You can also specify top , left , right . If left out, no caption will be placed. options options is a series of key=value arguments that control the placement of the images, such as width=100 , height=200 . CSS The optional parameters height and width control the height and width of the images. The value should be a valid CSS length, such as 100px , 10em , 32vw . If you pass an integer, or a decimal string without unit, your value will be converted to that many px . These parameters are interpreted as setting a maximum value (in fact they will end up as max-width and max-height on the final <img/> element in the HTML. So if you specify both width and height , the image will be placed in tightly in a box of those dimensions without changing the aspect ratio. If you want to force that the width of height you pass is completely consumed, you can prefix your value with a ! . In that case the aspect ratio maybe changed. You can use the ! also for both height and width . In that case, the rectangle will be completely filled, and the aspect ratio will be adjusted to that of the rectangle. The way the effect of the ! is achieved, is by adding min-width and min-height properties to the <img/> element. local images The images will be called in by a little piece of generated HTML, using the <img/> tag. This only works if the image is within reach. To the images will be copied to a sister directory of the notebook. The name of this directory is cdli-imagery . It will be created on-the-fly when needed. Copying will only be done if needed. The names of the images will be changed, to prevent problems with systems that cannot handle | and + characters in file names well. A.imagery() 1 A . imagery ( objectType , kind ) Description Provides the sets of locally available images by object type. for tablets, it lists the P-numbers; for sign/quads: the ATF representations. objectType objectType is the type of thing: ideograph or tablet . kind kind is photo or lineart .","title":"Cuneiform Tablets"},{"location":"Api/Cunei/#cunei","text":"","title":"Cunei"},{"location":"Api/Cunei/#about","text":"The module cunei.py contains a number of handy functions to deal with TF nodes for cuneiform tablets and ATF transcriptions of them and CDLI photos and lineart. See also about , images , transcription .","title":"About"},{"location":"Api/Cunei/#minimal-incantation","text":"1 2 from tf.extra.bhsa import Cunei A = Cunei ( hoist = globals ()) Explanation The first line makes the Cunei API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Cunei API and gives it the name A . During start-up the following happens: (1) the Cunei data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) most optimized features of the Cunei dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation .","title":"Minimal incantation"},{"location":"Api/Cunei/#set-up","text":"from tf.extra.cunei import Cunei import Cunei The Cunei API is distributed with Text-Fabric. You have to import it into your program.","title":"Set up"},{"location":"Api/Cunei/#initialisation","text":"Cunei() 1 A = Cunei ( name = None , hoist = globals (), silent = False ) Description Text-Fabric will be started for you and load all features as explained above in the minimal incantation.. When Cunei is initializing, it scans the image directory of the data and reports how many photos and lineart images it sees. name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Cunei API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through.","title":"Initialisation"},{"location":"Api/Cunei/#linking","text":"A.cdli() 1 A . cdli ( tablet , linkText = None , asString = False ) Description Produces a link to a tablet page on CDLI, to be placed in an output cell. tablet tablet is either a node of type tablet or a P-number of a tablet. linkText You may provide the text to be displayed as the link. If you do not provide any, the P-number of the tablet will be used. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the HTML as string, just say asString=True . A.tabletLink() 1 A . tabletLink ( node , text = None , asString = False ) Description Produces a link to CDLI node node can be an arbitrary node. The link targets the tablet that contains the material contained by the node. text You may provide the text to be displayed as the link. If you do not provide a link text, the P-number of the tablet will be chosen. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the HTML as string, just say asString=True . Sign 10000 on CDLI 1 A . tabletLink ( 100000 )","title":"Linking"},{"location":"Api/Cunei/#plain-display","text":"Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , linked = True , withNodes = False , lineart = True , lineNumbers = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. linked linked indicates whether the result should be a link to CDLI to the tablet on which the node occurs. withNodes withNodes indicates whether node numbers should be displayed. lineart lineart indicates whether to display a lineart image in addition (only relevant for signs and quads) lineNumbers lineNumbers indicates whether corresponding line numbers in the ATF source should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 2 3 4 5 6 A . plainTuple ( nodes , seqNumber , linked = 1 , withNodes = False , lineart = True , lineNumbers = False , asString = False , ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the CDLI page of the containing tablet; (the first data column is column 1) withNodes, lineart, lineNumbers, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 9 A . table ( results , start = 1 , end = len ( results ), linked = 1 , withNodes = False , lineart = True , lineNumbers = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, withNodes, lineart, lineNumbers, asString Same as in A.plainTuple() .","title":"Plain display"},{"location":"Api/Cunei/#pretty-display","text":"Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.pretty() 1 A . pretty ( node , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. withNodes withNodes indicates whether node numbers should be displayed. lineart, lineNumbers Same as in A.plain() . suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as tablets and cases are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 9 A . prettyTuple ( nodes , seqNumber , withNodes = False , lineart = True , lineNumbers = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By tablet We examine all nodes in the tuple. We collect and show all tablets in which they occur and highlight the material corresponding to the all nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, withNodes, lineart, lineNumbers Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color: If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 11 A . show ( results , condensed = True , start = 1 , end = len ( results ), withNodes = False , lineart = True , lineNumbers = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all tablets with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. withNodes, lineart, lineNumbers, suppress, colorMap, highlights Same as in A.prettyTuple() .","title":"Pretty display"},{"location":"Api/Cunei/#search","text":"A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. For example, if you have a set specialCases of all case nodes that are special in some way, you can pass sets=dict(scase=specialCases) , and then in your query you can say 1 2 scase number=1a sign grapheme=UKKIN etc. search template reference See the search template reference","title":"Search"},{"location":"Api/Cunei/#atf-representation","text":"Generate ATF Signs and quads and clusters can be represented by an ascii string, in the so-called Ascii Text Format, ATF . We provide a bunch of function that, given a node, generate the appropriate ATF representation. A.atfFromSign() 1 A . atfFromSign ( node , flags = False ) Description Reproduces the ATF representation of a sign. node node must have node type sign . flags flags whether the flags associated with the sign will be included in the ATF. A.atfFromQuad() 1 A . atfFromQuad ( node , flags = False ) Description Reproduces the ATF representation of a quad. node node must have node type quad . flags flags whether the flags associated with the quad will be included in the ATF. A.atfFromOuterQuad() 1 A . atfFromOuterQuad ( node , flags = False ) Description Reproduces the ATF representation of a quad or sign. node node must have node type quad or sign . flags flags whether the flags associated with the quad will be included in the ATF. outer quads If you take an ATF transcription line with linguistic material on it, and you split it on white space, and you forget the brackets that cluster quads and signs, then you get a sequence of outer quads and signs. If you need to get the ATF representation for these items, this function does conveniently produce them. You do not have to worry yourself about the sign/quad distinction here. A.atfFromCluster() 1 A . atfFromCluster ( node , flags = False ) Description Reproduces the ATF representation of a cluster. node node must have node type quad . clusters Clusters are bracketings of quads that indicate proper names, uncertainty, or supplied material. In ATF they look like ( )a or [ ] or < > Sub-clusters Sub-clusters will also be represented. Signs belonging to multiple nested clusters will only be represented once. A.getSource() 1 A . getSource ( node , nodeType = None , lineNumbers = False ) Description Delivers the transcription source of nodes that correspond to the ATF source line level. This in contrast with the A.atfFromXxx() functions that work for nodes that correspond to parts of the ATF source lines. node node must have a type in tablet , face , column , comment , line , case . nodeType If nodeType is passed, only source lines of this type are returned. lineNumbers lineNumbers : if True , add line numbers to the result, these numbers say where the source line occurs in the source file. TF from ATF conversion The conversion of ATF to Text-Fabric has saved the original source lines and their line numbers in the features srcLn and srcLnNum respectively. This function makes use of those features.","title":"ATF representation"},{"location":"Api/Cunei/#sections","text":"Sections in tablets Text-Fabric supports 3 section levels in general. The Uruk corpus uses them for tablets , columns and lines . But lines may be divided in cases and subcases, which are also numbered. We need to mimick some functions of the Text-Fabric T Api for sections, so that we can retrieve cases more easily. Consider search Text-Fabric Search is a generic and powerful mechanism for information retrieval. In most cases it is easier to extract nodes by search than by hand-written code using the functions here. A.nodeFromCase() 1 A . nodeFromCase (( P - number , face : columnNumber , hLineNumber )) Description Gives you a node, if you specify a terminal case, i.e. a numbered transcription line. Compare T.nodeFromSection() This function is analogous to T.nodeFromSection() of Text-Fabric. case specification This function takes a single argument which must be a tuple ( tabletNumber , face : columnNumber , hierarchical-line-number ). dots The hierarchical number may contain the original . that they often have in the transcriptions, but you may also leave them out. Not found If no such node exists, you get None back. A.caseFromNode() 1 A . caseFromNode ( node ) Description Gives you a terminal case specification, if you give a node of a case or something inside a case or line. Compare T.sectionFromNode() This function is analogous to T.sectionFromNode() of Text-Fabric. case specification A case specification is a tuple ( tabletNumber , face : columnNumber , hierarchical-line-number ). The hierarchical line number will not contain dots. node node must be of a terminal case (these are the cases that have a full hierarchical number; these cases correspond to the individual numbered lines in the transcription sources). other nodes If node corresponds to something inside a transcription line, the node of the terminal case or line in which it is contained will be used. A.lineFromNode() 1 A . lineFromNode ( node ) Description If called on a node corresponding to something inside a transcription line, it will navigate to up to the terminal case or line in which it is contained, and return that node. node node must correspond to something inside a transcription line: sign , quad , cluster . A.casesByLevel() 1 A . casesByLevel ( k , terminal = True ) Description Grabs all (sub)cases of a specified level. You can choose to filter the result to those (sub)cases that are terminal , i.e. those which do not contain subcases anymore. Such cases correspond to individual lines in the ATF. k k is an integer, indicating the level of (sub)cases you want. 0 is lines, 1 is top-level cases, 2 is subcases, 3 is subsubcases, and so on. terminal terminal : if True , only lines and cases that have the feature terminal are delivered. Otherwise, all lines/cases of that level will be delivered. A.getOuterQuads() 1 A . getOuterQuads ( node ) Description Collects the outer quads and isolated signs under a node. node node is typically a tablet, face, column, line, or case. This is the container of the outer quads. Outer quads Outer quads and isolated signs is what you get if you split line material by white space and remove cluster brackets.","title":"Sections"},{"location":"Api/Cunei/#images","text":"A.photo() and A.lineart() 1 2 A . photo ( nodes , key = None , asLink = True , withCaption = 'bottom' , ** options ) A . lineart ( nodes , key = None , asLink = True , withCaption = 'bottom' , ** options ) Description Fetches photos or linearts for tablets, signs or quads, and returns it in a way that it can be embedded in an output cell. The images that show up are clickable and link through to an online, higher resolution version on CDLI. Images will have, by default, a caption that links to the relevant page on CDLI. Placement The result will be returned as a row of images. Subsequent calls to photo() and lineart() will result in vertically stacked rows. nodes nodes is one or more nodes . As far as they are of type tablet , quad or sign , a photo or lineart will be looked up for them. by name Instead of a node you may also supply the P-number of a tablet or the name of the sign or quad. key key is an optional string specifying which of the available images for this node you want to use. look up if you want to know which keys are available for a node, supply key='xxx' , or any non-existing key. asLink asLink=True : no image will be placed, only a link to the online image at CDLI. In this case the caption will be suppressed, unless explicitly given. withCaption withCaption='bottom' controls whether a CDLI link to the tablet page must be put under the image. You can also specify top , left , right . If left out, no caption will be placed. options options is a series of key=value arguments that control the placement of the images, such as width=100 , height=200 . CSS The optional parameters height and width control the height and width of the images. The value should be a valid CSS length, such as 100px , 10em , 32vw . If you pass an integer, or a decimal string without unit, your value will be converted to that many px . These parameters are interpreted as setting a maximum value (in fact they will end up as max-width and max-height on the final <img/> element in the HTML. So if you specify both width and height , the image will be placed in tightly in a box of those dimensions without changing the aspect ratio. If you want to force that the width of height you pass is completely consumed, you can prefix your value with a ! . In that case the aspect ratio maybe changed. You can use the ! also for both height and width . In that case, the rectangle will be completely filled, and the aspect ratio will be adjusted to that of the rectangle. The way the effect of the ! is achieved, is by adding min-width and min-height properties to the <img/> element. local images The images will be called in by a little piece of generated HTML, using the <img/> tag. This only works if the image is within reach. To the images will be copied to a sister directory of the notebook. The name of this directory is cdli-imagery . It will be created on-the-fly when needed. Copying will only be done if needed. The names of the images will be changed, to prevent problems with systems that cannot handle | and + characters in file names well. A.imagery() 1 A . imagery ( objectType , kind ) Description Provides the sets of locally available images by object type. for tablets, it lists the P-numbers; for sign/quads: the ATF representations. objectType objectType is the type of thing: ideograph or tablet . kind kind is photo or lineart .","title":"Images"},{"location":"Api/General/","text":"Text-Fabric API \u00b6 Tutorial The tutorials for the Hebrew Bible , Peshitta , Syriac New Testament , and the Uruk Cuneiform Tablets put the Text-Fabric API on show for vastly different corpora. Generic API This is the API of Text-Fabric in general. Text-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several additions that make working with specific corpora easier. Hebrew Bible: BHSA Peshitta: Peshitta Syriac New Testament: SyrNT Proto-cuneiform tablets from Uruk: Cunei Loading \u00b6 TF=Fabric() 1 2 from tf.fabric import Fabric TF = Fabric ( locations = directories , modules = subdirectories , silent = False ) Description Text-Fabric is initialized for a corpus. It will search a set of directories and catalog all .tf files it finds there. These are the features you can subsequently load. Here directories and subdirectories are strings with directory names separated by newlines, or iterables of directories. locations, modules The directories specified in locations will be searched for modules , which are paths that will be appended to the paths in locations . All .tf files (non-recursively) in any module will be added to the feature set to be loaded in this session. The order in modules is important, because if a feature occurs in multiple modules, the last one will be chosen. In this way you can easily override certain features in one module by features in an other module of your choice. otext@ in modules If modules contain features with a name starting with otext@ , then the format definitions in these features will be added to the format definitions in the regular otext feature (which is a WARP feature). In this way, modules that define new features for text representation, also can add new formats to the Text-API. Defaults The locations list has a few defaults: 1 2 3 ~/Downloads/text-fabric-data ~/text-fabric-data ~/github/text-fabric-data So if you have stored your main Text-Fabric dataset in text-fabric-data in one of these directories you do not have to pass a location to Fabric. The modules list defaults to [''] . So if you leave it out, Text-Fabric will just search the paths specified in locations . silent If silent=True is passed, banners and normal progress messages are suppressed. TF.explore() 1 2 features = TF . explore ( silent = False , show = True ) features or 1 2 TF . explore ( silent = False , show = False ) TF . featureSets Description This will give you a dictionary of all available features by kind. The kinds are: nodes , edges , configs , computeds . silent With silent=False a message containing the total numbers of features is issued. show The resulting dictionary is delivered in TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api = TF . load ( features , add = False , silent = False ) Description Reads the features indicated by features and loads them in memory ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. add If later on you want load more features, you can either: add the features to the original load() statement and just run it again make a new statement: TF.load(newfeatures, add=True) . The new features will be added to the same api, so you do not have to to call api.makeAvailableIn(globals()) again after this! silent The features will be loaded rather silently, most messages will be suppressed. Time consuming operations will always be announced, so that you know what Text-Fabric is doing. If silent=True is passed, all informational messages will be suppressed. This is handy I you want to load data as part of other methods, on-the-fly. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ()) Description This method will export every member of the API (such as N , F , E , L , T , info ) to the global namespace. From now on, we will omit the api. in our documentation. Contents of the API After having loaded the features by api = TF.load(...) , the api harbours your Text-Fabric API. You can access node feature mydata by api.F.mydata.v(node) , edge feature mylink by api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit tedious to write the initial api. all the time. By this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital letters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored Description If you want to know which features were found but ignored (because the feature is also present in another, later, location), you can use this attribute to inspect the ignored features and their locations. loadLog() 1 api . loadlog () Description After loading you can view all messages using this method. It also shows the messages that have been suppressed due to silent=True . Navigating nodes \u00b6 N() 1 2 for n in N (): action Description The result of N() is a generator that walks through all nodes in the canonical order (see below). Iterating over N() delivers you all words and structural elements of your corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in a suitable order. Occasionally, during the walk you might want to visit embedding or embedded nodes to glean some feature information from them. More ways of walking Later, under Features there is another convenient way to walk through nodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way that you can enumerate all nodes in the order you encounter them if you walk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence, because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a book node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis 1:1, then a sentence node, then a clause node, a phrase node, and the first word node. Then follow all word nodes in the first phrase, then the phrase node of the second phrase, followed by the word nodes in that phrase. When ever you enter a higher structure, you will first get the node corresponding to that structure, and after that the nodes corresponding to the building blocks of that structure. This concept follows the intuition that slot sets with smaller elements come before slot set with bigger elements, and embedding slot sets come before embedded slot sets. Hence, if you enumerate a set of nodes that happens to constitute a tree hierarchy based on slot set embedding, and you enumerate those nodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994), Text Databases. One Database Model and Several Retrieval Languages , number 14 in Language and Computers, Editions Rodopi, Amsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, http://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by Doedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements available when working with an element. That is the advantage of pre-order over post-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet ) Description delivers an iterable of nodes as a tuple sorted by the canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList = sorted ( nodes , key = sortKey ) Description A function that provides for each node the key to be used to sort nodes in the canonical ordering. That means that the following two pieces of code do the same thing: sortNodes(nodeSet) and sorted(nodeSet, key=sortKey) . Sorting tuples of nodes Handy to sort things that are not nodes themselves, but data structures with nodes in it, e.g. search results: if results is a list of tuples of nodes, we could sort them in canonical order like this: 1 sorted ( nodeSet , key = lambda r : sortKey ( r [ 0 ])) otypeRank 1 otypeRank [ 'sentence' ] The node types are ordered in C.levels.data , and if you reverse that list, you get the rank of a type by looking at the position in which that type occurs. The slotType has otypeRank 0, and the more comprehensive a type is, the higher its rank. Locality \u00b6 Local navigation Here are the methods by which you can navigate easily from a node to its neighbours: parents and children, previous and next siblings. L The Locality API is exposed as L or Locality . otype parameter In all of the following L -functions, if the otype parameter is passed, the result is filtered and only nodes with otype=nodeType are retained. Results of the L. functions are tuples, not single nodes Even if an L -function returns a single node, it is packed in a tuple . So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ] L.u() 1 L . u ( node , otype = nodeType ) Description Produces an ordered tuple of nodes upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered. The result never includes node itself. L.d() 1 L . d ( node , otype = nodeType ) Description Produces an ordered tuple of nodes downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered. The result never includes node itself. L.n() 1 L . n ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent next nodes. node The node whose right adjacent nodes will be delivered; i.e. the nodes whose first slot immediately follow the last slot of node . The result never includes node itself. L.p() 1 L . p ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent previous nodes from node , i.e. nodes whose last slot just precedes the first slot of node . Description Produces an ordered tuple of adjacent previous nodes. node The node whose lefy adjacent nodes will be delivered; i.e. the nodes whose last slot immediately precede the first slot of node . Locality and levels Here is something that is very important to be aware of when using sortNodes and the L.d(n) and L.u(n) methods. When we order nodes and report on which nodes embed which other nodes, we do not only take into account the sets of slots the nodes occupy, but also their level . See levels and text . Both the L.d(n) and L.u(n) work as follows: L.d(n) returns nodes such that embedding nodes come before embedded nodes words) L.u(n) returns nodes such that embedded nodes come before embedding nodes books) N.B.: Suppose you have node types verse and sentence , and usually a verse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode) will contain sentence nodes, L.d(sentenceNode) will not contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the same words, then that is a case where: L.d(verseNode) will contain one sentence node, L.d(sentenceNode) will contain one verse node. Text \u00b6 Overview Here are the functions that enable you to get the actual text in the dataset. There are several things to accomplish here, such as getting text given book, chapter, and verse; given a node, produce the book, chapter and verse indicators in which the node is contained; handle multilingual book names; switch between various text representations. The details of the Text API are dependent on the warp feature otext , which is a config feature. T The Text API is exposed as T or Text . Sections \u00b6 Section levels In otext the main section levels (usually book , chapter , verse ) can be defined. It loads the features it needs (so you do not have to specify those features, unless you want to use them via F ). And finally, it makes some functions available by which you can make handy use of that information. Section levels are generic In this documentation, we call the main section level book , the second level chapter , and the third level verse . Text-Fabric, however, is completely agnostic about how these levels are called. It is prepared to distinguish three section levels, but how they are called, must be configured in the dataset. The task of the otext feature is to declare which node type and feature correspond with which section level. Text-Fabric assumes that the first section level may have multilingual headings, but that section levels two and three have single language headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or integers, depending on whether your dataset has declared the corresponding feature valueType as str or as int . Conceivably, other works might have chapter and verse numbers like XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. otext is optional If otext is missing, the Text API will not be build. If it exists, but does not specify sections, that part of the Text API will not be built. Likewise for text representations. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the sense that node types that act as containers have a lower level than node types that act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for your dataset, you can configure the right order in the otext feature, by means of a key levels with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign T.sectionFromNode() 1 T . sectionFromNode ( node , lastSlot = False , lang = 'en' ) Description Returns the book/chapter/verse indications that correspond to the reference node, which is the first or last slot belonging n , dependent on lastSlot . The result is a tuple, consisting of the book name (in language lang ), the chapter number, and the verse number. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse. In those cases, this function will return a different results for lastSlot=False and lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a None in the corresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section , lang = 'en' ) Description Given a section tuple, return the node of it. section section consists of a book name (in language lang ), and a chapter number and a verse number (both as strings or number depending on the value type of the corresponding feature). The verse number may be left out, the result is then a chapter node. Both verse and chapter numbers may be left out, the result is then a book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent. Book names and languages \u00b6 Book names and nodes The names of the books may be available in multiple languages. The book names are stored in node features with names of the form book@ la , where la is the ISO 639 two-letter code for that language. Text-Fabric will always load these features. T.languages 1 T . languages Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node , lang = 'en' ) Description gives the name of the book in which a node occurs. node The node in question. lang The lang parameter is a two letter language code. The default is en (English). If there is no feature data for the language chosen, the value of the ordinary book feature of the dataset will be returned. Works for all nodes n may or may not be a book node. If not, bookName() retrieves the embedding book node first. T.bookNode() 1 T . bookNode ( name , lang = 'en' ) Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the name parameter. If lang can not be found, the value of the ordinary book feature of the dataset will be used. If name cannot be found in the specified language, None will be returned. Function name follows configured section level If your dataset has configured section level one under an other name, say tablet , then these two methods follow that name. Instead of T.bookName() and T.bookNode() we have then T.tabletName() and T.tabletNode() . Text representation \u00b6 Text formats Text can be represented in multiple ways. We provide a number of formats with structured names. A format name is a string of keywords separated by - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is text-orig-full , we assume that every TF dataset defines this format. TF datasets may also define formats of the form nodetype -default where nodetype is a valid type of node in the dataset. These formats have a special meaning for Text-Fabric. Remember that the formats are defined in the otext warp config feature of your set, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats. They do not have to complied with the above-mentioned scheme. T.formats 1 T . formats Description Show the text representation formats that have been defined in your dataset. T.text() 1 2 T . text ( nodes , fmt = None ) T . text ( node , fmt = None , descend = False ) Description Gives the text that corresponds to a bunch of nodes. nodes nodes can be an arbitrary iterable of nodes. No attempt will be made to sort the nodes. If you need order, it is better to order the nodes before you feed them to T.text() . fmt The format of text-representation is given with fmt , with default text-orig-full . If the fmt cannot be found, the default is taken. The default is text-orig-full if the first argument is an iterable of nodes . The default is nodetype -default if the first argument is a single node with type nodetype . If the default format is not defined in the otext feature of the dataset, what will happen is dependent on the first argument, nodes or node . If the argument is an iterable of nodes , for each node its node type and node number will be output, connected with a _ . If the argument is a single node , Text-Fabric will look up de slot nodes contained in it (by means of L.d(node, otype=slotType) and format them with text-orig-full . If the format is defined, and the first argument is a single node , Text-Fabric will apply that format to the node itself, not to the slots contained in that node. But you can override that by means of descend=True . In that case, regardless of the format, the node will be replaced by the slot nodes contained in it, before applying the format. The default is sensible Consider the simplest call to this function: T.text(node) . This will apply the default format to node . If node is non-slot, then in most cases the default format will be applied to the slots contained in node . But for special node types, where the best representation is not obtained by descending down to the contained slot nodes, the dataset may define special default types that use other features to furnish a decent representation. BHSA In the BHSA case this happens for the type of lexemes: lex . Lexemes contain their occurrences as slots, but the representation of a lexeme is not the string of its occurrences, but resides in a feature such as voc_lex_utf8 (vocalized lexeme in Unicode). The BHSA dataset defines the format lex-default={voc_lex_utf8} and this is the only thing needed to regulate the representation of a lexeme. Hence, T.text(lx) results in the vocalized lexeme representation of lx . But if you really want to print out all occurrences of lexeme lx , you can say T.text(lx, descend=True) . Beware of this, however: T.text(phr) prints the full text of phr if phr is a phrase node. T.text(phr, fmt='text-orig-full') prints the empty string. Why? T.text(phr, fmt='text-orig-full', descend=True) prints the full text of the phrase. In the first case there is no fmt argument, so the default is taken, which would be phrase-default . But this format does not exists, so Text-Fabric descends to the words of the phrase and applies text-orig-full to them. In the second case, there is a fmt argument, so Text-Fabric applies that to the node . But text-orig-full uses features that have values on words, not on phrases. The third case is what you probably wanted, and this is what you need if you want to print the phrase in non-default formats: T.text(phr, fmt='text-phono-full', descend=True) No error messages This function does not give error messages, because that could easily overwhelm the output stream, especially in a notebook. Non slot nodes allowed In most cases, the nodes fed to T.text() are slots, and the formats are templates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features defined for a non-slot node type. If, for example, your slot type is glyph , and you want a format that renders lexemes, which are not defined for glyphs but for words, you can just define a format in terms of word features. It is your responsibility to take care to use the formats for node types for which they make sense. Escape whitespace in formats When defining formats in otext.tf , if you need a newline or tab in the format, specify it as \\n and \\t . Searching \u00b6 What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are interested in has to be written as a search template , offered to S.search() which returns the matching results as tuples of nodes. A search template expresses a pattern of nodes and edges with additional conditions also known as quantifiers . A result of a search template is a tuple of nodes that instantiate the nodes in the pattern, in such a way that the edges of the pattern are also instantiated between the nodes of the result tuple. Moreover, the quantifiers will hold for each result tuple. S The Search API is exposed as S or Search . Search templates \u00b6 Search primer A search template consists of a bunch of lines, possibly indented, that specify objects to look for. Here is a simple example: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural word pos=noun gender=feminine number=singular This template looks for word combinations within a sentence within chapter 2 of either Genesis or Exodus, where one of the words is a verb and the other is a noun. Both have a feminine inflection, but the verb is plural and the noun is singular. The indentation signifies embedding, i.e. containment. The two words are contained in the same sentence, the sentence is contained in the chapter, the chapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for this. The order of the two words is immaterial. If there are cases where the verb follows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words intervening between the noun and the verb, they will be included in the results. Speaking of results: the S.search() function returns its results as tuples of nodes: 1 ( book , chapter , sentence , word1 , word2 ) With these nodes in hand, you can programmatically gather all information about the results that the corpus provides. If the order between the verb and the noun is important, you can specify that as an additional constraint. You can give the words a name, and state a relational condition. Here we state that the noun precedes the verb. 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 sentence vb:word pos=verb gender=feminine number=plural nn:word pos=noun gender=feminine number=singular nn &lt; vb This can be written a bit more economically as: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural &gt; word pos=noun gender=feminine number=singular If you insist that the noun immediately precedes the verb, you can use a different relational operator: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular There are more kinds of relational operators. If the noun must be the first word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence w:word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural s =: w or a bit more economically: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence =: word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural If the verb must be the last word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular &lt;: w:word pos=verb gender=feminine number=plural s := w or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural := You can also use the edge features in the corpus as relational operators as well. Suppose we have an edge feature sub between clauses, such that if main clause m has subordinate clauses s1 , s2 and s3 , then 1 E.sub.f(m) = (s1, s2, s3) You can use this relation in search. Suppose we want to find the noun verb pair in subordinate clauses only. We can use this template: 1 2 3 4 5 6 7 book name=Genesis|Exodus chapter number=2 m:clause s:clause word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular m -sub&gt; s or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 clause -sub&gt; clause word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular Read m -sub> s as: there is a sub -arrow from m to s . Edge features may have values. For example, the crossref feature is a set of edges between parallel verses, with the levels of confidence as values. This number is an integer between 0 and 100. We can ask for parallel verses in an unqualified way: 1 2 verse -crossref&gt; verse But we can also ask for the cases with a specific confidence: 1 2 verse -crossref=90&gt; verse or cases with a high confidence: 1 2 verse -crossref&gt;95&gt; verse or cases with a low confidence: 1 2 verse -crossref&lt;80&gt; verse All feature conditions that you can assert on node features, you can also assert for edge features. If an edge feature is integer valued, such as crossref you can use comparisons; if it is string valued, you can use regular expressions. In both cases you can also use the other constructs, such as 1 2 verse -crossref=66|77&gt; verse To get a more specific introduction to search, consult the search tutorials for Hebrew , Peshitta , SyrNT , and Cuneiform . Finally an example with quantifiers. We want all clauses where Pred-phrases consist of verbs only: 1 2 3 4 5 6 7 8 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ Search template reference We have these kinds of lines in a template: comment lines if a line starts with # it is a comment line`; you cannot comment out parts of lines, only whole lines; if a line is empty or has whitespace only, it is a comment line; comment lines are allowed everywhere; comment lines are ignored. atom lines (simple): indent name:otype-or-set features Examples word pos=verb gender=feminine vb:word pos=verb gender=feminine vb pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space characters, where tabs count for just 1. Avoid tabs! . The name: part is optional. If present, it defines a name for this atom that can be used in relational comparisons and other atoms. The otype-or-set part is optional. If it is absent, the name part must be present. The meaning of 1 2 p:phrase sp=verb p vs=qal is identical to the meaning of 1 2 3 p:phrase sp=verb pnew:phrase vs=qal p = pnew (with relop): indent op name:otype-or-set features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom and this atom. The preceding atom may be the parent, provided we are at its first child, or it may the preceding sibling. You can leave out the name:otype-or-set features bit. In that case, the relation holds between the preceding atom and its parent. The name: part is optional. Exactly as in the case without relop. The otype-or-set part is optional. Exactly as in the case without relop. The otype-or-set is either a node type that exists in your TF data set, or it is the name of a set that you have passed in the sets parameter alongside the query itself when you call S.search() or S.study() . See feature specifications below for all full variety of feature constraints on nodes and edges. feature lines: features Indent is not significant. Continuation of feature constraints after a preceding atom line or other feature line. This way you can divide lengthy feature constraints over multiple lines. See feature specifications below for the full variety of feature constraints on nodes and edges. relation lines: name operator name s := w m -sub> s m <sub- s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values. See relational operators below for the whole spectrum of relational constraints on nodes. quantifier sub-templates: Atom lines that contain an otype or set may be followed by Quantifiers consist of search templates themselves, demarcated by some special keywords: /without/ /where/ and /have/ /with/ and /or/ /-/ See quantifiers below for all the syntax and semantics. Feature specifications \u00b6 Features specs The features above is a specification of what features with which values to search for. This specification must be written as a white-space separated list of feature specs . A feature spec has the form name valueSpec , with no space between the name and the valueSpec . The valueSpec may have the following forms and meanings: form evaluates to True the feature name ... has any value except None # has value None * has arbitrary value = values has one of the values specified # values has none of the values specified > value is greater than value < value is less than value ~ regular expression has a value and it matches regular expression Why * ? The operator * after a feature name does not pose any restriction at all. It will not influence the search results. Why would you want to include such a \"criterion\"? Some applications, such as the Text-Fabric browser collect the features used in a query to retrieve result information to be presented to the user. So if you want to include the values of a particular feature, mention that feature with a * . All these forms are also valid as - name form > and < name form - , in which case they specify value constraints on edge features. This is only meaningful if the edge feature is declared to have values (most edge features do not have values). Additional constraints There may be no space around the =#<>~ . name must be a feature name that exists in the dataset. If it references a feature that is not yet loaded, the feature will be loaded automatically. values must be a | separated list of feature values, no quotes. No spaces around the | . If you need a space or | or \\ in a value, escape it by a \\ . Escape tabs and newlines as \\t and \\n . When comparing values with < and > : value must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on string-valued features. regular expression must be a string that conforms to the Python regular axpression syntax If you need a space in your regular expression, you have to escape it with a \\ . You can do regular expressions only on string-valued features, not on number-valued features. Relational operators \u00b6 Operator lines Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the same slots are still unequal) # : is unequal (meaning a different node, a clause and a verse that occupy the same slots are still unequal) < > : before and after (in the canonical ordering ) Slot comparison == : occupy the same slots (identical slot sets) && : overlap (the intersection of both slot sets is not empty) ## : occupy different slots (but they may overlap, the set of slots of the two are different as sets) || : occupy disjoint slots (no slot occupied by the one is also occupied by the other) [[ ]] : embeds and contains (slot set inclusion, in both directions) << >> : before and after (with respect to the slots occupied: left ends before right starts and vice versa) <: :> : adjacent before and after (with respect to the slots occupied: left ends immediately before right starts and vice versa) =: left and right start at the same slot := left and right end at the same slot :: left and right start and end at the same slot Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring that one slot is equal to an other slot, you can require that they are k-near , i.e. they are at most k apart. Here are the relationships where you can do that. Instead of typing the letter k , provide the actual number you want. <k: :k> : k - adjacent before and after (with respect to the slots occupied: left ends k -near where right starts and vice versa) =k: left and right start at k -near slots :k= left and right end at k -near slots :k: left and right start and end at k -near slots Based on edge features - name > < name - : connected by the edge feature name in both directions; these forms work for edges that do and do not have values; - name valueSpec > < name valueSpec - : connected by the edge feature name in both directions; these forms work only for edges that do have values. Quantifiers \u00b6 Quantifiers Experimental This part of search templates is still experimental. bugs may be discovered the syntax of quantifiers may change What is a quantifier? Quantifiers are powerful expressions in templates. They state conditions on a given atom in your template. The atom in question is called the parent atom. The conditions may involve many nodes that are related to the parent, as in: all embedded words are a verb ; without a following predicate phrase ; with a mother clause or a mother phrase . That is where the term quantifier comes from. A quantifier quantifies its parent atom. /without/ Syntax: 1 2 3 4 atom /without/ templateN /-/ Meaning: node r is a result of this template if and only if r is a result of atom and there is no tuple RN such that ( r , RN ) is a result of 1 2 atom templateN /where/ Syntax: 1 2 3 4 5 6 atom /where/ templateA /have/ templateH /-/ Meaning: node r is a result of this template if and only if r is a result of atom and for all tuples ( RA ) such that ( r , RA ) is a result of 1 2 atom templateA there is a tuple RH such that ( r , RA , RH ) is a result of 1 2 3 atom templateA templateH /with/ Syntax: 1 2 3 4 5 6 7 8 atom /with/ templateO1 /or/ templateO2 /or/ templateO3 /-/ Meaning: node r is a result of this template if and only if: there is a tuple R1 such that ( r , R1 ) is a result of 1 2 atom templateO1 or there is a tuple R2 such that ( r , R2 ) is a result of 1 2 atom templateO2 or there is a tuple R3 such that ( r , R3 ) is a result of 1 2 atom templateO3 1 or more alternatives This quantifier can be used with any number of /or/ keywords, including none at all. If there is no /or/ , there is just one alternative. The only difference between 1 2 3 4 atom /with/ template /-/ and 1 2 atom template is that the results of the first query contain tuples with only one element, corresponding to the atom . The second query contains tuples of which the first element corresponds to the atom , and the remaining members correspond to the template . Parent The atom bit is an atom line, it acts as the parent of the quantifier. Inside a quantifier, you may refer to the parent by the special name .. . So you do not have to give a name to the parent. Multiple quantifiers You may have multiple quantifiers for one parent. Not in result tuples Whereas a the search for a normal template proceeds by finding a tuples that instantiates all it nodes in such a way that all relationships expressed in the template hold, a quantifier template is not instantiated. It asserts a condition that has to be tested for all nodes relative its parent. None of the atoms in a template of a quantifier corresponds to a node in a final result tuple. May be nested Templates within a quantifier may contain other quantifiers. The idea is, that whenever a search template is evaluated, quantifiers at the outer level of get interpreted. This interpretation gives rise to one or more templates to be constructed and run. Those new templates have been stripped of the outer layer of quantifiers, and when these templates are executed, the quantifiers at the next level have become outer. And so on. Restrictions Due to the implementation of quantifiers there are certain restrictions. Quantifiers must be put immediately below their parents or below preceding quantifiers of the same parent. The keywords of a quantifier must appear on lines with exactly the same indentation as the atom they quantify. The templates of a quantifier must have equal or greater indent than its keywords; The names accessible to the templates inside a quantifier are: the name .. , which is the name of the atom that is quantified; this name is automagically valid in quantifier templates; the name of the atom that is quantified (if that atom has a given name); names defined in the template itself; in /where/ , templateH may use names defined in templateA ; but only if these names are defined outside any quantifier of templateA . The following situations block the visibility of names: in /with/ , templateO i may not use names defined in templateO j for j other than i ; names defined outer quantifiers are not accessible in inner quantifiers; names defined inner quantifiers are not accessible in outer quantifiers. When you nest quantifiers, think of the way they will be recomposed into ordinary templates. This dictates whether your quantifier is syntactically valid or not. Indentation The indentation in quantifiers relative to their parent atom will be preserved. Nested quantifiers Consider 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj The auxiliary templates that will be run are: For the outer quantifier: 1 2 clause phrase function=Pred and 1 2 3 4 5 clause phrase function=Pred /without/ word sp#verb /-/ For the inner quantifier: 1 2 phrase function=Pred word sp#verb Note that the auxiliary template for the inner quantifier is shifted in its entirety to the left, but that the relative indentation is exactly as it shows in the original template. Implementation Here is a description of the implementation of the quantifiers. It is not the real implementation, but it makes clear what is going on, and why the quantifiers have certain limitations, and how indentation works. The basic idea is: a quantifier leads to the execution of one or more separate searche templates; the results of these searches are combined by means of set operations: difference , intersection , union , dependent on the nature of the quantifier; the end result of this combination will fed as a custom set to the original template after stripping the whole quantifier from that template. So we replace a quantifier by a custom set. Suppose we have 1 2 3 4 5 6 clause typ=Wx0 QUANTIFIER1 QUANTIFIER2 ... QUANTIFIERn rest-of-template We compute a set of clauses filteredClauses1 based on 1 2 clause typ=Wx0 QUANTIFIER1 and then compute a new set filteredClauses2 based on 1 2 3 4 5 S.search(''' fclause typ=Wx0 QUANTIFIER2 ''', customSets=dict(fclause=filteredClauses1) and so on until we have had QUANTIFIERn, leading to a set filteredClausesN of clauses that pass all filters set by the quantifiers. Finally, we deliver the results of 1 2 3 4 5 S.search(''' fclause rest-of-template ''', customSets=dict(fclause=filteredClausesN) Search API \u00b6 S.relationsLegend() 1 S . relationsLegend () Description Gives dynamic help about the basic relations that you can use in your search template. It includes the edge features that are available in your dataset. S.search() 1 S . search ( query , limit = None , shallow = False , sets = None , withContext = None ) Description Searches for combinations of nodes that together match a search template. This method returns a generator which yields the results one by one. One result is a tuple of nodes, where each node corresponds to an atom -line in your search template . query The query is a search template, i.e. a string that conforms to the rules described above. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. limit If limit is a number, it will fetch only that many results. TF as Database By means of S.search(query) you can use one TF instance as a database that multiple clients can use without the need for each client to call the costly load methods. You have to come up with a process that runs TF, has all features loaded, and that can respond to queries from other processes. We call such a process a TF kernel . Webservers can use such a daemonized TF to build efficient controllers. A TF kernel and webserver are included in the Text-Fabric code base. See kernel and web . Generator versus tuple If limit is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and translated into a search plan. The following methods expose parts of the search process, and may provide you with useful information in case the search does not deliver what you expect. see the plan the method S.showPlan() below shows you at a glance the correspondence between the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate , strategy = None , silent = False , shallow = False , sets = None ) Description Your search template will be checked, studied, the search space will be narrowed down, and a plan for retrieving the results will be set up. If your query has quantifiers, the asscociated search templates will be constructed and executed. These searches will be reported clearly. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched matters a lot. The search strategy is an implementation detail, but we bring it to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x') and you will get a list of options reported to choose from. Feel free to experiment. To see what the strategies do, see the code . silent If you want to suppress most of the output, say silent=True . shallow, sets As in S.search() . S.showPlan() 1 S . showPlan ( details = False ) Description Search results are tuples of nodes and the plan shows which part of the tuple corresponds to which part of the search template. details If you say details=True , you also get an overview of the search space and a description of how the results will be retrieved. after S.study() This function is only meaningful after a call to S.study() . Search results \u00b6 Preparation versus result fetching The method S.search() above combines the interpretation of a given template, the setting up of a plan, the constraining of the search space and the fetching of results. Here are a few methods that do actual result fetching. They must be called after a previous S.search() or S.study() . S.count() 1 S . count ( progress = None , limit = None ) Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message. The frequency is progress results, default every 100. limit Fetch results up to a given limit , default 1000. Setting limit to 0 or a negative value means no limit: all results will be counted. why needed You typically need this in cases where result fetching turns out to be (very) slow. generator versus list len(S.results()) does not work, because S.results() is a generator that delivers its results as they come. S.fetch() 1 S . fetch ( limit = None ) Description Finally, you can retrieve the results. The result of fetch() is not a list of all results, but a generator . It will retrieve results as long as they are requested and their are still results. limit Tries to get that many results and collects them in a tuple. So if limit is not None , the result is a tuple with a known length. Iterating over the fetch() generator You typically fetch results by saying: 1 2 3 4 i = 0 for r in S . results (): do_something ( r [ 0 ]) do_something_else ( r [ 1 ]) Alternatively, you can set the limit parameter, to ask for just so many results. They will be fetched, and when they are all collected, returned as a tuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 ) gives you the first bunch of results quickly. S.glean() 1 S . glean ( r ) Description A search result is just a tuple of nodes that correspond to your template, as indicated by showPlan() . Nodes give you access to all information that the corpus has about it. The glean() function is here to just give you a first impression quickly. r Pass a raw result tuple r , and you get a string indicating where it occurs, in terms of sections, and what text is associated with the results. Inspecting results 1 2 for result in S . fetch ( limit = 10 ): print ( S . glean ( result )) is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been obtained by search or not. More ways of showing results If you work in one of the corpora for which the TF-API has been extended, you will be provided with more powerful methods show() and table() to display your results. See Cunei , Bhsa , Peshitta and Syrnt . Features \u00b6 Features TF can give you information of all features it has encountered. TF.featureSets 1 TF . featureSets Description Returns a dictionary with keys nodes , edges , configs , computeds . Under each key there is the set of feature names in that category. So you can easily test whether a node feature or edge feature is present in the dataset you are working with. configs These are config features, with metadata only, no data. E.g. otext . computeds These are blocks of precomputed data, available under the C. API, see below. May be unloaded The sets do not indicate whether a feature is loaded or not. There are other functions that give you the loaded node features ( Fall() ) and the loaded edge features ( Eall() ). Node features \u00b6 F The node features API is exposed as F ( Fs ) or Feature ( FeatureString ). Fall() aka AllFeatures() 1 2 Fall () AllFeatures () Description Returns a sorted list of all usable, loaded node feature names. F. feature aka Feature. feature 1 2 F . part_of_speech Feature . part_of_speech Description Returns a sub-api for retrieving data that is stored in node features. In this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Fs instead. Fs(feature) aka FeatureString(feature) 1 2 3 4 Fs ( feature ) FeatureString ( feature ) Fs ( 'part-of-speech' ) FeatureString ( 'part-of-speech' ) Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called part-of-speech . Note that this is not a valid name in Python, yet we can work with features with such names. Both methods have identical results Suppose we have just issued feature = 'pos'. Then the result of Fs(feature) and F.pos` is identical. In most cases F works just fine, but Fs is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .v(node) 1 F . part_of_speech . v ( node ) Description Get the value of a feature , such as part_of_speech for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1 2 F . part_of_speech . s ( value ) F . part_of_speech . s ( 'noun' ) Description Returns a generator of all nodes in the canonical order with a given value for a given feature. This is an other way to walk through nodes than using N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F. feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None ) Description Inspect the values of feature (in this example: part_of_speech ) and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those types will be counted. F.otype otype is a special node feature and has additional capabilities. Description F.otype.slotType is the node type that can fill the slots (usually: word ) F.otype.maxSlot is the largest slot number F.otype.maxNode is the largest node number F.otype.all is a list of all otypes from big to small (from books through clauses to words) F.otype.sInterval(otype) is like F.otype.s(otype) , but instead of returning you a range to iterate over, it will give you the starting and ending nodes of otype . This makes use of the fact that the data is so organized that all node types have single ranges of nodes as members. Edge features \u00b6 E The edge features API is exposed as E ( Es ) or Edge ( EdgeString ). Eall() aka AllEdges() 1 2 Eall () AllEdges () Description Returns a sorted list of all usable, loaded edge feature names. E. feature aka Edge. feature 1 2 E . head Feature . head Description Returns a sub-api for retrieving data that is stored in edge features. In this example, we assume there is a feature called head . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Es instead. Es(feature) aka EdgeString(feature) 1 2 3 4 Es ( feature ) EdgeString ( feature ) Es ( 'head' ) EdgeString ( 'head' ) Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called head . Both methods have identical results Suppose we have just issued feature = 'head'. Then the result of Es(feature) and E.pos` is identical. In most cases E works just fine, but Es is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .f(node) 1 E . head . f ( node ) Description Get the nodes reached by feature -edges from a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than None . node The node from which the edges in question start. E. feature .t(node) 1 E . head . t ( node ) Description Get the nodes reached by feature -edges to a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if feature describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges to n , the empty tuple is returned, rather than None . node The node to which the edges in question go. E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None , nodeTypesTo = None ) Description If the edge feature has no values, simply return the number of node pairs between an edge of this kind exists. If the edge feature does have values, we inspect them and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypesFrom If not None , only the values for edges that start from a node with type within nodeTypesFrom will be counted. nodeTypesTo If not None , only the values for edges that go to a node with type within nodeTypesTo will be counted. E.oslots oslots is a special edge feature and is mainly used to construct other parts of the API. It has less capabilities, and you will rarely need it. It does not have .f and .t methods, but an .s method instead. Description E.oslots.s(node) Gives the sorted list of slot numbers linked to a node, or put otherwise: the slots that support that node. node The node whose slots are being delivered. Messaging \u00b6 Timed messages Error and informational messages can be issued, with a time indication. info(), error() 1 info ( msg , tm = True , nl = True ) Description Sends a message to standard output, possibly with time and newline. if info() is being used, the message is sent to stdout ; if error() is being used, the message is sent to stderr ; In a Jupyter notebook, the standard error is displayed with a reddish background colour. tm If True , an indicator of the elapsed time will be prepended to the message. nl If True a newline will be appended. indent() 1 indent ( level = None , reset = False ) Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer. Subsequent info() and error() will display their messages with this indent. reset If True , the elapsed time to will be reset to 0 at the given level. Timers at different levels are independent of each other. Saving features \u00b6 TF.save() 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Description If you have collected feature data in dictionaries, keyed by the names of the features, and valued by their feature data, then you can save that data to .tf feature files on disk. It is this easy to export new data as features: collect the data and metadata of the features and feed it in an orderly way to TF.save() and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and strings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or dictionaries as values. These sets should be sets of nodes (integers!), and these dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from metaData , which is a dictionary mapping a feature name to its metadata. value types The type of the values should conform to @valueType ( int or str ), which must be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have to say so, by specifying edgeValues = True in the metadata for that feature. generic metadata metaData may also contain fields under the empty name. These fields will be added to all features in nodeFeatures and edgeFeatures . config features If you need to write the config feature otext , which is a metadata-only feature, just add the metadata under key otext in metaData and make sure that otext is not a key in nodeFeatures nor in edgeFeatures . These fields will be written into the separate config feature otext , with no data associated. save location The (meta)data will be written to the very last module in the list of locations that you specified when calling Fabric() or to what you passed as module in the same location. If that module does not exist, it will be created in the last location . If both locations and modules are empty, writing will take place in the current directory. Clearing the cache \u00b6 TF.clearCache() 1 TF . clearCache () Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the original data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, where you might want to clear the cache of precomputed results. Calling this function just does it, and it is equivalent with manually removing all .tfx files inside the hidden .tf directory inside your dataset. No need to load It is not needed to execute a TF.load() first. MQL \u00b6 Data interchange with MQL You can interchange with MQL data. Text-Fabric can read and write MQL dumps. An MQL dump is a text file, like an SQL dump. It contains the instructions to create and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName , dirName ) Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to dirName/dbName.mql . If dirName starts with ~ , the ~ will be expanded to your home directory. Likewise, .. will be expanded to the parent of the current directory, and . to the current directory, both only at the start of dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the Text-Fabric dataset it comes from: the TF slots correspond exactly with the MQL monads and have the same numbers; provided the monad numbers in the MQL dump are consecutive. In MQL this is not obligatory. Even if there gaps in the monads sequence, we will fill the holes during conversion, so the slots are tightly consecutive; the TF nodes correspond exactly with the MQL objects and have the same numbers Node features in MQL The values of TF features are of two types, int and str , and they translate to corresponding MQL types integer and string . The actual values do not undergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature. Only if the feature is a number feature, you may omit the quotes: 1 2 [word sp='verb'] [verse chapter=1 and verse=1] Enumeration types It is attractive to use eumeration types for the values of a feature, whereever possible, because then you can query those features in MQL with IN and without quotes: 1 [chapter book IN (Genesis, Exodus)] We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an enumeration. So we restrict ourselves to node features with string values. We put the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter, followed by letters, digits, or _ . The letters can only be plain ASCII letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type. Currently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features, we collect all enumerated values for all those features into one big enumeration type. The reason is that MQL considers equal values in different types as distinct values. If we had separate types, we could never compare values for different features. Values of edge features are ignored There is no place for edge values in MQL. There is only one concept of feature in MQL: object features, which are node features. But TF edges without values can be seen as node features: nodes are mapped onto sets of nodes to which the edges go. And that notion is supported by MQL: edge features are translated into MQL features of type LIST OF id_d , i.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers (yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do that by replacing illegal characters by _ , and, if the result does not start with a letter, we prepend an x . We do not check whether the name is a reserved C word. With these provisos: the given dbName correspond to the MQL database name the TF otypes correspond to the MQL objects the TF features correspond to the MQL features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible). It can be compressed greatly, especially by the program bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or directory with the same name as the MQL database, your import will fail spectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your text-fabric-data directory, e.g. to ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1 2 cd ~/Downloads rm dataset ; mql -b 3 < dataset.mql TF.importMQL() 1 TF . importMQL ( mqlFile , slotType = None , otext = None , meta = None ) Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this importMQL on a TF instance called with 1 locations = targetDir , modules = '' Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type, because TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the otext.tf feature. Pass it as a dictionary of keys and values, like so: 1 2 3 4 otext = { 'fmt:text-trans-plain' : '{glyphs}{trailer}' , 'sectionFeatures' : 'book,chapter,verse' , } meta Likewise, you can add a dictionary of keys and values that will added to the metadata of all features. Handy to add provenance data here: 1 2 3 4 5 meta = dict ( dataset = 'DLC' , datasetName = 'Digital Language Corpus' , author = \"That 's me\" , ) Computed data \u00b6 Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in quick-load format. Most of this data are the features, but there is some extra data needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable, so we have made it accessible in the C -api, which we document here. Call() aka AllComputeds() 1 2 Call () AllComputeds () Description Returns a sorted list of all usable, loaded computed data names. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1 ( otype , averageSlots , minNode , maxNode ) where otype is the name of the node type, averageSlots the average size of objects in this type, measured in slots (usually words). minNode is the first node of this type, maxNode the last, and the nodes of this node type are exactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of that type usually occupy. The bigger the average object, the lower the levels. Books have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type phrase and above it a node type cluster , i.e. phrases are contained in clusters, but not vice versa. If all phrases are contained in clusters, and some clusters have more than one phrase, the automatic level ranking of node types works out well in this case. But if clusters only have very small phrases, and the big phrases do not occur in clusters, then the algorithm may assign a lower rank to clusters than to phrases. In general, it is too expensive to try to compute the levels in a sophisticated way. In order to remedy cases where the algorithm assigns wrong levels, you can add a @levels key to the otext config feature. See text . C.order.data Description An array of all nodes in the correct order. This is the order in which N() alias Node() traverses all nodes. Rationale To order all nodes in the canonical ordering is quite a bit of work, and we need this ordering all the time. C.rank.data Description An array of all indices of all nodes in the canonical order array. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know which position each node takes in the canonical order, in other words, at what index we find it in the C.order.data array. C.levUp.data and C.levDown.data Description These tables feed the L.d() and L.u() functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized. It is not advisable to use them directly, it is far better to use the L functions. Only when every bit of performance waste has to be squeezed out, this raw data might be a deal. C.boundary.data Description These tables feed the L.n() and L.p() functions. It is a tuple consisting of firstSlots and lastSlots . They are indexes for the first slot and last slot of nodes. Slot index For each slot, firstSlot gives all nodes (except slots) that start at that slot, and lastSlot gives all nodes (except slots) that end at that slot. Both firstSlot and lastSlot are tuples, and the information for node n can be found at position n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is chapter , and that of level 3 is verse . And suppose that we have features, named bookHeading , chapterHeading , and verseHeading that give the names or numbers of these. Custom names Note that the terms book , chapter , verse are not baked into Text-Fabric. It is the corpus data, especially the otext config feature that spells out the names of the sections. Then C.section.data is a tuple of two mappings , let us call them chapters and verses . chapters is a mapping, keyed by book nodes , and then by by chapter headings , giving the corresponding chapter node s as values. verses is a mapping, keyed by book nodes , and then by chapter headings , and then by verse headings , giving the corresponding verse node s as values. Supporting the T -Api The T -api is good in mapping nodes unto sections, such as books, chapters, verses and back. It knows how many chapters each book has, and how many verses each chapter. The T api is meant to make your life easier when you have to find passage labels by nodes or vice versa. That is why you probably never need to consult the underlying data. But you can! That data is stored in Miscellaneous \u00b6 TF.version Description Contains the version number of the Text-Fabric library. TF.banner Description Contains the name and the version of the Text-Fabric library.","title":"General"},{"location":"Api/General/#text-fabric-api","text":"Tutorial The tutorials for the Hebrew Bible , Peshitta , Syriac New Testament , and the Uruk Cuneiform Tablets put the Text-Fabric API on show for vastly different corpora. Generic API This is the API of Text-Fabric in general. Text-Fabric has no baked in knowledge of particular corpora. However, Text-Fabric comes with several additions that make working with specific corpora easier. Hebrew Bible: BHSA Peshitta: Peshitta Syriac New Testament: SyrNT Proto-cuneiform tablets from Uruk: Cunei","title":"Text-Fabric API"},{"location":"Api/General/#loading","text":"TF=Fabric() 1 2 from tf.fabric import Fabric TF = Fabric ( locations = directories , modules = subdirectories , silent = False ) Description Text-Fabric is initialized for a corpus. It will search a set of directories and catalog all .tf files it finds there. These are the features you can subsequently load. Here directories and subdirectories are strings with directory names separated by newlines, or iterables of directories. locations, modules The directories specified in locations will be searched for modules , which are paths that will be appended to the paths in locations . All .tf files (non-recursively) in any module will be added to the feature set to be loaded in this session. The order in modules is important, because if a feature occurs in multiple modules, the last one will be chosen. In this way you can easily override certain features in one module by features in an other module of your choice. otext@ in modules If modules contain features with a name starting with otext@ , then the format definitions in these features will be added to the format definitions in the regular otext feature (which is a WARP feature). In this way, modules that define new features for text representation, also can add new formats to the Text-API. Defaults The locations list has a few defaults: 1 2 3 ~/Downloads/text-fabric-data ~/text-fabric-data ~/github/text-fabric-data So if you have stored your main Text-Fabric dataset in text-fabric-data in one of these directories you do not have to pass a location to Fabric. The modules list defaults to [''] . So if you leave it out, Text-Fabric will just search the paths specified in locations . silent If silent=True is passed, banners and normal progress messages are suppressed. TF.explore() 1 2 features = TF . explore ( silent = False , show = True ) features or 1 2 TF . explore ( silent = False , show = False ) TF . featureSets Description This will give you a dictionary of all available features by kind. The kinds are: nodes , edges , configs , computeds . silent With silent=False a message containing the total numbers of features is issued. show The resulting dictionary is delivered in TF.featureSets , but if you say show=True , the dictionary is returned as function result. api=TF.load() 1 api = TF . load ( features , add = False , silent = False ) Description Reads the features indicated by features and loads them in memory ready to be used in the rest of the program. features features is a string containing space separated feature names, or an iterable of feature names. The feature names are just the names of .tf files without directory information and without extension. add If later on you want load more features, you can either: add the features to the original load() statement and just run it again make a new statement: TF.load(newfeatures, add=True) . The new features will be added to the same api, so you do not have to to call api.makeAvailableIn(globals()) again after this! silent The features will be loaded rather silently, most messages will be suppressed. Time consuming operations will always be announced, so that you know what Text-Fabric is doing. If silent=True is passed, all informational messages will be suppressed. This is handy I you want to load data as part of other methods, on-the-fly. api.makeAvailableIn(globals()) 1 api . makeAvailableIn ( globals ()) Description This method will export every member of the API (such as N , F , E , L , T , info ) to the global namespace. From now on, we will omit the api. in our documentation. Contents of the API After having loaded the features by api = TF.load(...) , the api harbours your Text-Fabric API. You can access node feature mydata by api.F.mydata.v(node) , edge feature mylink by api.E.mylink.f(node) , and so on. If you are working with a single data source in your program, it is a bit tedious to write the initial api. all the time. By this methodd you can avoid that. Longer names There are also longer names which can be used as aliases to the single capital letters. This might or might not improve the readability of your program. short name long name N Nodes F Feature Fs FeatureString Fall AllFeatures E Edge Es EdgeString Eall AllEdges C Computed Cs ComputedString Call AllComputeds L Locality T Text S Search ignored 1 api . ignored Description If you want to know which features were found but ignored (because the feature is also present in another, later, location), you can use this attribute to inspect the ignored features and their locations. loadLog() 1 api . loadlog () Description After loading you can view all messages using this method. It also shows the messages that have been suppressed due to silent=True .","title":"Loading"},{"location":"Api/General/#navigating-nodes","text":"N() 1 2 for n in N (): action Description The result of N() is a generator that walks through all nodes in the canonical order (see below). Iterating over N() delivers you all words and structural elements of your corpus in a very natural order. Walking nodes Most processing boils down to walking through the nodes by visiting node sets in a suitable order. Occasionally, during the walk you might want to visit embedding or embedded nodes to glean some feature information from them. More ways of walking Later, under Features there is another convenient way to walk through nodes. canonical order The canonical order is a way to sort the nodes in your corpus in such a way that you can enumerate all nodes in the order you encounter them if you walk through your corpus. Briefly this means: embedder nodes come before the nodes that lie embedded in them; earlier stuff comes before later stuff, if a verse coincides with a sentence, the verse comes before the sentence, because verses generally contain sentences and not the other way round; if two objects are intersecting, but none embeds the other, the one with the smallest slot that does not occur in the other, comes first. first things first, big things first That means, roughly, that you start with a book node (Genesis), then a chapter node (Genesis 1), then a verse node, Genesis 1:1, then a sentence node, then a clause node, a phrase node, and the first word node. Then follow all word nodes in the first phrase, then the phrase node of the second phrase, followed by the word nodes in that phrase. When ever you enter a higher structure, you will first get the node corresponding to that structure, and after that the nodes corresponding to the building blocks of that structure. This concept follows the intuition that slot sets with smaller elements come before slot set with bigger elements, and embedding slot sets come before embedded slot sets. Hence, if you enumerate a set of nodes that happens to constitute a tree hierarchy based on slot set embedding, and you enumerate those nodes in the slot set order, you will walk the tree in pre-order. This order is a modification of the one as described in (Doedens 1994, 3.6.3). Doedens, Crist-Jan (1994), Text Databases. One Database Model and Several Retrieval Languages , number 14 in Language and Computers, Editions Rodopi, Amsterdam, Netherlands and Atlanta, USA. ISBN: 90-5183-729-1, http://books.google.nl/books?id=9ggOBRz1dO4C . The order as defined by Doedens corresponds to walking trees in post-order. For a lot of processing, it is handy to have a the stack of embedding elements available when working with an element. That is the advantage of pre-order over post-order. It is very much like SAX parsing in the XML world. sortNodes() 1 sortNodes ( nodeSet ) Description delivers an iterable of nodes as a tuple sorted by the canonical ordering . nodeSet An iterable of nodes to be sorted. sortKey 1 nodeList = sorted ( nodes , key = sortKey ) Description A function that provides for each node the key to be used to sort nodes in the canonical ordering. That means that the following two pieces of code do the same thing: sortNodes(nodeSet) and sorted(nodeSet, key=sortKey) . Sorting tuples of nodes Handy to sort things that are not nodes themselves, but data structures with nodes in it, e.g. search results: if results is a list of tuples of nodes, we could sort them in canonical order like this: 1 sorted ( nodeSet , key = lambda r : sortKey ( r [ 0 ])) otypeRank 1 otypeRank [ 'sentence' ] The node types are ordered in C.levels.data , and if you reverse that list, you get the rank of a type by looking at the position in which that type occurs. The slotType has otypeRank 0, and the more comprehensive a type is, the higher its rank.","title":"Navigating nodes"},{"location":"Api/General/#locality","text":"Local navigation Here are the methods by which you can navigate easily from a node to its neighbours: parents and children, previous and next siblings. L The Locality API is exposed as L or Locality . otype parameter In all of the following L -functions, if the otype parameter is passed, the result is filtered and only nodes with otype=nodeType are retained. Results of the L. functions are tuples, not single nodes Even if an L -function returns a single node, it is packed in a tuple . So to get the node itself, you have to dereference the tuple: 1 L . u ( node )[ 0 ] L.u() 1 L . u ( node , otype = nodeType ) Description Produces an ordered tuple of nodes upward , i.e. embedder nodes. node The node whose embedder nodes will be delivered. The result never includes node itself. L.d() 1 L . d ( node , otype = nodeType ) Description Produces an ordered tuple of nodes downward , i.e. embedded nodes. node The node whose embedded nodes will be delivered. The result never includes node itself. L.n() 1 L . n ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent next nodes. node The node whose right adjacent nodes will be delivered; i.e. the nodes whose first slot immediately follow the last slot of node . The result never includes node itself. L.p() 1 L . p ( node , otype = nodeType ) Description Produces an ordered tuple of adjacent previous nodes from node , i.e. nodes whose last slot just precedes the first slot of node . Description Produces an ordered tuple of adjacent previous nodes. node The node whose lefy adjacent nodes will be delivered; i.e. the nodes whose last slot immediately precede the first slot of node . Locality and levels Here is something that is very important to be aware of when using sortNodes and the L.d(n) and L.u(n) methods. When we order nodes and report on which nodes embed which other nodes, we do not only take into account the sets of slots the nodes occupy, but also their level . See levels and text . Both the L.d(n) and L.u(n) work as follows: L.d(n) returns nodes such that embedding nodes come before embedded nodes words) L.u(n) returns nodes such that embedded nodes come before embedding nodes books) N.B.: Suppose you have node types verse and sentence , and usually a verse has multiple sentences, but not vice versa. Then you expect that L.d(verseNode) will contain sentence nodes, L.d(sentenceNode) will not contain verse nodes. But if there is a verse with exactly one sentence, and both have exactly the same words, then that is a case where: L.d(verseNode) will contain one sentence node, L.d(sentenceNode) will contain one verse node.","title":"Locality"},{"location":"Api/General/#text","text":"Overview Here are the functions that enable you to get the actual text in the dataset. There are several things to accomplish here, such as getting text given book, chapter, and verse; given a node, produce the book, chapter and verse indicators in which the node is contained; handle multilingual book names; switch between various text representations. The details of the Text API are dependent on the warp feature otext , which is a config feature. T The Text API is exposed as T or Text .","title":"Text"},{"location":"Api/General/#sections","text":"Section levels In otext the main section levels (usually book , chapter , verse ) can be defined. It loads the features it needs (so you do not have to specify those features, unless you want to use them via F ). And finally, it makes some functions available by which you can make handy use of that information. Section levels are generic In this documentation, we call the main section level book , the second level chapter , and the third level verse . Text-Fabric, however, is completely agnostic about how these levels are called. It is prepared to distinguish three section levels, but how they are called, must be configured in the dataset. The task of the otext feature is to declare which node type and feature correspond with which section level. Text-Fabric assumes that the first section level may have multilingual headings, but that section levels two and three have single language headings (numbers of some kind). String versus number Chapter and verse numbers will be considered to be strings or integers, depending on whether your dataset has declared the corresponding feature valueType as str or as int . Conceivably, other works might have chapter and verse numbers like XIV , '3A', '4.5', and in those cases these numbers are obviously not int s. otext is optional If otext is missing, the Text API will not be build. If it exists, but does not specify sections, that part of the Text API will not be built. Likewise for text representations. levels of node types Usually, Text-Fabric computes the hierarchy of node types correctly, in the sense that node types that act as containers have a lower level than node types that act as containees. So books have the lowest level, words the highest. See levels . However, if this level assignment turns out to be wrong for your dataset, you can configure the right order in the otext feature, by means of a key levels with value a comma separated list of levels. Example: 1 @levels=tablet,face,column,line,case,cluster,quad,comment,sign T.sectionFromNode() 1 T . sectionFromNode ( node , lastSlot = False , lang = 'en' ) Description Returns the book/chapter/verse indications that correspond to the reference node, which is the first or last slot belonging n , dependent on lastSlot . The result is a tuple, consisting of the book name (in language lang ), the chapter number, and the verse number. node The node from which we obtain a section specification. lastSlot Whether the reference node will be the last slot contained by the node argument or the first node. lang The language to be used for the section parts, as far as they are language dependent. crossing verse boundaries Sometimes a sentence or clause in a verse continue into the next verse. In those cases, this function will return a different results for lastSlot=False and lastSlot=True . nodes outside sections Nodes that lie outside any book, chapter, verse will get a None in the corresponding members of the returned tuple. T.nodeFromSection() 1 T . nodeFromSection ( section , lang = 'en' ) Description Given a section tuple, return the node of it. section section consists of a book name (in language lang ), and a chapter number and a verse number (both as strings or number depending on the value type of the corresponding feature). The verse number may be left out, the result is then a chapter node. Both verse and chapter numbers may be left out, the result is then a book node. If all three are present, de result is a verse node. lang The language assumed for the section parts, as far as they are language dependent.","title":"Sections"},{"location":"Api/General/#book-names-and-languages","text":"Book names and nodes The names of the books may be available in multiple languages. The book names are stored in node features with names of the form book@ la , where la is the ISO 639 two-letter code for that language. Text-Fabric will always load these features. T.languages 1 T . languages Description A dictionary of the languages that are available for book names. T.bookName() 1 T . bookName ( node , lang = 'en' ) Description gives the name of the book in which a node occurs. node The node in question. lang The lang parameter is a two letter language code. The default is en (English). If there is no feature data for the language chosen, the value of the ordinary book feature of the dataset will be returned. Works for all nodes n may or may not be a book node. If not, bookName() retrieves the embedding book node first. T.bookNode() 1 T . bookNode ( name , lang = 'en' ) Description gives the node of the book identified by its name name The name of the book. lang The language in which the book name is supplied by the name parameter. If lang can not be found, the value of the ordinary book feature of the dataset will be used. If name cannot be found in the specified language, None will be returned. Function name follows configured section level If your dataset has configured section level one under an other name, say tablet , then these two methods follow that name. Instead of T.bookName() and T.bookNode() we have then T.tabletName() and T.tabletNode() .","title":"Book names and languages"},{"location":"Api/General/#text-representation","text":"Text formats Text can be represented in multiple ways. We provide a number of formats with structured names. A format name is a string of keywords separated by - : what - how - fullness - modifier For Hebrew any combination of the follwoing could be useful formats: keyword value meaning what text words as they belong to the text what lex lexemes of the words how orig in the original script (Hebrew, Greek, Syriac) (all Unicode) how trans in (latin) transliteration how phono in phonetic/phonological transcription fullness full complete with accents and all diacritical marks fullness plain with accents and diacritical marks stripped, in Hebrew only the consonants are left modifier ketiv (Hebrew): where there is ketiv/qere, follow ketiv instead of qere (default); The default format is text-orig-full , we assume that every TF dataset defines this format. TF datasets may also define formats of the form nodetype -default where nodetype is a valid type of node in the dataset. These formats have a special meaning for Text-Fabric. Remember that the formats are defined in the otext warp config feature of your set, not by Text-Fabric. Freedom of names for formats There is complete freedom of choosing names for text formats. They do not have to complied with the above-mentioned scheme. T.formats 1 T . formats Description Show the text representation formats that have been defined in your dataset. T.text() 1 2 T . text ( nodes , fmt = None ) T . text ( node , fmt = None , descend = False ) Description Gives the text that corresponds to a bunch of nodes. nodes nodes can be an arbitrary iterable of nodes. No attempt will be made to sort the nodes. If you need order, it is better to order the nodes before you feed them to T.text() . fmt The format of text-representation is given with fmt , with default text-orig-full . If the fmt cannot be found, the default is taken. The default is text-orig-full if the first argument is an iterable of nodes . The default is nodetype -default if the first argument is a single node with type nodetype . If the default format is not defined in the otext feature of the dataset, what will happen is dependent on the first argument, nodes or node . If the argument is an iterable of nodes , for each node its node type and node number will be output, connected with a _ . If the argument is a single node , Text-Fabric will look up de slot nodes contained in it (by means of L.d(node, otype=slotType) and format them with text-orig-full . If the format is defined, and the first argument is a single node , Text-Fabric will apply that format to the node itself, not to the slots contained in that node. But you can override that by means of descend=True . In that case, regardless of the format, the node will be replaced by the slot nodes contained in it, before applying the format. The default is sensible Consider the simplest call to this function: T.text(node) . This will apply the default format to node . If node is non-slot, then in most cases the default format will be applied to the slots contained in node . But for special node types, where the best representation is not obtained by descending down to the contained slot nodes, the dataset may define special default types that use other features to furnish a decent representation. BHSA In the BHSA case this happens for the type of lexemes: lex . Lexemes contain their occurrences as slots, but the representation of a lexeme is not the string of its occurrences, but resides in a feature such as voc_lex_utf8 (vocalized lexeme in Unicode). The BHSA dataset defines the format lex-default={voc_lex_utf8} and this is the only thing needed to regulate the representation of a lexeme. Hence, T.text(lx) results in the vocalized lexeme representation of lx . But if you really want to print out all occurrences of lexeme lx , you can say T.text(lx, descend=True) . Beware of this, however: T.text(phr) prints the full text of phr if phr is a phrase node. T.text(phr, fmt='text-orig-full') prints the empty string. Why? T.text(phr, fmt='text-orig-full', descend=True) prints the full text of the phrase. In the first case there is no fmt argument, so the default is taken, which would be phrase-default . But this format does not exists, so Text-Fabric descends to the words of the phrase and applies text-orig-full to them. In the second case, there is a fmt argument, so Text-Fabric applies that to the node . But text-orig-full uses features that have values on words, not on phrases. The third case is what you probably wanted, and this is what you need if you want to print the phrase in non-default formats: T.text(phr, fmt='text-phono-full', descend=True) No error messages This function does not give error messages, because that could easily overwhelm the output stream, especially in a notebook. Non slot nodes allowed In most cases, the nodes fed to T.text() are slots, and the formats are templates that use features that are defined for slots. But nothing prevents you to define a format for non-slot nodes, and use features defined for a non-slot node type. If, for example, your slot type is glyph , and you want a format that renders lexemes, which are not defined for glyphs but for words, you can just define a format in terms of word features. It is your responsibility to take care to use the formats for node types for which they make sense. Escape whitespace in formats When defining formats in otext.tf , if you need a newline or tab in the format, specify it as \\n and \\t .","title":"Text representation"},{"location":"Api/General/#searching","text":"What is Text-Fabric Search? You can query for graph like structures in your data set. The structure you are interested in has to be written as a search template , offered to S.search() which returns the matching results as tuples of nodes. A search template expresses a pattern of nodes and edges with additional conditions also known as quantifiers . A result of a search template is a tuple of nodes that instantiate the nodes in the pattern, in such a way that the edges of the pattern are also instantiated between the nodes of the result tuple. Moreover, the quantifiers will hold for each result tuple. S The Search API is exposed as S or Search .","title":"Searching"},{"location":"Api/General/#search-templates","text":"Search primer A search template consists of a bunch of lines, possibly indented, that specify objects to look for. Here is a simple example: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural word pos=noun gender=feminine number=singular This template looks for word combinations within a sentence within chapter 2 of either Genesis or Exodus, where one of the words is a verb and the other is a noun. Both have a feminine inflection, but the verb is plural and the noun is singular. The indentation signifies embedding, i.e. containment. The two words are contained in the same sentence, the sentence is contained in the chapter, the chapter in the book. The conditions you specify on book, chapter, word are all conditions in terms of node features . You can use all features in the corpus for this. The order of the two words is immaterial. If there are cases where the verb follows the noun, they will be included in the results. Also, the words do not have to be adjacent. If there are cases with words intervening between the noun and the verb, they will be included in the results. Speaking of results: the S.search() function returns its results as tuples of nodes: 1 ( book , chapter , sentence , word1 , word2 ) With these nodes in hand, you can programmatically gather all information about the results that the corpus provides. If the order between the verb and the noun is important, you can specify that as an additional constraint. You can give the words a name, and state a relational condition. Here we state that the noun precedes the verb. 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 sentence vb:word pos=verb gender=feminine number=plural nn:word pos=noun gender=feminine number=singular nn &lt; vb This can be written a bit more economically as: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural &gt; word pos=noun gender=feminine number=singular If you insist that the noun immediately precedes the verb, you can use a different relational operator: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular There are more kinds of relational operators. If the noun must be the first word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence w:word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural s =: w or a bit more economically: 1 2 3 4 5 book name=Genesis|Exodus chapter number=2 sentence =: word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural If the verb must be the last word in the sentence, you can specify it as 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular &lt;: w:word pos=verb gender=feminine number=plural s := w or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 s:sentence word pos=noun gender=feminine number=singular &lt;: word pos=verb gender=feminine number=plural := You can also use the edge features in the corpus as relational operators as well. Suppose we have an edge feature sub between clauses, such that if main clause m has subordinate clauses s1 , s2 and s3 , then 1 E.sub.f(m) = (s1, s2, s3) You can use this relation in search. Suppose we want to find the noun verb pair in subordinate clauses only. We can use this template: 1 2 3 4 5 6 7 book name=Genesis|Exodus chapter number=2 m:clause s:clause word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular m -sub&gt; s or a bit more economically: 1 2 3 4 5 6 book name=Genesis|Exodus chapter number=2 clause -sub&gt; clause word pos=verb gender=feminine number=plural :&gt; word pos=noun gender=feminine number=singular Read m -sub> s as: there is a sub -arrow from m to s . Edge features may have values. For example, the crossref feature is a set of edges between parallel verses, with the levels of confidence as values. This number is an integer between 0 and 100. We can ask for parallel verses in an unqualified way: 1 2 verse -crossref&gt; verse But we can also ask for the cases with a specific confidence: 1 2 verse -crossref=90&gt; verse or cases with a high confidence: 1 2 verse -crossref&gt;95&gt; verse or cases with a low confidence: 1 2 verse -crossref&lt;80&gt; verse All feature conditions that you can assert on node features, you can also assert for edge features. If an edge feature is integer valued, such as crossref you can use comparisons; if it is string valued, you can use regular expressions. In both cases you can also use the other constructs, such as 1 2 verse -crossref=66|77&gt; verse To get a more specific introduction to search, consult the search tutorials for Hebrew , Peshitta , SyrNT , and Cuneiform . Finally an example with quantifiers. We want all clauses where Pred-phrases consist of verbs only: 1 2 3 4 5 6 7 8 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ Search template reference We have these kinds of lines in a template: comment lines if a line starts with # it is a comment line`; you cannot comment out parts of lines, only whole lines; if a line is empty or has whitespace only, it is a comment line; comment lines are allowed everywhere; comment lines are ignored. atom lines (simple): indent name:otype-or-set features Examples word pos=verb gender=feminine vb:word pos=verb gender=feminine vb pos=verb gender=feminine The indent is significant. Indent is counted as the number of white space characters, where tabs count for just 1. Avoid tabs! . The name: part is optional. If present, it defines a name for this atom that can be used in relational comparisons and other atoms. The otype-or-set part is optional. If it is absent, the name part must be present. The meaning of 1 2 p:phrase sp=verb p vs=qal is identical to the meaning of 1 2 3 p:phrase sp=verb pnew:phrase vs=qal p = pnew (with relop): indent op name:otype-or-set features <: word pos=verb gender=feminine The relation operator specifies an extra constraint between a preceding atom and this atom. The preceding atom may be the parent, provided we are at its first child, or it may the preceding sibling. You can leave out the name:otype-or-set features bit. In that case, the relation holds between the preceding atom and its parent. The name: part is optional. Exactly as in the case without relop. The otype-or-set part is optional. Exactly as in the case without relop. The otype-or-set is either a node type that exists in your TF data set, or it is the name of a set that you have passed in the sets parameter alongside the query itself when you call S.search() or S.study() . See feature specifications below for all full variety of feature constraints on nodes and edges. feature lines: features Indent is not significant. Continuation of feature constraints after a preceding atom line or other feature line. This way you can divide lengthy feature constraints over multiple lines. See feature specifications below for the full variety of feature constraints on nodes and edges. relation lines: name operator name s := w m -sub> s m <sub- s Indents and spacing are ignored. There must be white-space around the operator. Operators that come from edge features may be enriched with values. See relational operators below for the whole spectrum of relational constraints on nodes. quantifier sub-templates: Atom lines that contain an otype or set may be followed by Quantifiers consist of search templates themselves, demarcated by some special keywords: /without/ /where/ and /have/ /with/ and /or/ /-/ See quantifiers below for all the syntax and semantics.","title":"Search templates"},{"location":"Api/General/#feature-specifications","text":"Features specs The features above is a specification of what features with which values to search for. This specification must be written as a white-space separated list of feature specs . A feature spec has the form name valueSpec , with no space between the name and the valueSpec . The valueSpec may have the following forms and meanings: form evaluates to True the feature name ... has any value except None # has value None * has arbitrary value = values has one of the values specified # values has none of the values specified > value is greater than value < value is less than value ~ regular expression has a value and it matches regular expression Why * ? The operator * after a feature name does not pose any restriction at all. It will not influence the search results. Why would you want to include such a \"criterion\"? Some applications, such as the Text-Fabric browser collect the features used in a query to retrieve result information to be presented to the user. So if you want to include the values of a particular feature, mention that feature with a * . All these forms are also valid as - name form > and < name form - , in which case they specify value constraints on edge features. This is only meaningful if the edge feature is declared to have values (most edge features do not have values). Additional constraints There may be no space around the =#<>~ . name must be a feature name that exists in the dataset. If it references a feature that is not yet loaded, the feature will be loaded automatically. values must be a | separated list of feature values, no quotes. No spaces around the | . If you need a space or | or \\ in a value, escape it by a \\ . Escape tabs and newlines as \\t and \\n . When comparing values with < and > : value must be an integer (negative values allowed); You can do numeric comparisons only on number-valued features, not on string-valued features. regular expression must be a string that conforms to the Python regular axpression syntax If you need a space in your regular expression, you have to escape it with a \\ . You can do regular expressions only on string-valued features, not on number-valued features.","title":"Feature specifications"},{"location":"Api/General/#relational-operators","text":"Operator lines Node comparison = : is equal (meaning the same node, a clause and a verse that occupy the same slots are still unequal) # : is unequal (meaning a different node, a clause and a verse that occupy the same slots are still unequal) < > : before and after (in the canonical ordering ) Slot comparison == : occupy the same slots (identical slot sets) && : overlap (the intersection of both slot sets is not empty) ## : occupy different slots (but they may overlap, the set of slots of the two are different as sets) || : occupy disjoint slots (no slot occupied by the one is also occupied by the other) [[ ]] : embeds and contains (slot set inclusion, in both directions) << >> : before and after (with respect to the slots occupied: left ends before right starts and vice versa) <: :> : adjacent before and after (with respect to the slots occupied: left ends immediately before right starts and vice versa) =: left and right start at the same slot := left and right end at the same slot :: left and right start and end at the same slot Nearness comparison Some of the adjacency relations can actually be weakened. Instead of requiring that one slot is equal to an other slot, you can require that they are k-near , i.e. they are at most k apart. Here are the relationships where you can do that. Instead of typing the letter k , provide the actual number you want. <k: :k> : k - adjacent before and after (with respect to the slots occupied: left ends k -near where right starts and vice versa) =k: left and right start at k -near slots :k= left and right end at k -near slots :k: left and right start and end at k -near slots Based on edge features - name > < name - : connected by the edge feature name in both directions; these forms work for edges that do and do not have values; - name valueSpec > < name valueSpec - : connected by the edge feature name in both directions; these forms work only for edges that do have values.","title":"Relational operators"},{"location":"Api/General/#quantifiers","text":"Quantifiers Experimental This part of search templates is still experimental. bugs may be discovered the syntax of quantifiers may change What is a quantifier? Quantifiers are powerful expressions in templates. They state conditions on a given atom in your template. The atom in question is called the parent atom. The conditions may involve many nodes that are related to the parent, as in: all embedded words are a verb ; without a following predicate phrase ; with a mother clause or a mother phrase . That is where the term quantifier comes from. A quantifier quantifies its parent atom. /without/ Syntax: 1 2 3 4 atom /without/ templateN /-/ Meaning: node r is a result of this template if and only if r is a result of atom and there is no tuple RN such that ( r , RN ) is a result of 1 2 atom templateN /where/ Syntax: 1 2 3 4 5 6 atom /where/ templateA /have/ templateH /-/ Meaning: node r is a result of this template if and only if r is a result of atom and for all tuples ( RA ) such that ( r , RA ) is a result of 1 2 atom templateA there is a tuple RH such that ( r , RA , RH ) is a result of 1 2 3 atom templateA templateH /with/ Syntax: 1 2 3 4 5 6 7 8 atom /with/ templateO1 /or/ templateO2 /or/ templateO3 /-/ Meaning: node r is a result of this template if and only if: there is a tuple R1 such that ( r , R1 ) is a result of 1 2 atom templateO1 or there is a tuple R2 such that ( r , R2 ) is a result of 1 2 atom templateO2 or there is a tuple R3 such that ( r , R3 ) is a result of 1 2 atom templateO3 1 or more alternatives This quantifier can be used with any number of /or/ keywords, including none at all. If there is no /or/ , there is just one alternative. The only difference between 1 2 3 4 atom /with/ template /-/ and 1 2 atom template is that the results of the first query contain tuples with only one element, corresponding to the atom . The second query contains tuples of which the first element corresponds to the atom , and the remaining members correspond to the template . Parent The atom bit is an atom line, it acts as the parent of the quantifier. Inside a quantifier, you may refer to the parent by the special name .. . So you do not have to give a name to the parent. Multiple quantifiers You may have multiple quantifiers for one parent. Not in result tuples Whereas a the search for a normal template proceeds by finding a tuples that instantiates all it nodes in such a way that all relationships expressed in the template hold, a quantifier template is not instantiated. It asserts a condition that has to be tested for all nodes relative its parent. None of the atoms in a template of a quantifier corresponds to a node in a final result tuple. May be nested Templates within a quantifier may contain other quantifiers. The idea is, that whenever a search template is evaluated, quantifiers at the outer level of get interpreted. This interpretation gives rise to one or more templates to be constructed and run. Those new templates have been stripped of the outer layer of quantifiers, and when these templates are executed, the quantifiers at the next level have become outer. And so on. Restrictions Due to the implementation of quantifiers there are certain restrictions. Quantifiers must be put immediately below their parents or below preceding quantifiers of the same parent. The keywords of a quantifier must appear on lines with exactly the same indentation as the atom they quantify. The templates of a quantifier must have equal or greater indent than its keywords; The names accessible to the templates inside a quantifier are: the name .. , which is the name of the atom that is quantified; this name is automagically valid in quantifier templates; the name of the atom that is quantified (if that atom has a given name); names defined in the template itself; in /where/ , templateH may use names defined in templateA ; but only if these names are defined outside any quantifier of templateA . The following situations block the visibility of names: in /with/ , templateO i may not use names defined in templateO j for j other than i ; names defined outer quantifiers are not accessible in inner quantifiers; names defined inner quantifiers are not accessible in outer quantifiers. When you nest quantifiers, think of the way they will be recomposed into ordinary templates. This dictates whether your quantifier is syntactically valid or not. Indentation The indentation in quantifiers relative to their parent atom will be preserved. Nested quantifiers Consider 1 2 3 4 5 6 7 8 9 clause /where/ phrase function=Pred /have/ /without/ word sp#verb /-/ /-/ phrase function=Subj The auxiliary templates that will be run are: For the outer quantifier: 1 2 clause phrase function=Pred and 1 2 3 4 5 clause phrase function=Pred /without/ word sp#verb /-/ For the inner quantifier: 1 2 phrase function=Pred word sp#verb Note that the auxiliary template for the inner quantifier is shifted in its entirety to the left, but that the relative indentation is exactly as it shows in the original template. Implementation Here is a description of the implementation of the quantifiers. It is not the real implementation, but it makes clear what is going on, and why the quantifiers have certain limitations, and how indentation works. The basic idea is: a quantifier leads to the execution of one or more separate searche templates; the results of these searches are combined by means of set operations: difference , intersection , union , dependent on the nature of the quantifier; the end result of this combination will fed as a custom set to the original template after stripping the whole quantifier from that template. So we replace a quantifier by a custom set. Suppose we have 1 2 3 4 5 6 clause typ=Wx0 QUANTIFIER1 QUANTIFIER2 ... QUANTIFIERn rest-of-template We compute a set of clauses filteredClauses1 based on 1 2 clause typ=Wx0 QUANTIFIER1 and then compute a new set filteredClauses2 based on 1 2 3 4 5 S.search(''' fclause typ=Wx0 QUANTIFIER2 ''', customSets=dict(fclause=filteredClauses1) and so on until we have had QUANTIFIERn, leading to a set filteredClausesN of clauses that pass all filters set by the quantifiers. Finally, we deliver the results of 1 2 3 4 5 S.search(''' fclause rest-of-template ''', customSets=dict(fclause=filteredClausesN)","title":"Quantifiers"},{"location":"Api/General/#search-api","text":"S.relationsLegend() 1 S . relationsLegend () Description Gives dynamic help about the basic relations that you can use in your search template. It includes the edge features that are available in your dataset. S.search() 1 S . search ( query , limit = None , shallow = False , sets = None , withContext = None ) Description Searches for combinations of nodes that together match a search template. This method returns a generator which yields the results one by one. One result is a tuple of nodes, where each node corresponds to an atom -line in your search template . query The query is a search template, i.e. a string that conforms to the rules described above. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. limit If limit is a number, it will fetch only that many results. TF as Database By means of S.search(query) you can use one TF instance as a database that multiple clients can use without the need for each client to call the costly load methods. You have to come up with a process that runs TF, has all features loaded, and that can respond to queries from other processes. We call such a process a TF kernel . Webservers can use such a daemonized TF to build efficient controllers. A TF kernel and webserver are included in the Text-Fabric code base. See kernel and web . Generator versus tuple If limit is specified, the result is not a generator but a tuple of results. More info on the search plan Searching is complex. The search template must be parsed, interpreted, and translated into a search plan. The following methods expose parts of the search process, and may provide you with useful information in case the search does not deliver what you expect. see the plan the method S.showPlan() below shows you at a glance the correspondence between the nodes in each result tuple and your search template. S.study() 1 S . study ( searchTemplate , strategy = None , silent = False , shallow = False , sets = None ) Description Your search template will be checked, studied, the search space will be narrowed down, and a plan for retrieving the results will be set up. If your query has quantifiers, the asscociated search templates will be constructed and executed. These searches will be reported clearly. searchTemplate The search template is a string that conforms to the rules described above. strategy In order to tame the performance of search, the strategy by which results are fetched matters a lot. The search strategy is an implementation detail, but we bring it to the surface nevertheless. To see the names of the available strategies, just call S.study('', strategy='x') and you will get a list of options reported to choose from. Feel free to experiment. To see what the strategies do, see the code . silent If you want to suppress most of the output, say silent=True . shallow, sets As in S.search() . S.showPlan() 1 S . showPlan ( details = False ) Description Search results are tuples of nodes and the plan shows which part of the tuple corresponds to which part of the search template. details If you say details=True , you also get an overview of the search space and a description of how the results will be retrieved. after S.study() This function is only meaningful after a call to S.study() .","title":"Search API"},{"location":"Api/General/#search-results","text":"Preparation versus result fetching The method S.search() above combines the interpretation of a given template, the setting up of a plan, the constraining of the search space and the fetching of results. Here are a few methods that do actual result fetching. They must be called after a previous S.search() or S.study() . S.count() 1 S . count ( progress = None , limit = None ) Description Counts the results, with progress messages, optionally up to a limit. progress Every so often it shows a progress message. The frequency is progress results, default every 100. limit Fetch results up to a given limit , default 1000. Setting limit to 0 or a negative value means no limit: all results will be counted. why needed You typically need this in cases where result fetching turns out to be (very) slow. generator versus list len(S.results()) does not work, because S.results() is a generator that delivers its results as they come. S.fetch() 1 S . fetch ( limit = None ) Description Finally, you can retrieve the results. The result of fetch() is not a list of all results, but a generator . It will retrieve results as long as they are requested and their are still results. limit Tries to get that many results and collects them in a tuple. So if limit is not None , the result is a tuple with a known length. Iterating over the fetch() generator You typically fetch results by saying: 1 2 3 4 i = 0 for r in S . results (): do_something ( r [ 0 ]) do_something_else ( r [ 1 ]) Alternatively, you can set the limit parameter, to ask for just so many results. They will be fetched, and when they are all collected, returned as a tuple. Fetching a limited amount of results 1 S . fetch ( limit = 10 ) gives you the first bunch of results quickly. S.glean() 1 S . glean ( r ) Description A search result is just a tuple of nodes that correspond to your template, as indicated by showPlan() . Nodes give you access to all information that the corpus has about it. The glean() function is here to just give you a first impression quickly. r Pass a raw result tuple r , and you get a string indicating where it occurs, in terms of sections, and what text is associated with the results. Inspecting results 1 2 for result in S . fetch ( limit = 10 ): print ( S . glean ( result )) is a handy way to get an impression of the first bunch of results. Universal This function works on all tuples of nodes, whether they have been obtained by search or not. More ways of showing results If you work in one of the corpora for which the TF-API has been extended, you will be provided with more powerful methods show() and table() to display your results. See Cunei , Bhsa , Peshitta and Syrnt .","title":"Search results"},{"location":"Api/General/#features","text":"Features TF can give you information of all features it has encountered. TF.featureSets 1 TF . featureSets Description Returns a dictionary with keys nodes , edges , configs , computeds . Under each key there is the set of feature names in that category. So you can easily test whether a node feature or edge feature is present in the dataset you are working with. configs These are config features, with metadata only, no data. E.g. otext . computeds These are blocks of precomputed data, available under the C. API, see below. May be unloaded The sets do not indicate whether a feature is loaded or not. There are other functions that give you the loaded node features ( Fall() ) and the loaded edge features ( Eall() ).","title":"Features"},{"location":"Api/General/#node-features","text":"F The node features API is exposed as F ( Fs ) or Feature ( FeatureString ). Fall() aka AllFeatures() 1 2 Fall () AllFeatures () Description Returns a sorted list of all usable, loaded node feature names. F. feature aka Feature. feature 1 2 F . part_of_speech Feature . part_of_speech Description Returns a sub-api for retrieving data that is stored in node features. In this example, we assume there is a feature called part_of_speech . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Fs instead. Fs(feature) aka FeatureString(feature) 1 2 3 4 Fs ( feature ) FeatureString ( feature ) Fs ( 'part-of-speech' ) FeatureString ( 'part-of-speech' ) Description Returns a sub-api for retrieving data that is stored in node features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called part-of-speech . Note that this is not a valid name in Python, yet we can work with features with such names. Both methods have identical results Suppose we have just issued feature = 'pos'. Then the result of Fs(feature) and F.pos` is identical. In most cases F works just fine, but Fs is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. F. feature .v(node) 1 F . part_of_speech . v ( node ) Description Get the value of a feature , such as part_of_speech for a node. node The node whose value for the feature is being retrieved. F. feature .s(value) 1 2 F . part_of_speech . s ( value ) F . part_of_speech . s ( 'noun' ) Description Returns a generator of all nodes in the canonical order with a given value for a given feature. This is an other way to walk through nodes than using N() . value The test value: all nodes with this value are yielded, the others pass through. nouns The second line gives you all nodes which are nouns according to the corpus. F. feature .freqList() 1 F . part_of_speech . freqList ( nodeTypes = None ) Description Inspect the values of feature (in this example: part_of_speech ) and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypes If you pass a set of nodeTypes, only the values for nodes within those types will be counted. F.otype otype is a special node feature and has additional capabilities. Description F.otype.slotType is the node type that can fill the slots (usually: word ) F.otype.maxSlot is the largest slot number F.otype.maxNode is the largest node number F.otype.all is a list of all otypes from big to small (from books through clauses to words) F.otype.sInterval(otype) is like F.otype.s(otype) , but instead of returning you a range to iterate over, it will give you the starting and ending nodes of otype . This makes use of the fact that the data is so organized that all node types have single ranges of nodes as members.","title":"Node features"},{"location":"Api/General/#edge-features","text":"E The edge features API is exposed as E ( Es ) or Edge ( EdgeString ). Eall() aka AllEdges() 1 2 Eall () AllEdges () Description Returns a sorted list of all usable, loaded edge feature names. E. feature aka Edge. feature 1 2 E . head Feature . head Description Returns a sub-api for retrieving data that is stored in edge features. In this example, we assume there is a feature called head . Tricky feature names If the feature name is not a valid python identifier, you can not use this function, you should use Es instead. Es(feature) aka EdgeString(feature) 1 2 3 4 Es ( feature ) EdgeString ( feature ) Es ( 'head' ) EdgeString ( 'head' ) Description Returns a sub-api for retrieving data that is stored in edge features. feature In this example, in line 1 and 2, the feature name is contained in the variable feature . In lines 3 and 4, we assume there is a feature called head . Both methods have identical results Suppose we have just issued feature = 'head'. Then the result of Es(feature) and E.pos` is identical. In most cases E works just fine, but Es is needed in two cases: if we need to work with a feature whose name is not a valid Python name; if we determine the feature we work with dynamically, at run time. Simple forms In the sequel we'll give examples based on the simple form only. E. feature .f(node) 1 E . head . f ( node ) Description Get the nodes reached by feature -edges from a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if head describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges from the node, the empty tuple is returned, rather than None . node The node from which the edges in question start. E. feature .t(node) 1 E . head . t ( node ) Description Get the nodes reached by feature -edges to a certain node. These edges must be specified in feature , in this case head . The result is an ordered tuple (again, in the canonical order . The members of the result are just nodes, if feature describes edges without values. Otherwise the members are pairs (tuples) of a node and a value. If there are no edges to n , the empty tuple is returned, rather than None . node The node to which the edges in question go. E. feature .freqList() 1 E . op . freqList ( nodeTypesFrom = None , nodeTypesTo = None ) Description If the edge feature has no values, simply return the number of node pairs between an edge of this kind exists. If the edge feature does have values, we inspect them and see how often they occur. The result is a list of pairs (value, frequency) , ordered by frequency , highest frequencies first. nodeTypesFrom If not None , only the values for edges that start from a node with type within nodeTypesFrom will be counted. nodeTypesTo If not None , only the values for edges that go to a node with type within nodeTypesTo will be counted. E.oslots oslots is a special edge feature and is mainly used to construct other parts of the API. It has less capabilities, and you will rarely need it. It does not have .f and .t methods, but an .s method instead. Description E.oslots.s(node) Gives the sorted list of slot numbers linked to a node, or put otherwise: the slots that support that node. node The node whose slots are being delivered.","title":"Edge features"},{"location":"Api/General/#messaging","text":"Timed messages Error and informational messages can be issued, with a time indication. info(), error() 1 info ( msg , tm = True , nl = True ) Description Sends a message to standard output, possibly with time and newline. if info() is being used, the message is sent to stdout ; if error() is being used, the message is sent to stderr ; In a Jupyter notebook, the standard error is displayed with a reddish background colour. tm If True , an indicator of the elapsed time will be prepended to the message. nl If True a newline will be appended. indent() 1 indent ( level = None , reset = False ) Description Changes the level of indentation of messages and possibly resets the time. level The level of indentation, an integer. Subsequent info() and error() will display their messages with this indent. reset If True , the elapsed time to will be reset to 0 at the given level. Timers at different levels are independent of each other.","title":"Messaging"},{"location":"Api/General/#saving-features","text":"TF.save() 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Description If you have collected feature data in dictionaries, keyed by the names of the features, and valued by their feature data, then you can save that data to .tf feature files on disk. It is this easy to export new data as features: collect the data and metadata of the features and feed it in an orderly way to TF.save() and there you go. nodeFeatures The data of a node feature is a dictionary with nodes as keys (integers!) and strings or numbers as (feature) values. edgeFeatures The data of an edge feature is a dictionary with nodes as keys, and sets or dictionaries as values. These sets should be sets of nodes (integers!), and these dictionaries should have nodes as keys and strings or numbers as values. metadata Every feature will receive metadata from metaData , which is a dictionary mapping a feature name to its metadata. value types The type of the values should conform to @valueType ( int or str ), which must be stated in the metadata. edge values If you save an edge feature, and there are values in that edge feature, you have to say so, by specifying edgeValues = True in the metadata for that feature. generic metadata metaData may also contain fields under the empty name. These fields will be added to all features in nodeFeatures and edgeFeatures . config features If you need to write the config feature otext , which is a metadata-only feature, just add the metadata under key otext in metaData and make sure that otext is not a key in nodeFeatures nor in edgeFeatures . These fields will be written into the separate config feature otext , with no data associated. save location The (meta)data will be written to the very last module in the list of locations that you specified when calling Fabric() or to what you passed as module in the same location. If that module does not exist, it will be created in the last location . If both locations and modules are empty, writing will take place in the current directory.","title":"Saving features"},{"location":"Api/General/#clearing-the-cache","text":"TF.clearCache() 1 TF . clearCache () Description Text-Fabric precomputes data for you, so that it can be loaded faster. If the original data is updated, Text-Fabric detects it, and will recompute that data. But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, where you might want to clear the cache of precomputed results. Calling this function just does it, and it is equivalent with manually removing all .tfx files inside the hidden .tf directory inside your dataset. No need to load It is not needed to execute a TF.load() first.","title":"Clearing the cache"},{"location":"Api/General/#mql","text":"Data interchange with MQL You can interchange with MQL data. Text-Fabric can read and write MQL dumps. An MQL dump is a text file, like an SQL dump. It contains the instructions to create and fill a complete database. TF.exportMQL() 1 TF . exportMQL ( dbName , dirName ) Description Exports the complete TF dataset into single MQL database. dirName, dbName The exported file will be written to dirName/dbName.mql . If dirName starts with ~ , the ~ will be expanded to your home directory. Likewise, .. will be expanded to the parent of the current directory, and . to the current directory, both only at the start of dirName . Correspondence TF and MQL The resulting MQL database has the following properties with respect to the Text-Fabric dataset it comes from: the TF slots correspond exactly with the MQL monads and have the same numbers; provided the monad numbers in the MQL dump are consecutive. In MQL this is not obligatory. Even if there gaps in the monads sequence, we will fill the holes during conversion, so the slots are tightly consecutive; the TF nodes correspond exactly with the MQL objects and have the same numbers Node features in MQL The values of TF features are of two types, int and str , and they translate to corresponding MQL types integer and string . The actual values do not undergo any transformation. That means that in MQL queries, you use quotes if the feature is a string feature. Only if the feature is a number feature, you may omit the quotes: 1 2 [word sp='verb'] [verse chapter=1 and verse=1] Enumeration types It is attractive to use eumeration types for the values of a feature, whereever possible, because then you can query those features in MQL with IN and without quotes: 1 [chapter book IN (Genesis, Exodus)] We will generate enumerations for eligible features. Integer values can already be queried like this, even if they are not part of an enumeration. So we restrict ourselves to node features with string values. We put the following extra restrictions: the number of distinct values is less than 1000 all values must be legal C names, in practice: starting with a letter, followed by letters, digits, or _ . The letters can only be plain ASCII letters, uppercase and lowercase. Features that comply with these restrictions will get an enumeration type. Currently, we provide no ways to configure this in more detail. Merged enumeration types Instead of creating separate enumeration types for individual features, we collect all enumerated values for all those features into one big enumeration type. The reason is that MQL considers equal values in different types as distinct values. If we had separate types, we could never compare values for different features. Values of edge features are ignored There is no place for edge values in MQL. There is only one concept of feature in MQL: object features, which are node features. But TF edges without values can be seen as node features: nodes are mapped onto sets of nodes to which the edges go. And that notion is supported by MQL: edge features are translated into MQL features of type LIST OF id_d , i.e. lists of object identifiers. Legal names in MQL MQL names for databases, object types and features must be valid C identifiers (yes, the computer language C). The requirements are: start with a letter (ASCII, upper-case or lower-case) follow by any sequence of ASCII upper/lower-case letters or digits or underscores ( _ ) avoid being a reserved word in the C language So, we have to change names coming from TF if they are invalid in MQL. We do that by replacing illegal characters by _ , and, if the result does not start with a letter, we prepend an x . We do not check whether the name is a reserved C word. With these provisos: the given dbName correspond to the MQL database name the TF otypes correspond to the MQL objects the TF features correspond to the MQL features File size The MQL export is usually quite massive (500 MB for the Hebrew Bible). It can be compressed greatly, especially by the program bzip2 . Exisiting database If you try to import an MQL file in Emdros, and there exists already a file or directory with the same name as the MQL database, your import will fail spectacularly. So do not do that. A good way to prevent it is: export the MQL to outside your text-fabric-data directory, e.g. to ~/Downloads ; before importing the MQL file, delete the previous copy; Delete existing copy 1 2 cd ~/Downloads rm dataset ; mql -b 3 < dataset.mql TF.importMQL() 1 TF . importMQL ( mqlFile , slotType = None , otext = None , meta = None ) Description Converts an MQL database dump to a Text-Fabric dataset. Destination directory It is recommended to call this importMQL on a TF instance called with 1 locations = targetDir , modules = '' Then the resulting features will be written in the targetDir. In fact, the rules are exactly the same as for TF.save() . slotType You have to tell which object type in the MQL file acts as the slot type, because TF cannot see that on its own. otext You can pass the information about sections and text formats as the parameter otext . This info will end up in the otext.tf feature. Pass it as a dictionary of keys and values, like so: 1 2 3 4 otext = { 'fmt:text-trans-plain' : '{glyphs}{trailer}' , 'sectionFeatures' : 'book,chapter,verse' , } meta Likewise, you can add a dictionary of keys and values that will added to the metadata of all features. Handy to add provenance data here: 1 2 3 4 5 meta = dict ( dataset = 'DLC' , datasetName = 'Digital Language Corpus' , author = \"That 's me\" , )","title":"MQL"},{"location":"Api/General/#computed-data","text":"Pre-computing In order to make the API work, Text-Fabric prepares some data and saves it in quick-load format. Most of this data are the features, but there is some extra data needed for the special functions of the WARP features and the L-API. Normally, you do not use this data, but since it is there, it might be valuable, so we have made it accessible in the C -api, which we document here. Call() aka AllComputeds() 1 2 Call () AllComputeds () Description Returns a sorted list of all usable, loaded computed data names. C.levels.data Description A sorted list of object types plus basic information about them. Each entry in the list has the shape 1 ( otype , averageSlots , minNode , maxNode ) where otype is the name of the node type, averageSlots the average size of objects in this type, measured in slots (usually words). minNode is the first node of this type, maxNode the last, and the nodes of this node type are exactly the nodes between these two values (including). Level computation and customization All node types have a level, defined by the average amount of slots object of that type usually occupy. The bigger the average object, the lower the levels. Books have the lowest level, words the highest level. However, this can be overruled. Suppose you have a node type phrase and above it a node type cluster , i.e. phrases are contained in clusters, but not vice versa. If all phrases are contained in clusters, and some clusters have more than one phrase, the automatic level ranking of node types works out well in this case. But if clusters only have very small phrases, and the big phrases do not occur in clusters, then the algorithm may assign a lower rank to clusters than to phrases. In general, it is too expensive to try to compute the levels in a sophisticated way. In order to remedy cases where the algorithm assigns wrong levels, you can add a @levels key to the otext config feature. See text . C.order.data Description An array of all nodes in the correct order. This is the order in which N() alias Node() traverses all nodes. Rationale To order all nodes in the canonical ordering is quite a bit of work, and we need this ordering all the time. C.rank.data Description An array of all indices of all nodes in the canonical order array. It can be viewed as its inverse. Order arbitrary node sets I we want to order a set of nodes in the canonical ordering, we need to know which position each node takes in the canonical order, in other words, at what index we find it in the C.order.data array. C.levUp.data and C.levDown.data Description These tables feed the L.d() and L.u() functions. Use with care They consist of a fair amount of megabytes, so they are heavily optimized. It is not advisable to use them directly, it is far better to use the L functions. Only when every bit of performance waste has to be squeezed out, this raw data might be a deal. C.boundary.data Description These tables feed the L.n() and L.p() functions. It is a tuple consisting of firstSlots and lastSlots . They are indexes for the first slot and last slot of nodes. Slot index For each slot, firstSlot gives all nodes (except slots) that start at that slot, and lastSlot gives all nodes (except slots) that end at that slot. Both firstSlot and lastSlot are tuples, and the information for node n can be found at position n-MaxSlot-1 . C.sections.data Description Let us assume for the sake of clarity, that the node type of section level 1 is book , that of level 2 is chapter , and that of level 3 is verse . And suppose that we have features, named bookHeading , chapterHeading , and verseHeading that give the names or numbers of these. Custom names Note that the terms book , chapter , verse are not baked into Text-Fabric. It is the corpus data, especially the otext config feature that spells out the names of the sections. Then C.section.data is a tuple of two mappings , let us call them chapters and verses . chapters is a mapping, keyed by book nodes , and then by by chapter headings , giving the corresponding chapter node s as values. verses is a mapping, keyed by book nodes , and then by chapter headings , and then by verse headings , giving the corresponding verse node s as values. Supporting the T -Api The T -api is good in mapping nodes unto sections, such as books, chapters, verses and back. It knows how many chapters each book has, and how many verses each chapter. The T api is meant to make your life easier when you have to find passage labels by nodes or vice versa. That is why you probably never need to consult the underlying data. But you can! That data is stored in","title":"Computed data"},{"location":"Api/General/#miscellaneous","text":"TF.version Description Contains the version number of the Text-Fabric library. TF.banner Description Contains the name and the version of the Text-Fabric library.","title":"Miscellaneous"},{"location":"Api/Peshitta/","text":"Peshitta \u00b6 About \u00b6 The module peshitta.py contains a number of handy functions on top of Text-Fabric and especially its Search part. Minimal incantation \u00b6 1 2 from tf.extra.peshitta import Peshitta A = Peshitta ( hoist = globals ()) Explanation The first line makes the Peshitta API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Peshitta API and gives it the name A . During start-up the following happens: (1) the Peshitta data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) all optimized features of the Peshitta dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation . Set up \u00b6 import Peshitta The Peshitta API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.peshitta import Peshitta Initialisation \u00b6 Peshitta() 1 A = Peshitta ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific Peshitta version The easiest way to load a specific version of the Peshitta is like so: 1 A = Peshitta ( version = '0.1' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '0.1' TF = Fabric ( locations = f '~/github/etcbc/peshitta/tf/{VERSION}' ) api = TF . load ( ''' word_etcbc ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the peshitta module by a call to Peshitta() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Peshitta API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through. Linking \u00b6 A.pshLink() 1 A . pshLink ( node , text = None ) Description Produces a link to ETCBC source of this node. The link ends at a file corresponding to a book, having the ETCBC/WIT format. node node can be an arbitrary node. The link targets the book that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 in ETCBC/WIT 1 A . pshLink ( 100000 ) Plain display \u00b6 Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , fmt = None , linked = True , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. linked linked indicates whether the result should be a link to the source. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , linked = 1 , withNodes = False , fmt = None , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in the source; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , start = 1 , end = len ( results ), linked = 1 , fmt = None , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() . Pretty display \u00b6 Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values. None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() . Search \u00b6 A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. search template reference See the search template reference","title":"Peshitta"},{"location":"Api/Peshitta/#peshitta","text":"","title":"Peshitta"},{"location":"Api/Peshitta/#about","text":"The module peshitta.py contains a number of handy functions on top of Text-Fabric and especially its Search part.","title":"About"},{"location":"Api/Peshitta/#minimal-incantation","text":"1 2 from tf.extra.peshitta import Peshitta A = Peshitta ( hoist = globals ()) Explanation The first line makes the Peshitta API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Peshitta API and gives it the name A . During start-up the following happens: (1) the Peshitta data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) all optimized features of the Peshitta dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation .","title":"Minimal incantation"},{"location":"Api/Peshitta/#set-up","text":"import Peshitta The Peshitta API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.peshitta import Peshitta","title":"Set up"},{"location":"Api/Peshitta/#initialisation","text":"Peshitta() 1 A = Peshitta ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific Peshitta version The easiest way to load a specific version of the Peshitta is like so: 1 A = Peshitta ( version = '0.1' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '0.1' TF = Fabric ( locations = f '~/github/etcbc/peshitta/tf/{VERSION}' ) api = TF . load ( ''' word_etcbc ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the peshitta module by a call to Peshitta() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Peshitta API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through.","title":"Initialisation"},{"location":"Api/Peshitta/#linking","text":"A.pshLink() 1 A . pshLink ( node , text = None ) Description Produces a link to ETCBC source of this node. The link ends at a file corresponding to a book, having the ETCBC/WIT format. node node can be an arbitrary node. The link targets the book that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 in ETCBC/WIT 1 A . pshLink ( 100000 )","title":"Linking"},{"location":"Api/Peshitta/#plain-display","text":"Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , fmt = None , linked = True , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. linked linked indicates whether the result should be a link to the source. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , linked = 1 , withNodes = False , fmt = None , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in the source; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , start = 1 , end = len ( results ), linked = 1 , fmt = None , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() .","title":"Plain display"},{"location":"Api/Peshitta/#pretty-display","text":"Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values. None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() .","title":"Pretty display"},{"location":"Api/Peshitta/#search","text":"A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. search template reference See the search template reference","title":"Search"},{"location":"Api/Syrnt/","text":"Syriac New Testament \u00b6 About \u00b6 The module syrnt.py contains a number of handy functions on top of Text-Fabric and especially its Search part. Minimal incantation \u00b6 1 2 from tf.extra.syrnt import Syrnt A = Syrnt ( hoist = globals ()) Explanation The first line makes the Syrnt API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Syrnt API and gives it the name A . During start-up the following happens: (1) the SyrNT data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) all optimized features of the SyrNT dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation . Set up \u00b6 import Syrnt The Syrnt API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.syrnt import Syrnt Initialisation \u00b6 Syrnt() 1 A = Syrnt ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific SyrNT version The easiest way to load a specific version of the SyrNT is like so: 1 A = Syrnt ( version = '0.1' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '0.1' TF = Fabric ( locations = f '~/github/etcbc/syrnt/tf/{VERSION}' ) api = TF . load ( ''' word_etcbc ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the syrnt module by a call to Syrnt() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Syrnt API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through. Linking \u00b6 A.sntLink() 1 A . sntLink ( node , text = None ) Description Produces a link to ETCBC source of this node. The link ends at a file corresponding to a book, having the ETCBC/WIT format. node node can be an arbitrary node. The link targets the book that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 in ETCBC/WIT 1 A . sntLink ( 100000 ) Plain display \u00b6 Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , linked = True , fmt = None , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. linked linked indicates whether the result should be a link to the source. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , linked = 1 , fmt = None , withNodes = False , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in the source; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , start = 1 , end = len ( results ), linked = 1 , fmt = None , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() . Pretty display \u00b6 Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values. None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() . Search \u00b6 A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. search template reference See the search template reference","title":"Syriac New Testament"},{"location":"Api/Syrnt/#syriac-new-testament","text":"","title":"Syriac New Testament"},{"location":"Api/Syrnt/#about","text":"The module syrnt.py contains a number of handy functions on top of Text-Fabric and especially its Search part.","title":"About"},{"location":"Api/Syrnt/#minimal-incantation","text":"1 2 from tf.extra.syrnt import Syrnt A = Syrnt ( hoist = globals ()) Explanation The first line makes the Syrnt API code, which is an app on top of Text-Fabric, accessible to your notebook. The second line starts up the Syrnt API and gives it the name A . During start-up the following happens: (1) the SyrNT data is downloaded to your ~/text-fabric-data directory, if not already present there; (2) if your data has been freshly downloaded, a series of optimizations are executed; (3) all optimized features of the SyrNT dataset are loaded; (4) hoist=globals() makes the API elements directly available: you can refer to F , L , T , etc. directly, instead of the more verbose A.api.F , A.api.L , A.api.T etc. If you are content with the minimal incantation, you can skip Set up and Initialisation .","title":"Minimal incantation"},{"location":"Api/Syrnt/#set-up","text":"import Syrnt The Syrnt API is distributed with Text-Fabric. You have to import it into your program: 1 from tf.extra.syrnt import Syrnt","title":"Set up"},{"location":"Api/Syrnt/#initialisation","text":"Syrnt() 1 A = Syrnt ( api = api , name = None , version = VERSION , silent = False ) Description Silently loads some additional features, and A will give access to some extra functions. Specific SyrNT version The easiest way to load a specific version of the SyrNT is like so: 1 A = Syrnt ( version = '0.1' ) api The API resulting from an earlier call TF.load() If you leave it out, an API will be created exactly like the TF-browser does it, with the same data version and the same set of data features. Set up This module comes in action after you have set up TF and loaded some features, e.g. 1 2 3 4 5 6 VERSION = '0.1' TF = Fabric ( locations = f '~/github/etcbc/syrnt/tf/{VERSION}' ) api = TF . load ( ''' word_etcbc ''' ) api . makeAvailableIn ( globals ()) Then we add the functionality of the syrnt module by a call to Syrnt() . name If you leave this argument out, Text-Fabric will determine the name of your notebook for you. If Text-Fabric finds the wrong name, you can override it here. This should be the name of your current notebook (without the .ipynb extension). The Syrnt API will use this to generate a link to your notebook on GitHub and NBViewer. silent If True , nearly all output will be suppressed, including the links to the loaded data, features, API methods, and online versions of the notebook. Error messages will still come through.","title":"Initialisation"},{"location":"Api/Syrnt/#linking","text":"A.sntLink() 1 A . sntLink ( node , text = None ) Description Produces a link to ETCBC source of this node. The link ends at a file corresponding to a book, having the ETCBC/WIT format. node node can be an arbitrary node. The link targets the book that contains the first word contained by the node. text You may provide the text to be displayed as the link. Then the passage indicator (book chapter:verse) will be put in the tooltip (title) of the link. If you do not provide a link text, the passage indicator (book chapter:verse) will be chosen. Word 100000 in ETCBC/WIT 1 A . sntLink ( 100000 )","title":"Linking"},{"location":"Api/Syrnt/#plain-display","text":"Straightforward display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a simple way, as rows and as a table. A.plain() 1 A . plain ( node , linked = True , fmt = None , withNodes = False , asString = False ) Description Displays the material that corresponds to a node in a simple way. node node is a node of arbitrary type. linked linked indicates whether the result should be a link to the source. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. asString Instead of displaying the result directly in the output of your code cell in a notebook, you can also deliver the markdown as string, just say asString=True . A.plainTuple() 1 A . plainTuple ( nodes , seqNumber , linked = 1 , fmt = None , withNodes = False , asString = False ) Description Displays the material that corresponds to a tuple of nodes in a simple way as a row of cells. nodes nodes is an iterable (list, tuple, set, etc) of arbitrary nodes. seqNumber seqNumber is an arbitrary number which will be displayed in the first cell. This prepares the way for displaying query results, which come as a sequence of tuples of nodes. linked linked=1 the column number where the cell contents is linked to the relevant passage in the source; (the first data column is column 1) fmt, withNodes, asString Same as in A.plain() . A.table() 1 2 3 4 5 6 7 8 A . table ( results , start = 1 , end = len ( results ), linked = 1 , fmt = None , withNodes = False , asString = False , ) Description Displays an iterable of tuples of nodes. The list is displayed as a compact markdown table. Every row is prepended with the sequence number in the iterable. results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. linked, fmt, withNodes, asString Same as in A.plainTuple() .","title":"Plain display"},{"location":"Api/Syrnt/#pretty-display","text":"Graphical display of things There are functions to display nodes, tuples of nodes, and iterables of tuples of nodes in a graphical way. A.prettySetup() Description In pretty displays, nodes are shown together with the values of a selected set of features. With this function you can add features to the display. features A string or iterable of feature names. These features will be loaded automatically. In pretty displays these features will show up as feature=value , provided the value is not None , or something like None. Automatic loading These features will load automatically, no explicit loading is necessary. noneValues A set of values for which no display should be generated. The default set is None and the strings NA , none , unknown . None is useful Keep None in the set. If not, all custom features will be displayed for all kinds of nodes. So you will see clause types on words, and part of speech on clause atoms, al with value None . Suppress common values You can use noneValues also to suppress the normal values of a feature, in order to attract attention to the more special values. None values affect all features Beware of putting to much in noneValues . The contents of noneValues affect the display of all features, not only the custom features. A.pretty() 1 A . pretty ( node , fmt = None , withNodes = False , suppress = set (), highlights = {}) Description Displays the material that corresponds to a node in a graphical way. node node is a node of arbitrary type. fmt fmt is the text format that will be used for the represantation. withNodes withNodes indicates whether node numbers should be displayed. suppress suppress=set() is a set of feature names that should NOT be displayed. By default, quite a number of features is displayed for a node. If you find they clutter the display, you can turn them off selectively. Highlighting When nodes such as verses and sentences are displayed by pretty() , their contents is also displayed. You can selectively highlight those parts. highlights highlights={} is a set or mapping of nodes that should be highlighted. Only nodes that are involved in the display will be highlighted. If highlights is a set, its nodes will be highlighted with a default color (yellow). If it is a dictionary, it should map nodes to colors. Any color that is a valid CSS color qualifies. If you map a node to the empty string, it will get the default highlight color. color names The link above points to a series of handy color names and their previews. A.prettyTuple() 1 2 3 4 5 6 7 8 A . prettyTuple ( nodes , seqNumber , fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays the material that corresponds to a tuple of nodes in a graphical way, with customizable highlighting of nodes. By verse We examine all nodes in the tuple. We collect and show all verses in which they occur and highlight the material corresponding to all the nodes in the tuple. The highlighting can be tweaked by the optional colorMap parameter. nodes, seqNumber, fmt, withNodes Same as in A.plainTuple() . suppress Same as in A.pretty() . colorMap The nodes of the tuple will be highlighted. If colorMap is None or missing, all nodes will be highlighted with the default highlight color, which is yellow. But you can assign different colors to the members of the tuple: colorMap must be a dictionary that maps the positions in a tuple to a color. If a position is not mapped, it will not be highlighted. If it is mapped to the empty string, it gets the default highlight color. Otherwise, it should be mapped to a string that is a valid CSS color . color names The link above points to a series of handy color names and their previews. highlights Same as in A.pretty() . highlights takes precedence over colorMap If both highlights and colorMap are given, colorMap is ignored. If you need to micro-manage, highlights is your thing. Whenever possible, use colorMap . one big highlights dictionary It is OK to first compose a big highlights dictionary for many tuples of nodes, and then run prettyTuple() for many different tuples with the same highlights . It does not harm performance if highlights maps lots of nodes outside the tuple as well. A.show() 1 2 3 4 5 6 7 8 9 10 A . show ( results , condensed = True , start = 1 , end = len ( results ), fmt = None , withNodes = False , suppress = set (), colorMap = None , highlights = None , ) Description Displays an iterable of tuples of nodes. The elements of the list are displayed by A.prettyTuple() . results results an iterable of tuples of nodes. The results of a search qualify, but it works no matter which process has produced the tuples. condensed condensed indicates one of two modes of displaying the result list: True : instead of showing all results one by one, we show all verses with all results in it highlighted. That way, we blur the distinction between the individual results, but it is easier to oversee where the results are. False : make a separate display for each result tuple. This gives the best account of the exact result set. mixing up highlights Condensing may mix-up the highlight coloring. If a node occurs in two results, at different positions in the tuple, the colorMap wants to assign it two colors! Yet one color will be chosen, and it is unpredictable which one. start start is the starting point in the results iterable (1 is the first one). Default 1. end end is the end point in the results iterable. Default the length of the iterable. fmt, withNodes, suppress, colorMap, highlights Same as in A.prettyTuple() .","title":"Pretty display"},{"location":"Api/Syrnt/#search","text":"A.search() 1 A . search ( query , silent = False , shallow = False , sets = None ) Description Searches in the same way as the generic Text-Fabric S.search() . But whereas the S version returns a generator which yields the results one by one, the A version collects all results and sorts them. It then reports the number of results. query query is the search template that has to be searched for. silent silent : if True it will suppress the reporting of the number of results. shallow If True or 1 , the result is a set of things that match the top-level element of the query . If 2 or a bigger number n , return the set of truncated result tuples: only the first n members of each tuple is retained. If False or 0 , a sorted list of all result tuples will be returned. sets If not None , it should be a dictionary of sets, keyed by a names. In query you can refer to those names to invoke those sets. search template reference See the search template reference","title":"Search"},{"location":"Api/Transcription/","text":"Transcription \u00b6 While Text-Fabric is a generic package to deal with text and annotations in a model of nodes, edges, and features, there is need for some additions. Transcription \u00b6 About transcription.py contains transliteration tables for Hebrew and Syriac that are being used in the BHSA , Peshitta , SyrNT , . It also calls functions to use these tables for converting Hebrew and Syriac ttext material to transliterated representations and back. There is also a phonetic transcription for Hebrew, designed in phono.ipynb Character tables Hebrew : full list of characters covered by the ETCBC and phonetic transcriptions Syriac : full list of characters covered by the ETCBC transcriptions Usage Invoke the transcription functionality as follows: 1 from tf.transcription import Transcription Some of the attributes and methods below are class attributes, others are instance attributes. A class attribute aaa can be retrieved by saying Transcription.aaa . To retrieve an instance attribute, you need an instance first, like 1 tr = Transcription () and then you can say tr.aaa . For each attribute we'll give a usage example. Transcription.hebrew_mapping Maps all ETCBC transliteration character combinations for Hebrew to Unicode. Example: print the sof-pasuq: 1 print ( Transcription . hebrew_mapping [ '00' ]) Output: 1 \u05c3 Transcription.syriac_mapping Maps all ETCBC transliteration character combinations for Syriac to Unicode. Example: print the semkath-final: 1 print ( Transcription . syriac_mapping [ 's' ]) Output: 1 \u0724 Transcription.suffix_and_finales(word) Given an ETCBC transliteration, split it into the word material and the interword material that follows it (space, punctuation). Replace the last consonant of the word material by its final form, if applicable. Output a tuple with the modified word material and the interword material. Example: 1 print ( Transcription . suffix_and_finales ( '71T_H@>@95REY00' )) Output: 1 ('71T_H@>@95REy', '00\\n') Note that the Y has been replaced by y . Transcription.suppress_space(word) Given an ETCBC transliteration of a word, match the end of the word for interpunction and spacing characters (sof pasuq, paseq, nun hafukha, setumah, petuhah, space, no-space) Example: 1 2 3 print ( Transcription . suppress_space ( 'B.:&' )) print ( Transcription . suppress_space ( 'B.@R@74>' )) print ( Transcription . suppress_space ( '71T_H@>@95REY00' )) Output: 1 2 3 <re.Match object; span=(3, 4), match='&'> None <re.Match object; span=(13, 15), match='00'> Transcription.to_etcbc_v(word) Given an ETCBC transliteration of a fully pointed word, strip all the non-vowel pointing (i.e. the accents). Example: 1 print ( Transcription . to_etcbc_v ( 'HAC.@MA73JIm' )) Output: 1 HAC.@MAJIm Transcription.to_etcbc_c(word) Given an ETCBC transliteration of a fully pointed word, strip everything except the consonants. Punctuation will also be stripped. Example: 1 print ( Transcription . to_etcbc_c ( 'HAC.@MA73JIm' )) Output: 1 H#MJM Note that the pointed shin ( C ) is replaced by an unpointed one ( # ). Transcription.to_hebrew(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Care will be taken that vowel pointing will be added to consonants before accent pointing. Example: 1 print ( Transcription . to_hebrew ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_x(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Vowel pointing and accent pointing will be applied in the order given by the input word. produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_x ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_v(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the accents. Example: 1 print ( Transcription . to_hebrew_v ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u05d9\u05b4\u05dd Transcription.to_hebrew_c(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_c ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05e9\u05de\u05d9\u05de Note that final consonant forms are not being used. Transcription.ph_simplify(pword) Given a phonological transliteration of a fully pointed word, produce a more coarse phonological transliteration. Example: 1 2 3 print ( Transcription . ph_simplify ( '\u0294\u1d49l\u014dh\u02c8\u00eem' )) print ( Transcription . ph_simplify ( 'm\u0101q\u02c8\u00f4m' )) print ( Transcription . ph_simplify ( 'kol' )) Output: 1 2 3 \u0294l\u014dh\u00eem m\u00e5q\u00f4m k\u00e5l Note that the simplified version transliterates the qamets gadol and qatan to the same character. tr.from_hebrew(word) Given a fully pointed word in Unicode Hebrew, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_hebrew ( '\u05d4\u05b8\u05d0\u05b8\u05bd\u05e8\u05b6\u05e5\u05c3' )) Output: 1 H@>@95REy00 tr.from_syriac(word) Given a word in Unicode Syriac, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_syriac ( '\u0721\u071f\u0723\u071d\u0722' )) Output: 1 MKSJN tr.to_syriac(word) Given a word in ETCBC transliteration, produce the word in Unicode Syriac. Example: 1 print ( tr . to_syriac ( 'MKSJN' )) Output: 1 \u0721\u071f\u0723\u071d\u0722","title":"Auxiliary"},{"location":"Api/Transcription/#transcription","text":"While Text-Fabric is a generic package to deal with text and annotations in a model of nodes, edges, and features, there is need for some additions.","title":"Transcription"},{"location":"Api/Transcription/#transcription_1","text":"About transcription.py contains transliteration tables for Hebrew and Syriac that are being used in the BHSA , Peshitta , SyrNT , . It also calls functions to use these tables for converting Hebrew and Syriac ttext material to transliterated representations and back. There is also a phonetic transcription for Hebrew, designed in phono.ipynb Character tables Hebrew : full list of characters covered by the ETCBC and phonetic transcriptions Syriac : full list of characters covered by the ETCBC transcriptions Usage Invoke the transcription functionality as follows: 1 from tf.transcription import Transcription Some of the attributes and methods below are class attributes, others are instance attributes. A class attribute aaa can be retrieved by saying Transcription.aaa . To retrieve an instance attribute, you need an instance first, like 1 tr = Transcription () and then you can say tr.aaa . For each attribute we'll give a usage example. Transcription.hebrew_mapping Maps all ETCBC transliteration character combinations for Hebrew to Unicode. Example: print the sof-pasuq: 1 print ( Transcription . hebrew_mapping [ '00' ]) Output: 1 \u05c3 Transcription.syriac_mapping Maps all ETCBC transliteration character combinations for Syriac to Unicode. Example: print the semkath-final: 1 print ( Transcription . syriac_mapping [ 's' ]) Output: 1 \u0724 Transcription.suffix_and_finales(word) Given an ETCBC transliteration, split it into the word material and the interword material that follows it (space, punctuation). Replace the last consonant of the word material by its final form, if applicable. Output a tuple with the modified word material and the interword material. Example: 1 print ( Transcription . suffix_and_finales ( '71T_H@>@95REY00' )) Output: 1 ('71T_H@>@95REy', '00\\n') Note that the Y has been replaced by y . Transcription.suppress_space(word) Given an ETCBC transliteration of a word, match the end of the word for interpunction and spacing characters (sof pasuq, paseq, nun hafukha, setumah, petuhah, space, no-space) Example: 1 2 3 print ( Transcription . suppress_space ( 'B.:&' )) print ( Transcription . suppress_space ( 'B.@R@74>' )) print ( Transcription . suppress_space ( '71T_H@>@95REY00' )) Output: 1 2 3 <re.Match object; span=(3, 4), match='&'> None <re.Match object; span=(13, 15), match='00'> Transcription.to_etcbc_v(word) Given an ETCBC transliteration of a fully pointed word, strip all the non-vowel pointing (i.e. the accents). Example: 1 print ( Transcription . to_etcbc_v ( 'HAC.@MA73JIm' )) Output: 1 HAC.@MAJIm Transcription.to_etcbc_c(word) Given an ETCBC transliteration of a fully pointed word, strip everything except the consonants. Punctuation will also be stripped. Example: 1 print ( Transcription . to_etcbc_c ( 'HAC.@MA73JIm' )) Output: 1 H#MJM Note that the pointed shin ( C ) is replaced by an unpointed one ( # ). Transcription.to_hebrew(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Care will be taken that vowel pointing will be added to consonants before accent pointing. Example: 1 print ( Transcription . to_hebrew ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_x(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew. Vowel pointing and accent pointing will be applied in the order given by the input word. produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_x ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u0596\u05d9\u05b4\u05dd Transcription.to_hebrew_v(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the accents. Example: 1 print ( Transcription . to_hebrew_v ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05b7\ufb2a\u05bc\u05b8\u05de\u05b7\u05d9\u05b4\u05dd Transcription.to_hebrew_c(word) Given a transliteration of a fully pointed word, produce the word in Unicode Hebrew, but without the pointing. Example: 1 print ( Transcription . to_hebrew_c ( 'HAC.@MA73JIm' )) Output: 1 \u05d4\u05e9\u05de\u05d9\u05de Note that final consonant forms are not being used. Transcription.ph_simplify(pword) Given a phonological transliteration of a fully pointed word, produce a more coarse phonological transliteration. Example: 1 2 3 print ( Transcription . ph_simplify ( '\u0294\u1d49l\u014dh\u02c8\u00eem' )) print ( Transcription . ph_simplify ( 'm\u0101q\u02c8\u00f4m' )) print ( Transcription . ph_simplify ( 'kol' )) Output: 1 2 3 \u0294l\u014dh\u00eem m\u00e5q\u00f4m k\u00e5l Note that the simplified version transliterates the qamets gadol and qatan to the same character. tr.from_hebrew(word) Given a fully pointed word in Unicode Hebrew, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_hebrew ( '\u05d4\u05b8\u05d0\u05b8\u05bd\u05e8\u05b6\u05e5\u05c3' )) Output: 1 H@>@95REy00 tr.from_syriac(word) Given a word in Unicode Syriac, produce the word in ETCBC transliteration. Example: 1 print ( tr . from_syriac ( '\u0721\u071f\u0723\u071d\u0722' )) Output: 1 MKSJN tr.to_syriac(word) Given a word in ETCBC transliteration, produce the word in Unicode Syriac. Example: 1 print ( tr . to_syriac ( 'MKSJN' )) Output: 1 \u0721\u071f\u0723\u071d\u0722","title":"Transcription"},{"location":"Code/Overview/","text":"Code organisation \u00b6 The code base of Text-Fabric is evolving to a considerable size . However, the code can be divided into a few major parts, each with their own, identifiable task. Base \u00b6 The generic API ( stats ) of Text-Fabric is responsible for: Data management Text-Fabric data consists of feature files . TF must be able to load them, save them, import/export from MQL. Provide an API TF must offer an API for handling its data in applications. That means: feature lookup, containment lookup, text serialization. Precomputation In order to make its API work efficiently, TF has to precompute certain compiled forms of the data. Search \u00b6 TF contains a search engine ( stats ) based on templates, which are little graphs of nodes and edges that must be instantiated against the corpus. Search vs MQL The template language is inspired by MQL, but has a different syntax. It is both weaker and stronger than MQL. Search vs hand coding Search templates are the most accessible way to get at the data, easier than hand-coding your own little programs. The underlying engine is quite complicated. Sometimes it is faster than hand coding, sometimes (much) slower. Apps \u00b6 TF contains corpus-dependent apps ( stats ). Display An app knows how to display a particular corpus. Download An app knows how to download a particular corpus from its online repository. Web interface An app can set up a local web interface for a particular corpus. Web interface \u00b6 TF contains a local web interface ( stats ) for interacting with your corpus without programming. Working with your corpus The local web interface lets you fire queries (search templates) to TF and interact with the results: expanding rows to pretty displays; condensing results to verious container types; exporting results as PDF and CSV.","title":"Overview"},{"location":"Code/Overview/#code-organisation","text":"The code base of Text-Fabric is evolving to a considerable size . However, the code can be divided into a few major parts, each with their own, identifiable task.","title":"Code organisation"},{"location":"Code/Overview/#base","text":"The generic API ( stats ) of Text-Fabric is responsible for: Data management Text-Fabric data consists of feature files . TF must be able to load them, save them, import/export from MQL. Provide an API TF must offer an API for handling its data in applications. That means: feature lookup, containment lookup, text serialization. Precomputation In order to make its API work efficiently, TF has to precompute certain compiled forms of the data.","title":"Base"},{"location":"Code/Overview/#search","text":"TF contains a search engine ( stats ) based on templates, which are little graphs of nodes and edges that must be instantiated against the corpus. Search vs MQL The template language is inspired by MQL, but has a different syntax. It is both weaker and stronger than MQL. Search vs hand coding Search templates are the most accessible way to get at the data, easier than hand-coding your own little programs. The underlying engine is quite complicated. Sometimes it is faster than hand coding, sometimes (much) slower.","title":"Search"},{"location":"Code/Overview/#apps","text":"TF contains corpus-dependent apps ( stats ). Display An app knows how to display a particular corpus. Download An app knows how to download a particular corpus from its online repository. Web interface An app can set up a local web interface for a particular corpus.","title":"Apps"},{"location":"Code/Overview/#web-interface","text":"TF contains a local web interface ( stats ) for interacting with your corpus without programming. Working with your corpus The local web interface lets you fire queries (search templates) to TF and interact with the results: expanding rows to pretty displays; condensing results to verious container types; exporting results as PDF and CSV.","title":"Web interface"},{"location":"Code/Stats/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.88 s (83.8 files/s, 32149.6 lines/s) Language files blank comment code Python 37 1445 1752 10202 Markdown 29 2708 0 7271 CSS 3 1300 10 3098 JavaScript 2 13 1 269 IPython Notebook 1 0 0 247 YAML 1 4 0 63 Bourne Shell 1 0 0 1 -------- -------- -------- -------- -------- SUM: 74 5470 1763 21151","title":"Stats (overall)"},{"location":"Code/StatsApps/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.05 s (175.5 files/s, 73130.4 lines/s) Language files blank comment code Python 9 299 770 2682 -------- -------- -------- -------- -------- SUM: 9 299 770 2682","title":"Stats (TF-apps)"},{"location":"Code/StatsBase/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.06 s (239.8 files/s, 75014.8 lines/s) Language files blank comment code Python 14 479 371 3529 -------- -------- -------- -------- -------- SUM: 14 479 371 3529","title":"Stats (TF-base)"},{"location":"Code/StatsSearch/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.05 s (195.9 files/s, 78008.1 lines/s) Language files blank comment code Python 9 424 330 2829 -------- -------- -------- -------- -------- SUM: 9 424 330 2829","title":"Stats (TF-search)"},{"location":"Code/StatsServer/","text":"cloc github.com/AlDanial/cloc v 1.78 T=0.04 s (226.8 files/s, 136737.5 lines/s) Language files blank comment code CSS 3 1300 10 3098 Python 5 202 258 879 JavaScript 2 13 1 269 -------- -------- -------- -------- -------- SUM: 10 1515 269 4246","title":"Stats (TF-server)"},{"location":"Create/CreateTF/","text":"Create a TF dataset \u00b6 We describe a conversion of an example text to Text-Fabric. This is not meant as a recipe, but as a description of the pieces of information that have to be assembled from the source text, and how to compose that into a Text-Fabric resource, which is a set of features. How you turn this insight into an executable program is dependent on how the source text is encoded and organized. Have a look at the following examples: the book of John in the Greek New Testament from XML-TEI to TF: pilot ; the Syriac New Testament: linksyr ; the Greek New Testament: tfFromSblgnt ; Sanskrit writings: tfFromSanskrit ; Sanskrit writings: tfFromSumerianTEI . Analysis \u00b6 The text is a string with a little bit of structure. Some of that structure gives us our node types, and other bits give us the features. Node types \u00b6 The text is divided into main sections, subsections, paragraphs, and sentences. The sentences are divided into words by white-space and/or punctuation. Step 1: define slots \u00b6 Make a copy of the text, strip out all headings and split the string on white-space. We get a sequence of \"words\". These words may contain punctuation or other non-alphabetical signs. We do not care for the moment. The indexes in this sequence, from 1 till the number of \"words\", are our slots. Let's say we have S of them. We start constructing a mapping from numbers to node types, called otype . We assign to numbers 1, ... , S the string word . That means, we have now S nodes, all of type word . Step 2: add higher level nodes \u00b6 For each level of section , subsection and paragraph , make new nodes. Nodes are numbers, and we start making new nodes directly after S . We have 4 main sections, so we extend the otype mapping as follows: S+1 ~ section S+2 ~ section S+3 ~ section S+4 ~ section Likewise, we have 11 subsections, so we continue extending: S+5 ~ subsection S+6 ~ subsection ... S+16 ~ subsection We do the same for paragraph . And after that, we break the paragraphs up into sentences (split on . ), and we add so many nodes of type sentence . The mapping otype is called a feature of nodes. Any mapping that assigns values to nodes, is called a (node-)feature. Containment \u00b6 We also have to record which words belong to which nodes. This information takes the shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we have to lists pairs of nodes (n, w) such that the word w \"belongs\" to the node n . This is in fact a set of edges (pairs of nodes are edges), and a set of edges is an edge feature . In general it is possible to assign values to pairs of nodes, but for our containment information we just assign the empty value to every pair we have in our set. The edge feature that records the containment of words in nodes, is called oslots . Step 3: map nodes to sets of words \u00b6 For each of the higher level nodes n (the ones beyond S ) we have to lookup/remember/compute which words w belong to it, and put that in the oslots mapping: S+1 ~ { 1, 2, 3, ... x, ..., y } S+2 ~ { y+1, y+2, ... } ... S+5 ~ { 1, 2, 3, ... x } S+6 ~ { x+1, x+2, ...} ... Features \u00b6 Now we have two features, a node feature otype and an edge feature oslots . This is merely the skeleton of our text, the warp so to speak. It contains the textual positions, and the information what the meaningful chunks are. Now it is time to weave the information in. Step 4: the actual text \u00b6 Remember the words with punctuation attached? We can split every word into three parts: text: the alphabetical characters in between prefix: the non-alphabetical leading characters suffix: the non-alphabetical trailing characters We can make three node features, prefix , text , and suffix . Remember that node features are mappings from numbers to values. Here we go: prefix[1] is the prefix of word 1 suffix[1] is the suffix of word 1 text[1] is the text of word 1 ... And so for all words. Step 5: more features \u00b6 For the sections and subsections we can make a feature heading , in which we store the headings of those sections. heading[S+1] is Introduction heading[S+5] is Basic concepts heading[S+16] is Identity ... For paragraphs we can figure out their sequence number within the subsection, and store that in a feature number : number[p] is 1 if p is the node corresponding to the first paragraph in a subsection. If you want absolute paragraph numbers, you can just add a feature for that: abs_number[p] is 23 if p is the node corresponding to the 23th paragraph in the corpus. Metadata \u00b6 You can supply metadata to all node features and edge features. Metadata must be given as a dictionary, where the keys are the names of the features in your dataset, and the values are themselves key-value pairs, where the values are just strings. You can mention where the source data comes from, who did the conversion, and you can give a description of the intention of this feature and the shape of its values. Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated key-value. You can also supply metadata for '' (the empty key). These key-values will be added to all other features. Here you can put stuff that pertains to the dataset as a whole, such as information about decisions that have been taken. You should also provide some special metadata to the key otext . This feature has no data, only metadata. It is not a node feature, not an edge feature, but a config feature. otext is responsible for sectioning and text representation. If you specify otext well, the T-API can make use of it, so that you have convenient, generic functions to get at your sections and to serialize your text in different formats. Step 6: sectioning metadata \u00b6 sectionTypes: 'section,subsection,paragraph' sectionFeatures: 'title,title,number' This tells Text-Fabric that node type section corresponds to section level 1, subsection to level 2, and paragraph to level 3. Moreover, Text-Fabric knows that the heading of sections at level 1 and 2 are in the feature title , and that the heading at level 3 is in the feature number . Step 7: text formats \u00b6 fmt:text-orig-plain: '{prefix}{text}{suffix}' fmt:text-orig-bare: '{text} ' fmt:text-orig-angle: ' <{text}> ' Here you have provided a bunch of text representation formats to Text-Fabric. The names of those formats are up to you, and the values as well. If you have a list of word nodes, say ws , then a user of your corpus can ask Text-Fabric: 1 T . text ( ws , fmt = 'text-orig-plain' ) This will spit out the full textual representation of those words, including the non-alphabetical stuff in their prefixes and suffixes. The second format, text-orig-bare , will leave prefix and suffix out. And if for whatever reason you need to wrap each word in angle brackets, you can achieve that with text-orig-angle . As an example of how text formats come in handy, have a look at the text formats that have been designed for Hebrew: 1 2 3 4 5 6 7 8 9 10 fmt:lex-orig-full: '{g_lex_utf8} ' fmt:lex-orig-plain: '{lex_utf8} ' fmt:lex-trans-full: '{g_lex} ' fmt:lex-trans-plain: '{lex0} ' fmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}' fmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}' fmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}' fmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}' fmt:text-trans-full-ketiv: '{g_word}{trailer}' fmt:text-trans-plain: '{g_cons}{trailer}' Note that the actual text-formats are not baked in into TF, but are supplied by you, the corpus designer. Writing out TF \u00b6 Once you have assembled your features and metadata as data structures in memory, you can use TF.save() to write out your data as a bunch of Text-Fabric files. Step 8: invoke TF.save() \u00b6 The call to make is 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Here you supply for nodeFeatures a dictionary keyed by your node feature names and valued by the feature data of those features. Likewise for the edge features. And the metadata you have composed goes into the metaData parameter. Finally, the module parameter dictates where on your system the TF-files will be written. First time usage \u00b6 When you start using your new dataset in Text-Fabric, you'll notice that there is some upfront computation going on. Text-Fabric computes derived data, especially about the relationships between nodes based on the slots they occupy. All that information comes from oslots . The oslots information is very terse, and using it directly would result in a hefty performance penalty. Likewise, all feature data will be read from the textual .tf files, represented in memory as a dictionary, and then that dictionary will be serialized and gzipped into a .tfx file in a hidden directory .tf . These .tfx files load an order of magnitude faster than the original .tf files. Text-Fabric uses the timestamps of the files to determine whether the .tfx files are outdated and need to be regenerated again. This whole machinery is invisible to you, the user, except for the delay at first time use. Enriching your corpus \u00b6 Maybe a linguistic friend of yours has a tool to determine the part of speech of each word in the text. Using TF itself it is not that hard to create a new feature pos , that maps each word node to the part of speech of that word. See for example how Cody Kingham adds the notion of linguistic head to the BHSA datasource of the Hebrew Bible. Step 9: add the new feature \u00b6 Once you have the feature pos , provide a bit of metadata, and call 1 2 3 4 5 TF . save ( nodeFeatures = { 'pos' : posData }, metaData = { 'pos' : posMetaData }, module = 'linguistics' , ) You get a TF module consisting of one feature pos.tf in the linguistics directory. Maybe you have more linguistic features to add. You do not have to create those features alongside the original corpus. It is perfectly possible to leave the corpus alone in its own GitHub repo, and write your new features in another repo. Users can just obtain the corpus and your linguistic module separately. When they call their Text-Fabric, they can point it to both locations, and Text-Fabric treats it as one dataset. Step 10: use the new feature \u00b6 The call to TF=Fabric() looks like this 1 TF = Fabric ( locations = [ corpusLocation , moduleLocation ]) All feature files found at these locations are loadable in your session.","title":"Make TF"},{"location":"Create/CreateTF/#create-a-tf-dataset","text":"We describe a conversion of an example text to Text-Fabric. This is not meant as a recipe, but as a description of the pieces of information that have to be assembled from the source text, and how to compose that into a Text-Fabric resource, which is a set of features. How you turn this insight into an executable program is dependent on how the source text is encoded and organized. Have a look at the following examples: the book of John in the Greek New Testament from XML-TEI to TF: pilot ; the Syriac New Testament: linksyr ; the Greek New Testament: tfFromSblgnt ; Sanskrit writings: tfFromSanskrit ; Sanskrit writings: tfFromSumerianTEI .","title":"Create a TF dataset"},{"location":"Create/CreateTF/#analysis","text":"The text is a string with a little bit of structure. Some of that structure gives us our node types, and other bits give us the features.","title":"Analysis"},{"location":"Create/CreateTF/#node-types","text":"The text is divided into main sections, subsections, paragraphs, and sentences. The sentences are divided into words by white-space and/or punctuation.","title":"Node types"},{"location":"Create/CreateTF/#step-1-define-slots","text":"Make a copy of the text, strip out all headings and split the string on white-space. We get a sequence of \"words\". These words may contain punctuation or other non-alphabetical signs. We do not care for the moment. The indexes in this sequence, from 1 till the number of \"words\", are our slots. Let's say we have S of them. We start constructing a mapping from numbers to node types, called otype . We assign to numbers 1, ... , S the string word . That means, we have now S nodes, all of type word .","title":"Step 1: define slots"},{"location":"Create/CreateTF/#step-2-add-higher-level-nodes","text":"For each level of section , subsection and paragraph , make new nodes. Nodes are numbers, and we start making new nodes directly after S . We have 4 main sections, so we extend the otype mapping as follows: S+1 ~ section S+2 ~ section S+3 ~ section S+4 ~ section Likewise, we have 11 subsections, so we continue extending: S+5 ~ subsection S+6 ~ subsection ... S+16 ~ subsection We do the same for paragraph . And after that, we break the paragraphs up into sentences (split on . ), and we add so many nodes of type sentence . The mapping otype is called a feature of nodes. Any mapping that assigns values to nodes, is called a (node-)feature.","title":"Step 2: add higher level nodes"},{"location":"Create/CreateTF/#containment","text":"We also have to record which words belong to which nodes. This information takes the shape of a mapping of nodes to sets of nodes. Or, with a slight twist, we have to lists pairs of nodes (n, w) such that the word w \"belongs\" to the node n . This is in fact a set of edges (pairs of nodes are edges), and a set of edges is an edge feature . In general it is possible to assign values to pairs of nodes, but for our containment information we just assign the empty value to every pair we have in our set. The edge feature that records the containment of words in nodes, is called oslots .","title":"Containment"},{"location":"Create/CreateTF/#step-3-map-nodes-to-sets-of-words","text":"For each of the higher level nodes n (the ones beyond S ) we have to lookup/remember/compute which words w belong to it, and put that in the oslots mapping: S+1 ~ { 1, 2, 3, ... x, ..., y } S+2 ~ { y+1, y+2, ... } ... S+5 ~ { 1, 2, 3, ... x } S+6 ~ { x+1, x+2, ...} ...","title":"Step 3: map nodes to sets of words"},{"location":"Create/CreateTF/#features","text":"Now we have two features, a node feature otype and an edge feature oslots . This is merely the skeleton of our text, the warp so to speak. It contains the textual positions, and the information what the meaningful chunks are. Now it is time to weave the information in.","title":"Features"},{"location":"Create/CreateTF/#step-4-the-actual-text","text":"Remember the words with punctuation attached? We can split every word into three parts: text: the alphabetical characters in between prefix: the non-alphabetical leading characters suffix: the non-alphabetical trailing characters We can make three node features, prefix , text , and suffix . Remember that node features are mappings from numbers to values. Here we go: prefix[1] is the prefix of word 1 suffix[1] is the suffix of word 1 text[1] is the text of word 1 ... And so for all words.","title":"Step 4: the actual text"},{"location":"Create/CreateTF/#step-5-more-features","text":"For the sections and subsections we can make a feature heading , in which we store the headings of those sections. heading[S+1] is Introduction heading[S+5] is Basic concepts heading[S+16] is Identity ... For paragraphs we can figure out their sequence number within the subsection, and store that in a feature number : number[p] is 1 if p is the node corresponding to the first paragraph in a subsection. If you want absolute paragraph numbers, you can just add a feature for that: abs_number[p] is 23 if p is the node corresponding to the 23th paragraph in the corpus.","title":"Step 5: more features"},{"location":"Create/CreateTF/#metadata","text":"You can supply metadata to all node features and edge features. Metadata must be given as a dictionary, where the keys are the names of the features in your dataset, and the values are themselves key-value pairs, where the values are just strings. You can mention where the source data comes from, who did the conversion, and you can give a description of the intention of this feature and the shape of its values. Later, when you save the whole dataset as TF, Text-Fabric will insert a datecreated key-value. You can also supply metadata for '' (the empty key). These key-values will be added to all other features. Here you can put stuff that pertains to the dataset as a whole, such as information about decisions that have been taken. You should also provide some special metadata to the key otext . This feature has no data, only metadata. It is not a node feature, not an edge feature, but a config feature. otext is responsible for sectioning and text representation. If you specify otext well, the T-API can make use of it, so that you have convenient, generic functions to get at your sections and to serialize your text in different formats.","title":"Metadata"},{"location":"Create/CreateTF/#step-6-sectioning-metadata","text":"sectionTypes: 'section,subsection,paragraph' sectionFeatures: 'title,title,number' This tells Text-Fabric that node type section corresponds to section level 1, subsection to level 2, and paragraph to level 3. Moreover, Text-Fabric knows that the heading of sections at level 1 and 2 are in the feature title , and that the heading at level 3 is in the feature number .","title":"Step 6: sectioning metadata"},{"location":"Create/CreateTF/#step-7-text-formats","text":"fmt:text-orig-plain: '{prefix}{text}{suffix}' fmt:text-orig-bare: '{text} ' fmt:text-orig-angle: ' <{text}> ' Here you have provided a bunch of text representation formats to Text-Fabric. The names of those formats are up to you, and the values as well. If you have a list of word nodes, say ws , then a user of your corpus can ask Text-Fabric: 1 T . text ( ws , fmt = 'text-orig-plain' ) This will spit out the full textual representation of those words, including the non-alphabetical stuff in their prefixes and suffixes. The second format, text-orig-bare , will leave prefix and suffix out. And if for whatever reason you need to wrap each word in angle brackets, you can achieve that with text-orig-angle . As an example of how text formats come in handy, have a look at the text formats that have been designed for Hebrew: 1 2 3 4 5 6 7 8 9 10 fmt:lex-orig-full: '{g_lex_utf8} ' fmt:lex-orig-plain: '{lex_utf8} ' fmt:lex-trans-full: '{g_lex} ' fmt:lex-trans-plain: '{lex0} ' fmt:text-orig-full: '{qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}' fmt:text-orig-full-ketiv: '{g_word_utf8}{trailer_utf8}' fmt:text-orig-plain: '{g_cons_utf8}{trailer_utf8}' fmt:text-trans-full: '{qere/g_word}{qere_trailer/trailer}' fmt:text-trans-full-ketiv: '{g_word}{trailer}' fmt:text-trans-plain: '{g_cons}{trailer}' Note that the actual text-formats are not baked in into TF, but are supplied by you, the corpus designer.","title":"Step 7: text formats"},{"location":"Create/CreateTF/#writing-out-tf","text":"Once you have assembled your features and metadata as data structures in memory, you can use TF.save() to write out your data as a bunch of Text-Fabric files.","title":"Writing out TF"},{"location":"Create/CreateTF/#step-8-invoke-tfsave","text":"The call to make is 1 TF . save ( nodeFeatures = {}, edgeFeatures = {}, metaData = {}, module = None ) Here you supply for nodeFeatures a dictionary keyed by your node feature names and valued by the feature data of those features. Likewise for the edge features. And the metadata you have composed goes into the metaData parameter. Finally, the module parameter dictates where on your system the TF-files will be written.","title":"Step 8: invoke TF.save()"},{"location":"Create/CreateTF/#first-time-usage","text":"When you start using your new dataset in Text-Fabric, you'll notice that there is some upfront computation going on. Text-Fabric computes derived data, especially about the relationships between nodes based on the slots they occupy. All that information comes from oslots . The oslots information is very terse, and using it directly would result in a hefty performance penalty. Likewise, all feature data will be read from the textual .tf files, represented in memory as a dictionary, and then that dictionary will be serialized and gzipped into a .tfx file in a hidden directory .tf . These .tfx files load an order of magnitude faster than the original .tf files. Text-Fabric uses the timestamps of the files to determine whether the .tfx files are outdated and need to be regenerated again. This whole machinery is invisible to you, the user, except for the delay at first time use.","title":"First time usage"},{"location":"Create/CreateTF/#enriching-your-corpus","text":"Maybe a linguistic friend of yours has a tool to determine the part of speech of each word in the text. Using TF itself it is not that hard to create a new feature pos , that maps each word node to the part of speech of that word. See for example how Cody Kingham adds the notion of linguistic head to the BHSA datasource of the Hebrew Bible.","title":"Enriching your corpus"},{"location":"Create/CreateTF/#step-9-add-the-new-feature","text":"Once you have the feature pos , provide a bit of metadata, and call 1 2 3 4 5 TF . save ( nodeFeatures = { 'pos' : posData }, metaData = { 'pos' : posMetaData }, module = 'linguistics' , ) You get a TF module consisting of one feature pos.tf in the linguistics directory. Maybe you have more linguistic features to add. You do not have to create those features alongside the original corpus. It is perfectly possible to leave the corpus alone in its own GitHub repo, and write your new features in another repo. Users can just obtain the corpus and your linguistic module separately. When they call their Text-Fabric, they can point it to both locations, and Text-Fabric treats it as one dataset.","title":"Step 9: add the new feature"},{"location":"Create/CreateTF/#step-10-use-the-new-feature","text":"The call to TF=Fabric() looks like this 1 TF = Fabric ( locations = [ corpusLocation , moduleLocation ]) All feature files found at these locations are loadable in your session.","title":"Step 10: use the new feature"},{"location":"Create/ExampleText/","text":"Introduction \u00b6 The Social Construction of Reality is a 1966 book about the sociology of knowledge by the sociologists Peter L. Berger and Thomas Luckmann. Berger and Luckmann introduced the term \"social construction\" into the social sciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their central concept is that people and groups interacting in a social system create, over time, concepts or mental representations of each other's actions, and that these concepts eventually become habituated into reciprocal roles played by the actors in relation to each other. When these roles are made available to other members of society to enter into and play out, the reciprocal interactions are said to be institutionalized. In the process, meaning is embedded in society. Knowledge and people's conceptions (and beliefs) of what reality is become embedded in the institutional fabric of society. Reality is therefore said to be socially constructed. In 1998 the International Sociological Association listed The Social Construction of Reality as the fifth-most important sociological book of the 20th century Basic concepts \u00b6 Social stock of knowledge \u00b6 Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max Weber, etc.) often focused too much on scientific and theoretical knowledge, but this is only a small part of social knowledge, concerning a very limited group. Customs, common interpretations, institutions, shared routines, habitualizations, the who-is-who and who-does-what in social processes and the division of labor, constitute a much larger part of knowledge in society. \u201c\u2026theoretical knowledge is only a small and by no means the most important part of what passed for knowledge in a society\u2026 the primary knowledge about the institutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019 about a social world, an assemblage of maxims, morals, proverbial nuggets of wisdom, values and beliefs, myths, and so forth\u201d (p.65) Semantic fields \u00b6 The general body of knowledge is socially distributed, and classified in semantic fields. The dynamic distribution and inter dependencies of these knowledge sectors provide structure to the social stock of knowledge: \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026 my knowledge of my own occupation and its world is very rich and specific, while I have only very sketchy knowledge of the occupational worlds of others\u201d (p.43) \u201cThe social distribution of knowledge thus begins with the simple fact that I do not know everything known to my fellowmen, and vice versa, and culminates in exceedingly complex and esoteric systems of expertise. Knowledge of how the socially available stock of knowledge is distributed, at least in outline, is an important element of that same stock of knowledge.\u201d (p.46) Language and signs \u00b6 Language also plays an important role in the analysis of integration of everyday reality. Language links up commonsense knowledge with finite provinces of meaning, thus enabling people, for example, to interpret dreams through understandings relevant in the daytime. \"Language is capable of transcending the reality of everyday life altogether. It can refer to experiences pertaining to finite provinces of meaning, it can span discrete spheres of reality...Language soars into regions that are not only de facto but also a priori unavailable to everyday experience.\"p. 40. Regarding the function of language and signs, Berger and Luckmann are indebted to George Herbert Mead and other figures in the field known as symbolic interactionism, as acknowledged in their Introduction, especially regarding the possibility of constructing objectivity. Signs and language provide interoperability for the construction of everyday reality: \u201cA sign [has the] explicit intention to serve as an index of subjective meanings \u2026 Language is capable of becoming the objective repository of vast accumulations of meaning and experience, which it can then preserve in time and transmit to following generations\u2026 Language also typifies experiences, allowing me to subsume them under broad categories in terms of which they have meaning not only to myself but also to my fellowmen\u201d (p.35-39) Social everyday reality \u00b6 Social everyday reality is characterized by Intersubjectivity (which refers to the coexistence of multiple realities in this context)(p. 23-25): \u201cCompared to the reality of everyday life, other realities appear as finite provinces of meaning, enclaves within the paramount reality marked by circumscribed meanings and modes of experience\u201d (p.25) This is in contrast to other realities, such as dreams, theoretical constructs, religious or mystic beliefs, artistic and imaginary worlds, etc. While individuals may visit other realities (such as watching a film), they are always brought back to everyday reality (once the film ends)(p. 25). Society as objective reality \u00b6 \u201c Social order is a human product, or more precisely, an ongoing human production \u201d Institutionalization[edit] Institutionalization of social processes grows out of the habitualization and customs, gained through mutual observation with subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces uncertainty and danger and allows our limited attention span to focus on more things at the same time, while institutionalized routines can be expected to continue \u201cas previously agreed\u201d: \u201cHabitualization carries with it the important psychological gain that choices are narrowed\u2026 the background of habitualized activity opens up a foreground for deliberation and innovation [which demand a higher level of attention]\u2026 The most important gain is that each [member of society] will be able to predict the other\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026 Many actions are possible on a low level of attention. Each action of one is no longer a source of astonishment and potential danger to the other\u201c (p.53-57). Social objective worlds \u00b6 Social (or institutional) objective worlds are one consequence of institutionalization, and are created when institutions are passed on to a new generation. This creates a reality that is vulnerable to the ideas of a minority which will then form the basis of social expectations in the future. The underlying reasoning is fully transparent to the creators of an institution, as they can reconstruct the circumstances under which they made agreements; while the second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and \u201cself-evident\u201d and they might not understand the underlying logic. \u201c\u2026a social world [is] a comprehensive and given reality confronting the individual in a manner analogous to the reality of the natural world\u2026 In early phases of socialization the child is quite incapable of distinguishing between the objectivity of natural phenomena and the objectivity of the social formations\u2026 The objective reality of institutions is not diminished if the individual does not understand their purpose or their mode of operation\u2026 He must \u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61) Division of labor \u00b6 Division of labor is another consequence of institutionalization. Institutions assign \u201croles\u201d to be performed by various actors, through typification of performances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As specialization increases in number as well as in size and sophistication, a civilization's culture contains more and more sections of knowledge specific to given roles or tasks, sections which become more and more esoteric to non-specialists. These areas of knowledge do not belong anymore to the common social world and culture. \u201cA society\u2019s stock of knowledge is structured in terms of what is generally relevant and what is relevant only to specific roles\u2026 the social distribution of knowledge entails a dichotomization in terms of general and role-specific relevance\u2026 because of the division of labor, role-specific knowledge will grow at a faster rate than generally relevant and accessible knowledge\u2026 The increasing number and complexity of [the resulting] sub universes [of specialized knowledge] make them increasingly inaccessible to outsiders (p.77-87) Symbolic universes \u00b6 Symbolic universes are created to provide legitimation to the created institutional structure. Symbolic universes are a set of beliefs \u201ceverybody knows\u201d that aim at making the institutionalized structure plausible and acceptable for the individual\u2014who might otherwise not understand or agree with the underlying logic of the institution. As an ideological system, the symbolic universe \u201cputs everything in its right place\u201d. It provides explanations for why we do things the way we do. Proverbs, moral maxims, wise sayings, mythology, religions and other theological thought, metaphysical traditions and other value systems are part of the symbolic universe. They are all (more or less sophisticated) ways to legitimize established institutions. \u201cThe function of legitimation is to make objectively available and subjectively plausible the \u2018first-order\u2019 objections that have been institutionalized\u2026 Proverbs, moral maxims and wise sayings are common on this level\u2026 [as well as] explicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a general theory of man\u2026 The symbolic universe also orders history. It locates all collective events in a cohesive unity that includes past, present and future.\u201d (p. 92-104) Universe-maintenance \u00b6 Universe-maintenance refers to specific procedures undertaken, often by an elite group, when the symbolic universe does not fulfill its purpose anymore, which is to legitimize the institutional structure in place. This happens, for example, in generational shifts, or when deviants create an internal movement against established institutions (e.g. against revolutions), or when a society is confronted with another society with a greatly different history and institutional structures. In primitive societies this happened through mythological systems, later on through theological thought. Today, an extremely complex set of science has secularized universe-maintenance. \u201cSpecific procedures of universe-maintenance become necessary when the symbolic universe has become a problem. As long as this is not the case, the symbolic universe is self-maintaining, that is self-legitimating. An intrinsic problem presents itself with the process of transmission of the symbolic universe from one generation to another\u2026 [additionally] two societies confronting each other with conflicting universes will both develop conceptual machinery designed to maintain their respective universes\u2026 mythology represents the most archaic form of universe-maintenance\u2026 theological thought may be distinguished from its mythological predecessor simply in terms of its greater degree of theoretical systematization\u2026 Modern science is an extreme step in this development. (p.104-116) Society as subjective reality \u00b6 Socialization \u00b6 Socialization is a two-step induction of the individual to participate in the social institutional structure, meaning in its objective reality. \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of society. In the life of every individual\u2026 there is a temporal sequence, in the course of which he is inducted into participation in the social dialectic\" (p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high degree of symmetry between objective and subjective reality\u201d (p. 163) Primary Socialization takes place as a child. It is highly charged emotionally and is not questioned. Secondary Socialization includes the acquisition of role-specific knowledge, thus taking one\u2019s place in the social division of labor. It is learned through training and specific rituals, and is not emotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s teacher\u201d. Training for secondary socialization can be very complex and depends on the complexity of division of labor in a society. Primary socialization is much less flexible than secondary socialization. E.g. shame for nudity comes from primary socialization, adequate dress code depends on secondary: A relatively minor shift in the subjective definition of reality would suffice for an individual to take for granted that one may go to the office without a tie. A much more drastic shift would be necessary to have him go, as a matter of course, without any clothes at all. \u201cThe child does not internalize the world of his significant others as one of many possible worlds\u2026 It is for this reason that the world internalized in primary socialization is so much more firmly entrenched in consciousness than worlds internalized in secondary socialization\u2026. Secondary socialization is the internalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of secondary socialization carry a high degree of anonymity\u2026 The same knowledge taught by one teacher could also be taught by another\u2026 The institutional distribution of tasks between primary and secondary socialization varies with the complexity of the social distribution of knowledge\u201d (p. 129-147) Conversation \u00b6 Conversation or verbal communication aims at reality-maintenance of the subjective reality. What seems to be a useless and unnecessary communication of redundant banalities is actually a constant mutual reconfirmation of each other's internal thoughts, in that it maintains subjective reality. \u201cOne may view the individual\u2019s everyday life in terms of the working away of a conversational apparatus that ongoingly maintains, modifies and reconstructs his subjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the station,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire world within which these apparently simple propositions make sense\u2026 the exchange confirms the subjective reality of this world\u2026 the great part, if not all, of everyday conversation maintains subjective reality\u2026 imagine the effect\u2026of an exchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine, darling, don\u2019t forget to take along your gun.\u2019 (p. 147-163) Identity \u00b6 Identity of an individual is subject to a struggle of affiliation to sometimes conflicting realities. For example, the reality from primary socialization (mother tells child not to steal) can be in contrast with second socialization (gang members teach teenager that stealing is cool). Our final social location in the institutional structure of society will ultimately also influence our body and organism. \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines how long and in what manner the individual organism shall live\u2026 Society also directly penetrates the organism in its functioning, most importantly in respect to sexuality and nutrition. While both sexuality and nutrition are grounded in biological drives\u2026 biological constitution does not tell him where he should seek sexual release and what he should eat.\u201d (p. 163-183)","title":"Example data"},{"location":"Create/ExampleText/#introduction","text":"The Social Construction of Reality is a 1966 book about the sociology of knowledge by the sociologists Peter L. Berger and Thomas Luckmann. Berger and Luckmann introduced the term \"social construction\" into the social sciences and were strongly influenced by the work of Alfred Sch\u00fctz. Their central concept is that people and groups interacting in a social system create, over time, concepts or mental representations of each other's actions, and that these concepts eventually become habituated into reciprocal roles played by the actors in relation to each other. When these roles are made available to other members of society to enter into and play out, the reciprocal interactions are said to be institutionalized. In the process, meaning is embedded in society. Knowledge and people's conceptions (and beliefs) of what reality is become embedded in the institutional fabric of society. Reality is therefore said to be socially constructed. In 1998 the International Sociological Association listed The Social Construction of Reality as the fifth-most important sociological book of the 20th century","title":"Introduction"},{"location":"Create/ExampleText/#basic-concepts","text":"","title":"Basic concepts"},{"location":"Create/ExampleText/#social-stock-of-knowledge","text":"Earlier theories (Max Scheler, Karl Mannheim, Werner Stark, Karl Marx, Max Weber, etc.) often focused too much on scientific and theoretical knowledge, but this is only a small part of social knowledge, concerning a very limited group. Customs, common interpretations, institutions, shared routines, habitualizations, the who-is-who and who-does-what in social processes and the division of labor, constitute a much larger part of knowledge in society. \u201c\u2026theoretical knowledge is only a small and by no means the most important part of what passed for knowledge in a society\u2026 the primary knowledge about the institutional order is knowledge\u2026 is the sum total of \u2018what everybody knows\u2019 about a social world, an assemblage of maxims, morals, proverbial nuggets of wisdom, values and beliefs, myths, and so forth\u201d (p.65)","title":"Social stock of knowledge"},{"location":"Create/ExampleText/#semantic-fields","text":"The general body of knowledge is socially distributed, and classified in semantic fields. The dynamic distribution and inter dependencies of these knowledge sectors provide structure to the social stock of knowledge: \u201cThe social stock of knowledge differentiates reality by degrees of familiarity\u2026 my knowledge of my own occupation and its world is very rich and specific, while I have only very sketchy knowledge of the occupational worlds of others\u201d (p.43) \u201cThe social distribution of knowledge thus begins with the simple fact that I do not know everything known to my fellowmen, and vice versa, and culminates in exceedingly complex and esoteric systems of expertise. Knowledge of how the socially available stock of knowledge is distributed, at least in outline, is an important element of that same stock of knowledge.\u201d (p.46)","title":"Semantic fields"},{"location":"Create/ExampleText/#language-and-signs","text":"Language also plays an important role in the analysis of integration of everyday reality. Language links up commonsense knowledge with finite provinces of meaning, thus enabling people, for example, to interpret dreams through understandings relevant in the daytime. \"Language is capable of transcending the reality of everyday life altogether. It can refer to experiences pertaining to finite provinces of meaning, it can span discrete spheres of reality...Language soars into regions that are not only de facto but also a priori unavailable to everyday experience.\"p. 40. Regarding the function of language and signs, Berger and Luckmann are indebted to George Herbert Mead and other figures in the field known as symbolic interactionism, as acknowledged in their Introduction, especially regarding the possibility of constructing objectivity. Signs and language provide interoperability for the construction of everyday reality: \u201cA sign [has the] explicit intention to serve as an index of subjective meanings \u2026 Language is capable of becoming the objective repository of vast accumulations of meaning and experience, which it can then preserve in time and transmit to following generations\u2026 Language also typifies experiences, allowing me to subsume them under broad categories in terms of which they have meaning not only to myself but also to my fellowmen\u201d (p.35-39)","title":"Language and signs"},{"location":"Create/ExampleText/#social-everyday-reality","text":"Social everyday reality is characterized by Intersubjectivity (which refers to the coexistence of multiple realities in this context)(p. 23-25): \u201cCompared to the reality of everyday life, other realities appear as finite provinces of meaning, enclaves within the paramount reality marked by circumscribed meanings and modes of experience\u201d (p.25) This is in contrast to other realities, such as dreams, theoretical constructs, religious or mystic beliefs, artistic and imaginary worlds, etc. While individuals may visit other realities (such as watching a film), they are always brought back to everyday reality (once the film ends)(p. 25).","title":"Social everyday reality"},{"location":"Create/ExampleText/#society-as-objective-reality","text":"\u201c Social order is a human product, or more precisely, an ongoing human production \u201d Institutionalization[edit] Institutionalization of social processes grows out of the habitualization and customs, gained through mutual observation with subsequent mutual agreement on the \u201cway of doing things\u201d. This reduces uncertainty and danger and allows our limited attention span to focus on more things at the same time, while institutionalized routines can be expected to continue \u201cas previously agreed\u201d: \u201cHabitualization carries with it the important psychological gain that choices are narrowed\u2026 the background of habitualized activity opens up a foreground for deliberation and innovation [which demand a higher level of attention]\u2026 The most important gain is that each [member of society] will be able to predict the other\u2019s actions. Concomitantly, the interaction of both becomes predictable\u2026 Many actions are possible on a low level of attention. Each action of one is no longer a source of astonishment and potential danger to the other\u201c (p.53-57).","title":"Society as objective reality"},{"location":"Create/ExampleText/#social-objective-worlds","text":"Social (or institutional) objective worlds are one consequence of institutionalization, and are created when institutions are passed on to a new generation. This creates a reality that is vulnerable to the ideas of a minority which will then form the basis of social expectations in the future. The underlying reasoning is fully transparent to the creators of an institution, as they can reconstruct the circumstances under which they made agreements; while the second generation inherits it as something \u201cgiven\u201d, \u201cunalterable\u201d and \u201cself-evident\u201d and they might not understand the underlying logic. \u201c\u2026a social world [is] a comprehensive and given reality confronting the individual in a manner analogous to the reality of the natural world\u2026 In early phases of socialization the child is quite incapable of distinguishing between the objectivity of natural phenomena and the objectivity of the social formations\u2026 The objective reality of institutions is not diminished if the individual does not understand their purpose or their mode of operation\u2026 He must \u2018go out\u2019 and learn about them, just as he must learn about nature\u2026 (p.59-61)","title":"Social objective worlds"},{"location":"Create/ExampleText/#division-of-labor","text":"Division of labor is another consequence of institutionalization. Institutions assign \u201croles\u201d to be performed by various actors, through typification of performances, such as \u201cfather-role\u201d, \u201cteacher-role\u201d, \u201chunter\u201d, \u201ccook\u201d, etc. As specialization increases in number as well as in size and sophistication, a civilization's culture contains more and more sections of knowledge specific to given roles or tasks, sections which become more and more esoteric to non-specialists. These areas of knowledge do not belong anymore to the common social world and culture. \u201cA society\u2019s stock of knowledge is structured in terms of what is generally relevant and what is relevant only to specific roles\u2026 the social distribution of knowledge entails a dichotomization in terms of general and role-specific relevance\u2026 because of the division of labor, role-specific knowledge will grow at a faster rate than generally relevant and accessible knowledge\u2026 The increasing number and complexity of [the resulting] sub universes [of specialized knowledge] make them increasingly inaccessible to outsiders (p.77-87)","title":"Division of labor"},{"location":"Create/ExampleText/#symbolic-universes","text":"Symbolic universes are created to provide legitimation to the created institutional structure. Symbolic universes are a set of beliefs \u201ceverybody knows\u201d that aim at making the institutionalized structure plausible and acceptable for the individual\u2014who might otherwise not understand or agree with the underlying logic of the institution. As an ideological system, the symbolic universe \u201cputs everything in its right place\u201d. It provides explanations for why we do things the way we do. Proverbs, moral maxims, wise sayings, mythology, religions and other theological thought, metaphysical traditions and other value systems are part of the symbolic universe. They are all (more or less sophisticated) ways to legitimize established institutions. \u201cThe function of legitimation is to make objectively available and subjectively plausible the \u2018first-order\u2019 objections that have been institutionalized\u2026 Proverbs, moral maxims and wise sayings are common on this level\u2026 [as well as] explicit theories\u2026 symbolic processes\u2026 a general theory of the cosmos and a general theory of man\u2026 The symbolic universe also orders history. It locates all collective events in a cohesive unity that includes past, present and future.\u201d (p. 92-104)","title":"Symbolic universes"},{"location":"Create/ExampleText/#universe-maintenance","text":"Universe-maintenance refers to specific procedures undertaken, often by an elite group, when the symbolic universe does not fulfill its purpose anymore, which is to legitimize the institutional structure in place. This happens, for example, in generational shifts, or when deviants create an internal movement against established institutions (e.g. against revolutions), or when a society is confronted with another society with a greatly different history and institutional structures. In primitive societies this happened through mythological systems, later on through theological thought. Today, an extremely complex set of science has secularized universe-maintenance. \u201cSpecific procedures of universe-maintenance become necessary when the symbolic universe has become a problem. As long as this is not the case, the symbolic universe is self-maintaining, that is self-legitimating. An intrinsic problem presents itself with the process of transmission of the symbolic universe from one generation to another\u2026 [additionally] two societies confronting each other with conflicting universes will both develop conceptual machinery designed to maintain their respective universes\u2026 mythology represents the most archaic form of universe-maintenance\u2026 theological thought may be distinguished from its mythological predecessor simply in terms of its greater degree of theoretical systematization\u2026 Modern science is an extreme step in this development. (p.104-116)","title":"Universe-maintenance"},{"location":"Create/ExampleText/#society-as-subjective-reality","text":"","title":"Society as subjective reality"},{"location":"Create/ExampleText/#socialization","text":"Socialization is a two-step induction of the individual to participate in the social institutional structure, meaning in its objective reality. \"The individual\u2026 is not born a member of society. He\u2026 becomes a member of society. In the life of every individual\u2026 there is a temporal sequence, in the course of which he is inducted into participation in the social dialectic\" (p. 129) \u201cBy \u2018successful socialization\u2019 we mean the establishment of a high degree of symmetry between objective and subjective reality\u201d (p. 163) Primary Socialization takes place as a child. It is highly charged emotionally and is not questioned. Secondary Socialization includes the acquisition of role-specific knowledge, thus taking one\u2019s place in the social division of labor. It is learned through training and specific rituals, and is not emotionally charged: \u201cit is necessary to love one\u2019s mother, but not one\u2019s teacher\u201d. Training for secondary socialization can be very complex and depends on the complexity of division of labor in a society. Primary socialization is much less flexible than secondary socialization. E.g. shame for nudity comes from primary socialization, adequate dress code depends on secondary: A relatively minor shift in the subjective definition of reality would suffice for an individual to take for granted that one may go to the office without a tie. A much more drastic shift would be necessary to have him go, as a matter of course, without any clothes at all. \u201cThe child does not internalize the world of his significant others as one of many possible worlds\u2026 It is for this reason that the world internalized in primary socialization is so much more firmly entrenched in consciousness than worlds internalized in secondary socialization\u2026. Secondary socialization is the internalization of institutional or institution-based \u2018sub worlds\u2019\u2026 The roles of secondary socialization carry a high degree of anonymity\u2026 The same knowledge taught by one teacher could also be taught by another\u2026 The institutional distribution of tasks between primary and secondary socialization varies with the complexity of the social distribution of knowledge\u201d (p. 129-147)","title":"Socialization"},{"location":"Create/ExampleText/#conversation","text":"Conversation or verbal communication aims at reality-maintenance of the subjective reality. What seems to be a useless and unnecessary communication of redundant banalities is actually a constant mutual reconfirmation of each other's internal thoughts, in that it maintains subjective reality. \u201cOne may view the individual\u2019s everyday life in terms of the working away of a conversational apparatus that ongoingly maintains, modifies and reconstructs his subjective reality\u2026 [for example] \u2018Well, it\u2019s time for me to get to the station,\u2019 and \u2018Fine, darling, have a good day at the office\u2019 implies an entire world within which these apparently simple propositions make sense\u2026 the exchange confirms the subjective reality of this world\u2026 the great part, if not all, of everyday conversation maintains subjective reality\u2026 imagine the effect\u2026of an exchange like this: \u2018Well, it\u2019s time for me to get to the station,\u2019 \u2018Fine, darling, don\u2019t forget to take along your gun.\u2019 (p. 147-163)","title":"Conversation"},{"location":"Create/ExampleText/#identity","text":"Identity of an individual is subject to a struggle of affiliation to sometimes conflicting realities. For example, the reality from primary socialization (mother tells child not to steal) can be in contrast with second socialization (gang members teach teenager that stealing is cool). Our final social location in the institutional structure of society will ultimately also influence our body and organism. \u201c\u2026life-expectancies of lower-class and upper-class [vary] \u2026society determines how long and in what manner the individual organism shall live\u2026 Society also directly penetrates the organism in its functioning, most importantly in respect to sexuality and nutrition. While both sexuality and nutrition are grounded in biological drives\u2026 biological constitution does not tell him where he should seek sexual release and what he should eat.\u201d (p. 163-183)","title":"Identity"},{"location":"Model/Data-Model/","text":"Text-Fabric Data Model \u00b6 Everything about us, everything around us, everything we know and can know of is composed ultimately of patterns of nothing; that\u2019s the bottom line, the final truth. So where we find we have any control over those patterns, why not make the most elegant ones, the most enjoyable and good ones, in our own terms?\" -- Iain M. Banks . \" Consider Phlebas : A Culture Novel (Culture series)\" At a glance \u00b6 Take a text, put a grid around the words, and then leave out the words. What is left, are the textual positions, or slots . Pieces of text correspond to phrases, clauses, sentences, verses, chapters, books. Draw circles around those pieces, and then leave out their contents. What is left, are the textual objects, or nodes . Nodes can be connected to other nodes by edges . A basic function of edges is to indicate containment : this node corresponds to a set of slots that is contained in the slots of that node. But edges can also denote more abstract, linguistic relations between nodes. Nodes have types. Types are just a label that we use to make distinctions between word nodes, phrase nodes, ..., book nodes. The type assignment is an example of a feature of nodes: a mapping that assigns a piece of information to each node. This type assignment has a name: otype , and every Text-Fabric dataset has such a feature. Nodes may be linked to textual positions or slots . Some nodes are linked to a single slot, others to a set of slots, and yet others to no slots at all. Nodes of the first kind are identified with their slots, they have the same number as slot as they have as node. Nodes of the second kind have an edge to every slot (which is also a node) that they are linked to. The collection of these edges from nodes of the second kind to nodes of the first kind, is an example of a feature of edges: a mapping that assigns to each pair of nodes a boolean value: is this pair a link or not? This particular edge feature is called oslots , and every Text-Fabric dataset has such a feature. Nodes of the third kind represent information that is not part of the main body of text. We could represent the lexicon in this way. However, it is also possible to consider lexeme as a node type, where every lexeme node is linked to the set of slots that have an occurrence of that lexeme. Fabric metaphor \u00b6 AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen Before we go on, we invite you to look at a few basic terms in the craft of weaving . A weaver sets up a set of fixed, parallel threads, the warp . He then picks a thread, usually a colourful one, and sends it in a perpendicular way through the warp. This thread is called the weft . The instrument that carries the wefts through the warp is called the loom . The weaver continues operating the loom, back and forth, occasionally selecting new wefts, until he has completed a rectangular piece of fabric, the weave . source Now Text-Fabric, the tool, can be seen as the loom that sends features (the wefts) through a warp (the system of nodes and edges). The features otype and oslots are the ones that set up the system of nodes and edges. That's why we call them warp features. Every Text-Fabric dataset contains these two warp features. (Later on we'll see a third member of the warp, otext ). They provide the structure of a text and its annotations, without any content. Even the text itself is left out! All other information is added to the warp as features (the wefts): node features and edge features. A feature is a special aspect of the textual information, isolated as a kind of module. It is a collection of values which can be woven as a weft into the warp. One of the more basic things to add to the warp is the text itself. Ancient texts often have several text representations, like original (Unicode) characters or transliterated characters, with or without the complete set of diacritical marks. In Text-Fabric we do not have to choose between them: we can package each representation into a feature, and add it to the fabric. A Text-Fabric data set is a warp ( otype , oslots ) plus a collection of wefts (all other features). We may add other features to the same warp. Data sets with only wefts, but no warps, are called modules. When you use modules with a dataset, the modules must have been constructed around the warp of the dataset. Whenever you use Text-Fabric to generate new data, you are weaving a weave. The resulting dataset is a tight fabric of individual features (wefts), whose values are taken for a set of nodes (warp). Some features deserve a privileged place. After all, we are dealing with text , so we need a bit of information about which features carry textual representations and sectioning information (e.g. books, chapters, verses). This information is not hard-wired into Text-Fabric, but it is given in the form of a config feature. A config feature has no data, only metadata. Every Text-Fabric dataset may contain a config feature called otext , which specifies which node types and features correspond to sectional units such as books, chapters, and verses. It also contains templates for generating text representations for the slots. The otext feature is optional, because not all Text-Fabric datasets are expected to have extensive sectioning and text representation definitions. Especially when you are in the process of converting a data source (such as a treebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load the data without bothering about these matters. Model \u00b6 We summarize in brief statements our data model, including ways to represent the data, serialize it, and compute with it. Text objects: occupy arbitrary compositions of slots; carry a type (just a label); all slots carry the same type, the slot type ; e.g. word or character ; can be annotated by features (key-value pairs) can be connected by directed, labelled links to other text objects. The model knows which feature assigned values to nodes and edges. If two different features assign a value to an edge or node, both values can be read off later; one through the one feature, and one through the other. The data in Text-Fabric is organized as an annotated directed graph with a bit of additional structure. The correspondence is text positions => the first so many slot numbers text objects => nodes links between text objects => edges information associated with text objects => node features labels on links between text objects => edge features NB: since every link is specified by an edge feature, every link is implicitly labelled by the name of the edge feature. If the edge feature assigns values to edges, those values come on top of the implicit label. types of text objects => a special node feature called otype (read: object type) extent of text objects in terms of slots => a special edge feature called oslots (read: object slots) optional specifications for sectioning and representing text => a special config feature called otext (read: object text) Together, the otype , oslots , and the optional otext features are called the warp of a Text-Fabric dataset. Representation \u00b6 We represent the elements that make up such a graph as follows: nodes are integers, starting with 1, without gaps; the first maxSlot nodes correspond exactly with the slots, in the same order, where maxSlot is the number of slots; nodes greater than maxSlot correspond to general text objects; node features are mappings of integers to values; edge features are mappings of pairs of integers to values; i.e. edges are ordered pairs of integers; labelled edges are ordered tuples of two nodes and a value; values (for nodes and for edges) are strings (Unicode, utf8) or numbers; the otype feature maps the integers 1..maxSlot (including) to the slot type , where maxSlot is the last slot , the integers maxSlot+1..maxNode (including) to the relevant text object types; the oslots feature is an valueless edge feature, mapping all non-slot nodes to sets of slots; so there is an oslots edge between each non-slot node and each slot contained by that node; a Text-Fabric dataset is a collection of node features and edge features containing at least the warp features otype , oslots , and, optionally otext . More about the warp \u00b6 The warp/weft distinction is a handy way of separating textual organisation from textual content. Let us discuss the warp features a bit more. otype: node feature \u00b6 Maps each node to a label. The label typically is the kind of object that the node represents, with values such as 1 2 3 4 5 6 7 book chapter verse sentence clause phrase word There is a special kind of object type, the slot type , which is the atomic building block of the text objects. It is assumed that the complete text is built from a sequence of slots , from slot 1 till slot maxSlot (including), where the slots are numbered consecutively. There must be at least one slot. All other objects are defined with respect to the slots they contain. The slot type does not have to be called slot literally. If your basic entity is word , you may also call it word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with characters . Other choices may be equally viable. The only requirement is that all slots correspond exactly with the first so many nodes. The otype feature will map node 1 to a node type, and this node type is the type of all subsequent slots and also of the things that fill the slots. Note also the sectional features book chapter verse here. They will play a role in the third, optional, warp feature otext . oslots: edge feature \u00b6 Defines which slots are occupied by which objects. It does so by specifying edges from nodes to the slots they contain. From the information in oslots we can compute the embedding relationships between all nodes. It gives also rise to a canonical ordering of nodes. otext: config feature (optional) \u00b6 Declares which node types correspond to the first three levels of sectioning, usually book , chapter , verse . Also declares the corresponding features to get the names or numbers of the sections in those levels. Text-Fabric uses this information to construct the so-called Text-API, with functions to convert nodes to section labels and vice versa, represent section names in multiple languages, print formatted text for node sets. If information about sections or text representations are missing, Text-Fabric will build a reduced Text-API for you, but it will continue. Serializing and precomputing \u00b6 When Text-Fabric works with a dataset, it reads feature data files, and offers an API to process that data. The main task of Text-Fabric is to make processing efficient, so that it can be done in interactive ways, such as in a Jupyter notebook. To that end, Text-Fabric optimizes feature data after reading it for the first time and stores it in binary form for fast loading in next invocations; precomputes additional data from the warp features in order to provide convenient API functions. In Text-Fabric, we have various ways of encoding this model: as plain text in .tf feature files, as Python data structures in memory, as compressed serializations of the same data structures inside .tfx files in .tf cache directories.","title":"Data"},{"location":"Model/Data-Model/#text-fabric-data-model","text":"Everything about us, everything around us, everything we know and can know of is composed ultimately of patterns of nothing; that\u2019s the bottom line, the final truth. So where we find we have any control over those patterns, why not make the most elegant ones, the most enjoyable and good ones, in our own terms?\" -- Iain M. Banks . \" Consider Phlebas : A Culture Novel (Culture series)\"","title":"Text-Fabric Data Model"},{"location":"Model/Data-Model/#at-a-glance","text":"Take a text, put a grid around the words, and then leave out the words. What is left, are the textual positions, or slots . Pieces of text correspond to phrases, clauses, sentences, verses, chapters, books. Draw circles around those pieces, and then leave out their contents. What is left, are the textual objects, or nodes . Nodes can be connected to other nodes by edges . A basic function of edges is to indicate containment : this node corresponds to a set of slots that is contained in the slots of that node. But edges can also denote more abstract, linguistic relations between nodes. Nodes have types. Types are just a label that we use to make distinctions between word nodes, phrase nodes, ..., book nodes. The type assignment is an example of a feature of nodes: a mapping that assigns a piece of information to each node. This type assignment has a name: otype , and every Text-Fabric dataset has such a feature. Nodes may be linked to textual positions or slots . Some nodes are linked to a single slot, others to a set of slots, and yet others to no slots at all. Nodes of the first kind are identified with their slots, they have the same number as slot as they have as node. Nodes of the second kind have an edge to every slot (which is also a node) that they are linked to. The collection of these edges from nodes of the second kind to nodes of the first kind, is an example of a feature of edges: a mapping that assigns to each pair of nodes a boolean value: is this pair a link or not? This particular edge feature is called oslots , and every Text-Fabric dataset has such a feature. Nodes of the third kind represent information that is not part of the main body of text. We could represent the lexicon in this way. However, it is also possible to consider lexeme as a node type, where every lexeme node is linked to the set of slots that have an occurrence of that lexeme.","title":"At a glance"},{"location":"Model/Data-Model/#fabric-metaphor","text":"AD 1425 Hausb\u00fccher der N\u00fcrnberger Zw\u00f6lfbr\u00fcderstiftungen Before we go on, we invite you to look at a few basic terms in the craft of weaving . A weaver sets up a set of fixed, parallel threads, the warp . He then picks a thread, usually a colourful one, and sends it in a perpendicular way through the warp. This thread is called the weft . The instrument that carries the wefts through the warp is called the loom . The weaver continues operating the loom, back and forth, occasionally selecting new wefts, until he has completed a rectangular piece of fabric, the weave . source Now Text-Fabric, the tool, can be seen as the loom that sends features (the wefts) through a warp (the system of nodes and edges). The features otype and oslots are the ones that set up the system of nodes and edges. That's why we call them warp features. Every Text-Fabric dataset contains these two warp features. (Later on we'll see a third member of the warp, otext ). They provide the structure of a text and its annotations, without any content. Even the text itself is left out! All other information is added to the warp as features (the wefts): node features and edge features. A feature is a special aspect of the textual information, isolated as a kind of module. It is a collection of values which can be woven as a weft into the warp. One of the more basic things to add to the warp is the text itself. Ancient texts often have several text representations, like original (Unicode) characters or transliterated characters, with or without the complete set of diacritical marks. In Text-Fabric we do not have to choose between them: we can package each representation into a feature, and add it to the fabric. A Text-Fabric data set is a warp ( otype , oslots ) plus a collection of wefts (all other features). We may add other features to the same warp. Data sets with only wefts, but no warps, are called modules. When you use modules with a dataset, the modules must have been constructed around the warp of the dataset. Whenever you use Text-Fabric to generate new data, you are weaving a weave. The resulting dataset is a tight fabric of individual features (wefts), whose values are taken for a set of nodes (warp). Some features deserve a privileged place. After all, we are dealing with text , so we need a bit of information about which features carry textual representations and sectioning information (e.g. books, chapters, verses). This information is not hard-wired into Text-Fabric, but it is given in the form of a config feature. A config feature has no data, only metadata. Every Text-Fabric dataset may contain a config feature called otext , which specifies which node types and features correspond to sectional units such as books, chapters, and verses. It also contains templates for generating text representations for the slots. The otext feature is optional, because not all Text-Fabric datasets are expected to have extensive sectioning and text representation definitions. Especially when you are in the process of converting a data source (such as a treebanks set) into a Text-Fabric dataset, it is handy that Text-Fabric can load the data without bothering about these matters.","title":"Fabric metaphor"},{"location":"Model/Data-Model/#model","text":"We summarize in brief statements our data model, including ways to represent the data, serialize it, and compute with it. Text objects: occupy arbitrary compositions of slots; carry a type (just a label); all slots carry the same type, the slot type ; e.g. word or character ; can be annotated by features (key-value pairs) can be connected by directed, labelled links to other text objects. The model knows which feature assigned values to nodes and edges. If two different features assign a value to an edge or node, both values can be read off later; one through the one feature, and one through the other. The data in Text-Fabric is organized as an annotated directed graph with a bit of additional structure. The correspondence is text positions => the first so many slot numbers text objects => nodes links between text objects => edges information associated with text objects => node features labels on links between text objects => edge features NB: since every link is specified by an edge feature, every link is implicitly labelled by the name of the edge feature. If the edge feature assigns values to edges, those values come on top of the implicit label. types of text objects => a special node feature called otype (read: object type) extent of text objects in terms of slots => a special edge feature called oslots (read: object slots) optional specifications for sectioning and representing text => a special config feature called otext (read: object text) Together, the otype , oslots , and the optional otext features are called the warp of a Text-Fabric dataset.","title":"Model"},{"location":"Model/Data-Model/#representation","text":"We represent the elements that make up such a graph as follows: nodes are integers, starting with 1, without gaps; the first maxSlot nodes correspond exactly with the slots, in the same order, where maxSlot is the number of slots; nodes greater than maxSlot correspond to general text objects; node features are mappings of integers to values; edge features are mappings of pairs of integers to values; i.e. edges are ordered pairs of integers; labelled edges are ordered tuples of two nodes and a value; values (for nodes and for edges) are strings (Unicode, utf8) or numbers; the otype feature maps the integers 1..maxSlot (including) to the slot type , where maxSlot is the last slot , the integers maxSlot+1..maxNode (including) to the relevant text object types; the oslots feature is an valueless edge feature, mapping all non-slot nodes to sets of slots; so there is an oslots edge between each non-slot node and each slot contained by that node; a Text-Fabric dataset is a collection of node features and edge features containing at least the warp features otype , oslots , and, optionally otext .","title":"Representation"},{"location":"Model/Data-Model/#more-about-the-warp","text":"The warp/weft distinction is a handy way of separating textual organisation from textual content. Let us discuss the warp features a bit more.","title":"More about the warp"},{"location":"Model/Data-Model/#otype-node-feature","text":"Maps each node to a label. The label typically is the kind of object that the node represents, with values such as 1 2 3 4 5 6 7 book chapter verse sentence clause phrase word There is a special kind of object type, the slot type , which is the atomic building block of the text objects. It is assumed that the complete text is built from a sequence of slots , from slot 1 till slot maxSlot (including), where the slots are numbered consecutively. There must be at least one slot. All other objects are defined with respect to the slots they contain. The slot type does not have to be called slot literally. If your basic entity is word , you may also call it word . Slots are then filled with words . You can model text on the basis of another atomic entity, such as character . In that case, slots are filled with characters . Other choices may be equally viable. The only requirement is that all slots correspond exactly with the first so many nodes. The otype feature will map node 1 to a node type, and this node type is the type of all subsequent slots and also of the things that fill the slots. Note also the sectional features book chapter verse here. They will play a role in the third, optional, warp feature otext .","title":"otype: node feature"},{"location":"Model/Data-Model/#oslots-edge-feature","text":"Defines which slots are occupied by which objects. It does so by specifying edges from nodes to the slots they contain. From the information in oslots we can compute the embedding relationships between all nodes. It gives also rise to a canonical ordering of nodes.","title":"oslots: edge feature"},{"location":"Model/Data-Model/#otext-config-feature-optional","text":"Declares which node types correspond to the first three levels of sectioning, usually book , chapter , verse . Also declares the corresponding features to get the names or numbers of the sections in those levels. Text-Fabric uses this information to construct the so-called Text-API, with functions to convert nodes to section labels and vice versa, represent section names in multiple languages, print formatted text for node sets. If information about sections or text representations are missing, Text-Fabric will build a reduced Text-API for you, but it will continue.","title":"otext: config feature (optional)"},{"location":"Model/Data-Model/#serializing-and-precomputing","text":"When Text-Fabric works with a dataset, it reads feature data files, and offers an API to process that data. The main task of Text-Fabric is to make processing efficient, so that it can be done in interactive ways, such as in a Jupyter notebook. To that end, Text-Fabric optimizes feature data after reading it for the first time and stores it in binary form for fast loading in next invocations; precomputes additional data from the warp features in order to provide convenient API functions. In Text-Fabric, we have various ways of encoding this model: as plain text in .tf feature files, as Python data structures in memory, as compressed serializations of the same data structures inside .tfx files in .tf cache directories.","title":"Serializing and precomputing"},{"location":"Model/File-formats/","text":"Text-Fabric File Format \u00b6 Overview \u00b6 A .tf feature file starts with a header , and is followed by the actual data. The whole file is a plain text in UNICODE-utf8. Header \u00b6 A .tf feature file always starts with one or more metadata lines of the form 1 @key or 1 @key=value The first line must be either 1 @node or 1 @edge or 1 @config This tells Text-Fabric whether the data in the feature file is a node feature or an edge feature. The value @config means that the file will be used as configuration info. It will only have metadata. There must also be a type declaration: 1 @valueType=type where type is str or int . @valueType declares the type of the values in this feature file. If it is anything other than str (= string ), Text-Fabric will convert it to that type when it reads the data from the file. Currently, the only other supported type is int for integers. In edge features, there may also be a declaration 1 @edgeValues indicating that the edge feature carries values. The default is that an edge does not carry values. The rest of the metadata is optional for now, but it is recommended to put a date stamp in it like this 1 @dateCreated=2016-11-20T13:26:59Z The time format should be ISO 8601 . Data \u00b6 After the metadata, there must be exactly one blank line, and every line after that is data. Data lines \u00b6 The form of a data line is 1 node_spec value for node features, and 1 node_spec node_spec value for edge features. These fields are separated by single tabs. NB : This is the default format. Under Optimizations below we shall describe the bits that can be left out, which will lead to significant improvement in space demands and processing speed. Node Specification \u00b6 Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of node_spec s. A node spec denotes a set of nodes. The simplest form of a node spec is just a single integer. Examples: 1 2 3 3 45 425000 Ranges are also allowed. Examples 1 2 3 1-10 5-13 28-57045 The nodes denoted by a range are all numbers between the endpoints of the range (including at both sides). So 1 2-4 denotes the nodes 2 , 3 , and 4 . You can also combine numbers and ranges arbitrarily by separating them with commas. Examples 1 1-3,5-10,15,23-37 Such a specification denotes the union of what is denoted by each comma-separated part. NB As node specs denote sets of nodes, the following node specs are in fact equivalent 1 2 3 1,1 and 1 2-3 and 3,2 1-5,2-7 and 1-7 We will be tolerant in that you may specify the end points of ranges in arbitrary order: 1 1-3 is the same as 3-1 Edges \u00b6 An edge is specified by an ordered pair of nodes. The edge is from the first node in the pair to the second one. An edge spec consists of two node specs. It denotes all edges that are from a node denoted by the first node spec to a node denoted by the second node spec. An edge might be labelled, in that case the label of the edge is specified by the value after the two node specs. Value \u00b6 The value is arbitrary text. The type of the value must conform to the @valueType declaration in the feature file. If it is missing, it is assumed to be str , which is the type of Unicode-utf8 strings. If it is int , it should be a valid representation of an integer number, There are a few escapes: \\\\ backslash \\t tab \\n newline These characters MUST always be escaped in a value string, otherwise the line as a whole might be ambiguous. NB: There is no representation for the absence of a value. The empty string as value means that there is a value and it is the empty string. If you want to describe the fact that node n does not have a value for the feature in question, the node must be left out of the feature. In order words, there should be no data line in the feature that targets this node. If the declared value type ( @valueType ) of a feature is int , then its empty values will be taken as absence of values, though. Consistency requirements \u00b6 There are a few additional requirements on feature data, having to do with the fact that features annotate nodes or edges of a graph. Single values \u00b6 It is assumed that a node feature assigns only one value to the same node. If the data contains multiple assignments to a node, only the last assignment will be honoured, the previous ones will be discarded. Likewise, it is assumed that an edge feature assigns only one value to the same edge. If the data contains multiple assignments to an edge, only the last assignment will be honoured. Violations maybe or may not be reported, and processing may continue without warnings.","title":"Format"},{"location":"Model/File-formats/#text-fabric-file-format","text":"","title":"Text-Fabric File Format"},{"location":"Model/File-formats/#overview","text":"A .tf feature file starts with a header , and is followed by the actual data. The whole file is a plain text in UNICODE-utf8.","title":"Overview"},{"location":"Model/File-formats/#header","text":"A .tf feature file always starts with one or more metadata lines of the form 1 @key or 1 @key=value The first line must be either 1 @node or 1 @edge or 1 @config This tells Text-Fabric whether the data in the feature file is a node feature or an edge feature. The value @config means that the file will be used as configuration info. It will only have metadata. There must also be a type declaration: 1 @valueType=type where type is str or int . @valueType declares the type of the values in this feature file. If it is anything other than str (= string ), Text-Fabric will convert it to that type when it reads the data from the file. Currently, the only other supported type is int for integers. In edge features, there may also be a declaration 1 @edgeValues indicating that the edge feature carries values. The default is that an edge does not carry values. The rest of the metadata is optional for now, but it is recommended to put a date stamp in it like this 1 @dateCreated=2016-11-20T13:26:59Z The time format should be ISO 8601 .","title":"Header"},{"location":"Model/File-formats/#data","text":"After the metadata, there must be exactly one blank line, and every line after that is data.","title":"Data"},{"location":"Model/File-formats/#data-lines","text":"The form of a data line is 1 node_spec value for node features, and 1 node_spec node_spec value for edge features. These fields are separated by single tabs. NB : This is the default format. Under Optimizations below we shall describe the bits that can be left out, which will lead to significant improvement in space demands and processing speed.","title":"Data lines"},{"location":"Model/File-formats/#node-specification","text":"Every line contains a feature value that pertains to all nodes defined by its node_spec , or to all edges defined by its pair of node_spec s. A node spec denotes a set of nodes. The simplest form of a node spec is just a single integer. Examples: 1 2 3 3 45 425000 Ranges are also allowed. Examples 1 2 3 1-10 5-13 28-57045 The nodes denoted by a range are all numbers between the endpoints of the range (including at both sides). So 1 2-4 denotes the nodes 2 , 3 , and 4 . You can also combine numbers and ranges arbitrarily by separating them with commas. Examples 1 1-3,5-10,15,23-37 Such a specification denotes the union of what is denoted by each comma-separated part. NB As node specs denote sets of nodes, the following node specs are in fact equivalent 1 2 3 1,1 and 1 2-3 and 3,2 1-5,2-7 and 1-7 We will be tolerant in that you may specify the end points of ranges in arbitrary order: 1 1-3 is the same as 3-1","title":"Node Specification"},{"location":"Model/File-formats/#edges","text":"An edge is specified by an ordered pair of nodes. The edge is from the first node in the pair to the second one. An edge spec consists of two node specs. It denotes all edges that are from a node denoted by the first node spec to a node denoted by the second node spec. An edge might be labelled, in that case the label of the edge is specified by the value after the two node specs.","title":"Edges"},{"location":"Model/File-formats/#value","text":"The value is arbitrary text. The type of the value must conform to the @valueType declaration in the feature file. If it is missing, it is assumed to be str , which is the type of Unicode-utf8 strings. If it is int , it should be a valid representation of an integer number, There are a few escapes: \\\\ backslash \\t tab \\n newline These characters MUST always be escaped in a value string, otherwise the line as a whole might be ambiguous. NB: There is no representation for the absence of a value. The empty string as value means that there is a value and it is the empty string. If you want to describe the fact that node n does not have a value for the feature in question, the node must be left out of the feature. In order words, there should be no data line in the feature that targets this node. If the declared value type ( @valueType ) of a feature is int , then its empty values will be taken as absence of values, though.","title":"Value"},{"location":"Model/File-formats/#consistency-requirements","text":"There are a few additional requirements on feature data, having to do with the fact that features annotate nodes or edges of a graph.","title":"Consistency requirements"},{"location":"Model/File-formats/#single-values","text":"It is assumed that a node feature assigns only one value to the same node. If the data contains multiple assignments to a node, only the last assignment will be honoured, the previous ones will be discarded. Likewise, it is assumed that an edge feature assigns only one value to the same edge. If the data contains multiple assignments to an edge, only the last assignment will be honoured. Violations maybe or may not be reported, and processing may continue without warnings.","title":"Single values"},{"location":"Model/Optimizations/","text":"File format Optimizations \u00b6 Rationale \u00b6 It is important to avoid an explosion of redundant data in .tf files. We want the .tf format to be suitable for archiving, transparent to the human eye, and easy (i.e. fast) to process. Using the implicit node \u00b6 You may leave out the node spec for node features, and the first node spec for edge features. When leaving out a node spec, you must also leave out the tab following the node spec. A line with the first node spec left out denotes the singleton node set consisting of the implicit node . Here are the rules for implicit nodes. On a line where there is an explicit node spec, the implicit node is equal to the highest node denoted by the explicit node spec; On a line without an explicit node spec, the implicit node is determined from the previous line as follows: if there is no previous line, take 1 ; else take the implicit node of the previous line and increment it by 1 . For edges, this optimization only happens for the first node spec. The second node spec must always be explicit. This optimizes some feature files greatly, e.g. the feature that contains the actual text of each word. Instead of 1 2 3 4 5 6 7 8 9 10 11 1 be 2 reshit 3 bara 4 elohim 5 et 6 ha 7 shamajim 8 we 9 et 10 ha 11 arets you can just say 1 2 3 4 5 6 7 8 9 10 11 be reshit bara elohim et ha shamajim we et ha arets This optimization is not obligatory. It is a device that may be used if you want to optimize the size of data files that you want to distribute. Omitting empty values \u00b6 If the value is the empty string, you may also leave out the preceding tab (if there is one). This is especially good for edge features, because most edges just consist of a node pair without any value. This optimization will cause a conceptual ambiguity if there is only one field present in a node feature, or if there are only two fields in an edge feature. It could mean that the (first) node spec has been left out, or that the value has been left out. In those cases we will assume that the node spec has been left out for node features. For edge features, it depends on whether the edge is declared to have values (with @edgeValues ). If the edge has values, then, as in the case of node features, we assume that the first node spec has been left out. But if the edge has no values, then we assume that both fields are node specs. So, in a node feature a line like this 1 42 means that the implicit node gets value 42 , and not that node 42 gets the empty value. Likewise, a line in an edge feature (without values) like this 1 42 43 means that there is an edge from 42 to 43 with empty value, and not that there is an edge from the implicit node to 42 with value 43. And, in the same edge, a line like this 1 42 means that there is an edge from the implicit node to 42 with the empty value. But, in an edge with values, the same lines are interpreted thus: 1 42 43 means that there is an edge from the implicit node to node 42 with value 43 . And 1 42 means that there is an edge from the implicit node to node 42 with empty value. The reason for these conventions is practical: edge features usually have empty labels, and there are many edges. In case of the Hebrew Text database, there are 1.5 million edges, so every extra character that is needed on a data line means that the file size increases with 1.5 MB. Nodes on the other hand, usually do not have empty values, and they are often specified in a consecutive way, especially slot (word) nodes. There are quite many distinct word features, and it would be a waste to have a column of half a million incremental integers in those files. Absence of values \u00b6 Say you have a node feature assigning a value to only 2000 of 400,000 nodes. (The Hebrew qere would be an example). It is better to make sure that the absent values are not coded as the empty string. So the feature data will look like 2000 lines, each with a node spec, rather than a sequence of 400,000 lines, most empty. If you want to leave out just a few isolated cases in a feature where most nodes get a value, you can do it like this: 1 2 3 4 5 6 7 8 9 @node x0000 ... x1000 1002 x1002 x1003 ... x9999 Here all 10,000 nodes get a value, except node 1001 . Note on redundancy \u00b6 Some features assign the same value to many nodes. It is tempting to make a value definition facility, so that values are coded by short codes, the most frequent values getting the shortest codes. After some experiments, it turned out that the overall gain was just 50%. I find this advantage too small to justify the increased code complexity, and above all, the reduced transparency of the .tf files. Examples \u00b6 Here are a few more and less contrived examples of legal feature data lines. Node features \u00b6 \\t\\n 2 2\\t3 foo\\nbar 1 Escape \\t as \\\\t meaning node 1 has value: tab newline node 2 has value: 2 tab 3 node 3 has value: foo newline bar node 1 gets a new value: Escape as \\t Edge features \u00b6 1 1 2 2 3 foo 1-2 2-3 bar meaning edge from 1 to 1 with no value edge from 1 to 2 with no value edge from 2 to 3 with value foo four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can go from a node to itself. Note also that this line reassigns a value to two edges: 1->2 and 2->3.","title":"Tweaks"},{"location":"Model/Optimizations/#file-format-optimizations","text":"","title":"File format Optimizations"},{"location":"Model/Optimizations/#rationale","text":"It is important to avoid an explosion of redundant data in .tf files. We want the .tf format to be suitable for archiving, transparent to the human eye, and easy (i.e. fast) to process.","title":"Rationale"},{"location":"Model/Optimizations/#using-the-implicit-node","text":"You may leave out the node spec for node features, and the first node spec for edge features. When leaving out a node spec, you must also leave out the tab following the node spec. A line with the first node spec left out denotes the singleton node set consisting of the implicit node . Here are the rules for implicit nodes. On a line where there is an explicit node spec, the implicit node is equal to the highest node denoted by the explicit node spec; On a line without an explicit node spec, the implicit node is determined from the previous line as follows: if there is no previous line, take 1 ; else take the implicit node of the previous line and increment it by 1 . For edges, this optimization only happens for the first node spec. The second node spec must always be explicit. This optimizes some feature files greatly, e.g. the feature that contains the actual text of each word. Instead of 1 2 3 4 5 6 7 8 9 10 11 1 be 2 reshit 3 bara 4 elohim 5 et 6 ha 7 shamajim 8 we 9 et 10 ha 11 arets you can just say 1 2 3 4 5 6 7 8 9 10 11 be reshit bara elohim et ha shamajim we et ha arets This optimization is not obligatory. It is a device that may be used if you want to optimize the size of data files that you want to distribute.","title":"Using the implicit node"},{"location":"Model/Optimizations/#omitting-empty-values","text":"If the value is the empty string, you may also leave out the preceding tab (if there is one). This is especially good for edge features, because most edges just consist of a node pair without any value. This optimization will cause a conceptual ambiguity if there is only one field present in a node feature, or if there are only two fields in an edge feature. It could mean that the (first) node spec has been left out, or that the value has been left out. In those cases we will assume that the node spec has been left out for node features. For edge features, it depends on whether the edge is declared to have values (with @edgeValues ). If the edge has values, then, as in the case of node features, we assume that the first node spec has been left out. But if the edge has no values, then we assume that both fields are node specs. So, in a node feature a line like this 1 42 means that the implicit node gets value 42 , and not that node 42 gets the empty value. Likewise, a line in an edge feature (without values) like this 1 42 43 means that there is an edge from 42 to 43 with empty value, and not that there is an edge from the implicit node to 42 with value 43. And, in the same edge, a line like this 1 42 means that there is an edge from the implicit node to 42 with the empty value. But, in an edge with values, the same lines are interpreted thus: 1 42 43 means that there is an edge from the implicit node to node 42 with value 43 . And 1 42 means that there is an edge from the implicit node to node 42 with empty value. The reason for these conventions is practical: edge features usually have empty labels, and there are many edges. In case of the Hebrew Text database, there are 1.5 million edges, so every extra character that is needed on a data line means that the file size increases with 1.5 MB. Nodes on the other hand, usually do not have empty values, and they are often specified in a consecutive way, especially slot (word) nodes. There are quite many distinct word features, and it would be a waste to have a column of half a million incremental integers in those files.","title":"Omitting empty values"},{"location":"Model/Optimizations/#absence-of-values","text":"Say you have a node feature assigning a value to only 2000 of 400,000 nodes. (The Hebrew qere would be an example). It is better to make sure that the absent values are not coded as the empty string. So the feature data will look like 2000 lines, each with a node spec, rather than a sequence of 400,000 lines, most empty. If you want to leave out just a few isolated cases in a feature where most nodes get a value, you can do it like this: 1 2 3 4 5 6 7 8 9 @node x0000 ... x1000 1002 x1002 x1003 ... x9999 Here all 10,000 nodes get a value, except node 1001 .","title":"Absence of values"},{"location":"Model/Optimizations/#note-on-redundancy","text":"Some features assign the same value to many nodes. It is tempting to make a value definition facility, so that values are coded by short codes, the most frequent values getting the shortest codes. After some experiments, it turned out that the overall gain was just 50%. I find this advantage too small to justify the increased code complexity, and above all, the reduced transparency of the .tf files.","title":"Note on redundancy"},{"location":"Model/Optimizations/#examples","text":"Here are a few more and less contrived examples of legal feature data lines.","title":"Examples"},{"location":"Model/Optimizations/#node-features","text":"\\t\\n 2 2\\t3 foo\\nbar 1 Escape \\t as \\\\t meaning node 1 has value: tab newline node 2 has value: 2 tab 3 node 3 has value: foo newline bar node 1 gets a new value: Escape as \\t","title":"Node features"},{"location":"Model/Optimizations/#edge-features","text":"1 1 2 2 3 foo 1-2 2-3 bar meaning edge from 1 to 1 with no value edge from 1 to 2 with no value edge from 2 to 3 with value foo four edges: 1->2, 1->3, 2->2, 2->3, all with value bar. Note that edges can go from a node to itself. Note also that this line reassigns a value to two edges: 1->2 and 2->3.","title":"Edge features"},{"location":"Model/Search/","text":"Search Design \u00b6 Fabric metaphor \u00b6 The search space is a massive fabric of interconnected material. In it we discern the structures we are interested in: little pieces of fabric, also with interconnected material. When we search, we have a fabric in mind, woven from specific material, stitched together in a specific manner. Search in Text-Fabric works exactly like this: you give a sample patch, and Text-Fabric fetches all pieces of the big fabric that match your patch. The textile metaphor is particularly suited for grasping the search part of Text-Fabric, so I'm going to stick to it for a while. I have used it in the actual code as well, and even in the proofs that certain parts of the algorithm terminate and are correct. Yet it remains a metaphor, and the fit is not exact. The basic pattern of search is this: textile text example take several fleeces pick the nodes corresponding to a node type word s, phrase s, clause s, verse s spin thick yarns from them filter by feature conditions part-of-speech=verb gender= f book=Genesis vt spin the yarns further into thin yarns throw away nodes that do not have the right connections feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa stitch the yarns together with thread build results by selecting a member for every filtered node set word node 123456 in phrase node 657890 in clause node 490567 in verse node 1403456 We will explain the stages of the fabrication process in detail. Fleece \u00b6 A fleece corresponds with a very simple search template that asks for all objects of a given type: 1 word or 1 clause or, asking for multiple types: 1 2 3 4 5 verse clause phrase lex word Fleeces are the raw material from which we fabricate our search results. Every node type, such as word , sentence , book corresponds to a fleece. In Text-Fabric, every node has exactly one node type, so the whole space is neatly divided into a small set of fleeces. The most important characteristic of a fleece is it size: the number of nodes in a node type. Spinning thick yarn \u00b6 Consider search templates where we ask for specific members of a node type, by giving feature constraints: 1 2 3 4 5 6 verse book=Genesis clause type=rela phrase determined=yes lex id=jc/ word number=pl vt vt Every line in this search templates we call an atom : a node type plus a feature specification. The result of an atom is the set of all nodes in that node type that satisfy those feature conditions. Finding the results of an atom corresponds with first thing that we do with a fleece: spin a thick yarn from it. Yarns in general are obtained by spinning fleeces, i.e. by filtering node sets that correspond to a node type. A search template may contain multiple atoms. Text-Fabric collects all atoms of a template, grabs the corresponding fleeces, and spins thick yarns from them. For each atom it will spin a yarn, and if there are several atoms referring to the same node type, there will be several yarns spun from that fleece. This spinning of thick yarns out of fleeces happens in just one go. All fleeces together contain exactly all nodes, so Text-Fabric walks in one pass over all nodes, applies the feature conditions, and puts the nodes into the yarns depending on which conditions apply. By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical pass requires a fraction of a second. The most important characteristic of a yarn is its thickness , which we define as the number of nodes in the yarn divided by the number of nodes in the fleece. For example, a yarn consisting of the book node of Genesis only, has a thickness of 1/39, because there are 39 books in the fleece. A much thicker yarn is that of the verbs, which has a thickness of roughly 1/6. A very thin thread is that of the word <CQH (which occurs only once) with a thickness of only 1/400,000. Spinning thin yarns \u00b6 In order to find results, we have to further narrow down the search space. In other words, we are going to spin our thick yarns into thinner and thinner yarns. Before we can do that, we should make one thing clear. Connected by constraints \u00b6 If the template above were complete, it would lead to a monstrous number of results. Because a result of a template like this is any combination of verse-, clause-, phrase-, lex-, word nodes that individually satisfy their own atom condition. So the number of results is the product of the number of results of the individual atoms, which is pretty enormous. It is hard to imagine a situation where these results could be consumed. Usually, there are constraints active between the atoms. For example in a template like this: 1 2 3 4 5 6 7 8 1 verse book=Genesis 2 clause type=rela 3 phrase determined=yes 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l The meaning of this template is that we look for a verse that (1) claims to be in Genesis (2) has a clause whose type is rela (3) which in turn has a phrase of a determined character (4) which contains a word in the plural and that has a verbal tense (vt) There should also be a (6) lex object, identified by jc/ which is connected to the rest by the constraint that (8) the word of line 4 is contained in it. Note that all atoms are linked by constraints into one network. In graph theoretical terms: this template consists of exactly one connected component . If this were not so, we would have in fact two independent search tasks, where the result set would be the (cartesian) product of the result sets of the separate components. For example, if line 8 were missing, we would effectively search for things that match lines 1-4, and, independently, for things that match line 6. And every result of the first part, combined with any result of the second part, would be a valid result of the whole. Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the same time, let alone that it wants to waste time to generate all the results in the product. You will have to fire those search tasks one by one, and it is up to you how you combine the results. The upshot it: the atoms in the search template should form a network, connected by constraints . Text-Fabric will check this, and will only work with search templates that have only one connected component. Terminology \u00b6 By now we have arrived at the idea that our search template is a graph underneath: what we have called atoms are in fact the nodes, and what we have called constraints , are the edges. From now on, we will call the atoms qnodes and the constraints qedges . The q is to distinguish the nodes and the edges from the nodes and the edges of your dataset, the text nodes and text edges. When we use the term nodes and edges we will always refer to text nodes and edges. When we are searching, we maintain a yarn for every qnode . This yarn starts out to be the thick yarn as described above, but we are going to thin them. We can also see how our query templates are really topographic : a query template is a piece of local geography that we want to match against the data. Finding results is nothing else than instantiating qnodes of the search template by text nodes in such a way that the qedges hold between the text edges. Spinning a qedge \u00b6 So, where were we? We have spun thick threads based on the qnodes individually, but we have not done anything with the qedges . That is going to change now. Consider this piece of search template: 1 2 3 4 5 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l So our qnodes are w and l , and our qedge is w ]] l . Note that a lex object is the set of all occurrences of a lexeme. So w ]] l says that w is embedded in l , in other words, w is a slot contained in the slots of l . It is nothing else than the word w is an instance of the lexeme jc/ . We will now check the pairs of (lex, word)-nodes in the text, where the lex node is taken from the yarn of the qnode l , and the word from the yarn of the qnode w . We can throw away some words from the yarn of w , namely those words that do not lie in a lexeme that is in the yarn of l . In other words: the words that are not instances of lexeme jc/ are out! Conversely, if the lexeme jc/ does not have occurrences in the plural and with a verbal tense, we must kick it out of the yarn of l , leaving no members in it. If a yarn gets empty, we have an early detection that the search yields no results, and the whole process stops. In our case, however, this is not so, and we continue. This pass over the yarns at both sides of a qedge is a spin action. We spin this qedge, and the result is that the two yarns become spun more thinly, hopefully. With the yarn of words severely spun out, we are going to the next qedge, the one between words and phrases. 1 2 3 phrase determined=yes 4 w:word number=pl vt The indent is an implicit way of saying that the \"embeds\" relation [[ holds between the phrase and the word . An equivalent formulation of the template is 1 2 3 4 p:phrase determined=yes w:word number=pl vt p [[ w We race along the yarn w of remaining words and check for each word if it is contained in a phrase in the yarn of p , the determined phrases. If it is not, we throw the word out of the yarn of w . Similarly, we can throw out some phrases from the yarn of p , namely those phrases that do not contain words in the yarn of w . In other words: the phrases without plural words and verbal tense are also out. We continue spinning, now between phrases and clauses. 1 2 2 clause type=rela 3 phrase determined=yes Here we loose the phrases that are not contained in a clause of type=rela , and we loose all clauses that do not embed one of the few phrases left. The last spin action corresponds with 1 2 1 verse book=Genesis 2 clause type=rela So we throw away all our results if they are outside Genesis. We end up with a set of thin yarns, severely thinned out, even. This will be a good starting point for the last stage: picking members from each yarn to form results. We call this stitching and we'll get there in a moment. The spread of a qedge \u00b6 A very important property of a qedge is its spread . A qedge links every node n in its from -yarn to zero, one, or more nodes in its to -yarn. The number of nodes in the to -yarn is a key property. The average number of nodes m in the to -yarn per linked node n in the from -yarn is the spread of the edge. A few examples: An edge that corresponds to ]] , n embeds m . If this edge goes from books to words, then every book node n is linked to every one of its words. So very n has hundreds or thousands m s. The spread will roughly be 425,000 / 39 =~ 10,000 The opposite edge has a spread of exactly 1, because every word belongs to exactly one book. Edges with spread 1 are very pleasant for our stitching algorithm later on. An edge corresponding to = . These qedges are super efficient, because their relation = is a breeze to compute, and they have always a spread 1 in both directions. An edge corresponding to # , the node inequality relation. The relation # is still a breeze to compute, but the result is heavy: the set of all nodes not equal to a given node. The spread is nearly 100% of the yarn length, in both directions. These edges are not worth to spin, because if you have two yarns, no node will be excluded: if you have an n in the from -yarn, you will always be able to find a different n in the to -yarn (except when bot yarns are equal, and contain just one node). An edge corresponding to == , the relation between nodes that are linked to the same set of slots. The spread of this relation is not too big, but the cost of computing it adds up quickly when applied to many cases. Spinning all qedges \u00b6 Let us describe the spinning of yarns along edges in a bit more general way, and reflect on what it does for us. We spin all qedges of a template. But after spinning a qedge, the yarns involved may have changed. If that is the case, it makes sense to re-spin other qedges that are involved in the changed yarns. That is exactly what we do. We keep spinning, until the yarns have stabilized. A few key questions need to be addressed: Do the yarns stabilize? If they stabilize, what have we got? Is this an efficient process? Termination of spinning \u00b6 Yes, spinning qedges until nothing changes any more, terminates, provided you do not try to spin qedges that are up-to-date. If the yarns around an edge have not changed, it does not make sense to spin that qedge. See here for proof. What have we got? \u00b6 After spinning, it is guaranteed that we have not thrown away results. All nodes that are parts of valid results, are still in the yarns. But, conversely, can it be that there are still nodes in the yarns that are not part of a result? Yes, that is possible. Only when the graph of qnodes and qedges does not have a cycle, we know that all members of all yarns occur at least once in a result. See here for proof. Quite a few interesting queries, however, have cycles in in their graphs. So, in those cases, spinning qedges will not cause the maximal narrowing down of the search space. Efficiency \u00b6 And that raises the question: how effective is the process of spinning qedges? The answer is: it depends. If your qnodes have strong conditions on them, so that the first yarn is already very thin, then every yarn that is connected to this one by a qedge has also the chance to get very thin after spinning. In this case, the combined filtering effect of all edges can produce a rapid narrowing of the search space. Especially if we can implement edge spinning in an optimized way, this works like a charm. When we come to stitching results (which is potentially very expensive), we have already achieved a massive reduction of work. But if none of the yarns is thin at the outset, spinning qedges will not result in appreciable thinning of the yarns, while it might be an enormous amount of work, depending on the actual relations involved. The good news is that it is possible to detect those situations. Text-Fabric estimates whether it makes sense to spin a qedge, and if not, it will just skip spinning that edge. Which will make the final result gathering (stitching) more expensive. There is more to efficiency than this. It turns out that the strategy by which you select the next qedge to be spun, influences the efficiency. In general, it is best to always start with the thinnest yarns, and select edges that affect them. Also here there is complication: not every qedge is equally expensive when computed over a yarn. It might be better to compute a cheaper edge over a thicker yarn. Stitching \u00b6 The last step is actually getting results. A result is a bunch of nodes, one from each yarn, in such a way that result nodes on yarns fulfil the relationships that the qedges of the search template dictate. If we can find such a set of nodes, we have stitched the yarns together. We call such a result a stitch . A stitch is a tuple of text nodes, each corresponding to exactly one qnode. It is not completely trivial to find stitches, let alone to collect them efficiently. The general procedure is as follows: choose a yarn to start with; try a node in that yarn as starting point pick a qedge from the qnode associated with the yarn (the source yarn), to another qnode and consider that yarn (the target yarn), find a node in the target yarn that is in the right relationship with the node selected in the source yarn, and so on, until all qedges have been used, if all has gone well, deliver the nodes found as a result. Let us look to these steps in a bit more detail. There is an element of choice, and it is very important to study how big this element of choice is in the various stages. First we select a yarn, and in that yarn a node. Usually we have many choices and at least one, because result seeking only makes sense if all yarns are non-empty. The third choice is the related node in the target yarn. Here we may encounter anything from zero, one or many choices. If there are zero choices, then we know that our provisional stitching of yarns so far cannot be completed into a full stitching of all yarns. If we have made choices to get this far, then some of these choices have not been lucky. We have to back-track and try other alternatives. If there is just one choice, it is easy: we pick the one and only possible node in the target yarn, without introducing new points of choice. If there are many choices, we have to try them all, one by one. Some might lead to a full stitch, others not. An important situation to be aware of, is when a qedge leads the stitching process to a yarn, out of which a node has already been chosen by an earlier step. This is very well possible, since the search template might have cycles in the qedges, or multiple qedges arrive at the same qnode. When this happens, we do not have to select a target node, we only have to check whether the target node that has been selected before, stands in the right relationship to the current source node. The relationship, that is, which is dictated by the current qedge that we are at. If so, we can stitch on with other edges, without introducing choice points (very much like the one-choice above). If the relation fails to hold, this stitch is doomed, and we have to back-track (very much like the zero-choice above). Strategy of stitching \u00b6 The steps involved in stitching as described above are clear, but less clear is what yarn we shall select to start with, and in which order we shall follow the edges. We need a strategy, and multiple strategies might lead to the same results, albeit with varying efficiency. In Text-Fabric we employ a strategy, that makes the narrowest choices first. We call a choice narrow if there are few alternatives to choose from, and broad if there are many alternatives. By giving precedence to narrow choices, we prune larger parts of the search tree when we fail. If we are stitching, the more nodes we have gathered in our stitch, the greater the chance that a blocking relationship is encountered, i.e. a relationship that should hold between the nodes gathered so far, but which in fact does not hold. So we want to get as many nodes in our stitch as quickly as possible. If our search tree is narrowly branching near the root, and broadly branching near the leaves, the top n levels of the tree contain relatively few nodes. So we have relatively few possibilities to stitch n nodes together, and most reasons to fail will happen while visiting these n levels. If on the other hand our search tree is broadly branching near the root, and narrowly branching near the leaves, the top n levels of the tree contain many nodes. We will visit many nodes and try many stitchings of length n , of which a lot will fail. I have also tried a different, more complicated strategy, which is still implemented, and which can be used by means of an optional argument to S.study() , but results of this strategy were not particularly good. Small-first strategy \u00b6 Here is the small-first strategy in a bit more detail. we choose the smallest yarn to start with; for every qedge we estimate its current spread , i.e. how many targets it has per source on average, relative to the current source and target yarns; at every step there are three kinds of qedges: qedges that go between qnodes of which we have already stitched the yarns qedges that go from a yarn that is already part of the stitch to a yarn outside the stitch qedges that do not start at a yarn in the current stitch at every step, we first process all qedges of type (i), in arbitrary order; we select one edge with minimal spread out of type (ii) and process it; we postpone all edges of type (iii); we redetermine which edges are in all types. It cannot happen that at the end we have not visited all qnodes and yarns, because we have assumed that our search template consists of one connected component. Every qnode can be reached from every other through a series of qedges. So, as we perform step after step, as long as there are qnodes in type (iii), we can be sure that there are also qnodes in a path from the qnodes we have visited to the type (iii) qnodes. At least one of the qnodes in that path will be a type (ii) node. In the end there will no type (iii) nodes be left. We have added a few more things to optimize the process. A relationship between a source yarn and a target yarn can also be considered in the opposite direction. If its spread in the opposite direction is less than its spread in the normal direction, we use the opposite direction. Secondly, before we start stitching, we can compute the order of qedges that we will use for every stitch. We then sort the qnodes according to the order by which they will be encountered when we work through the qedges. When we are stitching, in the midst of a partial stitch, it is always the case that we have stitched qnodes 1 .. n for some n , and we still have to stitch all qnodes above n . That means that when we try to finish partial stitches of which an initial part has been fixed, the search process will not change that initial part of the stitch. Only when the algorithm has exhausted all possibilities based on that initial part, it will change the last node of the initial part, replace it by other options, and start searching further. This means that we just can maintain our partial stitch in a single list. We do not have to assemble many partial stitches as separate immutable tuples. We have implemented our deliver function as a generator, that walks over all stitch possibilities while maintaining just one current stitch. When the stitch has been completely filled in, a copy of it will be yielded, after which back-tracking occurs, by which the current stitch will get partly undefined, only to be filled up again by further searching. Read it all in the source code: def stitchOn(e) .","title":"Search"},{"location":"Model/Search/#search-design","text":"","title":"Search Design"},{"location":"Model/Search/#fabric-metaphor","text":"The search space is a massive fabric of interconnected material. In it we discern the structures we are interested in: little pieces of fabric, also with interconnected material. When we search, we have a fabric in mind, woven from specific material, stitched together in a specific manner. Search in Text-Fabric works exactly like this: you give a sample patch, and Text-Fabric fetches all pieces of the big fabric that match your patch. The textile metaphor is particularly suited for grasping the search part of Text-Fabric, so I'm going to stick to it for a while. I have used it in the actual code as well, and even in the proofs that certain parts of the algorithm terminate and are correct. Yet it remains a metaphor, and the fit is not exact. The basic pattern of search is this: textile text example take several fleeces pick the nodes corresponding to a node type word s, phrase s, clause s, verse s spin thick yarns from them filter by feature conditions part-of-speech=verb gender= f book=Genesis vt spin the yarns further into thin yarns throw away nodes that do not have the right connections feature conditions on a verse also affect the search space for sentences, clauses, etc., and vice versa stitch the yarns together with thread build results by selecting a member for every filtered node set word node 123456 in phrase node 657890 in clause node 490567 in verse node 1403456 We will explain the stages of the fabrication process in detail.","title":"Fabric metaphor"},{"location":"Model/Search/#fleece","text":"A fleece corresponds with a very simple search template that asks for all objects of a given type: 1 word or 1 clause or, asking for multiple types: 1 2 3 4 5 verse clause phrase lex word Fleeces are the raw material from which we fabricate our search results. Every node type, such as word , sentence , book corresponds to a fleece. In Text-Fabric, every node has exactly one node type, so the whole space is neatly divided into a small set of fleeces. The most important characteristic of a fleece is it size: the number of nodes in a node type.","title":"Fleece"},{"location":"Model/Search/#spinning-thick-yarn","text":"Consider search templates where we ask for specific members of a node type, by giving feature constraints: 1 2 3 4 5 6 verse book=Genesis clause type=rela phrase determined=yes lex id=jc/ word number=pl vt vt Every line in this search templates we call an atom : a node type plus a feature specification. The result of an atom is the set of all nodes in that node type that satisfy those feature conditions. Finding the results of an atom corresponds with first thing that we do with a fleece: spin a thick yarn from it. Yarns in general are obtained by spinning fleeces, i.e. by filtering node sets that correspond to a node type. A search template may contain multiple atoms. Text-Fabric collects all atoms of a template, grabs the corresponding fleeces, and spins thick yarns from them. For each atom it will spin a yarn, and if there are several atoms referring to the same node type, there will be several yarns spun from that fleece. This spinning of thick yarns out of fleeces happens in just one go. All fleeces together contain exactly all nodes, so Text-Fabric walks in one pass over all nodes, applies the feature conditions, and puts the nodes into the yarns depending on which conditions apply. By the way, for the Hebrew dataset, we have 1.4 million nodes, and a typical pass requires a fraction of a second. The most important characteristic of a yarn is its thickness , which we define as the number of nodes in the yarn divided by the number of nodes in the fleece. For example, a yarn consisting of the book node of Genesis only, has a thickness of 1/39, because there are 39 books in the fleece. A much thicker yarn is that of the verbs, which has a thickness of roughly 1/6. A very thin thread is that of the word <CQH (which occurs only once) with a thickness of only 1/400,000.","title":"Spinning thick yarn"},{"location":"Model/Search/#spinning-thin-yarns","text":"In order to find results, we have to further narrow down the search space. In other words, we are going to spin our thick yarns into thinner and thinner yarns. Before we can do that, we should make one thing clear.","title":"Spinning thin yarns"},{"location":"Model/Search/#connected-by-constraints","text":"If the template above were complete, it would lead to a monstrous number of results. Because a result of a template like this is any combination of verse-, clause-, phrase-, lex-, word nodes that individually satisfy their own atom condition. So the number of results is the product of the number of results of the individual atoms, which is pretty enormous. It is hard to imagine a situation where these results could be consumed. Usually, there are constraints active between the atoms. For example in a template like this: 1 2 3 4 5 6 7 8 1 verse book=Genesis 2 clause type=rela 3 phrase determined=yes 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l The meaning of this template is that we look for a verse that (1) claims to be in Genesis (2) has a clause whose type is rela (3) which in turn has a phrase of a determined character (4) which contains a word in the plural and that has a verbal tense (vt) There should also be a (6) lex object, identified by jc/ which is connected to the rest by the constraint that (8) the word of line 4 is contained in it. Note that all atoms are linked by constraints into one network. In graph theoretical terms: this template consists of exactly one connected component . If this were not so, we would have in fact two independent search tasks, where the result set would be the (cartesian) product of the result sets of the separate components. For example, if line 8 were missing, we would effectively search for things that match lines 1-4, and, independently, for things that match line 6. And every result of the first part, combined with any result of the second part, would be a valid result of the whole. Well, Text-Fabric is nobody's fool: it refuses to accept two search tasks at the same time, let alone that it wants to waste time to generate all the results in the product. You will have to fire those search tasks one by one, and it is up to you how you combine the results. The upshot it: the atoms in the search template should form a network, connected by constraints . Text-Fabric will check this, and will only work with search templates that have only one connected component.","title":"Connected by constraints"},{"location":"Model/Search/#terminology","text":"By now we have arrived at the idea that our search template is a graph underneath: what we have called atoms are in fact the nodes, and what we have called constraints , are the edges. From now on, we will call the atoms qnodes and the constraints qedges . The q is to distinguish the nodes and the edges from the nodes and the edges of your dataset, the text nodes and text edges. When we use the term nodes and edges we will always refer to text nodes and edges. When we are searching, we maintain a yarn for every qnode . This yarn starts out to be the thick yarn as described above, but we are going to thin them. We can also see how our query templates are really topographic : a query template is a piece of local geography that we want to match against the data. Finding results is nothing else than instantiating qnodes of the search template by text nodes in such a way that the qedges hold between the text edges.","title":"Terminology"},{"location":"Model/Search/#spinning-a-qedge","text":"So, where were we? We have spun thick threads based on the qnodes individually, but we have not done anything with the qedges . That is going to change now. Consider this piece of search template: 1 2 3 4 5 4 w:word number=pl vt 5 6 l:lex id=jc/ 7 8 w ]] l So our qnodes are w and l , and our qedge is w ]] l . Note that a lex object is the set of all occurrences of a lexeme. So w ]] l says that w is embedded in l , in other words, w is a slot contained in the slots of l . It is nothing else than the word w is an instance of the lexeme jc/ . We will now check the pairs of (lex, word)-nodes in the text, where the lex node is taken from the yarn of the qnode l , and the word from the yarn of the qnode w . We can throw away some words from the yarn of w , namely those words that do not lie in a lexeme that is in the yarn of l . In other words: the words that are not instances of lexeme jc/ are out! Conversely, if the lexeme jc/ does not have occurrences in the plural and with a verbal tense, we must kick it out of the yarn of l , leaving no members in it. If a yarn gets empty, we have an early detection that the search yields no results, and the whole process stops. In our case, however, this is not so, and we continue. This pass over the yarns at both sides of a qedge is a spin action. We spin this qedge, and the result is that the two yarns become spun more thinly, hopefully. With the yarn of words severely spun out, we are going to the next qedge, the one between words and phrases. 1 2 3 phrase determined=yes 4 w:word number=pl vt The indent is an implicit way of saying that the \"embeds\" relation [[ holds between the phrase and the word . An equivalent formulation of the template is 1 2 3 4 p:phrase determined=yes w:word number=pl vt p [[ w We race along the yarn w of remaining words and check for each word if it is contained in a phrase in the yarn of p , the determined phrases. If it is not, we throw the word out of the yarn of w . Similarly, we can throw out some phrases from the yarn of p , namely those phrases that do not contain words in the yarn of w . In other words: the phrases without plural words and verbal tense are also out. We continue spinning, now between phrases and clauses. 1 2 2 clause type=rela 3 phrase determined=yes Here we loose the phrases that are not contained in a clause of type=rela , and we loose all clauses that do not embed one of the few phrases left. The last spin action corresponds with 1 2 1 verse book=Genesis 2 clause type=rela So we throw away all our results if they are outside Genesis. We end up with a set of thin yarns, severely thinned out, even. This will be a good starting point for the last stage: picking members from each yarn to form results. We call this stitching and we'll get there in a moment.","title":"Spinning a qedge"},{"location":"Model/Search/#the-spread-of-a-qedge","text":"A very important property of a qedge is its spread . A qedge links every node n in its from -yarn to zero, one, or more nodes in its to -yarn. The number of nodes in the to -yarn is a key property. The average number of nodes m in the to -yarn per linked node n in the from -yarn is the spread of the edge. A few examples: An edge that corresponds to ]] , n embeds m . If this edge goes from books to words, then every book node n is linked to every one of its words. So very n has hundreds or thousands m s. The spread will roughly be 425,000 / 39 =~ 10,000 The opposite edge has a spread of exactly 1, because every word belongs to exactly one book. Edges with spread 1 are very pleasant for our stitching algorithm later on. An edge corresponding to = . These qedges are super efficient, because their relation = is a breeze to compute, and they have always a spread 1 in both directions. An edge corresponding to # , the node inequality relation. The relation # is still a breeze to compute, but the result is heavy: the set of all nodes not equal to a given node. The spread is nearly 100% of the yarn length, in both directions. These edges are not worth to spin, because if you have two yarns, no node will be excluded: if you have an n in the from -yarn, you will always be able to find a different n in the to -yarn (except when bot yarns are equal, and contain just one node). An edge corresponding to == , the relation between nodes that are linked to the same set of slots. The spread of this relation is not too big, but the cost of computing it adds up quickly when applied to many cases.","title":"The spread of a qedge"},{"location":"Model/Search/#spinning-all-qedges","text":"Let us describe the spinning of yarns along edges in a bit more general way, and reflect on what it does for us. We spin all qedges of a template. But after spinning a qedge, the yarns involved may have changed. If that is the case, it makes sense to re-spin other qedges that are involved in the changed yarns. That is exactly what we do. We keep spinning, until the yarns have stabilized. A few key questions need to be addressed: Do the yarns stabilize? If they stabilize, what have we got? Is this an efficient process?","title":"Spinning all qedges"},{"location":"Model/Search/#termination-of-spinning","text":"Yes, spinning qedges until nothing changes any more, terminates, provided you do not try to spin qedges that are up-to-date. If the yarns around an edge have not changed, it does not make sense to spin that qedge. See here for proof.","title":"Termination of spinning"},{"location":"Model/Search/#what-have-we-got","text":"After spinning, it is guaranteed that we have not thrown away results. All nodes that are parts of valid results, are still in the yarns. But, conversely, can it be that there are still nodes in the yarns that are not part of a result? Yes, that is possible. Only when the graph of qnodes and qedges does not have a cycle, we know that all members of all yarns occur at least once in a result. See here for proof. Quite a few interesting queries, however, have cycles in in their graphs. So, in those cases, spinning qedges will not cause the maximal narrowing down of the search space.","title":"What have we got?"},{"location":"Model/Search/#efficiency","text":"And that raises the question: how effective is the process of spinning qedges? The answer is: it depends. If your qnodes have strong conditions on them, so that the first yarn is already very thin, then every yarn that is connected to this one by a qedge has also the chance to get very thin after spinning. In this case, the combined filtering effect of all edges can produce a rapid narrowing of the search space. Especially if we can implement edge spinning in an optimized way, this works like a charm. When we come to stitching results (which is potentially very expensive), we have already achieved a massive reduction of work. But if none of the yarns is thin at the outset, spinning qedges will not result in appreciable thinning of the yarns, while it might be an enormous amount of work, depending on the actual relations involved. The good news is that it is possible to detect those situations. Text-Fabric estimates whether it makes sense to spin a qedge, and if not, it will just skip spinning that edge. Which will make the final result gathering (stitching) more expensive. There is more to efficiency than this. It turns out that the strategy by which you select the next qedge to be spun, influences the efficiency. In general, it is best to always start with the thinnest yarns, and select edges that affect them. Also here there is complication: not every qedge is equally expensive when computed over a yarn. It might be better to compute a cheaper edge over a thicker yarn.","title":"Efficiency"},{"location":"Model/Search/#stitching","text":"The last step is actually getting results. A result is a bunch of nodes, one from each yarn, in such a way that result nodes on yarns fulfil the relationships that the qedges of the search template dictate. If we can find such a set of nodes, we have stitched the yarns together. We call such a result a stitch . A stitch is a tuple of text nodes, each corresponding to exactly one qnode. It is not completely trivial to find stitches, let alone to collect them efficiently. The general procedure is as follows: choose a yarn to start with; try a node in that yarn as starting point pick a qedge from the qnode associated with the yarn (the source yarn), to another qnode and consider that yarn (the target yarn), find a node in the target yarn that is in the right relationship with the node selected in the source yarn, and so on, until all qedges have been used, if all has gone well, deliver the nodes found as a result. Let us look to these steps in a bit more detail. There is an element of choice, and it is very important to study how big this element of choice is in the various stages. First we select a yarn, and in that yarn a node. Usually we have many choices and at least one, because result seeking only makes sense if all yarns are non-empty. The third choice is the related node in the target yarn. Here we may encounter anything from zero, one or many choices. If there are zero choices, then we know that our provisional stitching of yarns so far cannot be completed into a full stitching of all yarns. If we have made choices to get this far, then some of these choices have not been lucky. We have to back-track and try other alternatives. If there is just one choice, it is easy: we pick the one and only possible node in the target yarn, without introducing new points of choice. If there are many choices, we have to try them all, one by one. Some might lead to a full stitch, others not. An important situation to be aware of, is when a qedge leads the stitching process to a yarn, out of which a node has already been chosen by an earlier step. This is very well possible, since the search template might have cycles in the qedges, or multiple qedges arrive at the same qnode. When this happens, we do not have to select a target node, we only have to check whether the target node that has been selected before, stands in the right relationship to the current source node. The relationship, that is, which is dictated by the current qedge that we are at. If so, we can stitch on with other edges, without introducing choice points (very much like the one-choice above). If the relation fails to hold, this stitch is doomed, and we have to back-track (very much like the zero-choice above).","title":"Stitching"},{"location":"Model/Search/#strategy-of-stitching","text":"The steps involved in stitching as described above are clear, but less clear is what yarn we shall select to start with, and in which order we shall follow the edges. We need a strategy, and multiple strategies might lead to the same results, albeit with varying efficiency. In Text-Fabric we employ a strategy, that makes the narrowest choices first. We call a choice narrow if there are few alternatives to choose from, and broad if there are many alternatives. By giving precedence to narrow choices, we prune larger parts of the search tree when we fail. If we are stitching, the more nodes we have gathered in our stitch, the greater the chance that a blocking relationship is encountered, i.e. a relationship that should hold between the nodes gathered so far, but which in fact does not hold. So we want to get as many nodes in our stitch as quickly as possible. If our search tree is narrowly branching near the root, and broadly branching near the leaves, the top n levels of the tree contain relatively few nodes. So we have relatively few possibilities to stitch n nodes together, and most reasons to fail will happen while visiting these n levels. If on the other hand our search tree is broadly branching near the root, and narrowly branching near the leaves, the top n levels of the tree contain many nodes. We will visit many nodes and try many stitchings of length n , of which a lot will fail. I have also tried a different, more complicated strategy, which is still implemented, and which can be used by means of an optional argument to S.study() , but results of this strategy were not particularly good.","title":"Strategy of stitching"},{"location":"Model/Search/#small-first-strategy","text":"Here is the small-first strategy in a bit more detail. we choose the smallest yarn to start with; for every qedge we estimate its current spread , i.e. how many targets it has per source on average, relative to the current source and target yarns; at every step there are three kinds of qedges: qedges that go between qnodes of which we have already stitched the yarns qedges that go from a yarn that is already part of the stitch to a yarn outside the stitch qedges that do not start at a yarn in the current stitch at every step, we first process all qedges of type (i), in arbitrary order; we select one edge with minimal spread out of type (ii) and process it; we postpone all edges of type (iii); we redetermine which edges are in all types. It cannot happen that at the end we have not visited all qnodes and yarns, because we have assumed that our search template consists of one connected component. Every qnode can be reached from every other through a series of qedges. So, as we perform step after step, as long as there are qnodes in type (iii), we can be sure that there are also qnodes in a path from the qnodes we have visited to the type (iii) qnodes. At least one of the qnodes in that path will be a type (ii) node. In the end there will no type (iii) nodes be left. We have added a few more things to optimize the process. A relationship between a source yarn and a target yarn can also be considered in the opposite direction. If its spread in the opposite direction is less than its spread in the normal direction, we use the opposite direction. Secondly, before we start stitching, we can compute the order of qedges that we will use for every stitch. We then sort the qnodes according to the order by which they will be encountered when we work through the qedges. When we are stitching, in the midst of a partial stitch, it is always the case that we have stitched qnodes 1 .. n for some n , and we still have to stitch all qnodes above n . That means that when we try to finish partial stitches of which an initial part has been fixed, the search process will not change that initial part of the stitch. Only when the algorithm has exhausted all possibilities based on that initial part, it will change the last node of the initial part, replace it by other options, and start searching further. This means that we just can maintain our partial stitch in a single list. We do not have to assemble many partial stitches as separate immutable tuples. We have implemented our deliver function as a generator, that walks over all stitch possibilities while maintaining just one current stitch. When the stitch has been completely filled in, a copy of it will be yielded, after which back-tracking occurs, by which the current stitch will get partly undefined, only to be filled up again by further searching. Read it all in the source code: def stitchOn(e) .","title":"Small-first strategy"},{"location":"Server/Common/","text":"Common Server Related Functions \u00b6 About \u00b6 About Here are functions that are being used by various parts of the TF browser infrastructure, such as kernel.py web.py start.py Argument parsing \u00b6 Apologies Really, we should replace the whole adhoc argument parsing by a decent use of the Python module argparse . getDebug() Checks whether one of the arguments with which the script is called is a -d . getParam(interactive=False) Checks whether a dataSource parameter has been passed on the command line. If so, it checks whether it specifies an existing app. If no dataSource has been passed, and interactive is true, presents the user with a list of valid choices and asks for input. Locating the app \u00b6 The problem The data source specific apps are bundled inside the TF package. The webserver of the TF browser needs the files in those apps, not as Python modules, but just as files on disk. So we have to tell the webserver where they are, and we really do not know that in advance, because it is dependent on how the text-fabric package has been installed by pip3 on your machine. Yet we have found a way through the labyrinth! getConfig(dataSource) Retrieves the config.py from the specified dataSource by dynamically importing it as a module from one of the *-app packages in tf.extra See also App structure getAppdir(myDir, dataSource) The code in web.py will pass its file location as myDir . Form there this function computes the locstion of the file in which the webapp of the dataSource resides: the location of the dataSource -app package in tf.extra . See also App structure Getting and setting form values \u00b6 Request and response The TF browser user interacts with the web app by clicking and typing, as a result of which a HTML form gets filled in. This form as regularly submitted to the webserver with a request for a new incarnation of the page: a response. The values that come with a request, must be peeled out of the form, and stored as logical values. Most of the data has a known function to the webserver, but there is also a list of webapp dependent options. The following functions deal with option values. getValues(options, form) Given a tuple of option specifications and form data from a web request, returns a dictionary of filled in values for those options. The options are specified in the config.py of an app. An option specification is a tuple of the following bits of information: name of the input element in the HTML form type of input (e.g. checkbox) value of html id attribute of the input element label for the input element Cunei options The options for the C, such as the phono transcriptions, you can sayunei app are: 1 2 3 4 options = ( ( 'lineart' , 'checkbox' , 'linea' , 'show lineart' ), ( 'lineNumbers' , 'checkbox' , 'linen' , 'show line numbers' ), ) This function isolates the option values from the rest of the form values, so that it can be passed as a whole ( **values ) to the app specific API. setValues(options, source, form) Fills in a form dictionary based on values in a source dictionary, but only insofar the keys to be filled out occur in the options specs, and with a cast of checkbox values to booleans. This function is used right after reading the form off a request. Raw form data is turned into logical data for further processing by the webserver. HTML formatting \u00b6 HTML generation Here we generate the HTML for bigger chunks on the page. pageLinks(nResults, position, spread=10) Provide navigation links for results sets, big or small. It creates links around position in a set of nResults . The spread indicates how many links before and after position are generated in each column. There will be multiple columns. The right most column contains links to results position - spread to position + spread . Left of that there is a column for results position - spread*spread to position + spread*spread , stepping by spread . And so on, until the stepping factor becomes bigger than the result set. shapeMessages Wraps error messages into HTML. The messages come from the TF API, through the TF kernel, in response to wrong search templates and other mistaken user input. shapeOptions Wraps the options, specified by the option specification in config.py into HTML. See also App structure shapeCondense Provides a radio-buttoned chooser for the condense types .","title":"Common"},{"location":"Server/Common/#common-server-related-functions","text":"","title":"Common Server Related Functions"},{"location":"Server/Common/#about","text":"About Here are functions that are being used by various parts of the TF browser infrastructure, such as kernel.py web.py start.py","title":"About"},{"location":"Server/Common/#argument-parsing","text":"Apologies Really, we should replace the whole adhoc argument parsing by a decent use of the Python module argparse . getDebug() Checks whether one of the arguments with which the script is called is a -d . getParam(interactive=False) Checks whether a dataSource parameter has been passed on the command line. If so, it checks whether it specifies an existing app. If no dataSource has been passed, and interactive is true, presents the user with a list of valid choices and asks for input.","title":"Argument parsing"},{"location":"Server/Common/#locating-the-app","text":"The problem The data source specific apps are bundled inside the TF package. The webserver of the TF browser needs the files in those apps, not as Python modules, but just as files on disk. So we have to tell the webserver where they are, and we really do not know that in advance, because it is dependent on how the text-fabric package has been installed by pip3 on your machine. Yet we have found a way through the labyrinth! getConfig(dataSource) Retrieves the config.py from the specified dataSource by dynamically importing it as a module from one of the *-app packages in tf.extra See also App structure getAppdir(myDir, dataSource) The code in web.py will pass its file location as myDir . Form there this function computes the locstion of the file in which the webapp of the dataSource resides: the location of the dataSource -app package in tf.extra . See also App structure","title":"Locating the app"},{"location":"Server/Common/#getting-and-setting-form-values","text":"Request and response The TF browser user interacts with the web app by clicking and typing, as a result of which a HTML form gets filled in. This form as regularly submitted to the webserver with a request for a new incarnation of the page: a response. The values that come with a request, must be peeled out of the form, and stored as logical values. Most of the data has a known function to the webserver, but there is also a list of webapp dependent options. The following functions deal with option values. getValues(options, form) Given a tuple of option specifications and form data from a web request, returns a dictionary of filled in values for those options. The options are specified in the config.py of an app. An option specification is a tuple of the following bits of information: name of the input element in the HTML form type of input (e.g. checkbox) value of html id attribute of the input element label for the input element Cunei options The options for the C, such as the phono transcriptions, you can sayunei app are: 1 2 3 4 options = ( ( 'lineart' , 'checkbox' , 'linea' , 'show lineart' ), ( 'lineNumbers' , 'checkbox' , 'linen' , 'show line numbers' ), ) This function isolates the option values from the rest of the form values, so that it can be passed as a whole ( **values ) to the app specific API. setValues(options, source, form) Fills in a form dictionary based on values in a source dictionary, but only insofar the keys to be filled out occur in the options specs, and with a cast of checkbox values to booleans. This function is used right after reading the form off a request. Raw form data is turned into logical data for further processing by the webserver.","title":"Getting and setting form values"},{"location":"Server/Common/#html-formatting","text":"HTML generation Here we generate the HTML for bigger chunks on the page. pageLinks(nResults, position, spread=10) Provide navigation links for results sets, big or small. It creates links around position in a set of nResults . The spread indicates how many links before and after position are generated in each column. There will be multiple columns. The right most column contains links to results position - spread to position + spread . Left of that there is a column for results position - spread*spread to position + spread*spread , stepping by spread . And so on, until the stepping factor becomes bigger than the result set. shapeMessages Wraps error messages into HTML. The messages come from the TF API, through the TF kernel, in response to wrong search templates and other mistaken user input. shapeOptions Wraps the options, specified by the option specification in config.py into HTML. See also App structure shapeCondense Provides a radio-buttoned chooser for the condense types .","title":"HTML formatting"},{"location":"Server/Kernel/","text":"Text-Fabric kernel \u00b6 About \u00b6 TF kernel Text-Fabric can be used as a service. The full API of Text-Fabric needs a lot of memory, which makes it unusably for rapid successions of loading and unloading, like when used in a webserver context. However, you can start TF as a service process, after which many clients can connect to it, all looking at the same (read-only) data. We call this a TF kernel . The API that the TF kernel offers is limited, it is primarily template search that is offered. see Kernel API below. The code in tf.server.kernel explains how it works. Start \u00b6 Run You can run the TF kernel as follows: 1 python3 -m tf.server.kernel ddd where ddd is one of the supported apps Example See the start-up script of the text-fabric browser. Connect \u00b6 Connect The TF kernel can be connected by an other Python program as follows: 1 2 3 from tf.server.kernel import makeTfConnection TF = makeTfConnection ( host , port ) api = TF . connect () After this, api can be used to obtain information from the TF kernel. Example See the webserver of the text-fabric browser. Kernel API \u00b6 About The API of the TF kernel is created by the function makeTfKernel in the data module of the server subpackage. It returns a class TfKernel with a number of exposed methods that can be called by other programs. For the machinery of interprocess communication we rely on the rpyc module. See especially the docs on services . Shadow objects The way rpyc works in the case of data transmission has a pitfall. When a service returns a Python object to the client, it does not return the object itself, but only a shadow object so called netref objects. This strategy is called boxing . To the client the shadow object looks like the real thing, but when the client needs to access members, they will be fetched on the fly. This is a performance problem when the service sends a big list or dict, and the client iterates over all its items. Each item will be fetched in a separate interprocess call, which causes an enormous overhead. Boxing only happens for mutable objects. And here lies the work-around: The service must send big chunks of data as immutable objects, such as tuples. They are sent within a single interprocess call, and fly swiftly through the connecting pipe. header() Calls the header() method of the extraApi, which fetches all the stuff to create a header on the page with links to data and documentation of the data source. provenance() Calls the provenance() method of the extraApi, which fetches provenance metadata to be shown on exported pages. css() Calls the loadCSS() method of the extraApi, which delivers the CSS code to be inserted on the browser page. condenseTypes() Fetches several things from the extraApi and the generic TF api: condenseType : the default node type that acts as a container for representing query results; for Bhsa it is verse , for Cunei it is tablet ; exampleSection : an example for the help text for this data source; levels : information about the node types in this data source. search() The work horse of this API. Executes a TF search template, retrieves formatted results, retrieves formatted results for additional nodes and sections. Parameters: query Search template to be executed. Typically coming from the search pad in the browser. tuples Tuples of nodes to look up features for. Typically coming from the node pad in the browser. sections Sections to look up features for. Typically coming from the section pad in the browser. For the Bhsa these are verse references, for Cunei these are tablets by P-number . condensed Whether or not the results should be condensed . Normally, results come as tuples of nodes, and each tuple is shown in a corresponding table row in plain or pretty display. But you can also condense results in container objects. All tuples will be inspected, and the nodes of each tuple will be gathered in containers, and these containers will be displayed in table rows. What is lost is the notion of an individual result, and what is gained is a better overview of where the parts of the results are. condenseType When condensing results, you can choose the node type that acts as container. Nodes get suppressed Nodes in result tuples that have a type that is bigger than the condenseType, will be skipped. E.g. if you have chapter nodes in your results, but you condense to verses, the chapter nodes will not show up. But if you condense to books, they will show up. batch The number of table rows to show on one page in the browser. position=1 The position that is central in the browser. The navigation links take this position as the focus point, and enable the user to navigate to neighbouring results, in ever bigger strides. opened=set() Which results have been expanded and need extended results. Normally, only the information to provide a plain representation of a result is being fetched, but for the opened ones information is gathered for pretty displays. withNodes=False Whether to include the node numbers into the formatted results. linked=1 Which column in the results should be hyperlinked to online representations closest to the objects in that column. Counting columns starts at 1. options Additional keyword arguments are passed as options to the underlying API. For example, the Cunei API accepts linenumbers and lineart , which will ask to include line numbers and lineart in the formatted results. csvs() This is an other workhorse. It also asks for the things search() is asking for, but it does not want formatted results. It will get tabular data of result nodes, one for the sections , one for the node tuples , and one for the search results . For every node that occurs in this tabular data, features will be looked up. All loaded features will be looked up for those nodes. The result is a big table of nodes and feature values. The parameters are query , tuples , sections , condensed , condenseType and have the same meaning as in search() above.","title":"Kernel"},{"location":"Server/Kernel/#text-fabric-kernel","text":"","title":"Text-Fabric kernel"},{"location":"Server/Kernel/#about","text":"TF kernel Text-Fabric can be used as a service. The full API of Text-Fabric needs a lot of memory, which makes it unusably for rapid successions of loading and unloading, like when used in a webserver context. However, you can start TF as a service process, after which many clients can connect to it, all looking at the same (read-only) data. We call this a TF kernel . The API that the TF kernel offers is limited, it is primarily template search that is offered. see Kernel API below. The code in tf.server.kernel explains how it works.","title":"About"},{"location":"Server/Kernel/#start","text":"Run You can run the TF kernel as follows: 1 python3 -m tf.server.kernel ddd where ddd is one of the supported apps Example See the start-up script of the text-fabric browser.","title":"Start"},{"location":"Server/Kernel/#connect","text":"Connect The TF kernel can be connected by an other Python program as follows: 1 2 3 from tf.server.kernel import makeTfConnection TF = makeTfConnection ( host , port ) api = TF . connect () After this, api can be used to obtain information from the TF kernel. Example See the webserver of the text-fabric browser.","title":"Connect"},{"location":"Server/Kernel/#kernel-api","text":"About The API of the TF kernel is created by the function makeTfKernel in the data module of the server subpackage. It returns a class TfKernel with a number of exposed methods that can be called by other programs. For the machinery of interprocess communication we rely on the rpyc module. See especially the docs on services . Shadow objects The way rpyc works in the case of data transmission has a pitfall. When a service returns a Python object to the client, it does not return the object itself, but only a shadow object so called netref objects. This strategy is called boxing . To the client the shadow object looks like the real thing, but when the client needs to access members, they will be fetched on the fly. This is a performance problem when the service sends a big list or dict, and the client iterates over all its items. Each item will be fetched in a separate interprocess call, which causes an enormous overhead. Boxing only happens for mutable objects. And here lies the work-around: The service must send big chunks of data as immutable objects, such as tuples. They are sent within a single interprocess call, and fly swiftly through the connecting pipe. header() Calls the header() method of the extraApi, which fetches all the stuff to create a header on the page with links to data and documentation of the data source. provenance() Calls the provenance() method of the extraApi, which fetches provenance metadata to be shown on exported pages. css() Calls the loadCSS() method of the extraApi, which delivers the CSS code to be inserted on the browser page. condenseTypes() Fetches several things from the extraApi and the generic TF api: condenseType : the default node type that acts as a container for representing query results; for Bhsa it is verse , for Cunei it is tablet ; exampleSection : an example for the help text for this data source; levels : information about the node types in this data source. search() The work horse of this API. Executes a TF search template, retrieves formatted results, retrieves formatted results for additional nodes and sections. Parameters: query Search template to be executed. Typically coming from the search pad in the browser. tuples Tuples of nodes to look up features for. Typically coming from the node pad in the browser. sections Sections to look up features for. Typically coming from the section pad in the browser. For the Bhsa these are verse references, for Cunei these are tablets by P-number . condensed Whether or not the results should be condensed . Normally, results come as tuples of nodes, and each tuple is shown in a corresponding table row in plain or pretty display. But you can also condense results in container objects. All tuples will be inspected, and the nodes of each tuple will be gathered in containers, and these containers will be displayed in table rows. What is lost is the notion of an individual result, and what is gained is a better overview of where the parts of the results are. condenseType When condensing results, you can choose the node type that acts as container. Nodes get suppressed Nodes in result tuples that have a type that is bigger than the condenseType, will be skipped. E.g. if you have chapter nodes in your results, but you condense to verses, the chapter nodes will not show up. But if you condense to books, they will show up. batch The number of table rows to show on one page in the browser. position=1 The position that is central in the browser. The navigation links take this position as the focus point, and enable the user to navigate to neighbouring results, in ever bigger strides. opened=set() Which results have been expanded and need extended results. Normally, only the information to provide a plain representation of a result is being fetched, but for the opened ones information is gathered for pretty displays. withNodes=False Whether to include the node numbers into the formatted results. linked=1 Which column in the results should be hyperlinked to online representations closest to the objects in that column. Counting columns starts at 1. options Additional keyword arguments are passed as options to the underlying API. For example, the Cunei API accepts linenumbers and lineart , which will ask to include line numbers and lineart in the formatted results. csvs() This is an other workhorse. It also asks for the things search() is asking for, but it does not want formatted results. It will get tabular data of result nodes, one for the sections , one for the node tuples , and one for the search results . For every node that occurs in this tabular data, features will be looked up. All loaded features will be looked up for those nodes. The result is a big table of nodes and feature values. The parameters are query , tuples , sections , condensed , condenseType and have the same meaning as in search() above.","title":"Kernel API"},{"location":"Server/Web/","text":"Web interface \u00b6 About \u00b6 Local web interface TF contains a local web interface in which you can enter a search template and view the results. This is realized by a web app based on bottle . This web app connects to the TF kernel and merges the retrieved data into a set of templates . See the code in tf.server.web . Start up \u00b6 Start up TF kernel, webserver and browser page are started up by means of a script called text-fabric , which will be installed in an executable directory by the pip installer. What the script does is the same as: 1 python3 -m tf.server.start Process management During start up the following happens: Kill previous processes The system is searched for non-terminated incarnations of the processes it wants to start up. If they are encountered, they will be killed, so that they cannot prevent a successful start up. TF kernel A TF kernel is started. This process loads the bulk of the TF data, so it can take a while. When it has loaded the data, it sends out a message that loading is done, which is picked up by the script. TF webserver A short while after receiving the \"data loading done\" message, the TF webserver is started. Debug mode If you have passed -d to the text-fabric script, the bottle will be started in debug and reload mode. That means that if you modify web.py or a module it imports, the webserver will reload itself automatically. When you refresh the browser you see the changes. If you have changed templates, the css, or the javascript, you should do a \"refresh from origin\". Load web page After a short while, the default web browser will be started with a url and port at which the webserver will listen. You see your browser being started up and the TF page being loaded. Waiting The script now waits till the webserver is finished. You finish it by pressing Ctrl-C, and if you have used the -d flag, you have to press it twice. Terminate the TF kernel At this point, the text-fabric script will terminate the TF kernel. Clean up Now all processes that have started up have been killed. If something went wrong in this sequence, chances are that a process keeps running. It will be terminated next time you call the text-fabric . You can kill too If you run 1 text-fabric -k all tf-browser-related processes will be killed. 1 text-fabric -k ddd will kill all such processes as far as they are for data source ddd . Routes \u00b6 Routes There are 4 kinds of routes in the web app: url pattern effect /server/static/... serves a static file from the server-wide static folder /data/static/... serves a static file from the app specific static folder /local/static/... serves a static file from a local directory specified by the app anything else submits the form with user data and return the processed request Templates \u00b6 Templates There are two templates in views : index : the normal template for returning responses to user requests; export : the template used for exporting results; it has printer/PDF-friendly formatting: good page breaks. Pretty displays always occur on a page by their own. It has very few user interaction controls. When saved as PDF from the browser, it is a neat record of work done, with DOI links to the corpus and to Text-Fabric. CSS \u00b6 CSS We format the web pages with CSS, with extensive use of flexbox . There are three sources of CSS formatting: the CSS loaded from the app dependent extraApi, used for pretty displays; main.css : the formatting of the index web page with which the user interacts; inside the export template, for formatting the exported page. Javascript \u00b6 Javascript We use a modest amount of Javascript on top of JQuery . For collapsing and expanding elements we use the details element. This is a convenient, Javascript-free way to manage collapsing. Unfortunately it is not supported by the Microsoft browsers, not even Edge. On Windows? Windows users should install Chrome of Firefox. Safari is fine.","title":"Web"},{"location":"Server/Web/#web-interface","text":"","title":"Web interface"},{"location":"Server/Web/#about","text":"Local web interface TF contains a local web interface in which you can enter a search template and view the results. This is realized by a web app based on bottle . This web app connects to the TF kernel and merges the retrieved data into a set of templates . See the code in tf.server.web .","title":"About"},{"location":"Server/Web/#start-up","text":"Start up TF kernel, webserver and browser page are started up by means of a script called text-fabric , which will be installed in an executable directory by the pip installer. What the script does is the same as: 1 python3 -m tf.server.start Process management During start up the following happens: Kill previous processes The system is searched for non-terminated incarnations of the processes it wants to start up. If they are encountered, they will be killed, so that they cannot prevent a successful start up. TF kernel A TF kernel is started. This process loads the bulk of the TF data, so it can take a while. When it has loaded the data, it sends out a message that loading is done, which is picked up by the script. TF webserver A short while after receiving the \"data loading done\" message, the TF webserver is started. Debug mode If you have passed -d to the text-fabric script, the bottle will be started in debug and reload mode. That means that if you modify web.py or a module it imports, the webserver will reload itself automatically. When you refresh the browser you see the changes. If you have changed templates, the css, or the javascript, you should do a \"refresh from origin\". Load web page After a short while, the default web browser will be started with a url and port at which the webserver will listen. You see your browser being started up and the TF page being loaded. Waiting The script now waits till the webserver is finished. You finish it by pressing Ctrl-C, and if you have used the -d flag, you have to press it twice. Terminate the TF kernel At this point, the text-fabric script will terminate the TF kernel. Clean up Now all processes that have started up have been killed. If something went wrong in this sequence, chances are that a process keeps running. It will be terminated next time you call the text-fabric . You can kill too If you run 1 text-fabric -k all tf-browser-related processes will be killed. 1 text-fabric -k ddd will kill all such processes as far as they are for data source ddd .","title":"Start up"},{"location":"Server/Web/#routes","text":"Routes There are 4 kinds of routes in the web app: url pattern effect /server/static/... serves a static file from the server-wide static folder /data/static/... serves a static file from the app specific static folder /local/static/... serves a static file from a local directory specified by the app anything else submits the form with user data and return the processed request","title":"Routes"},{"location":"Server/Web/#templates","text":"Templates There are two templates in views : index : the normal template for returning responses to user requests; export : the template used for exporting results; it has printer/PDF-friendly formatting: good page breaks. Pretty displays always occur on a page by their own. It has very few user interaction controls. When saved as PDF from the browser, it is a neat record of work done, with DOI links to the corpus and to Text-Fabric.","title":"Templates"},{"location":"Server/Web/#css","text":"CSS We format the web pages with CSS, with extensive use of flexbox . There are three sources of CSS formatting: the CSS loaded from the app dependent extraApi, used for pretty displays; main.css : the formatting of the index web page with which the user interacts; inside the export template, for formatting the exported page.","title":"CSS"},{"location":"Server/Web/#javascript","text":"Javascript We use a modest amount of Javascript on top of JQuery . For collapsing and expanding elements we use the details element. This is a convenient, Javascript-free way to manage collapsing. Unfortunately it is not supported by the Microsoft browsers, not even Edge. On Windows? Windows users should install Chrome of Firefox. Safari is fine.","title":"Javascript"}]}