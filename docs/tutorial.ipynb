{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loom.png\" align=\"left\" width=\"40%\"/>\n",
    "\n",
    "AD 1425 [Hausbücher der Nürnberger Zwölfbrüderstiftungen](http://www.nuernberger-hausbuecher.de/75-Amb-2-317-4-v/data)\n",
    "\n",
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# Tutorial\n",
    "\n",
    "This notebook gets you hands-on with\n",
    "[Text-Fabric](https://github.com/ETCBC/text-fabric) an API on richly annotated text.\n",
    "\n",
    "Below we show most of its functions in action on the BHSA data set (Hebrew Bible).\n",
    "\n",
    "In no time you will be a professional weaver.\n",
    "\n",
    "The tutorial is best understood after having familiarized yourself with the underlying\n",
    "[data model](https://github.com/ETCBC/text-fabric/wiki/Data-model).\n",
    "\n",
    "If you want to *get* this all, see the \n",
    "[home page](https://github.com/ETCBC/text-fabric/wiki)\n",
    "of the Text-Fabric wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Text-Fabric\n",
    "\n",
    "Everything starts by setting up Text-Fabric.\n",
    "It needs to know where to look for data.\n",
    "\n",
    "If you do not specify anything particular, text-fabric looks in some standard locations for a\n",
    "`text-fabric-data` directory. \n",
    "\n",
    "But here we want to illustrate text-fabric with the Hebrew Bible Text Database, a resource with a lot of\n",
    "high quality data in it.\n",
    "\n",
    "The data in this resource is so comprehensive, that some of the data is in a core repository,\n",
    "and some other pieces are in other repositories, as modules.\n",
    "\n",
    "All this data is versioned together.\n",
    "\n",
    "For the rest, we assume you have cloned [bhsa](https://github.com/ETCBC/bhsa)\n",
    "and [phono](https://github.com/ETCBC/phono)\n",
    "in your directory `~/github/etcbc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "118 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "DATABASE = '~/github/etcbc'\n",
    "VERSION = '2017'\n",
    "BHSA = f'bhsa/tf/{VERSION}'\n",
    "PHONO = f'phono/tf/{VERSION}'\n",
    "TF = Fabric(locations=[DATABASE], modules=[BHSA, PHONO], silent=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have added a module: `phono`. \n",
    "The  core data has a special 1-1 transcription from Hebrew to ASCII, but not a *phonetic* transcription,\n",
    "taking into account the usual pronunciation rules.\n",
    "\n",
    "I have made a \n",
    "[notebook](https://github.com/ETCBC/phono/blob/master/programs/phono.ipynb)\n",
    "that tries hard to find phonological representations for all the words.\n",
    "The notebook has added the result to Text-Fabric as a module.\n",
    "We'll encounter that later.\n",
    "\n",
    "**NB:** This is a real-world example of how to add data to an existing datasource as a module.\n",
    "\n",
    "And if you want to see what those standard locations are, look\n",
    "[here](https://github.com/Dans-labs/text-fabric/blob/master/tf/fabric.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features\n",
    "Specify the features to load, and receive the API to work with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.15s B g_word               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B qere                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B qere_trailer         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B trailer              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B voc_lex_utf8         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.27s B language             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B freq_lex             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B gloss                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.23s B mother               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s Feature overview: 111 for nodes; 5 for edges; 2 configs; 7 computed\n",
      "  6.27s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    sp lex voc_lex_utf8\n",
    "    g_word trailer\n",
    "    qere qere_trailer\n",
    "    language freq_lex gloss\n",
    "    mother\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see that `loadLog()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.03s B otype                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.60s B oslots               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M otext                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M otext@phono          from /Users/dirk/github/etcbc/phono/tf/2017\n",
      "   |     0.01s B book                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B chapter              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B verse                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B g_cons               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.19s B g_cons_utf8          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B g_lex                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.20s B g_lex_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.15s B g_word               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.22s B g_word_utf8          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.12s B lex0                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.18s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.21s B phono                from /Users/dirk/github/etcbc/phono/tf/2017\n",
      "   |     0.07s B phono_trailer        from /Users/dirk/github/etcbc/phono/tf/2017\n",
      "   |     0.00s B qere                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B qere_trailer         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B qere_trailer_utf8    from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B qere_utf8            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B trailer              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.09s B trailer_utf8         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B __levels__           from otype, oslots\n",
      "   |     0.04s B __order__            from otype, oslots, __levels__\n",
      "   |     0.03s B __rank__             from otype, __order__\n",
      "   |     1.17s B __levUp__            from otype, oslots, __rank__\n",
      "   |     0.92s B __levDown__          from otype, __levUp__, __rank__\n",
      "   |     0.33s B __boundary__         from otype, oslots, __rank__\n",
      "   |     0.02s B __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.13s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B voc_lex_utf8         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.27s B language             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B freq_lex             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B gloss                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.23s B mother               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@am              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@ar              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@bn              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@da              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@de              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@el              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@en              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@es              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@fa              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@fr              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@he              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@hi              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@id              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@ja              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@ko              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@la              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@nl              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@pa              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@pt              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@ru              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@sw              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@syc             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@tr              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@ur              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@yo              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B book@zh              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M code                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M det                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M dist                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M dist_unit            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M distributional_parent from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M domain               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M freq_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M function             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M functional_parent    from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_nme                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_nme_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_pfm                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_pfm_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_prs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_prs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_uvf                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_uvf_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbe                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbe_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M gn                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M instruction          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M is_root              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kind                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kq_hybrid            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kq_hybrid_utf8       from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M label                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M languageISO          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M lexeme_count         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M ls                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M mother_object_type   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nametype             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nme                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nu                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M number               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M omap@2016-2017       from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M otext                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M otext@phono          from /Users/dirk/github/etcbc/phono/tf/2017\n",
      "   |     0.00s M pargr                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M pdp                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M pfm                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_gn               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_nu               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_ps               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M ps                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rank_lex             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rank_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rela                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M root                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M st                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_gender        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_number        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_person        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M tab                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M txt                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M typ                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M uvf                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vbe                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vbs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M voc_lex              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vs                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vt                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n"
     ]
    }
   ],
   "source": [
    "api.loadLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it so that the members of the API are directly accessible as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to show that `F` and `N` are defined and store the same values as `api.F` and `api.N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.api.NodeFeatures object at 0x18f42d940>\n",
      "<tf.api.NodeFeatures object at 0x18f42d940>\n",
      "<bound method Api.N of <tf.api.Api object at 0x116828198>>\n",
      "<bound method Api.N of <tf.api.Api object at 0x116828198>>\n"
     ]
    }
   ],
   "source": [
    "print(F)\n",
    "print(api.F)\n",
    "print(N)\n",
    "print(api.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "In order to get acquainted with the data, we start with simple tasks: counting.\n",
    "\n",
    "## Count all nodes\n",
    "We use the \n",
    "[`N()` generator](https://github.com/ETCBC/text-fabric/wiki/Api#walking-through-nodes)\n",
    "to walk us through the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.38s 1446635 nodes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Counting nodes ...')\n",
    "i = 0\n",
    "for n in N(): i += 1\n",
    "info('{} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort some nodes\n",
    "\n",
    "Get some nodes, \n",
    "[slot](https://github.com/ETCBC/text-fabric/wiki/Data-model#summary)\n",
    "and non-slot, and sort them in the \n",
    "[canonical order](https://github.com/ETCBC/text-fabric/wiki/Api#sorting-nodes).\n",
    "\n",
    "The [`otype` feature](https://github.com/ETCBC/text-fabric/wiki/Data-model#otype-node-feature)\n",
    "is a\n",
    "[GRID feature](https://github.com/ETCBC/text-fabric/wiki/Data-model#more-about-the-grid),\n",
    "a special feature that provides defining characteristics for the\n",
    "data set as a whole. \n",
    "It tells us where the slots end and the other nodes start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[426585,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 426586,\n",
       " 426587,\n",
       " 426588,\n",
       " 426589,\n",
       " 426590,\n",
       " 426591,\n",
       " 426592,\n",
       " 426593]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortNodes(list(range(F.otype.maxSlot+1, F.otype.maxSlot+10))+list(range(1,11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers in the otype feature\n",
    "Get more information that is readily available in the \n",
    "[GRID feature](https://github.com/ETCBC/text-fabric/wiki/Data-model#more-about-the-grid)\n",
    "[`otype`](https://github.com/ETCBC/text-fabric/wiki/Data-model#otype-node-feature),\n",
    "namely what types of objects there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slotType  = word\n",
      "maxSlot  =426584\n",
      "maxNode  =1446635\n",
      "All otypes:\n",
      "\tbook\n",
      "\tchapter\n",
      "\tlex\n",
      "\tverse\n",
      "\thalf_verse\n",
      "\tsentence\n",
      "\tsentence_atom\n",
      "\tclause\n",
      "\tclause_atom\n",
      "\tphrase\n",
      "\tphrase_atom\n",
      "\tsubphrase\n",
      "\tword\n"
     ]
    }
   ],
   "source": [
    "info('{:<9} = {}\\n{:<9}={}\\n{:<9}={}'.format(\n",
    "    'slotType', F.otype.slotType,\n",
    "    'maxSlot', F.otype.maxSlot,\n",
    "    'maxNode', F.otype.maxNode,\n",
    "), tm=False)\n",
    "info('All otypes:\\n\\t', nl=False, tm=False)\n",
    "info('\\n\\t'.join(F.otype.all), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count individual object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s counting objects ...\n",
      "   |     0.00s      39 books\n",
      "   |     0.00s     929 chapters\n",
      "   |     0.00s    9233 lexs\n",
      "   |     0.01s   23213 verses\n",
      "   |     0.01s   45180 half_verses\n",
      "   |     0.01s   63711 sentences\n",
      "   |     0.01s   64486 sentence_atoms\n",
      "   |     0.01s   88101 clauses\n",
      "   |     0.02s   90669 clause_atoms\n",
      "   |     0.04s  253187 phrases\n",
      "   |     0.04s  267519 phrase_atoms\n",
      "   |     0.02s  113784 subphrases\n",
      "   |     0.06s  426584 words\n",
      "  0.25s Done\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('counting objects ...')\n",
    "for otype in F.otype.all:\n",
    "    i = 0\n",
    "    indent(level=1, reset=True)\n",
    "    for n in F.otype.s(otype): i+=1\n",
    "    info('{:>7} {}s'.format(i, otype))\n",
    "indent(level=0)\n",
    "info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "\n",
    "The content data resides in the features.\n",
    "The\n",
    "[`F` function](https://github.com/ETCBC/text-fabric/wiki/Api#node-features)\n",
    "gives access to that data.\n",
    "Every feature has a method\n",
    "[`freqList()`](https://github.com/ETCBC/text-fabric/wiki/Api#node-features)\n",
    "to generate a frequency list of its values, ordered by highest frequency first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('subs', 125558),\n",
       " ('verb', 75450),\n",
       " ('prep', 73298),\n",
       " ('conj', 62737),\n",
       " ('nmpr', 35696),\n",
       " ('art', 30387),\n",
       " ('adjv', 10075),\n",
       " ('nega', 6059),\n",
       " ('prps', 5035),\n",
       " ('advb', 4603),\n",
       " ('prde', 2678),\n",
       " ('intj', 1912),\n",
       " ('inrg', 1303),\n",
       " ('prin', 1026))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sp.freqList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexeme matters\n",
    "\n",
    "## Top 10 frequent verbs\n",
    "\n",
    "If we count the frequency of words, we usually mean the frequency of their\n",
    "corresponding lexemes.\n",
    "\n",
    "There are several methods for working with lexemes.\n",
    "\n",
    "### Method 1: counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.33s Done\n",
      ">MR[: 5378\n",
      "HJH[: 3561\n",
      "<FH[: 2629\n",
      "BW>[: 2570\n",
      "NTN[: 2017\n",
      "HLK[: 1554\n",
      "R>H[: 1298\n",
      "CM<[: 1168\n",
      "DBR[: 1138\n",
      "JCB[: 1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "indent(reset=True)\n",
    "info('Collecting data')\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    verbs[F.lex.v(w)] +=1\n",
    "info('Done')\n",
    "print(''.join(\n",
    "    '{}: {}\\n'.format(verb, cnt) for (verb, cnt) in sorted(\n",
    "        verbs.items() , key=lambda x: (-x[1], x[0]))[0:10],\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: counting lexemes\n",
    "\n",
    "An alternative way to do this is to use the feature `freq_lex`, defined for `lex` nodes.\n",
    "Now we walk the lexemes instead of the occurrences.\n",
    "Note that the feature `sp` (part-of-speech) is defined for nodes of type `word` as well as `lex`.\n",
    "Both also have the `lex` feature.\n",
    "Note that we might encounter Hebrew lexemes as well as Aramaic lexemes, so we still have to\n",
    "accumulate the `freq_lex`es of the lexeme nodes with the same lexeme value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.02s Done\n",
      ">MR[: 5378\n",
      "HJH[: 3561\n",
      "<FH[: 2629\n",
      "BW>[: 2570\n",
      "NTN[: 2017\n",
      "HLK[: 1554\n",
      "R>H[: 1298\n",
      "CM<[: 1168\n",
      "DBR[: 1138\n",
      "JCB[: 1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "indent(reset=True)\n",
    "info('Collecting data')\n",
    "for w in F.otype.s('lex'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    verbs[F.lex.v(w)] += F.freq_lex.v(w)\n",
    "info('Done')\n",
    "print(''.join(\n",
    "    '{}: {}\\n'.format(verb, cnt) for (verb, cnt) in sorted(\n",
    "        verbs.items() , key=lambda x: (-x[1], x[0]))[0:10],\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexeme distribution\n",
    "\n",
    "Let's do a bit more fancy lexeme stuff.\n",
    "\n",
    "### Hapaxes\n",
    "\n",
    "A hapax can be found by inspecting lexemes and see to how many word nodes they are linked.\n",
    "If that is number is one, we have a hapax.\n",
    "\n",
    "We print 10 hapaxes with their gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.16s 3072 hapaxes found\n",
      "No zeroes found\n",
      "\tPJCWN/   Pishon\n",
      "\tCWP[     bruise\n",
      "\tHRWN/    pregnancy\n",
      "\tZ<H/     sweat\n",
      "\tLHV/     flame\n",
      "\tNWD/     Nod\n",
      "\tXNWK=/   Enoch\n",
      "\tMXWJ>L/  Mehujael\n",
      "\tMXJJ>L/  Mehujael\n",
      "\tJBL=/    Jabal\n"
     ]
    }
   ],
   "source": [
    "hapax = []\n",
    "zero = set()\n",
    "\n",
    "indent(reset=True)\n",
    "for l in F.otype.s('lex'):\n",
    "    occs = L.d(l, otype='word')\n",
    "    n = len(occs)\n",
    "    if n == 0: # that's weird: should not happen\n",
    "        zero.add(l)\n",
    "    elif n == 1: # hapax found!\n",
    "        hapax.append(l)\n",
    "info('{} hapaxes found'.format(len(hapax)))\n",
    "if zero:\n",
    "    error('{} zeroes found'.format(len(zero)), tm=False)\n",
    "else:\n",
    "    info('No zeroes found', tm=False)\n",
    "for h in hapax[0:10]:\n",
    "    print('\\t{:<8} {}'.format(F.lex.v(h), F.gloss.v(h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small occurrence base\n",
    "\n",
    "The occurrence base of a lexeme are the verses, chapters and books in which occurs.\n",
    "Let's look for lexemes that occur in a single chapter.\n",
    "\n",
    "If a lexeme occurs in a single chapter, its slots are a subset of the slots of that chapter.\n",
    "So, if you go *up* from the lexeme, you encounter the chapter.\n",
    "\n",
    "Normally, lexemes occur in many chapters, and then none of them totally includes all occurrences of it,\n",
    "so if you go up from such lexemes, you don not find chapters.\n",
    "\n",
    "Let's check it out.\n",
    "\n",
    "Oh yes, we have already found the hapaxes, we will skip them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single chapter lexemes\n",
      "  0.15s 450 single chapter lexemes found\n",
      "No chapter embedders of multiple lexemes found\n",
      "Genesis 4:1          QJN=/ \n",
      "Genesis 4:2          HBL=/ \n",
      "Genesis 4:18         <JRD/ \n",
      "Genesis 4:18         MTWC>L/\n",
      "Genesis 4:19         YLH/  \n",
      "Genesis 4:22         TWBL_QJN/\n",
      "Genesis 10:11        KLX=/ \n",
      "Genesis 14:1         >MRPL/\n",
      "Genesis 14:1         >RJWK/\n",
      "Genesis 14:1         >LSR/ \n"
     ]
    }
   ],
   "source": [
    "singleCh = []\n",
    "multiple = []\n",
    "indent(reset=True)\n",
    "info('Finding single chapter lexemes')\n",
    "for l in F.otype.s('lex'):\n",
    "    chapters = L.u(l, 'chapter')\n",
    "    if len(chapters) == 1:\n",
    "        if l not in hapax:\n",
    "            singleCh.append(l)\n",
    "    elif len(chapters) > 0: # should not happen\n",
    "        multipleCh.append(l)\n",
    "info('{} single chapter lexemes found'.format(len(singleCh)))\n",
    "if multiple:\n",
    "    error('{} chapter embedders of multiple lexemes found'.format(len(multiple)), tm=False)\n",
    "else:\n",
    "    info('No chapter embedders of multiple lexemes found', tm=False)\n",
    "for s in singleCh[0:10]:\n",
    "    print('{:<20} {:<6}'.format(\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(s)),\n",
    "        F.lex.v(s),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confined to books\n",
    "\n",
    "As a final exercise with lexemes, lets make a list of all books, and show their total number of lexemes and\n",
    "the number of lexemes that occur exclusively in that book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Making book-lexeme index\n",
      "  4.88s Found 9233 lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "\n",
    "allBook = collections.defaultdict(set)\n",
    "allLex = set()\n",
    "\n",
    "info('Making book-lexeme index')\n",
    "for b in F.otype.s('book'):\n",
    "    for w in L.d(b, 'word'):\n",
    "        l = L.u(w, 'lex')[0]\n",
    "        allBook[b].add(l)\n",
    "        allLex.add(l)\n",
    "info('Found {} lexemes'.format(len(allLex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single book lexemes\n",
      "  0.06s found 4226 single book lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "\n",
    "singleBook = collections.defaultdict(lambda:0)\n",
    "info('Finding single book lexemes')\n",
    "for l in F.otype.s('lex'):\n",
    "    book = L.u(l, 'book')\n",
    "    if len(book) == 1:\n",
    "        singleBook[book[0]] += 1\n",
    "info('found {} single book lexemes'.format(sum(singleBook.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                 #all #own %own\n",
      "-----------------------------------\n",
      "Daniel               1121  428 38.2%\n",
      "1_Chronicles         2015  488 24.2%\n",
      "Ezra                  991  199 20.1%\n",
      "Joshua               1175  206 17.5%\n",
      "Esther                472   67 14.2%\n",
      "Isaiah               2553  350 13.7%\n",
      "Numbers              1457  197 13.5%\n",
      "Ezekiel              1718  212 12.3%\n",
      "Song_of_songs         503   60 11.9%\n",
      "Job                  1717  202 11.8%\n",
      "Genesis              1817  208 11.4%\n",
      "Nehemiah             1076  110 10.2%\n",
      "Psalms               2251  216  9.6%\n",
      "Leviticus             960   89  9.3%\n",
      "Judges               1210   99  8.2%\n",
      "Ecclesiastes          575   46  8.0%\n",
      "Proverbs             1356  103  7.6%\n",
      "Jeremiah             1949  147  7.5%\n",
      "2_Samuel             1304   89  6.8%\n",
      "1_Samuel             1256   85  6.8%\n",
      "2_Kings              1266   85  6.7%\n",
      "Exodus               1425   92  6.5%\n",
      "1_Kings              1291   81  6.3%\n",
      "Deuteronomy          1449   80  5.5%\n",
      "Lamentations          592   31  5.2%\n",
      "2_Chronicles         1411   67  4.7%\n",
      "Nahum                 357   16  4.5%\n",
      "Hosea                 742   33  4.4%\n",
      "Ruth                  319   14  4.4%\n",
      "Habakkuk              393   17  4.3%\n",
      "Amos                  652   27  4.1%\n",
      "Joel                  398   14  3.5%\n",
      "Zechariah             726   25  3.4%\n",
      "Obadiah               167    5  3.0%\n",
      "Micah                 586   16  2.7%\n",
      "Zephaniah             367   10  2.7%\n",
      "Jonah                 252    5  2.0%\n",
      "Haggai                208    3  1.4%\n",
      "Malachi               314    4  1.3%\n"
     ]
    }
   ],
   "source": [
    "print('{:<20}{:>5}{:>5}{:>5}\\n{}'.format(\n",
    "    'book', '#all', '#own', '%own',\n",
    "    '-'*35,\n",
    "))\n",
    "booklist = []\n",
    "\n",
    "for b in F.otype.s('book'):\n",
    "    book = T.bookName(b)\n",
    "    a = len(allBook[b])\n",
    "    o = singleBook.get(b, 0)\n",
    "    p = 100 * o / a\n",
    "    booklist.append((book, a, o, p))\n",
    "\n",
    "for x in sorted(booklist, key=lambda e: (-e[3], -e[1], e[0])):\n",
    "    print('{:<20} {:>4} {:>4} {:>4.1f}%'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech counting\n",
    "We count the words of each part of speech, and we list to top 10 of frequent lexemes.\n",
    "\n",
    "**NB**: This is not so much about lexemes as well as\n",
    "generating pretty progress messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Starting tasks\n",
      "   |     0.00s Counting the words by part-of-speech ...\n",
      "   |     0.43s Done: 14 categories\n",
      "   |      |   subs   : 121483x\n",
      "   |      |   verb   :  73710x\n",
      "   |      |   prep   :  73273x\n",
      "   |      |   conj   :  62722x\n",
      "   |      |   nmpr   :  33081x\n",
      "   |      |   art    :  30386x\n",
      "   |      |   adjv   :   9464x\n",
      "   |      |   nega   :   6053x\n",
      "   |      |   prps   :   5011x\n",
      "   |      |   advb   :   4550x\n",
      "   |      |   prde   :   2660x\n",
      "   |      |   intj   :   1885x\n",
      "   |      |   inrg   :   1285x\n",
      "   |      |   prin   :   1021x\n",
      "   |     0.00s Listing the top 10 frequent words ...\n",
      "   |     0.42s Done: 8773 lexemes\n",
      "   |      |   W      :  51003x\n",
      "   |      |   H      :  30392x\n",
      "   |      |   L      :  20447x\n",
      "   |      |   B      :  15768x\n",
      "   |      |   >T     :  10997x\n",
      "   |      |   MN     :   7681x\n",
      "   |      |   JHWH/  :   6828x\n",
      "   |      |   <L     :   5870x\n",
      "   |      |   >L     :   5521x\n",
      "   |      |   >CR    :   5500x\n",
      "  0.88s All tasks completed\n"
     ]
    }
   ],
   "source": [
    "partOfSpeech = collections.Counter()\n",
    "freqLex = collections.Counter()\n",
    "\n",
    "indent(level=0, reset=True)\n",
    "info('Starting tasks')\n",
    "indent(level=1, reset=True)\n",
    "info('Counting the words by part-of-speech ...')\n",
    "for w in F.otype.s('word'):\n",
    "    partOfSpeech[F.sp.v(w)] += 1\n",
    "info('Done: {} categories'.format(len(partOfSpeech)))\n",
    "indent(level=2)\n",
    "info('\\n'.join('{:<7}: {:>6}x'.format(*x) for x in sorted(\n",
    "    partOfSpeech.items(),\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")), tm=False)\n",
    "indent(level=1, reset=True)\n",
    "info('Listing the top 10 frequent words ...')\n",
    "for w in F.otype.s('word'):\n",
    "    freqLex[F.lex.v(w)] += 1\n",
    "info('Done: {} lexemes'.format(len(freqLex)))\n",
    "indent(level=2)\n",
    "info('\\n'.join('{:<7}: {:>6}x'.format(*x) for x in sorted(\n",
    "    freqLex.items(),\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")[0:10]), tm=False)\n",
    "indent(level=0)\n",
    "info('All tasks completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer API\n",
    "We travel upwards and downwards, forwards and backwards through the nodes.\n",
    "The Layer-API (`L`) provides functions: `u()` for going up, and `d()` for going down,\n",
    "`n()` for going to next nodes and `p()` for going to previous nodes.\n",
    "\n",
    "These directions are indirect notions: nodes are just numbers, but by means of the\n",
    "`oslots` feature they are linked to slots. One node *contains* an other node, if the one is linked to a set of slots that contains the set of slots that the other is linked to.\n",
    "And one if next or previous to an other, if its slots follow of precede the slots of the other one.\n",
    "\n",
    "`L.u(node)` **Up** is going to nodes that embed `node`.\n",
    "\n",
    "`L.d(node)` **Down** is the opposite direction, to those that are contained in `node`.\n",
    "\n",
    "`L.n(node)` **Next** are the next *adjacent* nodes, i.e. nodes whose first slot comes immediately after the last slot of `node`.\n",
    "\n",
    "`L.p(node)` **Previous** are the previous *adjacent* nodes, i.e. nodes whose last slot comes immediately before the first slot of `node`.\n",
    "\n",
    "All these functions yield nodes of all possible otypes.\n",
    "By passing an optional parameter, you can restrict the results to nodes of that type.\n",
    "\n",
    "The result is ordered in the canonical node ordering.\n",
    "The functions return always a tuple, even if there is just one node in the result.\n",
    "\n",
    "## Going up\n",
    "We go from the first word to the book it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426585\n"
     ]
    }
   ],
   "source": [
    "firstBook = L.u(1, otype='book')[0]\n",
    "print(firstBook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see all the containing objects of word 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 3 is contained in book 426585\n",
      "word 3 is contained in chapter 426624\n",
      "word 3 is contained in lex 1437405\n",
      "word 3 is contained in verse 1414190\n",
      "word 3 is contained in half_verse 606323\n",
      "word 3 is contained in sentence 1172209\n",
      "word 3 is contained in sentence_atom 1235920\n",
      "word 3 is contained in clause 427553\n",
      "word 3 is contained in clause_atom 515654\n",
      "word 3 is contained in phrase 651504\n",
      "word 3 is contained in phrase_atom 904691\n",
      "word 3 is contained in subphrase x\n"
     ]
    }
   ],
   "source": [
    "w = 3\n",
    "for otype in F.otype.all:\n",
    "    if otype == F.otype.slotType: continue\n",
    "    up = L.u(w, otype=otype)\n",
    "    upNode = 'x' if len(up) == 0 else up[0]\n",
    "    print('word {} is contained in {} {}'.format(w, otype, upNode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going next\n",
    "Let's go to the next nodes of the first book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  28764: word          first slot=28764 , last slot=28764 \n",
      " 923447: phrase_atom   first slot=28764 , last slot=28764 \n",
      " 669484: phrase        first slot=28764 , last slot=28764 \n",
      " 521793: clause_atom   first slot=28764 , last slot=28768 \n",
      " 433543: clause        first slot=28764 , last slot=28768 \n",
      " 609323: half_verse    first slot=28764 , last slot=28771 \n",
      "1240568: sentence_atom first slot=28764 , last slot=28773 \n",
      "1176828: sentence      first slot=28764 , last slot=28792 \n",
      "1415723: verse         first slot=28764 , last slot=28777 \n",
      " 426674: chapter       first slot=28764 , last slot=29112 \n",
      " 426586: book          first slot=28764 , last slot=52511 \n"
     ]
    }
   ],
   "source": [
    "afterFirstBook = L.n(firstBook)\n",
    "for n in afterFirstBook:\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))\n",
    "secondBook = L.n(firstBook, otype='book')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going previous\n",
    "\n",
    "And let's see what is right before the second book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 426585: book          first slot=1     , last slot=28763 \n",
      " 426673: chapter       first slot=28259 , last slot=28763 \n",
      "1415722: verse         first slot=28746 , last slot=28763 \n",
      " 609322: half_verse    first slot=28754 , last slot=28763 \n",
      "1176827: sentence      first slot=28757 , last slot=28763 \n",
      "1240567: sentence_atom first slot=28757 , last slot=28763 \n",
      " 433542: clause        first slot=28757 , last slot=28763 \n",
      " 521792: clause_atom   first slot=28757 , last slot=28763 \n",
      " 669483: phrase        first slot=28762 , last slot=28763 \n",
      " 923446: phrase_atom   first slot=28762 , last slot=28763 \n",
      "  28763: word          first slot=28763 , last slot=28763 \n"
     ]
    }
   ],
   "source": [
    "for n in L.p(secondBook):\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the chapters of the second book, and just count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "chapters = L.d(secondBook, otype='chapter')\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first verse\n",
    "We pick the first verse and the first word, and explore what is above and below them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "   |   UP\n",
      "   |      |   1437403         lex\n",
      "   |      |   904690          phrase_atom\n",
      "   |      |   651503          phrase\n",
      "   |      |   606323          half_verse\n",
      "   |      |   515654          clause_atom\n",
      "   |      |   427553          clause\n",
      "   |      |   1235920         sentence_atom\n",
      "   |      |   1172209         sentence\n",
      "   |      |   1414190         verse\n",
      "   |      |   426624          chapter\n",
      "   |      |   426585          book\n",
      "   |   DOWN\n",
      "   |      |   \n",
      "Node 1414190\n",
      "   |   UP\n",
      "   |      |   426624          chapter\n",
      "   |      |   426585          book\n",
      "   |   DOWN\n",
      "   |      |   1172209         sentence\n",
      "   |      |   1235920         sentence_atom\n",
      "   |      |   427553          clause\n",
      "   |      |   515654          clause_atom\n",
      "   |      |   606323          half_verse\n",
      "   |      |   651503          phrase\n",
      "   |      |   904690          phrase_atom\n",
      "   |      |   1               word\n",
      "   |      |   2               word\n",
      "   |      |   651504          phrase\n",
      "   |      |   904691          phrase_atom\n",
      "   |      |   3               word\n",
      "   |      |   651505          phrase\n",
      "   |      |   904692          phrase_atom\n",
      "   |      |   4               word\n",
      "   |      |   606324          half_verse\n",
      "   |      |   651506          phrase\n",
      "   |      |   904693          phrase_atom\n",
      "   |      |   1300406         subphrase\n",
      "   |      |   5               word\n",
      "   |      |   6               word\n",
      "   |      |   7               word\n",
      "   |      |   8               word\n",
      "   |      |   1300407         subphrase\n",
      "   |      |   9               word\n",
      "   |      |   10              word\n",
      "   |      |   11              word\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for n in [1, L.u(1, otype='verse')[0]]:\n",
    "    indent(level=0)\n",
    "    info('Node {}'.format(n), tm=False)\n",
    "    indent(level=1)\n",
    "    info('UP', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.u(n)]), tm=False)\n",
    "    indent(level=1)\n",
    "    info('DOWN', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.d(n)]), tm=False)\n",
    "indent(level=0)\n",
    "info('Done', tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text API\n",
    "\n",
    "We examine the functions of the Text API: `T`.\n",
    "\n",
    "## Formats\n",
    "First the formats that we have available to represent the actual text.\n",
    "These formats have been defined in the `otext` feature.\n",
    "This is an optional GRID config feature: it has only metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lex-orig-full',\n",
       " 'lex-orig-plain',\n",
       " 'lex-trans-full',\n",
       " 'lex-trans-plain',\n",
       " 'text-orig-full',\n",
       " 'text-orig-full-ketiv',\n",
       " 'text-orig-plain',\n",
       " 'text-phono-full',\n",
       " 'text-trans-full',\n",
       " 'text-trans-full-ketiv',\n",
       " 'text-trans-plain']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(T.formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `text-phono-full` format here.\n",
    "It does not come from the main data source `etcbc4c`, but from the module `phono`.\n",
    "Look in your data directory, find `text-fabric-data/hebrew/phono/otext@phono.tf`,\n",
    "and you'll see this format defined there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the formats\n",
    "Now let's use those formats to print out the first verse of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lex-orig-full:\n",
      "\tבְּ רֵאשִׁית בָּרָא אֱלֹה אֵת הַ שָּׁמַי וְ אֵת הָ אָרֶץ \n",
      "lex-orig-plain:\n",
      "\tב ראשׁית ברא אלהים את ה שׁמים ו את ה ארץ \n",
      "lex-trans-full:\n",
      "\tB.:- R;>CIJT B.@R@> >:ELOH >;T HA- C.@MAJ W:- >;T H@- >@REY \n",
      "lex-trans-plain:\n",
      "\tB R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "text-orig-full:\n",
      "\tבְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "text-orig-full-ketiv:\n",
      "\tבְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "text-orig-plain:\n",
      "\tבראשׁית ברא אלהים את השׁמים ואת הארץ׃ \n",
      "text-phono-full:\n",
      "\tbᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ . \n",
      "text-trans-full:\n",
      "\tB.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "text-trans-full-ketiv:\n",
      "\tB.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "text-trans-plain:\n",
      "\tBR>CJT BR> >LHJM >T HCMJM W>T H>RY00 \n"
     ]
    }
   ],
   "source": [
    "for fmt in sorted(T.formats):\n",
    "    print('{}:\\n\\t{}'.format(fmt, T.text(range(1,12), fmt=fmt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify a format, the **default** format is used (`text-orig-full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n"
     ]
    }
   ],
   "source": [
    "print(T.text(range(1,12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole text in all formats in just 10 seconds\n",
    "We are going to produce the complete text of the Hebrew Bible in all available formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s writing plain text of whole Bible in all formats\n",
      "  9.88s done 11 formats\n",
      "lex-orig-full\n",
      "בְּ רֵאשִׁית בָּרָא אֱלֹה אֵת הַ שָּׁמַי וְ אֵת הָ אָרֶץ \n",
      "וְ הָ אָרֶץ הָי תֹהוּ וָ בֹהוּ וְ חֹשֶׁךְ עַל פְּן תְהֹום וְ רוּחַ אֱלֹה רַחֶף עַל פְּן הַ מָּי \n",
      "וַ אמֶר אֱלֹה הִי אֹור וַ הִי אֹור \n",
      "וַ רְא אֱלֹה אֶת הָ אֹור כִּי טֹוב וַ בְדֵּל אֱלֹה בֵּין הָ אֹור וּ בֵין הַ חֹשֶׁךְ \n",
      "וַ קְרָא אֱלֹה לָ  אֹור יֹום וְ לַ  חֹשֶׁךְ קָרָא לָיְלָה וַ הִי עֶרֶב וַ הִי בֹקֶר יֹום אֶחָד \n",
      "\n",
      "lex-orig-plain\n",
      "ב ראשׁית ברא אלהים את ה שׁמים ו את ה ארץ \n",
      "ו ה ארץ היה תהו ו בהו ו חשׁך על פנה תהום ו רוח אלהים רחף על פנה ה מים \n",
      "ו אמר אלהים היה אור ו היה אור \n",
      "ו ראה אלהים את ה אור כי טוב ו בדל אלהים בין ה אור ו בין ה חשׁך \n",
      "ו קרא אלהים ל ה אור יום ו ל ה חשׁך קרא לילה ו היה ערב ו היה בקר יום אחד \n",
      "\n",
      "lex-trans-full\n",
      "B.:- R;>CIJT B.@R@> >:ELOH >;T HA- C.@MAJ W:- >;T H@- >@REY \n",
      "W:- H@- >@REY H@J TOHW. W@- BOHW. W:- XOCEK: <AL P.:N T:HOWM W:- RW.XA >:ELOH RAXEP <AL P.:N HA- M.@J \n",
      "WA- >MER >:ELOH HIJ >OWR WA- HIJ >OWR \n",
      "WA- R:> >:ELOH >ET H@- >OWR K.IJ VOWB WA- B:D.;L >:ELOH B.;JN H@- >OWR W.- B;JN HA- XOCEK: \n",
      "WA- Q:R@> >:ELOH L@- - >OWR JOWM W:- LA- - XOCEK: Q@R@> L@J:L@H WA- HIJ <EREB WA- HIJ BOQER JOWM >EX@D \n",
      "\n",
      "lex-trans-plain\n",
      "B R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "W H >RY HJH THW W BHW W XCK <L PNH THWM W RWX >LHJM RXP <L PNH H MJM \n",
      "W >MR >LHJM HJH >WR W HJH >WR \n",
      "W R>H >LHJM >T H >WR KJ VWB W BDL >LHJM BJN H >WR W BJN H XCK \n",
      "W QR> >LHJM L H >WR JWM W L H XCK QR> LJLH W HJH <RB W HJH BQR JWM >XD \n",
      "\n",
      "text-orig-full\n",
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְהֹ֑ום וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃ \n",
      "וַיֹּ֥אמֶר אֱלֹהִ֖ים יְהִ֣י אֹ֑ור וַֽיְהִי־אֹֽור׃ \n",
      "וַיַּ֧רְא אֱלֹהִ֛ים אֶת־הָאֹ֖ור כִּי־טֹ֑וב וַיַּבְדֵּ֣ל אֱלֹהִ֔ים בֵּ֥ין הָאֹ֖ור וּבֵ֥ין הַחֹֽשֶׁךְ׃ \n",
      "וַיִּקְרָ֨א אֱלֹהִ֤ים׀ לָאֹור֙ יֹ֔ום וְלַחֹ֖שֶׁךְ קָ֣רָא לָ֑יְלָה וַֽיְהִי־עֶ֥רֶב וַֽיְהִי־בֹ֖קֶר יֹ֥ום אֶחָֽד׃ פ \n",
      "\n",
      "text-orig-full-ketiv\n",
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְהֹ֑ום וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃ \n",
      "וַיֹּ֥אמֶר אֱלֹהִ֖ים יְהִ֣י אֹ֑ור וַֽיְהִי־אֹֽור׃ \n",
      "וַיַּ֧רְא אֱלֹהִ֛ים אֶת־הָאֹ֖ור כִּי־טֹ֑וב וַיַּבְדֵּ֣ל אֱלֹהִ֔ים בֵּ֥ין הָאֹ֖ור וּבֵ֥ין הַחֹֽשֶׁךְ׃ \n",
      "וַיִּקְרָ֨א אֱלֹהִ֤ים׀ לָאֹור֙ יֹ֔ום וְלַחֹ֖שֶׁךְ קָ֣רָא לָ֑יְלָה וַֽיְהִי־עֶ֥רֶב וַֽיְהִי־בֹ֖קֶר יֹ֥ום אֶחָֽד׃ פ \n",
      "\n",
      "text-orig-plain\n",
      "בראשׁית ברא אלהים את השׁמים ואת הארץ׃ \n",
      "והארץ היתה תהו ובהו וחשׁך על־פני תהום ורוח אלהים מרחפת על־פני המים׃ \n",
      "ויאמר אלהים יהי אור ויהי־אור׃ \n",
      "וירא אלהים את־האור כי־טוב ויבדל אלהים בין האור ובין החשׁך׃ \n",
      "ויקרא אלהים׀ לאור יום ולחשׁך קרא לילה ויהי־ערב ויהי־בקר יום אחד׃ פ \n",
      "\n",
      "text-phono-full\n",
      "bᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ . \n",
      "wᵊhāʔˈāreṣ hāyᵊṯˌā ṯˈōhû wāvˈōhû wᵊḥˌōšeḵ ʕal-pᵊnˈê ṯᵊhˈôm wᵊrˈûₐḥ ʔᵉlōhˈîm mᵊraḥˌefeṯ ʕal-pᵊnˌê hammˈāyim . \n",
      "wayyˌōmer ʔᵉlōhˌîm yᵊhˈî ʔˈôr wˈayᵊhî-ʔˈôr . \n",
      "wayyˈar ʔᵉlōhˈîm ʔeṯ-hāʔˌôr kî-ṭˈôv wayyavdˈēl ʔᵉlōhˈîm bˌên hāʔˌôr ûvˌên haḥˈōšeḵ . \n",
      "wayyiqrˌā ʔᵉlōhˈîm lāʔôr yˈôm wᵊlaḥˌōšeḵ qˈārā lˈāyᵊlā wˈayᵊhî-ʕˌerev wˈayᵊhî-vˌōqer yˌôm ʔeḥˈāḏ . f \n",
      "\n",
      "text-trans-full\n",
      "B.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "W:-H@->@81REY H@J:T@71H TO33HW.03 W@-BO80HW. W:-XO73CEK: <AL&P.:N;74J T:HO92WM W:-R74W.XA >:ELOHI80JM M:RAXE73PET <AL&P.:N;71J HA-M.@75JIM00 \n",
      "WA-J.O71>MER >:ELOHI73JM J:HI74J >O92WR WA45-J:HIJ&>O75WR00 \n",
      "WA-J.A94R:> >:ELOHI91JM >ET&H@->O73WR K.IJ&VO92WB WA-J.AB:D.;74L >:ELOHI80JM B.;71JN H@->O73WR W.-B;71JN HA-XO75CEK:00 \n",
      "WA-J.IQ:R@63> >:ELOHI70JM05 L@-->OWR03 JO80WM W:-LA--XO73CEK: Q@74R@> L@92J:L@H WA45-J:HIJ&<E71REB WA45-J:HIJ&BO73QER JO71WM >EX@75D00_P \n",
      "\n",
      "text-trans-full-ketiv\n",
      "B.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "W:-H@->@81REY H@J:T@71H TO33HW.03 W@-BO80HW. W:-XO73CEK: <AL&P.:N;74J T:HO92WM W:-R74W.XA >:ELOHI80JM M:RAXE73PET <AL&P.:N;71J HA-M.@75JIM00 \n",
      "WA-J.O71>MER >:ELOHI73JM J:HI74J >O92WR WA45-J:HIJ&>O75WR00 \n",
      "WA-J.A94R:> >:ELOHI91JM >ET&H@->O73WR K.IJ&VO92WB WA-J.AB:D.;74L >:ELOHI80JM B.;71JN H@->O73WR W.-B;71JN HA-XO75CEK:00 \n",
      "WA-J.IQ:R@63> >:ELOHI70JM05 L@-->OWR03 JO80WM W:-LA--XO73CEK: Q@74R@> L@92J:L@H WA45-J:HIJ&<E71REB WA45-J:HIJ&BO73QER JO71WM >EX@75D00_P \n",
      "\n",
      "text-trans-plain\n",
      "BR>CJT BR> >LHJM >T HCMJM W>T H>RY00 \n",
      "WH>RY HJTH THW WBHW WXCK <L&PNJ THWM WRWX >LHJM MRXPT <L&PNJ HMJM00 \n",
      "WJ>MR >LHJM JHJ >WR WJHJ&>WR00 \n",
      "WJR> >LHJM >T&H>WR KJ&VWB WJBDL >LHJM BJN H>WR WBJN HXCK00 \n",
      "WJQR> >LHJM05 L>WR JWM WLXCK QR> LJLH WJHJ&<RB WJHJ&BQR JWM >XD00_P \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = collections.defaultdict(list)\n",
    "indent(reset=True)\n",
    "info('writing plain text of whole Bible in all formats')\n",
    "for v in F.otype.s('verse'):\n",
    "    words = L.d(v, 'word')\n",
    "    for fmt in sorted(T.formats):\n",
    "        text[fmt].append(T.text(words, fmt=fmt))\n",
    "info('done {} formats'.format(len(text)))\n",
    "for fmt in sorted(text):\n",
    "    print('{}\\n{}\\n'.format(fmt, '\\n'.join(text[fmt][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full plain text\n",
    "We write the default plain text version to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser('~/Downloads/text-orig-full.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(text['text-orig-full']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book names\n",
    "\n",
    "For Bible book names, we can use several languages.\n",
    "\n",
    "### Languages\n",
    "Here are the languages that we can use for book names.\n",
    "These languages come from the features `book@ll`, where `ll` is a two letter\n",
    "ISO language code. Have a look in your data directory, you can't miss them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': {'language': 'ኣማርኛ', 'languageEnglish': 'amharic'},\n",
       " 'ar': {'language': 'العَرَبِية', 'languageEnglish': 'arabic'},\n",
       " 'bn': {'language': 'বাংলা', 'languageEnglish': 'bengali'},\n",
       " 'da': {'language': 'Dansk', 'languageEnglish': 'danish'},\n",
       " 'de': {'language': 'Deutsch', 'languageEnglish': 'german'},\n",
       " 'el': {'language': 'Ελληνικά', 'languageEnglish': 'greek'},\n",
       " 'en': {'language': 'English', 'languageEnglish': 'english'},\n",
       " 'es': {'language': 'Español', 'languageEnglish': 'spanish'},\n",
       " 'fa': {'language': 'فارسی', 'languageEnglish': 'farsi'},\n",
       " 'fr': {'language': 'Français', 'languageEnglish': 'french'},\n",
       " 'he': {'language': 'עברית', 'languageEnglish': 'hebrew'},\n",
       " 'hi': {'language': 'हिन्दी', 'languageEnglish': 'hindi'},\n",
       " 'id': {'language': 'Bahasa Indonesia', 'languageEnglish': 'indonesian'},\n",
       " 'ja': {'language': '日本語', 'languageEnglish': 'japanese'},\n",
       " 'ko': {'language': '한국어', 'languageEnglish': 'korean'},\n",
       " 'la': {'language': 'Latina', 'languageEnglish': 'latin'},\n",
       " 'nl': {'language': 'Nederlands', 'languageEnglish': 'dutch'},\n",
       " 'pa': {'language': 'ਪੰਜਾਬੀ', 'languageEnglish': 'punjabi'},\n",
       " 'pt': {'language': 'Português', 'languageEnglish': 'portuguese'},\n",
       " 'ru': {'language': 'Русский', 'languageEnglish': 'russian'},\n",
       " 'sw': {'language': 'Kiswahili', 'languageEnglish': 'swahili'},\n",
       " 'syc': {'language': 'ܠܫܢܐ ܣܘܪܝܝܐ', 'languageEnglish': 'syriac'},\n",
       " 'tr': {'language': 'Türkçe', 'languageEnglish': 'turkish'},\n",
       " 'ur': {'language': 'اُردُو', 'languageEnglish': 'urdu'},\n",
       " 'yo': {'language': 'èdè Yorùbá', 'languageEnglish': 'yoruba'},\n",
       " 'zh': {'language': '中文', 'languageEnglish': 'chinese'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book names in Swahili\n",
    "Get the book names in Swahili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426585 = Mwanzo\n",
      "426586 = Kutoka\n",
      "426587 = Mambo_ya_Walawi\n",
      "426588 = Hesabu\n",
      "426589 = Kumbukumbu_la_Torati\n",
      "426590 = Yoshua\n",
      "426591 = Waamuzi\n",
      "426592 = 1_Samweli\n",
      "426593 = 2_Samweli\n",
      "426594 = 1_Wafalme\n",
      "426595 = 2_Wafalme\n",
      "426596 = Isaya\n",
      "426597 = Yeremia\n",
      "426598 = Ezekieli\n",
      "426599 = Hosea\n",
      "426600 = Yoeli\n",
      "426601 = Amosi\n",
      "426602 = Obadia\n",
      "426603 = Yona\n",
      "426604 = Mika\n",
      "426605 = Nahumu\n",
      "426606 = Habakuki\n",
      "426607 = Sefania\n",
      "426608 = Hagai\n",
      "426609 = Zekaria\n",
      "426610 = Malaki\n",
      "426611 = Zaburi\n",
      "426612 = Ayubu\n",
      "426613 = Mithali\n",
      "426614 = Ruthi\n",
      "426615 = Wimbo_Ulio_Bora\n",
      "426616 = Mhubiri\n",
      "426617 = Maombolezo\n",
      "426618 = Esta\n",
      "426619 = Danieli\n",
      "426620 = Ezra\n",
      "426621 = Nehemia\n",
      "426622 = 1_Mambo_ya_Nyakati\n",
      "426623 = 2_Mambo_ya_Nyakati\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodeToSwahili = ''\n",
    "for b in F.otype.s('book'):\n",
    "    nodeToSwahili += '{} = {}\\n'.format(b, T.bookName(b, lang='sw'))\n",
    "print(nodeToSwahili)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book nodes from Swahili\n",
    "OK, there they are. We copy them into a string, and do the opposite: get the nodes back.\n",
    "We check whether we get exactly the same nodes as the ones we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going from nodes to booknames and back yields the original nodes\n"
     ]
    }
   ],
   "source": [
    "swahiliNames = '''\n",
    "Mwanzo\n",
    "Kutoka\n",
    "Mambo_ya_Walawi\n",
    "Hesabu\n",
    "Kumbukumbu_la_Torati\n",
    "Yoshua\n",
    "Waamuzi\n",
    "1_Samweli\n",
    "2_Samweli\n",
    "1_Wafalme\n",
    "2_Wafalme\n",
    "Isaya\n",
    "Yeremia\n",
    "Ezekieli\n",
    "Hosea\n",
    "Yoeli\n",
    "Amosi\n",
    "Obadia\n",
    "Yona\n",
    "Mika\n",
    "Nahumu\n",
    "Habakuki\n",
    "Sefania\n",
    "Hagai\n",
    "Zekaria\n",
    "Malaki\n",
    "Zaburi\n",
    "Ayubu\n",
    "Mithali\n",
    "Ruthi\n",
    "Wimbo_Ulio_Bora\n",
    "Mhubiri\n",
    "Maombolezo\n",
    "Esta\n",
    "Danieli\n",
    "Ezra\n",
    "Nehemia\n",
    "1_Mambo_ya_Nyakati\n",
    "2_Mambo_ya_Nyakati\n",
    "'''.strip().split()\n",
    "\n",
    "swahiliToNode = ''\n",
    "for nm in swahiliNames:\n",
    "    swahiliToNode += '{} = {}\\n'.format(T.bookNode(nm, lang='sw'), nm)\n",
    "    \n",
    "if swahiliToNode != nodeToSwahili:\n",
    "    print('Something is not right with the book names')\n",
    "else:\n",
    "    print('Going from nodes to booknames and back yields the original nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences spanning multiple verses\n",
    "If you go up from a sentence node, you expect to find a verse node.\n",
    "But some sentences span multiple verses, and in that case, you will not find the enclosing\n",
    "verse node, because it is not there.\n",
    "\n",
    "Here is a piece of code to detect and list all cases where sentences span multiple verses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892 sentences span multiple verses\n",
      "The first one is 1172254\n"
     ]
    }
   ],
   "source": [
    "verseSpanners = []\n",
    "for s in F.otype.s('sentence'):\n",
    "    if not L.u(s, otype='verse'):\n",
    "        verseSpanners.append(s)\n",
    "print(f'{len(verseSpanners)} sentences span multiple verses')\n",
    "firstSpanner = verseSpanners[0]\n",
    "print(f'The first one is {firstSpanner}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "\n",
    "A section in the Hebrew bible is a book, a chapter or a verse.\n",
    "Knowledge of sections is not baked into Text-Fabric. \n",
    "The config feature `otext.tf` may specify three section levels, and tell\n",
    "what the corresponding node types and features are.\n",
    "\n",
    "From that knowledge it can construct mappings from nodes to sections, e.g. from verse\n",
    "nodes to tuples of the form:\n",
    "\n",
    "    (bookName, chapterNumber, verseNumber)\n",
    "   \n",
    "Here are examples of getting the section that corresponds to a node and vice versa.\n",
    "\n",
    "**NB:** `sectionFromNode` always delivers a verse specification, either from the\n",
    "first slot belonging to that node, or, if `lastSlot`, from the last slot\n",
    "belonging to that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section of first word          ('Genesis', 1, 1)\n",
      "node of Gen 1:1                1414190\n",
      "idem                           1414190\n",
      "node of book Genesis           426585\n",
      "node of Genesis 1              426624\n",
      "section of sentence node       ('Genesis', 1, 17)\n",
      "idem, now last word            ('Genesis', 1, 18)\n",
      "section of chapter node        ('1_Kings', 13, 1)\n",
      "idem, now last word            ('1_Kings', 13, 34)\n"
     ]
    }
   ],
   "source": [
    "allChapters = list(F.otype.s('chapter'))\n",
    "chapter300 = allChapters[299]\n",
    "\n",
    "for x in (\n",
    "    ('section of first word',    T.sectionFromNode(1)                            ),\n",
    "    ('node of Gen 1:1',          T.nodeFromSection(('Genesis', 1, 1))            ),\n",
    "    ('idem',                     T.nodeFromSection(('Mwanzo', 1, 1), lang='sw')  ),\n",
    "    ('node of book Genesis',     T.nodeFromSection(('Genesis',))                 ),\n",
    "    ('node of Genesis 1',        T.nodeFromSection(('Genesis', 1))               ),\n",
    "    ('section of sentence node', T.sectionFromNode(firstSpanner)                 ),\n",
    "    ('idem, now last word',      T.sectionFromNode(firstSpanner, lastSlot=True)  ),\n",
    "    ('section of chapter node',  T.sectionFromNode(chapter300)                   ),\n",
    "    ('idem, now last word',      T.sectionFromNode(chapter300, lastSlot=True)    ),\n",
    "): print('{:<30} {}'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets return to the verse spanning sentences and show the first 10 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genesis 1:17  \"וַיִּתֵּ֥ן אֹתָ֛ם אֱלֹהִ֖ים בִּרְקִ֣יעַ הַשָּׁמָ֑יִם לְהָאִ֖יר עַל־הָאָֽרֶץ׃ \" \n",
      "Genesis 1:18 ... \"וְלִמְשֹׁל֙ בַּיֹּ֣ום וּבַלַּ֔יְלָה וּֽלֲהַבְדִּ֔יל בֵּ֥ין הָאֹ֖ור וּבֵ֣ין הַחֹ֑שֶׁךְ \" \n",
      "\n",
      "Genesis 1:29  \"וַיֹּ֣אמֶר אֱלֹהִ֗ים הִנֵּה֩ נָתַ֨תִּי לָכֶ֜ם אֶת־כָּל־עֵ֣שֶׂב׀ זֹרֵ֣עַ זֶ֗רַע אֲשֶׁר֙ עַל־פְּנֵ֣י כָל־הָאָ֔רֶץ וְאֶת־כָּל־הָעֵ֛ץ אֲשֶׁר־בֹּ֥ו פְרִי־עֵ֖ץ זֹרֵ֣עַ זָ֑רַע לָכֶ֥ם יִֽהְיֶ֖ה לְאָכְלָֽה׃ \" ...\n",
      "Genesis 1:30 ... \"וּֽלְכָל־חַיַּ֣ת הָ֠אָרֶץ וּלְכָל־עֹ֨וף הַשָּׁמַ֜יִם וּלְכֹ֣ל׀ רֹומֵ֣שׂ עַל־הָאָ֗רֶץ אֲשֶׁר־בֹּו֙ נֶ֣פֶשׁ חַיָּ֔ה אֶת־כָּל־יֶ֥רֶק עֵ֖שֶׂב לְאָכְלָ֑ה \" \n",
      "\n",
      "Genesis 2:4  \"אֵ֣לֶּה תֹולְדֹ֧ות הַשָּׁמַ֛יִם וְהָאָ֖רֶץ בְּהִבָּֽרְאָ֑ם בְּיֹ֗ום עֲשֹׂ֛ות יְהוָ֥ה אֱלֹהִ֖ים אֶ֥רֶץ וְשָׁמָֽיִם׃ \" ...\n",
      "Genesis 2:5  \"וְכֹ֣ל׀ שִׂ֣יחַ הַשָּׂדֶ֗ה טֶ֚רֶם יִֽהְיֶ֣ה בָאָ֔רֶץ וְכָל־עֵ֥שֶׂב הַשָּׂדֶ֖ה טֶ֣רֶם יִצְמָ֑ח כִּי֩ לֹ֨א הִמְטִ֜יר יְהוָ֤ה אֱלֹהִים֙ עַל־הָאָ֔רֶץ וְאָדָ֣ם אַ֔יִן לַֽעֲבֹ֖ד אֶת־הָֽאֲדָמָֽה׃ \" \n",
      "Genesis 2:6  \"וְאֵ֖ד יַֽעֲלֶ֣ה מִן־הָאָ֑רֶץ וְהִשְׁקָ֖ה אֶֽת־כָּל־פְּנֵֽי־הָֽאֲדָמָֽה׃ \" \n",
      "Genesis 2:7 ... \"וַיִּיצֶר֩ יְהוָ֨ה אֱלֹהִ֜ים אֶת־הָֽאָדָ֗ם עָפָר֙ מִן־הָ֣אֲדָמָ֔ה \" \n",
      "\n",
      "Genesis 7:2  \"מִכֹּ֣ל׀ הַבְּהֵמָ֣ה הַטְּהֹורָ֗ה תִּֽקַּח־לְךָ֛ שִׁבְעָ֥ה שִׁבְעָ֖ה אִ֣ישׁ וְאִשְׁתֹּ֑ו וּמִן־הַבְּהֵמָ֡ה אֲ֠שֶׁר לֹ֣א טְהֹרָ֥ה הִ֛וא שְׁנַ֖יִם אִ֥ישׁ וְאִשְׁתֹּֽו׃ \" \n",
      "Genesis 7:3  \"גַּ֣ם מֵעֹ֧וף הַשָּׁמַ֛יִם שִׁבְעָ֥ה שִׁבְעָ֖ה זָכָ֣ר וּנְקֵבָ֑ה לְחַיֹּ֥ות זֶ֖רַע עַל־פְּנֵ֥י כָל־הָאָֽרֶץ׃ \" \n",
      "\n",
      "Genesis 7:8  \"מִן־הַבְּהֵמָה֙ הַטְּהֹורָ֔ה וּמִן־הַ֨בְּהֵמָ֔ה אֲשֶׁ֥ר אֵינֶ֖נָּה טְהֹרָ֑ה וּמִ֨ן־הָעֹ֔וף וְכֹ֥ל אֲשֶׁר־רֹמֵ֖שׂ עַל־הָֽאֲדָמָֽה׃ \" \n",
      "Genesis 7:9  \"שְׁנַ֨יִם שְׁנַ֜יִם בָּ֧אוּ אֶל־נֹ֛חַ אֶל־הַתֵּבָ֖ה זָכָ֣ר וּנְקֵבָ֑ה כַּֽאֲשֶׁ֛ר צִוָּ֥ה אֱלֹהִ֖ים אֶת־נֹֽחַ׃ \" \n",
      "\n",
      "Genesis 7:13  \"בְּעֶ֨צֶם הַיֹּ֤ום הַזֶּה֙ בָּ֣א נֹ֔חַ וְשֵׁם־וְחָ֥ם וָיֶ֖פֶת בְּנֵי־נֹ֑חַ וְאֵ֣שֶׁת נֹ֗חַ וּשְׁלֹ֧שֶׁת נְשֵֽׁי־בָנָ֛יו אִתָּ֖ם אֶל־הַתֵּבָֽה׃ \" \n",
      "Genesis 7:14  \"הֵ֜מָּה וְכָל־הַֽחַיָּ֣ה לְמִינָ֗הּ וְכָל־הַבְּהֵמָה֙ לְמִינָ֔הּ וְכָל־הָרֶ֛מֶשׂ הָרֹמֵ֥שׂ עַל־הָאָ֖רֶץ לְמִינֵ֑הוּ וְכָל־הָעֹ֣וף לְמִינֵ֔הוּ כֹּ֖ל צִפֹּ֥ור כָּל־כָּנָֽף׃ \" \n",
      "\n",
      "Genesis 9:9  \"וַאֲנִ֕י הִנְנִ֥י מֵקִ֛ים אֶת־בְּרִיתִ֖י אִתְּכֶ֑ם וְאֶֽת־זַרְעֲכֶ֖ם אַֽחֲרֵיכֶֽם׃ \" \n",
      "Genesis 9:10  \"וְאֵ֨ת כָּל־נֶ֤פֶשׁ הַֽחַיָּה֙ אֲשֶׁ֣ר אִתְּכֶ֔ם בָּעֹ֧וף בַּבְּהֵמָ֛ה וּֽבְכָל־חַיַּ֥ת הָאָ֖רֶץ אִתְּכֶ֑ם מִכֹּל֙ יֹצְאֵ֣י הַתֵּבָ֔ה לְכֹ֖ל חַיַּ֥ת הָאָֽרֶץ׃ \" \n",
      "\n",
      "Genesis 10:11  \"מִן־הָאָ֥רֶץ הַהִ֖וא יָצָ֣א אַשּׁ֑וּר וַיִּ֨בֶן֙ אֶת־נִ֣ינְוֵ֔ה וְאֶת־רְחֹבֹ֥ת עִ֖יר וְאֶת־כָּֽלַח׃ \" ...\n",
      "Genesis 10:12 ... \"וְֽאֶת־רֶ֔סֶן בֵּ֥ין נִֽינְוֵ֖ה וּבֵ֣ין כָּ֑לַח \" \n",
      "\n",
      "Genesis 10:13  \"וּמִצְרַ֡יִם יָלַ֞ד אֶת־לוּדִ֧ים וְאֶת־עֲנָמִ֛ים וְאֶת־לְהָבִ֖ים וְאֶת־נַפְתֻּחִֽים׃ \" \n",
      "Genesis 10:14  \"וְֽאֶת־פַּתְרֻסִ֞ים וְאֶת־כַּסְלֻחִ֗ים אֲשֶׁ֨ר יָצְא֥וּ מִשָּׁ֛ם פְּלִשְׁתִּ֖ים וְאֶת־כַּפְתֹּרִֽים׃ ס \" \n",
      "\n",
      "Genesis 10:15  \"וּכְנַ֗עַן יָלַ֛ד אֶת־צִידֹ֥ן בְּכֹרֹ֖ו וְאֶת־חֵֽת׃ \" \n",
      "Genesis 10:16  \"וְאֶת־הַיְבוּסִי֙ וְאֶת־הָ֣אֱמֹרִ֔י וְאֵ֖ת הַגִּרְגָּשִֽׁי׃ \" \n",
      "Genesis 10:17  \"וְאֶת־הַֽחִוִּ֥י וְאֶת־הַֽעַרְקִ֖י וְאֶת־הַסִּינִֽי׃ \" \n",
      "Genesis 10:18 ... \"וְאֶת־הָֽאַרְוָדִ֥י וְאֶת־הַצְּמָרִ֖י וְאֶת־הַֽחֲמָתִ֑י \" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def showSpanSentence(s):\n",
    "    words = L.d(s, otype='word')\n",
    "    firstWord = words[0]\n",
    "    lastWord = words[-1]\n",
    "    currentWord = words[0]\n",
    "    while currentWord <= lastWord:\n",
    "        currentVerse = L.u(currentWord, otype='verse')[0]\n",
    "        currentWords = L.d(currentVerse, otype='word')\n",
    "        currentWordsInSentence = [w for w in currentWords if w <= lastWord]\n",
    "        currentVerseBefore = currentWords[0] < firstWord\n",
    "        currentVerseAfter = currentWords[-1] > lastWord\n",
    "        currentPassage = '{} {}:{}'.format(*T.sectionFromNode(currentVerse))\n",
    "        print('{} {} \"{}\" {}'.format(\n",
    "            currentPassage,\n",
    "            '...' if currentVerseAfter else '',\n",
    "            T.text(currentWordsInSentence),\n",
    "            '...' if currentVerseBefore else '',\n",
    "        ))\n",
    "        currentWord = currentWords[-1] + 1\n",
    "    print('')\n",
    "\n",
    "for s in verseSpanners[0:10]:\n",
    "    showSpanSentence(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ketiv Qere\n",
    "Let us explore where Ketiv/Qere pairs are and how they render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1892 qeres\n",
      "3897: ketiv = \"*HWY>\"+\" \" qere = \"HAJ:Y;74>\"+\" \"\n",
      "4420: ketiv = \"*>HLH\"+\" \" qere = \">@H:@LO75W\"+\"00\"\n",
      "5645: ketiv = \"*>HLH\"+\" \" qere = \">@H:@LO92W\"+\" \"\n",
      "5912: ketiv = \"*>HLH\"+\" \" qere = \">@95H:@LOW03\"+\" \"\n",
      "6246: ketiv = \"*YBJJM\"+\" \" qere = \"Y:BOWJI80m\"+\" \"\n",
      "6354: ketiv = \"*YBJJM\"+\" \" qere = \"Y:BOWJI80m\"+\" \"\n",
      "11761: ketiv = \"*W-\"+\"\" qere = \"WA\"+\"\"\n",
      "11762: ketiv = \"*JJFM\"+\" \" qere = \"J.W.FA70m\"+\" \"\n",
      "12783: ketiv = \"*GJJM\"+\" \" qere = \"GOWJIm03\"+\" \"\n",
      "13684: ketiv = \"*YJDH\"+\" \" qere = \"Y@75JID\"+\"00\"\n"
     ]
    }
   ],
   "source": [
    "qeres = [w for w in F.otype.s('word') if F.qere.v(w) != None]\n",
    "print('{} qeres'.format(len(qeres)))\n",
    "for w in qeres[0:10]:\n",
    "    print('{}: ketiv = \"{}\"+\"{}\" qere = \"{}\"+\"{}\"'.format(\n",
    "        w, F.g_word.v(w), F.trailer.v(w), F.qere.v(w), F.qere_trailer.v(w),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a ketiv-qere pair\n",
    "Let us print all text representations of the verse in which word node 4419 occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genesis 9:21\n",
      "text-orig-full            וַיֵּ֥שְׁתְּ מִן־הַיַּ֖יִן וַיִּשְׁכָּ֑ר וַיִּתְגַּ֖ל בְּתֹ֥וךְ אָהֳלֹֽו׃\n",
      "text-orig-full-ketiv      וַיֵּ֥שְׁתְּ מִן־הַיַּ֖יִן וַיִּשְׁכָּ֑ר וַיִּתְגַּ֖ל בְּתֹ֥וךְ אהלה \n",
      "text-orig-plain           וישׁת מן־היין וישׁכר ויתגל בתוך אהלה \n",
      "text-phono-full           wayyˌēšt min-hayyˌayin wayyiškˈār wayyiṯgˌal bᵊṯˌôḵ *ʔohᵒlˈô .\n",
      "text-trans-full           WA-J.;71C:T.: MIN&HA-J.A73JIN WA-J.IC:K.@92R WA-J.IT:G.A73L B.:-TO71WK: >@H:@LO75W00\n",
      "text-trans-full-ketiv     WA-J.;71C:T.: MIN&HA-J.A73JIN WA-J.IC:K.@92R WA-J.IT:G.A73L B.:-TO71WK: *>HLH \n",
      "text-trans-plain          WJCT MN&HJJN WJCKR WJTGL BTWK >HLH \n"
     ]
    }
   ],
   "source": [
    "refWord = qeres[1]\n",
    "vn = L.u(refWord, otype='verse')[0]\n",
    "ws = L.d(vn, otype='word')\n",
    "print('{} {}:{}'.format(*T.sectionFromNode(refWord)))\n",
    "for fmt in sorted(T.formats):\n",
    "    if fmt.startswith('text-'):\n",
    "        print('{:<25} {}'.format(fmt, T.text(ws, fmt=fmt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge features: mother\n",
    "\n",
    "Let us do a few basic enquiries on an edge feature:\n",
    "[mother](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/mother).\n",
    "\n",
    "We count how many mothers nodes can have (it turns to be 0 or 1).\n",
    "We walk through all nodes and per node we retrieve the mother nodes, and\n",
    "we store the lengths (if non-zero) in a dictionary (`mother_len`).\n",
    "\n",
    "We see that nodes have at most one mother.\n",
    "\n",
    "We also count the inverse relationship: daughters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11s Counting edges\n",
      "    14s 182159 nodes have mothers\n",
      "    14s 144059 nodes have daughters\n",
      "mothers Counter({1: 182159})\n",
      "daughters Counter({1: 117926, 2: 17408, 3: 6272, 4: 1843, 5: 462, 6: 122, 7: 21, 8: 5})\n"
     ]
    }
   ],
   "source": [
    "motherLen = {}\n",
    "daughterLen = {}\n",
    "info('Counting edges')\n",
    "for c in N():\n",
    "    lms = E.mother.f(c) or []\n",
    "    lds = E.mother.t(c) or []\n",
    "    nms = len(lms)\n",
    "    nds = len(lds)\n",
    "    if nms: motherLen[c] = nms\n",
    "    if nds: daughterLen[c] = nds\n",
    "info('{} nodes have mothers'.format(len(motherLen)))\n",
    "info('{} nodes have daughters'.format(len(daughterLen)))\n",
    "\n",
    "motherCount = collections.Counter()\n",
    "daughterCount = collections.Counter()\n",
    "\n",
    "for (n, lm) in motherLen.items(): motherCount[lm] += 1\n",
    "for (n, ld) in daughterLen.items(): daughterCount[ld] += 1\n",
    "\n",
    "print('mothers', motherCount)\n",
    "print('daughters', daughterCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Emdros MQL\n",
    "\n",
    "[EMDROS](http://emdros.org), written by Ulrik Petersen.\n",
    "is a text database system with the powerful *topographic* query language MQL.\n",
    "The ideas are based on a model devised by Christ-Jan Doedens in\n",
    "[Text Databases: One Database Model and Several Retrieval Languages](https://books.google.nl/books?id=9ggOBRz1dO4C).\n",
    "\n",
    "Text-Fabric's model of slots, nodes and edges is a fairly straightforward translation of the models of Christ-Jan Doedens and Ulrik Petersen.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) uses EMDROS to offer users to execute and save MQL queries against the Hebrew Text Database [BHSA](https://github.com/ETCBC/BHSA).\n",
    "\n",
    "So it is kind of logical and convenient to be able to work with a Text-Fabric resource through MQL.\n",
    "\n",
    "After the `Fabric(modules=...)` call, you can call `exportMQL()` in order to save all features of the\n",
    "indicated modules into a big MQL dump, which can be imported by an EMDROS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Checking features of dataset BHSA2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   |     0.00s feature \"book@am\" => \"book_am\"\n",
      "   |     0.00s feature \"book@ar\" => \"book_ar\"\n",
      "   |     0.00s feature \"book@bn\" => \"book_bn\"\n",
      "   |     0.00s feature \"book@da\" => \"book_da\"\n",
      "   |     0.00s feature \"book@de\" => \"book_de\"\n",
      "   |     0.00s feature \"book@el\" => \"book_el\"\n",
      "   |     0.00s feature \"book@en\" => \"book_en\"\n",
      "   |     0.00s feature \"book@es\" => \"book_es\"\n",
      "   |     0.00s feature \"book@fa\" => \"book_fa\"\n",
      "   |     0.00s feature \"book@fr\" => \"book_fr\"\n",
      "   |     0.00s feature \"book@he\" => \"book_he\"\n",
      "   |     0.00s feature \"book@hi\" => \"book_hi\"\n",
      "   |     0.00s feature \"book@id\" => \"book_id\"\n",
      "   |     0.00s feature \"book@ja\" => \"book_ja\"\n",
      "   |     0.00s feature \"book@ko\" => \"book_ko\"\n",
      "   |     0.00s feature \"book@la\" => \"book_la\"\n",
      "   |     0.00s feature \"book@nl\" => \"book_nl\"\n",
      "   |     0.00s feature \"book@pa\" => \"book_pa\"\n",
      "   |     0.00s feature \"book@pt\" => \"book_pt\"\n",
      "   |     0.00s feature \"book@ru\" => \"book_ru\"\n",
      "   |     0.00s feature \"book@sw\" => \"book_sw\"\n",
      "   |     0.00s feature \"book@syc\" => \"book_syc\"\n",
      "   |     0.00s feature \"book@tr\" => \"book_tr\"\n",
      "   |     0.00s feature \"book@ur\" => \"book_ur\"\n",
      "   |     0.00s feature \"book@yo\" => \"book_yo\"\n",
      "   |     0.00s feature \"book@zh\" => \"book_zh\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M code                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M det                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M dist                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M dist_unit            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M distributional_parent from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M domain               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M freq_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M function             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M functional_parent    from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_nme                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_nme_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_pfm                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_pfm_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_prs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_prs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_uvf                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_uvf_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbe                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbe_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M g_vbs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M gn                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M instruction          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M is_root              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kind                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kq_hybrid            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M kq_hybrid_utf8       from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M label                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M languageISO          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M lexeme_count         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M ls                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M mother_object_type   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nametype             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nme                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M nu                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M number               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M omap@2016-2017       from /Users/dirk/github/etcbc/bhsa/tf/2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   |     0.00s feature \"omap@2016-2017\" => \"omap_2016_2017\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext@phono          from /Users/dirk/github/etcbc/phono/tf/2017\n",
      "   |     0.00s M pargr                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M pdp                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M pfm                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_gn               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_nu               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M prs_ps               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M ps                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rank_lex             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rank_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M rela                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M root                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M st                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_gender        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_number        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M suffix_person        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M tab                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M txt                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M typ                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M uvf                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vbe                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vbs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M voc_lex              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vs                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s M vt                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "  0.17s 114 features to export to MQL ...\n",
      "  0.23s Loading 114 features\n",
      "   |     0.04s B code                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.19s B det                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.17s B dist                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.19s B dist_unit            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     1.44s B distributional_parent from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.02s B domain               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.10s B freq_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B function             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     1.92s B functional_parent    from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_nme                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.11s B g_nme_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_pfm                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_pfm_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_prs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.09s B g_prs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B g_uvf                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B g_uvf_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_vbe                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B g_vbe_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B g_vbs                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B g_vbs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.10s B gn                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.03s B instruction          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.03s B is_root              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.03s B kind                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B kq_hybrid            from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.07s B kq_hybrid_utf8       from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.01s B label                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.15s B languageISO          from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.08s B lexeme_count         from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.19s B ls                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.10s B mother_object_type   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B nametype             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B nme                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.15s B nu                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.25s B number               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.91s B omap@2016-2017       from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.03s B pargr                from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.15s B pdp                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.16s B pfm                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.17s B prs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.16s B prs_gn               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B prs_nu               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B prs_ps               from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.12s B ps                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.09s B rank_lex             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.09s B rank_occ             from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.23s B rela                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B root                 from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.10s B st                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B suffix_gender        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B suffix_number        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B suffix_person        from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.02s B tab                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.02s B txt                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.23s B typ                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B uvf                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B vbe                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.13s B vbs                  from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.00s B voc_lex              from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B vs                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "   |     0.14s B vt                   from /Users/dirk/github/etcbc/bhsa/tf/2017\n",
      "    11s Writing enumerations\n",
      "\tbook_am        :   39 values, 39 not a name, e.g. «መኃልየ_መኃልይ_ዘሰሎሞን»\n",
      "\tbook_ar        :   39 values, 39 not a name, e.g. «1_اخبار»\n",
      "\tbook_bn        :   39 values, 39 not a name, e.g. «আদিপুস্তক»\n",
      "\tbook_da        :   39 values, 13 not a name, e.g. «1.Kongebog»\n",
      "\tbook_de        :   39 values, 7 not a name, e.g. «1_Chronik»\n",
      "\tbook_el        :   39 values, 39 not a name, e.g. «Άσμα_Ασμάτων»\n",
      "\tbook_en        :   39 values, 6 not a name, e.g. «1_Chronicles»\n",
      "\tbook_es        :   39 values, 22 not a name, e.g. «1_Crónicas»\n",
      "\tbook_fa        :   39 values, 39 not a name, e.g. «استر»\n",
      "\tbook_fr        :   39 values, 19 not a name, e.g. «1_Chroniques»\n",
      "\tbook_he        :   39 values, 39 not a name, e.g. «איוב»\n",
      "\tbook_hi        :   39 values, 39 not a name, e.g. «1_इतिहास»\n",
      "\tbook_id        :   39 values, 7 not a name, e.g. «1_Raja-raja»\n",
      "\tbook_ja        :   39 values, 39 not a name, e.g. «アモス書»\n",
      "\tbook_ko        :   39 values, 39 not a name, e.g. «나훔»\n",
      "\tbook_nl        :   39 values, 8 not a name, e.g. «1_Koningen»\n",
      "\tbook_pa        :   39 values, 39 not a name, e.g. «1_ਇਤਹਾਸ»\n",
      "\tbook_pt        :   39 values, 21 not a name, e.g. «1_Crônicas»\n",
      "\tbook_ru        :   39 values, 39 not a name, e.g. «1-я_Паралипоменон»\n",
      "\tbook_sw        :   39 values, 6 not a name, e.g. «1_Mambo_ya_Nyakati»\n",
      "\tbook_syc       :   39 values, 39 not a name, e.g. «ܐ_ܒܪܝܡܝܢ»\n",
      "\tbook_tr        :   39 values, 16 not a name, e.g. «1_Krallar»\n",
      "\tbook_ur        :   39 values, 39 not a name, e.g. «احبار»\n",
      "\tbook_yo        :   39 values, 8 not a name, e.g. «Amọsi»\n",
      "\tbook_zh        :   38 values, 37 not a name, e.g. «以斯帖记»\n",
      "\tdomain         :    4 values, 1 not a name, e.g. «?»\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tg_nme          :  108 values, 108 not a name, e.g. «»\n",
      "\tg_nme_utf8     :  106 values, 106 not a name, e.g. «»\n",
      "\tg_pfm          :   87 values, 87 not a name, e.g. «»\n",
      "\tg_pfm_utf8     :   86 values, 86 not a name, e.g. «»\n",
      "\tg_prs          :  127 values, 127 not a name, e.g. «»\n",
      "\tg_prs_utf8     :  126 values, 126 not a name, e.g. «»\n",
      "\tg_uvf          :   19 values, 19 not a name, e.g. «»\n",
      "\tg_uvf_utf8     :   17 values, 17 not a name, e.g. «»\n",
      "\tg_vbe          :  101 values, 101 not a name, e.g. «»\n",
      "\tg_vbe_utf8     :   97 values, 97 not a name, e.g. «»\n",
      "\tg_vbs          :   66 values, 66 not a name, e.g. «»\n",
      "\tg_vbs_utf8     :   65 values, 65 not a name, e.g. «»\n",
      "\tinstruction    :   35 values, 20 not a name, e.g. «.#»\n",
      "\tnametype       :    9 values, 4 not a name, e.g. «gens,topo»\n",
      "\tnme            :   20 values, 7 not a name, e.g. «»\n",
      "\tpfm            :   11 values, 4 not a name, e.g. «»\n",
      "\tphono_trailer  :    4 values, 4 not a name, e.g. «»\n",
      "\tprs            :   22 values, 4 not a name, e.g. «H=»\n",
      "\tqere_trailer   :    5 values, 5 not a name, e.g. «»\n",
      "\tqere_trailer_utf8:    5 values, 5 not a name, e.g. «»\n",
      "\troot           :  648 values, 187 not a name, e.g. «<Assyrian>»\n",
      "\ttrailer        :   12 values, 12 not a name, e.g. «»\n",
      "\ttrailer_utf8   :   12 values, 12 not a name, e.g. «»\n",
      "\ttxt            :  136 values, 59 not a name, e.g. «?»\n",
      "\tuvf            :    6 values, 1 not a name, e.g. «>»\n",
      "\tvbe            :   19 values, 6 not a name, e.g. «»\n",
      "\tvbs            :   11 values, 3 not a name, e.g. «>»\n",
      "   |     2.02s Writing an all-in-one enum with  232 values\n",
      "    12s Mapping 114 features onto 13 object types\n",
      "    19s Writing 114 features as data in 13 object types\n",
      "   |     0.00s word data ...\n",
      "   |      |     5.86s batch of size               46.6MB with   50000 of   50000 words\n",
      "   |      |       11s batch of size               46.7MB with   50000 of  100000 words\n",
      "   |      |       17s batch of size               46.8MB with   50000 of  150000 words\n",
      "   |      |       22s batch of size               46.8MB with   50000 of  200000 words\n",
      "   |      |       27s batch of size               47.1MB with   50000 of  250000 words\n",
      "   |      |       33s batch of size               47.0MB with   50000 of  300000 words\n",
      "   |      |       38s batch of size               47.2MB with   50000 of  350000 words\n",
      "   |      |       44s batch of size               47.0MB with   50000 of  400000 words\n",
      "   |      |       47s batch of size               24.9MB with   26584 of  426584 words\n",
      "   |       47s word data: 426584 objects\n",
      "   |     0.00s subphrase data ...\n",
      "   |      |     0.73s batch of size                7.3MB with   50000 of   50000 subphrases\n",
      "   |      |     1.41s batch of size                7.2MB with   50000 of  100000 subphrases\n",
      "   |      |     1.64s batch of size                2.0MB with   13784 of  113784 subphrases\n",
      "   |     1.64s subphrase data: 113784 objects\n",
      "   |     0.00s phrase_atom data ...\n",
      "   |      |     1.21s batch of size               10.9MB with   50000 of   50000 phrase_atoms\n",
      "   |      |     2.27s batch of size               11.0MB with   50000 of  100000 phrase_atoms\n",
      "   |      |     3.38s batch of size               11.1MB with   50000 of  150000 phrase_atoms\n",
      "   |      |     4.50s batch of size               11.1MB with   50000 of  200000 phrase_atoms\n",
      "   |      |     5.57s batch of size               11.0MB with   50000 of  250000 phrase_atoms\n",
      "   |      |     5.94s batch of size                3.9MB with   17519 of  267519 phrase_atoms\n",
      "   |     5.94s phrase_atom data: 267519 objects\n",
      "   |     0.00s phrase data ...\n",
      "   |      |     0.99s batch of size                9.9MB with   50000 of   50000 phrases\n",
      "   |      |     1.99s batch of size               10.0MB with   50000 of  100000 phrases\n",
      "   |      |     3.06s batch of size               10.0MB with   50000 of  150000 phrases\n",
      "   |      |     4.41s batch of size               10.0MB with   50000 of  200000 phrases\n",
      "   |      |     5.57s batch of size               10.0MB with   50000 of  250000 phrases\n",
      "   |      |     5.65s batch of size              652.1KB with    3187 of  253187 phrases\n",
      "   |     5.65s phrase data: 253187 objects\n",
      "   |     0.00s clause_atom data ...\n",
      "   |      |     1.45s batch of size               13.3MB with   50000 of   50000 clause_atoms\n",
      "   |      |     2.70s batch of size               10.8MB with   40669 of   90669 clause_atoms\n",
      "   |     2.70s clause_atom data: 90669 objects\n",
      "   |     0.00s clause data ...\n",
      "   |      |     1.32s batch of size               12.3MB with   50000 of   50000 clauses\n",
      "   |      |     2.89s batch of size                9.4MB with   38101 of   88101 clauses\n",
      "   |     2.89s clause data: 88101 objects\n",
      "   |     0.00s sentence_atom data ...\n",
      "   |      |     1.15s batch of size                6.7MB with   50000 of   50000 sentence_atoms\n",
      "   |      |     1.35s batch of size                1.9MB with   14486 of   64486 sentence_atoms\n",
      "   |     1.35s sentence_atom data: 64486 objects\n",
      "   |     0.00s sentence data ...\n",
      "   |      |     0.60s batch of size                5.2MB with   50000 of   50000 sentences\n",
      "   |      |     0.74s batch of size                1.4MB with   13711 of   63711 sentences\n",
      "   |     0.74s sentence data: 63711 objects\n",
      "   |     0.00s half_verse data ...\n",
      "   |      |     0.50s batch of size                4.6MB with   45180 of   45180 half_verses\n",
      "   |     0.51s half_verse data: 45180 objects\n",
      "   |     0.00s verse data ...\n",
      "   |      |     0.37s batch of size                3.5MB with   23213 of   23213 verses\n",
      "   |     0.37s verse data: 23213 objects\n",
      "   |     0.00s lex data ...\n",
      "   |      |     0.66s batch of size                4.8MB with    9233 of    9233 lexs\n",
      "   |     0.66s lex data: 9233 objects\n",
      "   |     0.00s chapter data ...\n",
      "   |      |     0.09s batch of size              111.1KB with     929 of     929 chapters\n",
      "   |     0.09s chapter data: 929 objects\n",
      "   |     0.00s book data ...\n",
      "   |      |     0.08s batch of size               28.4KB with      39 of      39 books\n",
      "   |     0.09s book data: 39 objects\n",
      " 1m 28s Done\n"
     ]
    }
   ],
   "source": [
    "TF.exportMQL(f'BHSA{VERSION}','~/Downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a file `~/Downloads/BHSA2017.mql` of ca. 600 MB.\n",
    "You can import it into an Emdros database by saying:\n",
    "\n",
    "    cd ~/Downloads\n",
    "    rm BHSA2017\n",
    "    mql -b 3 < BHSA2017.mql\n",
    "    \n",
    "It will take some time!\n",
    "You'll see progress messages like\n",
    "\n",
    "```\n",
    "dirk:~/Downloads > mql -b 3 < BHSA2017.mql\n",
    "Dropping indices on word_objects...!\n",
    "+------------------------+\n",
    "| object_count : integer |\n",
    "+------------------------+\n",
    "| 50000                  |\n",
    "+------------------------+\n",
    "+------------------------+\n",
    "| object_count : integer |\n",
    "+------------------------+\n",
    "| 50000                  |\n",
    "+------------------------+\n",
    "```\n",
    "\n",
    "The result is an sqlite3 database `BHSA2017` in the same directory (168 MB).\n",
    "You can run a query against it by creating a text file test.mql with this contents:\n",
    "\n",
    "    select all objects where\n",
    "    [lex gloss ~ 'create'\n",
    "        [word FOCUS]\n",
    "    ]\n",
    "    \n",
    "And then say\n",
    "\n",
    "    mql -b 3 -d BHSA2017 test.mql\n",
    "    \n",
    "You will see raw query results: all word occurrences that belong to lexemes with `create` in their gloss.\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "```\n",
    " //  <  < [ lex 1437405 { 3, 381, 535, 545, 550, 724, 736, 2126, 2137, 2148, 2740, 47988, 80620, 95897, 213379, 225969, 226014, 226387, 226589, 226941, 227051, 227178, 227979, 227984, 228010, 228065, 228171, 228186, 228896, 231015, 231028, 231865, 234285, 234310, 234313, 251053, 275157, 278572, 278597, 296496, 309970, 318972, 325849, 326167, 327907, 328521, 335786, 363235 } false  //  <  < [ word 3 { 3 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 381 { 381 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 535 { 535 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 545 { 545 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 550 { 550 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 724 { 724 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 736 { 736 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 2126 { 2126 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 2137 { 2137 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 2148 { 2148 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 2740 { 2740 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 47988 { 47988 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 80620 { 80620 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 95897 { 95897 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 213379 { 213379 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 225969 { 225969 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 226014 { 226014 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 226387 { 226387 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 226589 { 226589 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 226941 { 226941 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 227051 { 227051 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 227178 { 227178 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 227979 { 227979 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 227984 { 227984 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 228010 { 228010 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 228065 { 228065 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 228171 { 228171 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 228186 { 228186 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 228896 { 228896 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 231015 { 231015 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 231028 { 231028 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 231865 { 231865 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 234285 { 234285 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 234310 { 234310 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 234313 { 234313 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 251053 { 251053 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 275157 { 275157 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 278572 { 278572 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 278597 { 278597 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 296496 { 296496 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 309970 { 309970 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 318972 { 318972 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 325849 { 325849 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 326167 { 326167 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 327907 { 327907 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 328521 { 328521 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 335786 { 335786 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 363235 { 363235 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " > \n",
    " ]\n",
    " > \n",
    " < [ lex 1437665 { 1682, 6579, 6625, 111521, 334458, 349029 } false  //  <  < [ word 1682 { 1682 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 6579 { 6579 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 6625 { 6625 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 111521 { 111521 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 334458 { 334458 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " < [ word 349029 { 349029 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " > \n",
    " ]\n",
    " > \n",
    " < [ lex 1440298 { 80619 } false  //  <  < [ word 80619 { 80619 } true  //  <  > \n",
    " ]\n",
    " > \n",
    " > \n",
    " ]\n",
    " > \n",
    " > \n",
    "\n",
    "```\n",
    "     \n",
    "It is not very pretty, and probably you should use a more visual Emdros tool to run those queries.\n",
    "But, while we're at it, observe how the word object ids coincide with their monad numbers.\n",
    "\n",
    "And let's look up lexeme 1437665 here in TF, together with its 6 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNH=[ קנה \"create\"\n",
      "Genesis 4:1 Q@NI71JTIJ\n",
      "\tוְהָ֣אָדָ֔ם יָדַ֖ע אֶת־חַוָּ֣ה אִשְׁתֹּ֑ו וַתַּ֨הַר֙ וַתֵּ֣לֶד אֶת־קַ֔יִן וַתֹּ֕אמֶר קָנִ֥יתִי אִ֖ישׁ אֶת־יְהוָֽה׃ \n",
      "Genesis 14:19 QON;73H\n",
      "\tוַֽיְבָרְכֵ֖הוּ וַיֹּאמַ֑ר בָּר֤וּךְ אַבְרָם֙ לְאֵ֣ל עֶלְיֹ֔ון קֹנֵ֖ה שָׁמַ֥יִם וָאָֽרֶץ׃ \n",
      "Genesis 14:22 QON;73H\n",
      "\tוַיֹּ֥אמֶר אַבְרָ֖ם אֶל־מֶ֣לֶךְ סְדֹ֑ם הֲרִימֹ֨תִי יָדִ֤י אֶל־יְהוָה֙ אֵ֣ל עֶלְיֹ֔ון קֹנֵ֖ה שָׁמַ֥יִם וָאָֽרֶץ׃ \n",
      "Deuteronomy 32:6 Q.@NE80K@\n",
      "\tהֲ־לַיְהוָה֙ תִּגְמְלוּ־זֹ֔את עַ֥ם נָבָ֖ל וְלֹ֣א חָכָ֑ם הֲלֹוא־הוּא֙ אָבִ֣יךָ קָּנֶ֔ךָ ה֥וּא עָֽשְׂךָ֖ וַֽיְכֹנְנֶֽךָ׃ \n",
      "Psalms 139:13 Q@NI74JT@\n",
      "\tכִּֽי־אַ֭תָּה קָנִ֣יתָ כִלְיֹתָ֑י תְּ֝סֻכֵּ֗נִי בְּבֶ֣טֶן אִמִּֽי׃ \n",
      "Proverbs 8:22 13Q@N@NIJ\n",
      "\tיְֽהוָ֗ה קָ֭נָנִי רֵאשִׁ֣ית דַּרְכֹּ֑ו קֶ֖דֶם מִפְעָלָ֣יו מֵאָֽז׃ \n"
     ]
    }
   ],
   "source": [
    "lexNode = 1437665\n",
    "print('{} {} \"{}\"'.format(\n",
    "    F.lex.v(lexNode),\n",
    "    F.voc_lex_utf8.v(lexNode),\n",
    "    F.gloss.v(lexNode),\n",
    "))\n",
    "for wordNode in L.d(lexNode, otype='word'):\n",
    "    verseNode = L.u(wordNode, otype='verse')[0]\n",
    "    print('{} {}\\n\\t{}'.format(\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(wordNode)),\n",
    "        F.g_word.v(wordNode),\n",
    "        T.text(L.d(verseNode, otype='word')),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean caches\n",
    "\n",
    "Text-Fabric precomputes data for you, so that it can be loaded faster.\n",
    "If the original data is updated, Text-Fabric detects it, and will recompute that data.\n",
    "\n",
    "But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, that you might\n",
    "want to clear the cache of precomputed results.\n",
    "\n",
    "There are two ways to do that:\n",
    "\n",
    "* Locate the `.tf` directory of your dataset, and remove all `.tfx` files in it.\n",
    "  This might be a bit awkward to do, because the `.tf` directory is hidden on Unix-like systems.\n",
    "* Call `TF.clearCache()`, which does exactly the same.\n",
    "\n",
    "It is not handy to execute the following cell all the time, that's why I have commented it out.\n",
    "So if you really want to clear the cache, remove the comment sign below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
