{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# Tutorial\n",
    "\n",
    "This notebook gets you started with\n",
    "[Text-Fabric](https://github.com/dirkroorda/text-fabric) an API on richly annotated text.\n",
    "Below we show most of its functions in action on the ETCBC4C data set (Hebrew Bible).\n",
    "\n",
    "The tutorial is best understood after having familiarized yourself with the underlying\n",
    "[data model](https://github.com/dirkroorda/text-fabric/wiki/Data-model).\n",
    "\n",
    "If you want to *get* this all, see the \n",
    "[home page](https://github.com/dirkroorda/text-fabric/wiki)\n",
    "of Text-Fabric wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Text-Fabric\n",
    "\n",
    "Everything starts by setting up Text-Fabric.\n",
    "It needs to know where to look for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 1.1.0\n",
      "Api reference    : https://github.com/dirkroorda/text-fabric/wiki/Api\n",
      "Tutorial         : https://github.com/dirkroorda/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources     : https://github.com/dirkroorda/text-fabric-data\n",
      "Data feature docs: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "109 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "ETCBC = 'hebrew/etcbc4c'\n",
    "PHONO = 'hebrew/phono'\n",
    "TF = Fabric( modules=[ETCBC, PHONO] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have added a module: `phono`. \n",
    "The etcbc4c data has a special 1-1 transcription from Hebrew to ASCII, but they do not do a *phonetic* transcription, taking into account the usual pronunciation rules.\n",
    "\n",
    "I have made a \n",
    "[notebook](https://github.com/dirkroorda/text-fabric/blob/master/phono/phonoTf.ipynb)\n",
    "that tries hard to find phonological representations for all the words.\n",
    "The notebook has added the result to Text-Fabric as a module.\n",
    "We'll encounter that later.\n",
    "\n",
    "**NB:** This is a real-world example of how to add data to an existing datasource as a module.\n",
    "\n",
    "**NB:** If you keep your data at a non-standard location, you can pass a search location as `locations=...` to the `Fabric` call.\n",
    "Here, however, we did not need that.\n",
    "\n",
    "And if you want to see what those standard locations are, look\n",
    "[here](https://github.com/dirkroorda/text-fabric/blob/master/tf/fabric.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features\n",
    "Specify the features to load, and receive the API to work with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M otext@phono          from /Users/dirk/github/text-fabric-data/hebrew/phono\n",
      "  0.02s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    sp lex voc_utf8\n",
    "    g_word trailer\n",
    "    qere qere_trailer\n",
    "    language freq_lex gloss\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see that `loadLog()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M otext@phono          from /Users/dirk/github/text-fabric-data/hebrew/phono\n",
      "   |     0.00s B book@am              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ar              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@bn              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@da              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@de              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@el              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@en              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@es              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@fa              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@fr              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@he              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@hi              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@id              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ja              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ko              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@la              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@nl              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@pa              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@pt              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ru              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@sw              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@syc             from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@tr              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ur              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@yo              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@zh              from /Users/dirk/github/text-fabric-data/hebrew/etcbc4c\n"
     ]
    }
   ],
   "source": [
    "api.loadLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it so that the members of the API are directly accessible as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just check that `F` and `N` are defined and look like `api.F` and `api.N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.api.NodeFeatures object at 0x140dd0f98>\n",
      "<tf.api.NodeFeatures object at 0x140dd0f98>\n",
      "<bound method Api.N of <tf.api.Api object at 0x114bbe5f8>>\n",
      "<bound method Api.N of <tf.api.Api object at 0x114bbe5f8>>\n"
     ]
    }
   ],
   "source": [
    "print(F)\n",
    "print(api.F)\n",
    "print(N)\n",
    "print(api.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "In order to get acquainted with the data, we start with simple tasks: counting.\n",
    "\n",
    "## Count all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.30s 1446130 nodes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Counting nodes ...')\n",
    "i = 0\n",
    "for n in N(): i += 1\n",
    "info('{} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort some nodes\n",
    "\n",
    "Get some nodes, slot and non-slot, and sort them in the canonical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[426582,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 426583,\n",
       " 426584,\n",
       " 426585,\n",
       " 426586,\n",
       " 426587,\n",
       " 426588,\n",
       " 426589,\n",
       " 426590]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortNodes(list(range(F.otype.maxSlot+1, F.otype.maxSlot+10))+list(range(1,11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers in the otype feature\n",
    "Get information that is readily available in the GRID feature `otype`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slotType  = word\n",
      "maxSlot  =426581\n",
      "maxNode  =1446130\n",
      "All otypes:\n",
      "\tbook\n",
      "\tchapter\n",
      "\tlex\n",
      "\tverse\n",
      "\thalf_verse\n",
      "\tsentence\n",
      "\tsentence_atom\n",
      "\tclause\n",
      "\tclause_atom\n",
      "\tphrase\n",
      "\tphrase_atom\n",
      "\tsubphrase\n",
      "\tword\n"
     ]
    }
   ],
   "source": [
    "info('{:<9} = {}\\n{:<9}={}\\n{:<9}={}'.format(\n",
    "    'slotType', F.otype.slotType,\n",
    "    'maxSlot', F.otype.maxSlot,\n",
    "    'maxNode', F.otype.maxNode,\n",
    "), tm=False)\n",
    "info('All otypes:\\n\\t', nl=False, tm=False)\n",
    "info('\\n\\t'.join(F.otype.all), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count individual object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s counting objects ...\n",
      "   |     0.00s      39 books\n",
      "   |     0.00s     929 chapters\n",
      "   |     0.00s    9236 lexs\n",
      "   |     0.00s   23213 verses\n",
      "   |     0.01s   45180 half_verses\n",
      "   |     0.01s   63570 sentences\n",
      "   |     0.01s   64339 sentence_atoms\n",
      "   |     0.02s   88000 clauses\n",
      "   |     0.04s   90562 clause_atoms\n",
      "   |     0.04s  253174 phrases\n",
      "   |     0.04s  267515 phrase_atoms\n",
      "   |     0.02s  113792 subphrases\n",
      "   |     0.06s  426581 words\n",
      "  0.28s Done\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('counting objects ...')\n",
    "for otype in F.otype.all:\n",
    "    i = 0\n",
    "    indent(level=1, reset=True)\n",
    "    for n in F.otype.s(otype): i+=1\n",
    "    info('{:>7} {}s'.format(i, otype))\n",
    "indent(level=0)\n",
    "info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "\n",
    "Every feature has a method to generate a frequency list of its values, ordered by highest frequency first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('subs', 125558),\n",
       " ('verb', 75450),\n",
       " ('prep', 73298),\n",
       " ('conj', 62737),\n",
       " ('nmpr', 35698),\n",
       " ('art', 30385),\n",
       " ('adjv', 10075),\n",
       " ('nega', 6059),\n",
       " ('prps', 5035),\n",
       " ('advb', 4603),\n",
       " ('prde', 2678),\n",
       " ('intj', 1912),\n",
       " ('inrg', 1303),\n",
       " ('prin', 1026))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sp.freqList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexeme matters\n",
    "\n",
    "## Top 10 frequent verbs\n",
    "\n",
    "If we count the frequency of words, we usually mean the frequency of their\n",
    "corresponding lexemes.\n",
    "\n",
    "There are several methods for working with lexemes.\n",
    "\n",
    "### Method 1: counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.31s Done\n",
      ">MR[: 5378\n",
      "HJH[: 3561\n",
      "<FH[: 2629\n",
      "BW>[: 2570\n",
      "NTN[: 2017\n",
      "HLK[: 1554\n",
      "R>H[: 1298\n",
      "CM<[: 1168\n",
      "DBR[: 1138\n",
      "JCB[: 1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "indent(reset=True)\n",
    "info('Collecting data')\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    verbs[F.lex.v(w)] +=1\n",
    "info('Done')\n",
    "print(''.join(\n",
    "    '{}: {}\\n'.format(verb, cnt) for (verb, cnt) in sorted(\n",
    "        verbs.items() , key=lambda x: (-x[1], x[0]))[0:10],\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: counting lexemes\n",
    "\n",
    "An alternative way to do this is to use the feature `freq_lex`, defined for `lex` nodes.\n",
    "Now we walk the lexemes instead of the occurrences.\n",
    "Note that the feature `sp` (part-of-speech) is defined for nodes of type `word` as well as `lex`.\n",
    "Both also have the `lex` feature.\n",
    "Note that we might encounter Hebrew lexemes as well as Aramaic lexemes, so we still have to\n",
    "accumulate the `freq_lex`es of the lexeme nodes with the same lexeme value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.02s Done\n",
      ">MR[: 5378\n",
      "HJH[: 3561\n",
      "<FH[: 2629\n",
      "BW>[: 2570\n",
      "NTN[: 2017\n",
      "HLK[: 1554\n",
      "R>H[: 1298\n",
      "CM<[: 1168\n",
      "DBR[: 1138\n",
      "JCB[: 1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "indent(reset=True)\n",
    "info('Collecting data')\n",
    "for w in F.otype.s('lex'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    verbs[F.lex.v(w)] += F.freq_lex.v(w)\n",
    "info('Done')\n",
    "print(''.join(\n",
    "    '{}: {}\\n'.format(verb, cnt) for (verb, cnt) in sorted(\n",
    "        verbs.items() , key=lambda x: (-x[1], x[0]))[0:10],\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexeme distribution\n",
    "\n",
    "Let's do a bit more fancy lexeme stuff.\n",
    "\n",
    "### Hapaxes\n",
    "\n",
    "A hapax can be found by inspecting lexemes and see to how many word nodes they are linked.\n",
    "If that is number is one, we have a hapax.\n",
    "\n",
    "We print 10 hapaxes with their gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.18s 3074 hapaxes found\n",
      "No zeroes found\n",
      "\tPJCWN/   Pishon\n",
      "\tCWP[     bruise\n",
      "\tHRWN/    pregnancy\n",
      "\tZ<H/     sweat\n",
      "\tLHV/     flame\n",
      "\tNWD/     Nod\n",
      "\tXNWK=/   Enoch\n",
      "\tMXWJ>L/  Mehujael\n",
      "\tMXJJ>L/  Mehujael\n",
      "\tJBL=/    Jabal\n"
     ]
    }
   ],
   "source": [
    "hapax = []\n",
    "zero = set()\n",
    "\n",
    "indent(reset=True)\n",
    "for l in F.otype.s('lex'):\n",
    "    occs = L.d(l, otype='word')\n",
    "    n = len(occs)\n",
    "    if n == 0: # that's weird: should not happen\n",
    "        zero.add(l)\n",
    "    elif n == 1: # hapax found!\n",
    "        hapax.append(l)\n",
    "info('{} hapaxes found'.format(len(hapax)))\n",
    "if zero:\n",
    "    error('{} zeroes found'.format(len(zero)), tm=False)\n",
    "else:\n",
    "    info('No zeroes found', tm=False)\n",
    "for h in hapax[0:10]:\n",
    "    print('\\t{:<8} {}'.format(F.lex.v(h), F.gloss.v(h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small occurrence base\n",
    "\n",
    "The occurrence base of a lexeme are the verses, chapters and books in which occurs.\n",
    "Let's look for lexemes that occur in a single chapter.\n",
    "\n",
    "If a lexeme occurs in a single chapter, its slots are a subset of the slots of that chapter.\n",
    "So, if you go *up* from the lexeme, you encounter the chapter.\n",
    "\n",
    "Normally, lexemes occur in many chapters, and then none of them totally includes all occurrences of it,\n",
    "so if you go up from such lexemes, you don not find chapters.\n",
    "\n",
    "Let's check it out.\n",
    "\n",
    "Oh yes, we have already found the hapaxes, we will skip them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single chapter lexemes\n",
      "  0.19s 450 single chapter lexemes found\n",
      "No chapter embedders of multiple lexemes found\n",
      "Genesis 4:1          QJN=/ \n",
      "Genesis 4:2          HBL=/ \n",
      "Genesis 4:18         <JRD/ \n",
      "Genesis 4:18         MTWC>L/\n",
      "Genesis 4:19         YLH/  \n",
      "Genesis 4:22         TWBL_QJN/\n",
      "Genesis 10:11        KLX=/ \n",
      "Genesis 14:1         >MRPL/\n",
      "Genesis 14:1         >RJWK/\n",
      "Genesis 14:1         >LSR/ \n"
     ]
    }
   ],
   "source": [
    "singleCh = []\n",
    "multiple = []\n",
    "indent(reset=True)\n",
    "info('Finding single chapter lexemes')\n",
    "for l in F.otype.s('lex'):\n",
    "    chapters = L.u(l, 'chapter')\n",
    "    if len(chapters) == 1:\n",
    "        if l not in hapax:\n",
    "            singleCh.append(l)\n",
    "    elif len(chapters) > 0: # should not happen\n",
    "        multipleCh.append(l)\n",
    "info('{} single chapter lexemes found'.format(len(singleCh)))\n",
    "if multiple:\n",
    "    error('{} chapter embedders of multiple lexemes found'.format(len(multiple)), tm=False)\n",
    "else:\n",
    "    info('No chapter embedders of multiple lexemes found', tm=False)\n",
    "for s in singleCh[0:10]:\n",
    "    print('{:<20} {:<6}'.format(\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(s)),\n",
    "        F.lex.v(s),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confined to books\n",
    "\n",
    "As a final exercise with lexemes, lets make a list of all books, and show their total number of lexemes and\n",
    "the number of lexemes that occur exclusively in that book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Making book-lexeme index\n",
      "  4.50s Found 9236 lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "\n",
    "allBook = collections.defaultdict(set)\n",
    "allLex = set()\n",
    "\n",
    "info('Making book-lexeme index')\n",
    "for b in F.otype.s('book'):\n",
    "    for w in L.d(b, 'word'):\n",
    "        l = L.u(w, 'lex')[0]\n",
    "        allBook[b].add(l)\n",
    "        allLex.add(l)\n",
    "info('Found {} lexemes'.format(len(allLex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single book lexemes\n",
      "  0.09s found 4228 single book lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "\n",
    "singleBook = collections.defaultdict(lambda:0)\n",
    "info('Finding single book lexemes')\n",
    "for l in F.otype.s('lex'):\n",
    "    book = L.u(l, 'book')\n",
    "    if len(book) == 1:\n",
    "        singleBook[book[0]] += 1\n",
    "info('found {} single book lexemes'.format(sum(singleBook.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                 #all #own %own\n",
      "-----------------------------------\n",
      "Daniel               1121  428 38.2%\n",
      "1_Chronicles         2016  489 24.3%\n",
      "Ezra                  991  199 20.1%\n",
      "Joshua               1175  206 17.5%\n",
      "Esther                472   67 14.2%\n",
      "Isaiah               2554  350 13.7%\n",
      "Numbers              1457  197 13.5%\n",
      "Ezekiel              1718  212 12.3%\n",
      "Song_of_songs         503   60 11.9%\n",
      "Job                  1718  202 11.8%\n",
      "Genesis              1817  208 11.4%\n",
      "Nehemiah             1076  110 10.2%\n",
      "Psalms               2251  216  9.6%\n",
      "Leviticus             960   89  9.3%\n",
      "Judges               1210   99  8.2%\n",
      "Ecclesiastes          575   46  8.0%\n",
      "Proverbs             1356  103  7.6%\n",
      "Jeremiah             1950  147  7.5%\n",
      "1_Samuel             1257   86  6.8%\n",
      "2_Samuel             1304   89  6.8%\n",
      "2_Kings              1266   85  6.7%\n",
      "Exodus               1425   92  6.5%\n",
      "1_Kings              1291   81  6.3%\n",
      "Deuteronomy          1449   80  5.5%\n",
      "Lamentations          592   31  5.2%\n",
      "2_Chronicles         1411   67  4.7%\n",
      "Nahum                 357   16  4.5%\n",
      "Hosea                 742   33  4.4%\n",
      "Ruth                  319   14  4.4%\n",
      "Habakkuk              393   17  4.3%\n",
      "Amos                  652   27  4.1%\n",
      "Joel                  398   14  3.5%\n",
      "Zechariah             726   25  3.4%\n",
      "Obadiah               167    5  3.0%\n",
      "Micah                 586   16  2.7%\n",
      "Zephaniah             367   10  2.7%\n",
      "Jonah                 252    5  2.0%\n",
      "Haggai                208    3  1.4%\n",
      "Malachi               314    4  1.3%\n"
     ]
    }
   ],
   "source": [
    "print('{:<20}{:>5}{:>5}{:>5}\\n{}'.format(\n",
    "    'book', '#all', '#own', '%own',\n",
    "    '-'*35,\n",
    "))\n",
    "booklist = []\n",
    "\n",
    "for b in F.otype.s('book'):\n",
    "    book = T.bookName(b)\n",
    "    a = len(allBook[b])\n",
    "    o = singleBook.get(b, 0)\n",
    "    p = 100 * o / a\n",
    "    booklist.append((book, a, o, p))\n",
    "\n",
    "for x in sorted(booklist, key=lambda e: (-e[3], -e[1], e[0])):\n",
    "    print('{:<20} {:>4} {:>4} {:>4.1f}%'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech counting\n",
    "We count the words of each part of speech, and we list to top 10 of frequent lexemes.\n",
    "\n",
    "**NB**: This is not so much about lexemes as well as\n",
    "generating pretty progress messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Starting tasks\n",
      "   |     0.00s Counting the words by part-of-speech ...\n",
      "   |     0.42s Done: 14 categories\n",
      "   |      |   subs   : 121481x\n",
      "   |      |   verb   :  73710x\n",
      "   |      |   prep   :  73273x\n",
      "   |      |   conj   :  62722x\n",
      "   |      |   nmpr   :  33082x\n",
      "   |      |   art    :  30384x\n",
      "   |      |   adjv   :   9464x\n",
      "   |      |   nega   :   6053x\n",
      "   |      |   prps   :   5011x\n",
      "   |      |   advb   :   4550x\n",
      "   |      |   prde   :   2660x\n",
      "   |      |   intj   :   1885x\n",
      "   |      |   inrg   :   1285x\n",
      "   |      |   prin   :   1021x\n",
      "   |     0.00s Listing the top 10 frequent words ...\n",
      "   |     0.42s Done: 8776 lexemes\n",
      "   |      |   W      :  51003x\n",
      "   |      |   H      :  30390x\n",
      "   |      |   L      :  20447x\n",
      "   |      |   B      :  15768x\n",
      "   |      |   >T     :  11002x\n",
      "   |      |   MN     :   7681x\n",
      "   |      |   JHWH/  :   6828x\n",
      "   |      |   <L     :   5870x\n",
      "   |      |   >L     :   5521x\n",
      "   |      |   >CR    :   5500x\n",
      "  0.87s All tasks completed\n"
     ]
    }
   ],
   "source": [
    "partOfSpeech = collections.Counter()\n",
    "freqLex = collections.Counter()\n",
    "\n",
    "indent(level=0, reset=True)\n",
    "info('Starting tasks')\n",
    "indent(level=1, reset=True)\n",
    "info('Counting the words by part-of-speech ...')\n",
    "for w in F.otype.s('word'):\n",
    "    partOfSpeech[F.sp.v(w)] += 1\n",
    "info('Done: {} categories'.format(len(partOfSpeech)))\n",
    "indent(level=2)\n",
    "info('\\n'.join('{:<7}: {:>6}x'.format(*x) for x in sorted(\n",
    "    partOfSpeech.items(),\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")), tm=False)\n",
    "indent(level=1, reset=True)\n",
    "info('Listing the top 10 frequent words ...')\n",
    "for w in F.otype.s('word'):\n",
    "    freqLex[F.lex.v(w)] += 1\n",
    "info('Done: {} lexemes'.format(len(freqLex)))\n",
    "indent(level=2)\n",
    "info('\\n'.join('{:<7}: {:>6}x'.format(*x) for x in sorted(\n",
    "    freqLex.items(),\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")[0:10]), tm=False)\n",
    "indent(level=0)\n",
    "info('All tasks completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer API\n",
    "We travel upwards and downwards through the node hierarchy.\n",
    "The Layer-API (`L`) provides two functions: `u()` for going up, and `d()` for going down.\n",
    "\n",
    "Upwards is going to nodes that embed the node you started from.\n",
    "Downwards is the opposite direction, to those that are contained in the start node.\n",
    "\n",
    "Embedding and containment are an indirect notion: nodes are just numbers, but by means of the\n",
    "`oslots` feature they are linked to slots. One node *contains* an other node, if the one is linked to a set of slots that contains the set of slots that the other is linked to.\n",
    "\n",
    "Both functions yield nodes of all possible otypes. By passing an optional parameter, you can restrict the results to nodes of that type.\n",
    "\n",
    "The result is ordered in the canonical node ordering.\n",
    "Both functions return always a tuple, even if there is just one node in the result.\n",
    "\n",
    "## Going up\n",
    "We go from the first word to the book it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1367534,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.u(1, otype='book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the chapters of the that book, and just count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "chapters = L.d(L.u(1, otype='book')[0], otype='chapter')\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first verse\n",
    "We pick the first verse and the first word, and explore what is above and below them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "   |   UP\n",
      "   |      |   1436895         lex\n",
      "   |      |   605144          phrase\n",
      "   |      |   858318          phrase_atom\n",
      "   |      |   1368502         half_verse\n",
      "   |      |   514582          clause_atom\n",
      "   |      |   426582          clause\n",
      "   |      |   1413682         verse\n",
      "   |      |   1189403         sentence_atom\n",
      "   |      |   1125833         sentence\n",
      "   |      |   1367573         chapter\n",
      "   |      |   1367534         book\n",
      "   |   DOWN\n",
      "   |      |   \n",
      "Node 1413682\n",
      "   |   UP\n",
      "   |      |   1189403         sentence_atom\n",
      "   |      |   1125833         sentence\n",
      "   |      |   1367573         chapter\n",
      "   |      |   1367534         book\n",
      "   |   DOWN\n",
      "   |      |   426582          clause\n",
      "   |      |   514582          clause_atom\n",
      "   |      |   1368502         half_verse\n",
      "   |      |   858318          phrase_atom\n",
      "   |      |   605144          phrase\n",
      "   |      |   1               word\n",
      "   |      |   2               word\n",
      "   |      |   858319          phrase_atom\n",
      "   |      |   3               word\n",
      "   |      |   605145          phrase\n",
      "   |      |   858320          phrase_atom\n",
      "   |      |   4               word\n",
      "   |      |   605146          phrase\n",
      "   |      |   858321          phrase_atom\n",
      "   |      |   1368503         half_verse\n",
      "   |      |   605147          phrase\n",
      "   |      |   1253742         subphrase\n",
      "   |      |   5               word\n",
      "   |      |   6               word\n",
      "   |      |   7               word\n",
      "   |      |   8               word\n",
      "   |      |   1253743         subphrase\n",
      "   |      |   9               word\n",
      "   |      |   10              word\n",
      "   |      |   11              word\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for n in [1, L.u(1, otype='verse')[0]]:\n",
    "    indent(level=0)\n",
    "    info('Node {}'.format(n), tm=False)\n",
    "    indent(level=1)\n",
    "    info('UP', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.u(n)]), tm=False)\n",
    "    indent(level=1)\n",
    "    info('DOWN', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.d(n)]), tm=False)\n",
    "indent(level=0)\n",
    "info('Done', tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text API\n",
    "\n",
    "We examine the functions of the Text API: `T`.\n",
    "\n",
    "## Formats\n",
    "First the formats that we have available to represent the actual text.\n",
    "These formats have been defined in the `otext` feature.\n",
    "This is an optional GRID config feature: it has only metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lex-orig-full',\n",
       " 'lex-orig-plain',\n",
       " 'lex-trans-full',\n",
       " 'lex-trans-plain',\n",
       " 'text-orig-full',\n",
       " 'text-orig-full-ketiv',\n",
       " 'text-orig-plain',\n",
       " 'text-phono-full',\n",
       " 'text-trans-full',\n",
       " 'text-trans-full-ketiv',\n",
       " 'text-trans-plain']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(T.formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `text-phono-full` format here.\n",
    "It does not come from the main data source `etcbc4c`, but from the module `phono`.\n",
    "Look in your data directory, find `text-fabric-data/hebrew/phono/otext@phono.tf`,\n",
    "and you'll see this format defined there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the formats\n",
    "Now let's use those formats to print out the first verse of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lex-orig-full:\n",
      "\tבְּ רֵאשִׁית בָּרָא אֱלֹה אֵת הַ שָּׁמַי וְ אֵת הָ אָרֶץ \n",
      "lex-orig-plain:\n",
      "\tב ראשׁית ברא אלהימ את ה שׁמימ ו את ה ארצ \n",
      "lex-trans-full:\n",
      "\tB.:- R;>CIJT B.@R@> >:ELOH >;T HA- C.@MAJ W:- >;T H@- >@REY \n",
      "lex-trans-plain:\n",
      "\tB R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "text-orig-full:\n",
      "\tבְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "text-orig-full-ketiv:\n",
      "\tבְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "text-orig-plain:\n",
      "\tבראשׁית ברא אלהים את השׁמים ואת הארץ׃ \n",
      "text-phono-full:\n",
      "\tbᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ . \n",
      "text-trans-full:\n",
      "\tB.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "text-trans-full-ketiv:\n",
      "\tB.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "text-trans-plain:\n",
      "\tBR>CJT BR> >LHJM >T HCMJM W>T H>RY00 \n"
     ]
    }
   ],
   "source": [
    "for fmt in sorted(T.formats):\n",
    "    print('{}:\\n\\t{}'.format(fmt, T.text(range(1,12), fmt=fmt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify a format, the **default** format is used (`text-orig-full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n"
     ]
    }
   ],
   "source": [
    "print(T.text(range(1,12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole text in all formats in just 10 seconds\n",
    "We are going to produce the complete text of the Hebrew Bible in all available formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s writing plain text of whole Bible in all formats\n",
      "    10s done 11 formats\n",
      "lex-orig-full\n",
      "בְּ רֵאשִׁית בָּרָא אֱלֹה אֵת הַ שָּׁמַי וְ אֵת הָ אָרֶץ \n",
      "וְ הָ אָרֶץ הָי תֹהוּ וָ בֹהוּ וְ חֹשֶׁךְ עַל פְּן תְהֹום וְ רוּחַ אֱלֹה רַחֶף עַל פְּן הַ מָּי \n",
      "וַ אמֶר אֱלֹה הִי אֹור וַ הִי אֹור \n",
      "וַ רְא אֱלֹה אֶת הָ אֹור כִּי טֹוב וַ בְדֵּל אֱלֹה בֵּין הָ אֹור וּ בֵין הַ חֹשֶׁךְ \n",
      "וַ קְרָא אֱלֹה לָ  אֹור יֹום וְ לַ  חֹשֶׁךְ קָרָא לָיְלָה וַ הִי עֶרֶב וַ הִי בֹקֶר יֹום אֶחָד \n",
      "\n",
      "lex-orig-plain\n",
      "ב ראשׁית ברא אלהימ את ה שׁמימ ו את ה ארצ \n",
      "ו ה ארצ היה תהו ו בהו ו חשׁכ על פנה תהומ ו רוח אלהימ רחפ על פנה ה מימ \n",
      "ו אמר אלהימ היה אור ו היה אור \n",
      "ו ראה אלהימ את ה אור כי טוב ו בדל אלהימ בינ ה אור ו בינ ה חשׁכ \n",
      "ו קרא אלהימ ל ה אור יומ ו ל ה חשׁכ קרא לילה ו היה ערב ו היה בקר יומ אחד \n",
      "\n",
      "lex-trans-full\n",
      "B.:- R;>CIJT B.@R@> >:ELOH >;T HA- C.@MAJ W:- >;T H@- >@REY \n",
      "W:- H@- >@REY H@J TOHW. W@- BOHW. W:- XOCEK: <AL P.:N T:HOWM W:- RW.XA >:ELOH RAXEP <AL P.:N HA- M.@J \n",
      "WA- >MER >:ELOH HIJ >OWR WA- HIJ >OWR \n",
      "WA- R:> >:ELOH >ET H@- >OWR K.IJ VOWB WA- B:D.;L >:ELOH B.;JN H@- >OWR W.- B;JN HA- XOCEK: \n",
      "WA- Q:R@> >:ELOH L@- - >OWR JOWM W:- LA- - XOCEK: Q@R@> L@J:L@H WA- HIJ <EREB WA- HIJ BOQER JOWM >EX@D \n",
      "\n",
      "lex-trans-plain\n",
      "B R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "W H >RY HJH THW W BHW W XCK <L PNH THWM W RWX >LHJM RXP <L PNH H MJM \n",
      "W >MR >LHJM HJH >WR W HJH >WR \n",
      "W R>H >LHJM >T H >WR KJ VWB W BDL >LHJM BJN H >WR W BJN H XCK \n",
      "W QR> >LHJM L H >WR JWM W L H XCK QR> LJLH W HJH <RB W HJH BQR JWM >XD \n",
      "\n",
      "text-orig-full\n",
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְהֹ֑ום וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃ \n",
      "וַיֹּ֥אמֶר אֱלֹהִ֖ים יְהִ֣י אֹ֑ור וַֽיְהִי־אֹֽור׃ \n",
      "וַיַּ֧רְא אֱלֹהִ֛ים אֶת־הָאֹ֖ור כִּי־טֹ֑וב וַיַּבְדֵּ֣ל אֱלֹהִ֔ים בֵּ֥ין הָאֹ֖ור וּבֵ֥ין הַחֹֽשֶׁךְ׃ \n",
      "וַיִּקְרָ֨א אֱלֹהִ֤ים׀ לָאֹור֙ יֹ֔ום וְלַחֹ֖שֶׁךְ קָ֣רָא לָ֑יְלָה וַֽיְהִי־עֶ֥רֶב וַֽיְהִי־בֹ֖קֶר יֹ֥ום אֶחָֽד׃ פ \n",
      "\n",
      "text-orig-full-ketiv\n",
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃ \n",
      "וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְהֹ֑ום וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃ \n",
      "וַיֹּ֥אמֶר אֱלֹהִ֖ים יְהִ֣י אֹ֑ור וַֽיְהִי־אֹֽור׃ \n",
      "וַיַּ֧רְא אֱלֹהִ֛ים אֶת־הָאֹ֖ור כִּי־טֹ֑וב וַיַּבְדֵּ֣ל אֱלֹהִ֔ים בֵּ֥ין הָאֹ֖ור וּבֵ֥ין הַחֹֽשֶׁךְ׃ \n",
      "וַיִּקְרָ֨א אֱלֹהִ֤ים׀ לָאֹור֙ יֹ֔ום וְלַחֹ֖שֶׁךְ קָ֣רָא לָ֑יְלָה וַֽיְהִי־עֶ֥רֶב וַֽיְהִי־בֹ֖קֶר יֹ֥ום אֶחָֽד׃ פ \n",
      "\n",
      "text-orig-plain\n",
      "בראשׁית ברא אלהים את השׁמים ואת הארץ׃ \n",
      "והארץ היתה תהו ובהו וחשׁך על־פני תהום ורוח אלהים מרחפת על־פני המים׃ \n",
      "ויאמר אלהים יהי אור ויהי־אור׃ \n",
      "וירא אלהים את־האור כי־טוב ויבדל אלהים בין האור ובין החשׁך׃ \n",
      "ויקרא אלהים׀ לאור יום ולחשׁך קרא לילה ויהי־ערב ויהי־בקר יום אחד׃ פ \n",
      "\n",
      "text-phono-full\n",
      "bᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ . \n",
      "wᵊhāʔˈāreṣ hāyᵊṯˌā ṯˈōhû wāvˈōhû wᵊḥˌōšeḵ ʕal-pᵊnˈê ṯᵊhˈôm wᵊrˈûₐḥ ʔᵉlōhˈîm mᵊraḥˌefeṯ ʕal-pᵊnˌê hammˈāyim . \n",
      "wayyˌōmer ʔᵉlōhˌîm yᵊhˈî ʔˈôr wˈayᵊhî-ʔˈôr . \n",
      "wayyˈar ʔᵉlōhˈîm ʔeṯ-hāʔˌôr kî-ṭˈôv wayyavdˈēl ʔᵉlōhˈîm bˌên hāʔˌôr ûvˌên haḥˈōšeḵ . \n",
      "wayyiqrˌā ʔᵉlōhˈîm lāʔôr yˈôm wᵊlaḥˌōšeḵ qˈārā lˈāyᵊlā wˈayᵊhî-ʕˌerev wˈayᵊhî-vˌōqer yˌôm ʔeḥˈāḏ . f \n",
      "\n",
      "text-trans-full\n",
      "B.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "W:-H@->@81REY H@J:T@71H TO33HW.03 W@-BO80HW. W:-XO73CEK: <AL&P.:N;74J T:HO92WM W:-R74W.XA >:ELOHI80JM M:RAXE73PET <AL&P.:N;71J HA-M.@75JIM00 \n",
      "WA-J.O71>MER >:ELOHI73JM J:HI74J >O92WR WA45-J:HIJ&>O75WR00 \n",
      "WA-J.A94R:> >:ELOHI91JM >ET&H@->O73WR K.IJ&VO92WB WA-J.AB:D.;74L >:ELOHI80JM B.;71JN H@->O73WR W.-B;71JN HA-XO75CEK:00 \n",
      "WA-J.IQ:R@63> >:ELOHI70JM05 L@-->OWR03 JO80WM W:-LA--XO73CEK: Q@74R@> L@92J:L@H WA45-J:HIJ&<E71REB WA45-J:HIJ&BO73QER JO71WM >EX@75D00_P \n",
      "\n",
      "text-trans-full-ketiv\n",
      "B.:-R;>CI73JT B.@R@74> >:ELOHI92JM >;71T HA-C.@MA73JIM W:->;71T H@->@75REY00 \n",
      "W:-H@->@81REY H@J:T@71H TO33HW.03 W@-BO80HW. W:-XO73CEK: <AL&P.:N;74J T:HO92WM W:-R74W.XA >:ELOHI80JM M:RAXE73PET <AL&P.:N;71J HA-M.@75JIM00 \n",
      "WA-J.O71>MER >:ELOHI73JM J:HI74J >O92WR WA45-J:HIJ&>O75WR00 \n",
      "WA-J.A94R:> >:ELOHI91JM >ET&H@->O73WR K.IJ&VO92WB WA-J.AB:D.;74L >:ELOHI80JM B.;71JN H@->O73WR W.-B;71JN HA-XO75CEK:00 \n",
      "WA-J.IQ:R@63> >:ELOHI70JM05 L@-->OWR03 JO80WM W:-LA--XO73CEK: Q@74R@> L@92J:L@H WA45-J:HIJ&<E71REB WA45-J:HIJ&BO73QER JO71WM >EX@75D00_P \n",
      "\n",
      "text-trans-plain\n",
      "BR>CJT BR> >LHJM >T HCMJM W>T H>RY00 \n",
      "WH>RY HJTH THW WBHW WXCK <L&PNJ THWM WRWX >LHJM MRXPT <L&PNJ HMJM00 \n",
      "WJ>MR >LHJM JHJ >WR WJHJ&>WR00 \n",
      "WJR> >LHJM >T&H>WR KJ&VWB WJBDL >LHJM BJN H>WR WBJN HXCK00 \n",
      "WJQR> >LHJM05 L>WR JWM WLXCK QR> LJLH WJHJ&<RB WJHJ&BQR JWM >XD00_P \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = collections.defaultdict(list)\n",
    "indent(reset=True)\n",
    "info('writing plain text of whole Bible in all formats')\n",
    "for v in F.otype.s('verse'):\n",
    "    words = L.d(v, 'word')\n",
    "    for fmt in sorted(T.formats):\n",
    "        text[fmt].append(T.text(words, fmt=fmt))\n",
    "info('done {} formats'.format(len(text)))\n",
    "for fmt in sorted(text):\n",
    "    print('{}\\n{}\\n'.format(fmt, '\\n'.join(text[fmt][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book names\n",
    "\n",
    "For Bible book names, we can use several languages.\n",
    "\n",
    "### Languages\n",
    "Here are the languages that we can use for book names.\n",
    "These languages come from the features `book@ll`, where `ll` is a two letter\n",
    "ISO language code. Have a look in your data directory, you can't miss them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': {'language': 'ኣማርኛ', 'languageEnglish': 'amharic'},\n",
       " 'ar': {'language': 'العَرَبِية', 'languageEnglish': 'arabic'},\n",
       " 'bn': {'language': 'বাংলা', 'languageEnglish': 'bengali'},\n",
       " 'da': {'language': 'Dansk', 'languageEnglish': 'danish'},\n",
       " 'de': {'language': 'Deutsch', 'languageEnglish': 'german'},\n",
       " 'el': {'language': 'Ελληνικά', 'languageEnglish': 'greek'},\n",
       " 'en': {'language': 'English', 'languageEnglish': 'english'},\n",
       " 'es': {'language': 'Español', 'languageEnglish': 'spanish'},\n",
       " 'fa': {'language': 'فارسی', 'languageEnglish': 'farsi'},\n",
       " 'fr': {'language': 'Français', 'languageEnglish': 'french'},\n",
       " 'he': {'language': 'עברית', 'languageEnglish': 'hebrew'},\n",
       " 'hi': {'language': 'हिन्दी', 'languageEnglish': 'hindi'},\n",
       " 'id': {'language': 'Bahasa Indonesia', 'languageEnglish': 'indonesian'},\n",
       " 'ja': {'language': '日本語', 'languageEnglish': 'japanese'},\n",
       " 'ko': {'language': '한국어', 'languageEnglish': 'korean'},\n",
       " 'la': {'language': 'Latina', 'languageEnglish': 'latin'},\n",
       " 'nl': {'language': 'Nederlands', 'languageEnglish': 'dutch'},\n",
       " 'pa': {'language': 'ਪੰਜਾਬੀ', 'languageEnglish': 'punjabi'},\n",
       " 'pt': {'language': 'Português', 'languageEnglish': 'portuguese'},\n",
       " 'ru': {'language': 'Русский', 'languageEnglish': 'russian'},\n",
       " 'sw': {'language': 'Kiswahili', 'languageEnglish': 'swahili'},\n",
       " 'syc': {'language': 'ܠܫܢܐ ܣܘܪܝܝܐ', 'languageEnglish': 'syriac'},\n",
       " 'tr': {'language': 'Türkçe', 'languageEnglish': 'turkish'},\n",
       " 'ur': {'language': 'اُردُو', 'languageEnglish': 'urdu'},\n",
       " 'yo': {'language': 'èdè Yorùbá', 'languageEnglish': 'yoruba'},\n",
       " 'zh': {'language': '中文', 'languageEnglish': 'chinese'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book names in Swahili\n",
    "Get the book names in Swahili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1367534 = Mwanzo\n",
      "1367535 = Kutoka\n",
      "1367536 = Mambo_ya_Walawi\n",
      "1367537 = Hesabu\n",
      "1367538 = Kumbukumbu_la_Torati\n",
      "1367539 = Yoshua\n",
      "1367540 = Waamuzi\n",
      "1367541 = 1_Samweli\n",
      "1367542 = 2_Samweli\n",
      "1367543 = 1_Wafalme\n",
      "1367544 = 2_Wafalme\n",
      "1367545 = Isaya\n",
      "1367546 = Yeremia\n",
      "1367547 = Ezekieli\n",
      "1367548 = Hosea\n",
      "1367549 = Yoeli\n",
      "1367550 = Amosi\n",
      "1367551 = Obadia\n",
      "1367552 = Yona\n",
      "1367553 = Mika\n",
      "1367554 = Nahumu\n",
      "1367555 = Habakuki\n",
      "1367556 = Sefania\n",
      "1367557 = Hagai\n",
      "1367558 = Zekaria\n",
      "1367559 = Malaki\n",
      "1367560 = Zaburi\n",
      "1367561 = Ayubu\n",
      "1367562 = Mithali\n",
      "1367563 = Ruthi\n",
      "1367564 = Wimbo_Ulio_Bora\n",
      "1367565 = Mhubiri\n",
      "1367566 = Maombolezo\n",
      "1367567 = Esta\n",
      "1367568 = Danieli\n",
      "1367569 = Ezra\n",
      "1367570 = Nehemia\n",
      "1367571 = 1_Mambo_ya_Nyakati\n",
      "1367572 = 2_Mambo_ya_Nyakati\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodeToSwahili = ''\n",
    "for b in F.otype.s('book'):\n",
    "    nodeToSwahili += '{} = {}\\n'.format(b, T.bookName(b, lang='sw'))\n",
    "print(nodeToSwahili)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book nodes from Swahili\n",
    "OK, there they are. We copy them into a string, and do the opposite: get the nodes back.\n",
    "We check whether we get exactly the same nodes as the ones we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going from nodes to booknames and back yields the original nodes\n"
     ]
    }
   ],
   "source": [
    "swahiliNames = '''\n",
    "Mwanzo\n",
    "Kutoka\n",
    "Mambo_ya_Walawi\n",
    "Hesabu\n",
    "Kumbukumbu_la_Torati\n",
    "Yoshua\n",
    "Waamuzi\n",
    "1_Samweli\n",
    "2_Samweli\n",
    "1_Wafalme\n",
    "2_Wafalme\n",
    "Isaya\n",
    "Yeremia\n",
    "Ezekieli\n",
    "Hosea\n",
    "Yoeli\n",
    "Amosi\n",
    "Obadia\n",
    "Yona\n",
    "Mika\n",
    "Nahumu\n",
    "Habakuki\n",
    "Sefania\n",
    "Hagai\n",
    "Zekaria\n",
    "Malaki\n",
    "Zaburi\n",
    "Ayubu\n",
    "Mithali\n",
    "Ruthi\n",
    "Wimbo_Ulio_Bora\n",
    "Mhubiri\n",
    "Maombolezo\n",
    "Esta\n",
    "Danieli\n",
    "Ezra\n",
    "Nehemia\n",
    "1_Mambo_ya_Nyakati\n",
    "2_Mambo_ya_Nyakati\n",
    "'''.strip().split()\n",
    "\n",
    "swahiliToNode = ''\n",
    "for nm in swahiliNames:\n",
    "    swahiliToNode += '{} = {}\\n'.format(T.bookNode(nm, lang='sw'), nm)\n",
    "    \n",
    "if swahiliToNode != nodeToSwahili:\n",
    "    print('Something is not right with the book names')\n",
    "else:\n",
    "    print('Going from nodes to booknames and back yields the original nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "\n",
    "A section in the Hebrew bible is a book, a chapter or a verse.\n",
    "Knowledge of sections is not baked into Text-Fabric. \n",
    "The config feature `otext.tf` may specify three section levels, and tell\n",
    "what the corresponding node types and features are.\n",
    "\n",
    "From that knowledge it can construct mappings from nodes to sections, e.g. from verse\n",
    "nodes to tuples of the form:\n",
    "\n",
    "    (bookName, chapterNumber, verseNumber)\n",
    "   \n",
    "Here are examples of getting the section that corresponds to a node and vice versa.\n",
    "\n",
    "**NB:** `sectionFromNode` always delivers a verse specification, either from the\n",
    "first slot belonging to that node, or, if `lastSlot`, from the last slot\n",
    "belonging to that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section of first word          ('Genesis', 1, 1)\n",
      "node of Gen 1:1                1413682\n",
      "idem                           1413682\n",
      "node of book Genesis           1367534\n",
      "node of Genesis 1              1367573\n",
      "section of book node           ('Genesis', 1, 1)\n",
      "idem, now last word            ('Genesis', 50, 26)\n",
      "section of chapter node        ('Genesis', 1, 1)\n",
      "idem, now last word            ('Genesis', 1, 31)\n"
     ]
    }
   ],
   "source": [
    "for x in (\n",
    "    ('section of first word',   T.sectionFromNode(1)                            ),\n",
    "    ('node of Gen 1:1',         T.nodeFromSection(('Genesis', 1, 1))            ),\n",
    "    ('idem',                    T.nodeFromSection(('Mwanzo', 1, 1), lang='sw')  ),\n",
    "    ('node of book Genesis',    T.nodeFromSection(('Genesis',))                 ),\n",
    "    ('node of Genesis 1',       T.nodeFromSection(('Genesis', 1))               ),\n",
    "    ('section of book node',    T.sectionFromNode(1367534)                      ),\n",
    "    ('idem, now last word',     T.sectionFromNode(1367534, lastSlot=True)       ),\n",
    "    ('section of chapter node', T.sectionFromNode(1367573)                      ),\n",
    "    ('idem, now last word',     T.sectionFromNode(1367573, lastSlot=True)       ),\n",
    "): print('{:<30} {}'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences spanning multiple verses\n",
    "If you go up from a sentence node, you expect to find a verse node.\n",
    "But some sentences span multiple verses, and in that case, you will not find the enclosing\n",
    "verse node, because it is not there.\n",
    "\n",
    "Here is a piece of code to detect and list all cases where sentences span multiple verses.\n",
    "\n",
    "The idea is to pick the first and the last word of a sentence, use `T.sectionFromNode` to\n",
    "discover the verse in which that word occurs, and if they are different: bingo!\n",
    "\n",
    "We show the first 10 of 915 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Get sentences that span multiple verses\n",
      "  5.18s Found 915 cases\n",
      "  5.18s \n",
      "Genesis 1:17-18\n",
      "Genesis 1:29-30\n",
      "Genesis 2:4-7\n",
      "Genesis 7:2-3\n",
      "Genesis 7:8-9\n",
      "Genesis 7:13-14\n",
      "Genesis 9:9-10\n",
      "Genesis 10:11-12\n",
      "Genesis 10:13-14\n",
      "Genesis 10:15-18\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Get sentences that span multiple verses')\n",
    "spanSentences = []\n",
    "for s in F.otype.s('sentence'):\n",
    "    f = T.sectionFromNode(s, lastSlot=False)\n",
    "    l = T.sectionFromNode(s, lastSlot=True)\n",
    "    if f != l:\n",
    "        spanSentences.append('{} {}:{}-{}'.format(f[0], f[1], f[2], l[2]))\n",
    "info('Found {} cases'.format(len(spanSentences)))\n",
    "info('\\n{}'.format('\\n'.join(spanSentences[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ketiv Qere\n",
    "Let us explore where Ketiv/Qere pairs are and how they render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1892 qeres\n",
      "3897: ketiv = \"*HWY>\"+\" \" qere = \"HAJ:Y;74>\"+\" \"\n",
      "4420: ketiv = \"*>HLH\"+\" \" qere = \">@H:@LO75W\"+\"00\"\n",
      "5645: ketiv = \"*>HLH\"+\" \" qere = \">@H:@LO92W\"+\" \"\n",
      "5912: ketiv = \"*>HLH\"+\" \" qere = \">@95H:@LOW03\"+\" \"\n",
      "6246: ketiv = \"*YBJJM\"+\" \" qere = \"Y:BOWJI80m\"+\" \"\n",
      "6354: ketiv = \"*YBJJM\"+\" \" qere = \"Y:BOWJI80m\"+\" \"\n",
      "11761: ketiv = \"*W-\"+\"\" qere = \"WA\"+\"\"\n",
      "11762: ketiv = \"*JJFM\"+\" \" qere = \"J.W.FA70m\"+\" \"\n",
      "12783: ketiv = \"*GJJM\"+\" \" qere = \"GOWJIm03\"+\" \"\n",
      "13684: ketiv = \"*YJDH\"+\" \" qere = \"Y@75JID\"+\"00\"\n"
     ]
    }
   ],
   "source": [
    "qeres = [w for w in F.otype.s('word') if F.qere.v(w) != None]\n",
    "print('{} qeres'.format(len(qeres)))\n",
    "for w in qeres[0:10]:\n",
    "    print('{}: ketiv = \"{}\"+\"{}\" qere = \"{}\"+\"{}\"'.format(\n",
    "        w, F.g_word.v(w), F.trailer.v(w), F.qere.v(w), F.qere_trailer.v(w),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a ketiv-qere pair\n",
    "Let us print all text representations of the verse in which word node 4419 occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genesis 9:21\n",
      "text-orig-full            וַיֵּ֥שְׁתְּ מִן־הַיַּ֖יִן וַיִּשְׁכָּ֑ר וַיִּתְגַּ֖ל בְּתֹ֥וךְ אָהֳלֹֽו׃\n",
      "text-orig-full-ketiv      וַיֵּ֥שְׁתְּ מִן־הַיַּ֖יִן וַיִּשְׁכָּ֑ר וַיִּתְגַּ֖ל בְּתֹ֥וךְ אהלה \n",
      "text-orig-plain           וישׁת מן־היין וישׁכר ויתגל בתוך אהלה \n",
      "text-phono-full           wayyˌēšt min-hayyˌayin wayyiškˈār wayyiṯgˌal bᵊṯˌôḵ *ʔohᵒlˈô .\n",
      "text-trans-full           WA-J.;71C:T.: MIN&HA-J.A73JIN WA-J.IC:K.@92R WA-J.IT:G.A73L B.:-TO71WK: >@H:@LO75W00\n",
      "text-trans-full-ketiv     WA-J.;71C:T.: MIN&HA-J.A73JIN WA-J.IC:K.@92R WA-J.IT:G.A73L B.:-TO71WK: *>HLH \n",
      "text-trans-plain          WJCT MN&HJJN WJCKR WJTGL BTWK >HLH \n"
     ]
    }
   ],
   "source": [
    "refWord = 4419\n",
    "vn = L.u(refWord, otype='verse')[0]\n",
    "ws = L.d(vn, otype='word')\n",
    "print('{} {}:{}'.format(*T.sectionFromNode(refWord)))\n",
    "for fmt in sorted(T.formats):\n",
    "    if fmt.startswith('text-'):\n",
    "        print('{:<25} {}'.format(fmt, T.text(ws, fmt=fmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Emdros MQL\n",
    "\n",
    "[EMDROS](http://emdros.org), written by Ulrik Petersen.\n",
    "is a text database system with the powerful *topographic* query language MQL.\n",
    "The ideas are based on a model devised by Christ-Jan Doedens in\n",
    "[Text Databases: One Database Model and Several Retrieval Languages](https://books.google.nl/books?id=9ggOBRz1dO4C).\n",
    "\n",
    "Text-Fabric's model of slots, nodes and edges is a fairly straightforward translation of the models of Christ-Jan Doedens and Ulrik Petersen.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) uses EMDROS to offer users to execute and save MQL queries against the Hebrew Text Database of the ETCBC.\n",
    "\n",
    "So it is kind of logical and convenient to be able to work with a Text-Fabric resource through MQL.\n",
    "\n",
    "After the `Fabric(modules=...)` call, you can call `exportMQL()` in order to save all features of the\n",
    "indicated modules into a big MQL dump, which can be imported by an EMDROS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Checking features of dataset etcbc4c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   |     0.00s feature \"book@am\" => \"book_am\"\n",
      "   |     0.00s feature \"book@ar\" => \"book_ar\"\n",
      "   |     0.00s feature \"book@bn\" => \"book_bn\"\n",
      "   |     0.00s feature \"book@da\" => \"book_da\"\n",
      "   |     0.00s feature \"book@de\" => \"book_de\"\n",
      "   |     0.00s feature \"book@el\" => \"book_el\"\n",
      "   |     0.00s feature \"book@en\" => \"book_en\"\n",
      "   |     0.00s feature \"book@es\" => \"book_es\"\n",
      "   |     0.00s feature \"book@fa\" => \"book_fa\"\n",
      "   |     0.00s feature \"book@fr\" => \"book_fr\"\n",
      "   |     0.00s feature \"book@he\" => \"book_he\"\n",
      "   |     0.00s feature \"book@hi\" => \"book_hi\"\n",
      "   |     0.00s feature \"book@id\" => \"book_id\"\n",
      "   |     0.00s feature \"book@ja\" => \"book_ja\"\n",
      "   |     0.00s feature \"book@ko\" => \"book_ko\"\n",
      "   |     0.00s feature \"book@la\" => \"book_la\"\n",
      "   |     0.00s feature \"book@nl\" => \"book_nl\"\n",
      "   |     0.00s feature \"book@pa\" => \"book_pa\"\n",
      "   |     0.00s feature \"book@pt\" => \"book_pt\"\n",
      "   |     0.00s feature \"book@ru\" => \"book_ru\"\n",
      "   |     0.00s feature \"book@sw\" => \"book_sw\"\n",
      "   |     0.00s feature \"book@syc\" => \"book_syc\"\n",
      "   |     0.00s feature \"book@tr\" => \"book_tr\"\n",
      "   |     0.00s feature \"book@ur\" => \"book_ur\"\n",
      "   |     0.00s feature \"book@yo\" => \"book_yo\"\n",
      "   |     0.00s feature \"book@zh\" => \"book_zh\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.04s 105 features to export to MQL ...\n",
      "  0.12s Loading 105 features\n",
      "  0.13s Mapping 105 features onto 13 object types\n",
      "  5.97s Writing 105 features as data in 13 object types\n",
      "   |     0.00s word data ...\n",
      "   |      |     4.51s batch of size               38.7MB with   50000 of   50000 words\n",
      "   |      |     9.11s batch of size               38.7MB with   50000 of  100000 words\n",
      "   |      |       14s batch of size               38.8MB with   50000 of  150000 words\n",
      "   |      |       18s batch of size               38.8MB with   50000 of  200000 words\n",
      "   |      |       23s batch of size               39.0MB with   50000 of  250000 words\n",
      "   |      |       27s batch of size               39.0MB with   50000 of  300000 words\n",
      "   |      |       31s batch of size               39.2MB with   50000 of  350000 words\n",
      "   |      |       36s batch of size               39.0MB with   50000 of  400000 words\n",
      "   |      |       38s batch of size               20.7MB with   26581 of  426581 words\n",
      "   |       38s word data: 426581 objects\n",
      "   |     0.00s subphrase data ...\n",
      "   |      |     0.50s batch of size                5.4MB with   50000 of   50000 subphrases\n",
      "   |      |     1.00s batch of size                5.4MB with   50000 of  100000 subphrases\n",
      "   |      |     1.15s batch of size                1.5MB with   13792 of  113792 subphrases\n",
      "   |     1.15s subphrase data: 113792 objects\n",
      "   |     0.00s phrase_atom data ...\n",
      "   |      |     1.04s batch of size               10.0MB with   50000 of   50000 phrase_atoms\n",
      "   |      |     2.05s batch of size               10.1MB with   50000 of  100000 phrase_atoms\n",
      "   |      |     3.08s batch of size               10.1MB with   50000 of  150000 phrase_atoms\n",
      "   |      |     4.09s batch of size               10.1MB with   50000 of  200000 phrase_atoms\n",
      "   |      |     5.10s batch of size               10.1MB with   50000 of  250000 phrase_atoms\n",
      "   |      |     5.46s batch of size                3.6MB with   17515 of  267515 phrase_atoms\n",
      "   |     5.47s phrase_atom data: 267515 objects\n",
      "   |     0.00s phrase data ...\n",
      "   |      |     0.95s batch of size                9.1MB with   50000 of   50000 phrases\n",
      "   |      |     1.89s batch of size                9.2MB with   50000 of  100000 phrases\n",
      "   |      |     2.83s batch of size                9.2MB with   50000 of  150000 phrases\n",
      "   |      |     3.79s batch of size                9.2MB with   50000 of  200000 phrases\n",
      "   |      |     4.72s batch of size                9.2MB with   50000 of  250000 phrases\n",
      "   |      |     4.78s batch of size              599.8KB with    3174 of  253174 phrases\n",
      "   |     4.78s phrase data: 253174 objects\n",
      "   |     0.00s clause_atom data ...\n",
      "   |      |     1.15s batch of size               10.7MB with   50000 of   50000 clause_atoms\n",
      "   |      |     2.08s batch of size                8.7MB with   40562 of   90562 clause_atoms\n",
      "   |     2.08s clause_atom data: 90562 objects\n",
      "   |     0.00s clause data ...\n",
      "   |      |     1.17s batch of size               11.5MB with   50000 of   50000 clauses\n",
      "   |      |     2.03s batch of size                8.8MB with   38000 of   88000 clauses\n",
      "   |     2.03s clause data: 88000 objects\n",
      "   |     0.00s sentence_atom data ...\n",
      "   |      |     0.54s batch of size                5.4MB with   50000 of   50000 sentence_atoms\n",
      "   |      |     0.70s batch of size                1.5MB with   14339 of   64339 sentence_atoms\n",
      "   |     0.70s sentence_atom data: 64339 objects\n",
      "   |     0.00s sentence data ...\n",
      "   |      |     0.38s batch of size                3.9MB with   50000 of   50000 sentences\n",
      "   |      |     0.48s batch of size                1.1MB with   13570 of   63570 sentences\n",
      "   |     0.48s sentence data: 63570 objects\n",
      "   |     0.00s half_verse data ...\n",
      "   |      |     0.36s batch of size                3.5MB with   45180 of   45180 half_verses\n",
      "   |     0.36s half_verse data: 45180 objects\n",
      "   |     0.00s verse data ...\n",
      "   |      |     0.32s batch of size                2.9MB with   23213 of   23213 verses\n",
      "   |     0.32s verse data: 23213 objects\n",
      "   |     0.00s lex data ...\n",
      "   |      |     0.62s batch of size                5.1MB with    9236 of    9236 lexs\n",
      "   |     0.62s lex data: 9236 objects\n",
      "   |     0.00s chapter data ...\n",
      "   |      |     0.07s batch of size               90.2KB with     929 of     929 chapters\n",
      "   |     0.07s chapter data: 929 objects\n",
      "   |     0.00s book data ...\n",
      "   |      |     0.07s batch of size               27.6KB with      39 of      39 books\n",
      "   |     0.07s book data: 39 objects\n",
      " 1m 02s Done\n"
     ]
    }
   ],
   "source": [
    "TF.exportMQL('etcbc4c','~/Downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a file `~/Downloads/etcbc4c.mql` of 530 MB.\n",
    "You can import it into an Emdros database by saying:\n",
    "\n",
    "    cd ~/Downloads\n",
    "    rm etcbc4c.mql\n",
    "    mql -b 3 < etcbc4c.mql\n",
    "    \n",
    "The result is an sqlite3 database `etcbc4c` in the same directory (168 MB).\n",
    "You can run a query against it by creating a text file test.mql with this contents:\n",
    "\n",
    "    select all objects where\n",
    "    [lex gloss ~ 'make'\n",
    "        [word FOCUS]\n",
    "    ]\n",
    "    \n",
    "And then say\n",
    "\n",
    "    mql -b 3 -d etcbc4c test.mql\n",
    "    \n",
    "You will see raw query results: all word occurrences that belong to lexemes with `make` in their gloss.\n",
    "\n",
    "The end looks like this:\n",
    "\n",
    "    < [ lex 1443661 { 313199, 339890, 343902, 343933, 343998, 344891 } false  //  <  < [ word 313199 { 313199 } true  //  <  > \n",
    "     ]\n",
    "     > \n",
    "     < [ word 339890 { 339890 } true  //  <  > \n",
    "     ]\n",
    "     > \n",
    "     < [ word 343902 { 343902 } true  //  <  > \n",
    "     ]\n",
    "     > \n",
    "     < [ word 343933 { 343933 } true  //  <  > \n",
    "     ]\n",
    "     > \n",
    "     < [ word 343998 { 343998 } true  //  <  > \n",
    "     ]\n",
    "     > \n",
    "     < [ word 344891 { 344891 } true  //  <  > \n",
    "     \n",
    "It is not very pretty, and probably you should use a more visual Emdros tool to run those queries.\n",
    "But, while we're at it, observe how the word object ids coincide with their monad numbers.\n",
    "\n",
    "And let's look up this last lexeme here in TF, together with its first three occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XWH=[ חוה \"make known\"\n",
      "Psalms 19:3 J:XAW.EH\n",
      "\tיֹ֣ום לְ֭יֹום יַבִּ֣יעַֽ אֹ֑מֶר וְלַ֥יְלָה לְּ֝לַ֗יְלָה יְחַוֶּה־דָּֽעַת׃ \n",
      "Job 15:17 >:AXAW:K@71\n",
      "\tאֲחַוְךָ֥ שְֽׁמַֽע־לִ֑י וְזֶֽה־חָ֝זִ֗יתִי וַאֲסַפֵּֽרָה׃ \n"
     ]
    }
   ],
   "source": [
    "lexNode = 1443661\n",
    "print('{} {} \"{}\"'.format(\n",
    "    F.lex.v(lexNode),\n",
    "    F.voc_utf8.v(lexNode),\n",
    "    F.gloss.v(lexNode),\n",
    "))\n",
    "for wordNode in L.d(lexNode, otype='word')[0:2]:\n",
    "    verseNode = L.u(wordNode, otype='verse')[0]\n",
    "    print('{} {}\\n\\t{}'.format(\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(wordNode)),\n",
    "        F.g_word.v(wordNode),\n",
    "        T.text(L.d(verseNode, otype='word')),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
