{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# Tutorial\n",
    "## SBLGNT and Text-Fabric\n",
    "This tutorial introduces basic queries on the SBL Greek New Testament dataset using [Text-Fabric](https://github.com/ETCBC/text-fabric)<br>\n",
    "It assumes at least a basic familiarity with the [data model](https://github.com/ETCBC/text-fabric/wiki/Data-model)<br>\n",
    "For documentation on Text-Fabric, see [Text-Fabric Wiki](https://github.com/ETCBC/text-fabric/wiki)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Loading Text-Fabric](#Loading-Text-Fabric)    \n",
    "    * &nbsp;[instantiate text-fabric](#instantiate-text-fabric)\n",
    "    * &nbsp;[load sblgnt features](#load-sblgnt-features)\n",
    "* [Intro to Nodes, Objects, and Features](#Intro-to-Nodes,-Objects,-and-Features)\n",
    "    * &nbsp;[what is a node?](#what-is-a-node?)\n",
    "    * &nbsp;[what is an object?](#what-is-an-object?)\n",
    "    * &nbsp;[what is a feature?](#what-is-a-feature?)\n",
    "* [Access Object Nodes](#Access-Object-Nodes)<br>\n",
    "    * &nbsp;[access nodes](#access-nodes)\n",
    "    * &nbsp;[count all object types](#count-all-object-types)\n",
    "    * &nbsp;[count features and values](#count-features-and-values)\n",
    "* [Example Query: WordOrder](#Example-Query:-WordOrder)\n",
    "    * &nbsp;[excursus: clause structure in iteration](#excursus:-clause-structure-in-iteration)\n",
    "    * &nbsp;[main code](#main-code)\n",
    "    * &nbsp;[results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Text-Fabric\n",
    "\n",
    "Import the Fabric module from text-fabric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiate text-fabric\n",
    "Load the module with its path in the `text-fabric-data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.6\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "63 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(modules='greek/sblgnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load sblgnt features\n",
    "\n",
    "Select which features to load from the data. The available features are in the [sblgnt features documentation](https://etcbc.github.io/text-fabric-data/features/greek/sblgnt/0_home.html). Features unique to text-fabric are lower-case while features native to sblgnt are upper. \n",
    "\n",
    "Features are loaded with the load method on the Fabric object. The method takes a string argument with all of the features. Features in the load string may be space or new-line separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s Feature overview: 60 for nodes; 2 for edges; 1 configs; 7 computed\n",
      "  0.08s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "                Cat Gender Tense\n",
    "                Unicode UnicodeLemma Mood\n",
    "                book chapter verse\n",
    "                otype function psp\n",
    "                freq_occ freq_lex\n",
    "                Head End\n",
    "                g_word\n",
    "              ''')\n",
    "\n",
    "api.makeAvailableIn(globals()) # optional line, but without it you must always append api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Nodes, Objects, and Features\n",
    "\n",
    "TF uses nodes, objects, and features as pointers to the data.\n",
    "\n",
    "### what is a node?\n",
    "\n",
    "A node is an arbitrary integer that TF uses to look up the data. Every datapoint in TF has its own unique node. We supply node numbers to TF python objects and get the value in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_node = 137795\n",
    "\n",
    "# What kind of data does example_node represent? \n",
    "# We can find out by supplying the node number to the otype feature object:\n",
    "\n",
    "F.otype.v(example_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which book does example_node represent? We can find out by supplying it to another feature object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matthew'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.book.v(example_node)  # the book feature returns the book's name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want the book name in Swahili:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mathayo', 1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sectionFromNode(example_node, lang='sw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the available languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': {'language': 'ኣማርኛ', 'languageEnglish': 'amharic'},\n",
       " 'ar': {'language': 'العَرَبِية', 'languageEnglish': 'arabic'},\n",
       " 'bn': {'language': 'বাংলা', 'languageEnglish': 'bengali'},\n",
       " 'da': {'language': 'Dansk', 'languageEnglish': 'danish'},\n",
       " 'de': {'language': 'Deutsch', 'languageEnglish': 'german'},\n",
       " 'el': {'language': 'Ελληνικά', 'languageEnglish': 'greek'},\n",
       " 'en': {'language': 'English', 'languageEnglish': 'english'},\n",
       " 'es': {'language': 'Español', 'languageEnglish': 'spanish'},\n",
       " 'fa': {'language': 'فارسی', 'languageEnglish': 'farsi'},\n",
       " 'fr': {'language': 'Français', 'languageEnglish': 'french'},\n",
       " 'he': {'language': 'עברית', 'languageEnglish': 'hebrew'},\n",
       " 'hi': {'language': 'हिन्दी', 'languageEnglish': 'hindi'},\n",
       " 'id': {'language': 'Bahasa Indonesia', 'languageEnglish': 'indonesian'},\n",
       " 'ja': {'language': '日本語', 'languageEnglish': 'japanese'},\n",
       " 'ko': {'language': '한국어', 'languageEnglish': 'korean'},\n",
       " 'la': {'language': 'Latina', 'languageEnglish': 'latin'},\n",
       " 'nl': {'language': 'Nederlands', 'languageEnglish': 'dutch'},\n",
       " 'pa': {'language': 'ਪੰਜਾਬੀ', 'languageEnglish': 'punjabi'},\n",
       " 'pt': {'language': 'Português', 'languageEnglish': 'portuguese'},\n",
       " 'ru': {'language': 'Русский', 'languageEnglish': 'russian'},\n",
       " 'sw': {'language': 'Kiswahili', 'languageEnglish': 'swahili'},\n",
       " 'syc': {'language': 'ܠܫܢܐ ܣܘܪܝܝܐ', 'languageEnglish': 'syriac'},\n",
       " 'tr': {'language': 'Türkçe', 'languageEnglish': 'turkish'},\n",
       " 'ur': {'language': 'اُردُو', 'languageEnglish': 'urdu'},\n",
       " 'yo': {'language': 'èdè Yorùbá', 'languageEnglish': 'yoruba'},\n",
       " 'zh': {'language': '中文', 'languageEnglish': 'chinese'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want all book names in \n",
    "[Syriac](https://en.wikipedia.org/wiki/Peshitta#Books_of_the_Peshitta_New_Testament):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew                                             ܣܦܪܐ_ܕܡܬܝ\n",
      "Mark                                              ܣܦܪܐ_ܕܡܪܩܘܣ\n",
      "Luke                                              ܣܦܪܐ_ܕܠܘܩܘܣ\n",
      "John                                              ܣܦܪܐ_ܕܝܘܚܢܢ\n",
      "Acts                                            ܦܪܟܣܣ_ܕܫܠܝ̈ܚܐ\n",
      "Romans                              ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܪ̈ܗܘܡܝܐ\n",
      "1_Corinthians              ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܩܘܪ̈ܝܢܬܝܐ_ܩܕܡܝܬܐ\n",
      "2_Corinthians              ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܩܘܪ̈ܝܢܬܝܐ_ܕܬܪܬܝܢ\n",
      "Galatians                            ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܓܠܛܝ̈ܐ\n",
      "Ephesians                            ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܐܦܣܝ̈ܐ\n",
      "Philippians                       ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܦܝܠܝܦܣܝ̈ܐ\n",
      "Colossians                          ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܩܘܠ̈ܣܝܐ\n",
      "1_Thessalonians           ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܬܣܠ̈ܘܢܝܩܝܐ_ܩܕܡܝܬܐ\n",
      "2_Thessalonians           ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܬܣܠ̈ܘܢܝܩܝܐ_ܕܬܪܬܝܢ\n",
      "1_Timothy                    ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܛܝܡܬܐܘܣ_ܩܕܡܝܬܐ\n",
      "2_Timothy                    ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܛܝܡܬܐܘܣ_ܕܬܪܬܝܢ\n",
      "Titus                                 ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܛܝܛܘܣ\n",
      "Philemon                            ܐܓܪܬܐ_ܕܦܘܠܘܣ_ܕܠܘܬ_ܦܝܠܝܡܘܢ\n",
      "Hebrews                                     ܐܓܪܬܐ_ܕܠܘܬ_ܥܒܪ̈ܝܐ\n",
      "James                                      ܐܓܪܬܐ_ܕܝܥܩܘܒ_ܫܠܝܚܐ\n",
      "1_Peter                                    ܐܓܪܬܐ_ܕܦܛܪܘܣ_ܫܠܝܚܐ\n",
      "2_Peter                                   ܐܓܪܬܐ_ܕܬܪܬܝܢ_ܕܦܛܪܘܣ\n",
      "1_John                                     ܐܓܪܬܐ_ܕܝܘܚܢܢ_ܫܠܝܚܐ\n",
      "2_John                                    ܐܓܪܬܐ_ܕܬܪܬܝܢ_ܕܝܘܚܢܢ\n",
      "3_John                                      ܐܓܪܬܐ_ܕܬܠܬ_ܕܝܘܚܢܢ\n",
      "Jude                                             ܐܓܪܬܐ_ܕܝܗܘܕܐ\n",
      "Revelation                                       ܓܠܝܢܐ_ܕܝܘܚܢܢ\n"
     ]
    }
   ],
   "source": [
    "for bn in F.otype.s('book'):\n",
    "    print('{:<20} {:>40}'.format(\n",
    "        T.bookName(bn, lang='en'),\n",
    "        T.bookName(bn, lang='syc'),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something else with this node. We'll supply example_node to a different kind of feature object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(F.Gender.v(example_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? Book nodes can't have gender features. But word nodes can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_node gender: Feminine\n",
      "word_node unicode: ἔρημον\n"
     ]
    }
   ],
   "source": [
    "word_node = 1231\n",
    "print('word_node gender:', F.Gender.v(word_node))\n",
    "print('word_node unicode:', F.Unicode.v(word_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because any given node represents different linguistic **objects**.\n",
    "\n",
    "### what is an object?\n",
    "\n",
    "Up to this point we've used the term 'object' in the usual Python sense. The sense we refer to from now on has *no* relation to programming objects. Rather, in the datamodel of TF, words, phrases, and clauses are defined as (linguistic) objects; likewise, sections like books, chapters, and verses are objects. For more information about how objects are defined, see the [data model documentation](https://github.com/ETCBC/text-fabric/wiki/Data-model). Every object has a `type`. As in the example above, some nodes are book object types, others are word object types, [and more](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/otype).\n",
    "\n",
    "### what is a feature?\n",
    "\n",
    "Features are strings that provide information on an object type. `book`, `gender`, `tense`, and `function` are all examples of features that can be looked up for a corresponding object type. See the feature documentation for a reference to all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Access Object Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### access nodes\n",
    "We've seen what we can do with nodes. But how do we get the nodes we want? \n",
    "\n",
    "#### iterate through all nodes with [node generator](https://github.com/ETCBC/text-fabric/wiki/Api#walking-through-nodes) &nbsp;&nbsp;&nbsp;&nbsp;   `N():` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nodes:  428430\n"
     ]
    }
   ],
   "source": [
    "node_count = 0 \n",
    "\n",
    "for node in N():\n",
    "    node_count += 1\n",
    "    \n",
    "print('total nodes: ', node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interate through certain object type nodes with [feature otype](https://github.com/ETCBC/text-fabric/wiki/Api#node-features) &nbsp;&nbsp;&nbsp;&nbsp;`F.otype.s()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total books nodes:  27\n"
     ]
    }
   ],
   "source": [
    "book_count = 0\n",
    "\n",
    "for book_node in F.otype.s('book'):\n",
    "    book_count += 1\n",
    "    \n",
    "print('total books nodes: ', book_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### access embedd[ed/ing] objects with \"level up\", \"level down\" [locality](https://github.com/ETCBC/text-fabric/wiki/Api#locality) &nbsp;&nbsp;&nbsp;&nbsp; `L.u()` / `L.d()`\n",
    "\n",
    "TF preserves embedding relationships between object types. For example, phrases are embedded in clauses. See the [datamodel discussion on levels](https://github.com/ETCBC/text-fabric/wiki/Api#locality-and-levels) to understand how this is encoded. The TF term for these relationships is 'levels.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import Random\n",
    "randomizer = Random()\n",
    "\n",
    "highest_word_node = F.otype.maxSlot\n",
    "random_word = randomizer.randint(1, highest_word_node)\n",
    "\n",
    "random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137797,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the book lookup returns a tuple containing the embedding book node:\n",
    "L.u(random_word,'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἔξω, luke, 4, 29, phraseFunction: pp'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see all the above information for random_word\n",
    "level_up = (\n",
    "            F.Unicode.v(random_word),\n",
    "            F.book.v(\n",
    "                        L.u(random_word, otype='book')[0]\n",
    "                     ),\n",
    "    \n",
    "            str(F.chapter.v(\n",
    "                        L.u(random_word, otype='chapter')[0]\n",
    "                     )),\n",
    "\n",
    "            str(F.verse.v(\n",
    "                        L.u(random_word, otype = 'verse')[0]\n",
    "                     )),\n",
    "            'phraseFunction: ' + F.function.v(\n",
    "                                L.u(random_word, otype='phrase')[0]\n",
    "                             ))\n",
    "\n",
    "', '.join(level_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### access section objects with [Text](https://github.com/ETCBC/text-fabric/wiki/Api#text) &nbsp;&nbsp;&nbsp;&nbsp; `T.nodeFromSection()` / `T.sectionFromNode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422594"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john316 = ('John',3,16)  # req. a tuple; verse/chapter optional\n",
    "john316_node = T.nodeFromSection(john316)\n",
    "\n",
    "john316_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Text api can conversely return section information from a given node (**`T.sectionFromNode`**). The T. api also provides a formatting function for formatting UTF8 text from a provided list of nodes.\n",
    "\n",
    "In the example below we do 3 things: \n",
    "1. Gather all of the word nodes in John 3:16 with a `L.d()` call (this returns a list). \n",
    "2. We feed the word nodes to **`T.text()`**, which requires an iterable of word nodes as an argument.\n",
    "3. And we print with the `T.text()` now formatted, and reverse the previous cell's step by re-gathering the section data from the `john316_node` (with `T.sectionfromNode())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "γὰρ Οὕτως ἠγάπησεν ὁ θεὸς τὸν κόσμον ὥστε τὸν υἱὸν τὸν μονογενῆ ἔδωκεν, ἵνα πᾶς ὁ πιστεύων εἰς αὐτὸν μὴ ἀπόληται ἀλλὰ ἔχῃ ζωὴν αἰώνιον.  ('John', 3, 16)\n"
     ]
    }
   ],
   "source": [
    "john316_words = L.d(john316_node, otype='word')\n",
    "\n",
    "print(T.text(john316_words), T.sectionFromNode(john316_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an other format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lex-orig-full', 'text-orig-full', 'text-orig-plain'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΓΑΡ ΟΥΤΩΣ ΗΓΑΠΗΣΕΝ Ο ΘΕΟΣ ΤΟΝ ΚΟΣΜΟΝ ΩΣΤΕ ΤΟΝ ΥΙΟΝ ΤΟΝ ΜΟΝΟΓΕΝΗ ΕΔΩΚΕΝ, ΙΝΑ ΠΑΣ Ο ΠΙΣΤΕΥΩΝ ΕΙΣ ΑΥΤΟΝ ΜΗ ΑΠΟΛΗΤΑΙ ΑΛΛΑ ΕΧΗ ΖΩΗΝ ΑΙΩΝΙΟΝ.  ('John', 3, 16)\n"
     ]
    }
   ],
   "source": [
    "print(T.text(john316_words, fmt='text-orig-plain'), T.sectionFromNode(john316_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "γάρ οὕτω(ς) ἀγαπάω ὁ θεός ὁ κόσμος ὥστε ὁ υἱός ὁ μονογενής δίδωμι ἵνα πᾶς ὁ πιστεύω εἰς αὐτός μή ἀπόλλυμι ἀλλά ἔχω ζωή αἰώνιος  ('John', 3, 16)\n"
     ]
    }
   ],
   "source": [
    "print(T.text(john316_words, fmt='lex-orig-full'), T.sectionFromNode(john316_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `Unicode` gives the text of a word, but also with trailing punctuation, and without any space.\n",
    "\n",
    "we also have features `g_word` and `trailer` which split `Unicode` into a word part and a trailer part.\n",
    "The trailer part is al the material after the word until the next word, including white space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count all object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('book', 'chapter', 'verse', 'sentence', 'clause', 'clause_atom', 'phrase', 'conjunction', 'wordx', 'word')\n",
      "10 object types in sblgnt\n"
     ]
    }
   ],
   "source": [
    "all_objects = F.otype.all # just a tuple of object types (not nodes!) in sblgnt\n",
    "print(all_objects)\n",
    "print(len(all_objects), 'object types in sblgnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                        27\n",
      "conjunction                172\n",
      "chapter                    260\n",
      "wordx                      879\n",
      "verse                     7939\n",
      "sentence                  8014\n",
      "clause                   54800\n",
      "clause_atom              75967\n",
      "word                    137794\n",
      "phrase                  142578\n"
     ]
    }
   ],
   "source": [
    "# how many instances of each object type?\n",
    "\n",
    "object_counts = collections.Counter() # we use a counter to number the instances\n",
    "\n",
    "for obj_type in all_objects:\n",
    "    for otype_node in F.otype.s(obj_type): # F.otype.s() to iterate through the given otype nodes\n",
    "        object_counts[obj_type] += 1\n",
    "\n",
    "for otype, count in sorted(object_counts.items(), key = lambda k: k[1]):\n",
    "    print('{:<15}{:>15}'.format(otype, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count features and values\n",
    "\n",
    "A special method can return the count of a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Masculine', 41418), ('Feminine', 18750), ('Neuter', 13813))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.Gender.freqList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use &nbsp; [`Fall()`](https://github.com/ETCBC/text-fabric/wiki/Api#node-features) to see all loaded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cat',\n",
       " 'End',\n",
       " 'Gender',\n",
       " 'Head',\n",
       " 'Mood',\n",
       " 'Tense',\n",
       " 'Unicode',\n",
       " 'UnicodeLemma',\n",
       " 'book',\n",
       " 'chapter',\n",
       " 'freq_lex',\n",
       " 'freq_occ',\n",
       " 'function',\n",
       " 'otype',\n",
       " 'psp',\n",
       " 'verse']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category\n",
      "-------------------------\n",
      "np                  86102\n",
      "CL                  54800\n",
      "vp                  28339\n",
      "noun                28277\n",
      "verb                28112\n",
      "V                   25142\n",
      "det                 19806\n",
      "ADV                 19523\n",
      "S                   19180\n",
      "conj                18422\n",
      "pron                16132\n",
      "pp                  11434\n",
      "prep                11039\n",
      "O                   10931\n",
      "adjp                 9651\n",
      "adj                  8906\n",
      "advp                 6535\n",
      "adv                  6314\n",
      "P                    3685\n",
      "IO                   2666\n",
      "VC                   2590\n",
      "ptcl                 1043\n",
      "nump                  517\n",
      "num                   477\n",
      "intj                  317\n",
      "O2                    264 \n",
      "\n",
      "\n",
      "         gender\n",
      "-------------------------\n",
      "Masculine           41418\n",
      "Feminine            18750\n",
      "Neuter              13813 \n",
      "\n",
      "\n",
      "          tense\n",
      "-------------------------\n",
      "Aorist              11596\n",
      "Present             11552\n",
      "Imperfect            1679\n",
      "Future               1624\n",
      "Perfect              1573\n",
      "Pluperfect             88 \n",
      "\n",
      "\n",
      "           mood\n",
      "-------------------------\n",
      "Indicative          15616\n",
      "Participle           6659\n",
      "Infinitive           2289\n",
      "Subjunctive           1857\n",
      "Imperative           1623\n",
      "Optative               68 \n",
      "\n",
      "\n",
      "   partOfSpeech\n",
      "-------------------------\n",
      "noun                28277\n",
      "verb                28112\n",
      "det                 19806\n",
      "conj                18250\n",
      "pron                16130\n",
      "prep                10898\n",
      "adj                  8670\n",
      "adv                  6176\n",
      "ptcl                  979\n",
      "num                   477\n",
      "intj                   19 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_features = {'category' : F.Cat, \n",
    "                   'gender' : F.Gender, \n",
    "                   'tense' : F.Tense, \n",
    "                   'mood' : F.Mood,\n",
    "                   'partOfSpeech' : F.psp,}\n",
    "\n",
    "for feature,TFObject in select_features.items():\n",
    "    \n",
    "    counts = '\\n'.join(list('{:10}{:>15}'.format(value, count) for value, count in TFObject.freqList()))\n",
    "    print('{:>15}\\n{:>15}'.format(feature,'-'*25))\n",
    "    print(counts, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Query: WordOrder\n",
    "\n",
    "Word order is notoriously tricky in Greek. Can we find any tendencies throughout the different NT books?\n",
    "\n",
    "For this search, we will look for clauses in which both a subject and a finite verb are present and measure which one comes first. The results will be presented on a book-by-book basis.\n",
    "\n",
    "To accomplish this query, we have to first understand a bit about the highly nuanced clause structure of sblgnt with gbi trees. An iteration through clauses is not yet as simple in TF as it is for the etcbc Hebrew data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excursus: clause structure in iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verse\t Ἐν ἀρχῇ ἦν ὁ λόγος, καὶ ὁ λόγος ἦν πρὸς τὸν θεόν, καὶ θεὸς ἦν ὁ λόγος. \n",
      "clause\t Ἐν ἀρχῇ ἦν ὁ λόγος, καὶ ὁ λόγος ἦν πρὸς τὸν θεόν, καὶ θεὸς ἦν ὁ λόγος. \n",
      "clause\t Ἐν ἀρχῇ ἦν ὁ λόγος, \n",
      "clause\t ὁ λόγος ἦν πρὸς τὸν θεόν, \n",
      "clause\t θεὸς ἦν ὁ λόγος. \n"
     ]
    }
   ],
   "source": [
    "John1_1 = T.nodeFromSection(('John',1,1)) # pull the verse node for John 1.1\n",
    "\n",
    "print('verse\\t', T.text(L.d(John1_1, otype = 'word'))) # print the words contained in verse\n",
    "\n",
    "for clause in L.d(John1_1, otype = 'clause'): # iterate through clauses contained in verse\n",
    "    words = L.d(clause, otype='word')           # find the word nodes contained in each clause\n",
    "    print('clause\\t', T.text(words))            # print the words as unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that embedding and embedded clause objects are not yet distinguishable from each other with a simple iteration in Text Fabric. One way to mitigate this problem is to search for only embedded clauses in order to find only those clauses that function at the lowest levels. But this solution only shifts the problem down one level, since embedded clauses can themselves embed other clauses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τὸ φῶς ἐν τῇ σκοτίᾳ φαίνει, καὶ ἡ σκοτία αὐτὸ οὐ κατέλαβεν. \n",
      "τὸ φῶς ἐν τῇ σκοτίᾳ φαίνει, \n",
      "ἡ σκοτία αὐτὸ οὐ κατέλαβεν. \n"
     ]
    }
   ],
   "source": [
    "John1_5 = T.nodeFromSection(('John',1,5))\n",
    "for clause in L.d(John1_5, otype = 'clause'):\n",
    "    if L.u(clause, otype = 'clause'):  # here's the qualification; i.e. ONLY pull embedded's\n",
    "        print(T.text(L.d(clause, otype = 'word')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add yet another qualification, that the clause itself cannot be an embedder (only clauses that are embedded but not embedding), we find that we end up losing important information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verse\t Ἐγένετο ἄνθρωπος ἀπεσταλμένος παρὰ θεοῦ, ὄνομα αὐτῷ Ἰωάννης· \n",
      "clause\t ἀπεσταλμένος παρὰ θεοῦ, \n",
      "clause\t ὄνομα αὐτῷ Ἰωάννης· \n"
     ]
    }
   ],
   "source": [
    "John1_6 = T.nodeFromSection(('John',1,6))\n",
    "\n",
    "print('verse\\t', T.text(L.d(John1_6, otype = 'word')))\n",
    "\n",
    "for clause in L.d(John1_6, otype = 'clause'):\n",
    "    # now we add a second qualification: clause cannot be an embedder...\n",
    "    if L.u(clause, otype = 'clause') and not L.d(clause, otype = 'clause'):\n",
    "        print('clause\\t', T.text(L.d(clause, otype = 'word')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we miss the clause, `Ἐγένετο ἄνθρωπος`, because of the second qualification.\n",
    "\n",
    "In order to retrieve non-overlapping clauses at the most basic level, we need to specify only those clauses that are:\n",
    "1. not embedding clauses OR\n",
    "2. an embedding clause with phrases not reflected in its \"children\" clauses.\n",
    "    * keep only those phrase nodes (currently `clause_atom`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clauseFilter(clNode):\n",
    "    '''\n",
    "    test whether clause is embedded but not embedding\n",
    "    return True or False\n",
    "    '''\n",
    "    motherClause = L.u(clNode, otype = 'clause')\n",
    "    daughterClause = L.d(clNode, otype = 'clause')\n",
    "    if motherClause and not daughterClause: # embedded but not embedding\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def keepPhrases(clNode): \n",
    "\n",
    "    allPhrases = L.d(clNode, otype = 'clause_atom')\n",
    "    daughterClauses = L.d(clNode, otype = 'clause')\n",
    "    daughterPhrases = set(\n",
    "                             ph for clause in daughterClauses \\\n",
    "                             for ph in L.d(clause, otype = 'clause_atom')\n",
    "                         )\n",
    "    goodPhrases = tuple(ph for ph in allPhrases if ph not in daughterPhrases)\n",
    "    return goodPhrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test it again on the sample passage John 1.6 that gave us trouble above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause1\n",
      "words\t\t Ἐγένετο ἄνθρωπος ἀπεσταλμένος παρὰ θεοῦ, \n",
      "phrNodes\t 221521 221522\n",
      "phrFunctions\t V\tS \n",
      "\n",
      "Clause2\n",
      "words\t\t ἀπεσταλμένος παρὰ θεοῦ, \n",
      "phrNodes\t 221523 221524\n",
      "phrFunctions\t V\tADV \n",
      "\n",
      "Clause3\n",
      "words\t\t ὄνομα αὐτῷ Ἰωάννης· \n",
      "phrNodes\t 221525 221526 221527\n",
      "phrFunctions\t S\tADV\tP \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, clause in enumerate(L.d(John1_6, otype = 'clause')):\n",
    "    if clauseFilter(clause):\n",
    "        words = L.d(clause, otype = 'word')\n",
    "        phrases = L.d(clause, otype = 'clause_atom')\n",
    "    elif keepPhrases(clause):\n",
    "        words = (w for phrase in keepPhrases(clause) for w in L.d(phrase, otype = 'word'))\n",
    "        phrases = keepPhrases(clause)\n",
    "    else: continue \n",
    "    print('Clause{}'.format(i))\n",
    "    print('words\\t\\t', T.text(L.d(clause, otype = 'word')))\n",
    "    print('phrNodes\\t',' '.join(str(ph) for ph in phrases)) # nodes\n",
    "    print('phrFunctions\\t','\\t'.join(F.function.v(ph) for ph in phrases),'\\n') # nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words from the clause1 still overlap with clause2! But if we look closer at the phrase nodes (`clause_atom`s), we find that even though the words overlap, the phrases and roles are both different because clause1 contains phrase nodes `(221521,221522)` and clause2 contains `(221523,221524)`. The roles are different too, with clause1 consisting of verb-subject functions and clause2 of verb-adverb.\n",
    "\n",
    "### main code\n",
    "Now we apply the functions in a large loop through all the books in the New Testament. We iterate through each word in each clause and test its phrase function by looking up its clause_atom (currently a phrase-type object in TF-sblgnt). We then check whether the phrasal clause_atom has the 'S' (subject) or 'V' (verb) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordOrder(section, find = set(), wordList = False):\n",
    "    for clause in L.d(section, otype = 'clause'):\n",
    "        if clauseFilter(clause):\n",
    "            phrases = L.d(clause, otype = 'clause_atom')\n",
    "        else:\n",
    "            phrases = keepPhrases(clause)\n",
    "        clauseLvlFunctions = ''\n",
    "        for phrase in phrases:\n",
    "            if F.function.v(phrase) in find:\n",
    "                clauseLvlFunctions += F.function.v(phrase)\n",
    "        if clauseLvlFunctions and not wordList:\n",
    "            yield clauseLvlFunctions\n",
    "        elif clauseLvlFunctions:\n",
    "            yield (clauseLvlFunctions, tuple(w for ph in phrases for w in L.d(ph, otype = 'word')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ἐν ἀρχῇ ἦν ὁ λόγος,  \n",
      " S \n",
      "\n",
      "ὁ λόγος ἦν πρὸς τὸν θεόν,  \n",
      " S \n",
      "\n",
      "θεὸς ἦν ὁ λόγος.  \n",
      " S \n",
      "\n",
      "οὗτος ἦν ἐν ἀρχῇ πρὸς τὸν θεόν.  \n",
      " S \n",
      "\n",
      "πάντα δι’ αὐτοῦ ἐγένετο,  \n",
      " SV \n",
      "\n"
     ]
    }
   ],
   "source": [
    "john1 = T.nodeFromSection(('John',1))\n",
    "\n",
    "for result in list(getWordOrder(john1, find = {'S','V'}, wordList = True))[:5]:\n",
    "    print(T.text(result[1]), '\\n', result[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the function to the whole NT. We'll only keep results that contain both a subject and a verb. This presents a good opportunity to utilise the previously undiscussed, built-in **`info()`** function from Text Fabric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.77s Use info!\n"
     ]
    }
   ],
   "source": [
    "# demo\n",
    "info('Use info!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1m 47s Processing word order for matthew\n",
      " 1m 48s Processing word order for mark\n",
      " 1m 48s Processing word order for luke\n",
      " 1m 49s Processing word order for john\n",
      " 1m 50s Processing word order for acts\n",
      " 1m 50s Processing word order for romans\n",
      " 1m 51s Processing word order for 1corinthians\n",
      " 1m 51s Processing word order for 2corinthians\n",
      " 1m 51s Processing word order for galatians\n",
      " 1m 51s Processing word order for ephesians\n",
      " 1m 51s Processing word order for philippians\n",
      " 1m 51s Processing word order for colossians\n",
      " 1m 51s Processing word order for 1thessalonians\n",
      " 1m 51s Processing word order for 2thessalonians\n",
      " 1m 52s Processing word order for 1timothy\n",
      " 1m 52s Processing word order for 2timothy\n",
      " 1m 52s Processing word order for titus\n",
      " 1m 52s Processing word order for philemon\n",
      " 1m 52s Processing word order for hebrews\n",
      " 1m 52s Processing word order for james\n",
      " 1m 52s Processing word order for 1peter\n",
      " 1m 52s Processing word order for 2peter\n",
      " 1m 52s Processing word order for 1john\n",
      " 1m 52s Processing word order for 2john\n",
      " 1m 52s Processing word order for 3john\n",
      " 1m 52s Processing word order for jude\n",
      " 1m 52s Processing word order for revelation\n",
      "\n",
      " 1m 53s Word order search complete with 7865 results\n"
     ]
    }
   ],
   "source": [
    "wordOrderCounts = collections.defaultdict(collections.Counter)\n",
    "find = {'S','V'}\n",
    "\n",
    "for book in F.otype.s('book'):\n",
    "    info('Processing word order for {}'.format(F.book.v(book)))\n",
    "    for result in getWordOrder(book, find=find):\n",
    "        if result in {'SV','VS'}:\n",
    "            wordOrderCounts[F.book.v(book)][result] += 1\n",
    "            wordOrderCounts[F.book.v(book)]['total'] += 1      \n",
    "\n",
    "allresults = sum(amt[1] for book in wordOrderCounts for amt in wordOrderCounts[book].items()\\\n",
    "                 if amt[0] != 'total')\n",
    "\n",
    "print()\n",
    "info('Word order search complete with {} results'.format(allresults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Book            SV                 VS\n",
      "-------------------------------------------------------\n",
      "        matthew     797   66.53%      401   33.47%\n",
      "           mark     470   68.02%      221   31.98%\n",
      "           luke     747   62.56%      447   37.44%\n",
      "           john     735   59.51%      500   40.49%\n",
      "           acts     695   60.54%      453   39.46%\n",
      "         romans     224   69.78%       97   30.22%\n",
      "   1corinthians     281   74.14%       98   25.86%\n",
      "   2corinthians     122   68.93%       55   31.07%\n",
      "      galatians      69   68.32%       32   31.68%\n",
      "      ephesians      34   60.71%       22   39.29%\n",
      "    philippians      35   77.78%       10   22.22%\n",
      "     colossians      24   66.67%       12   33.33%\n",
      " 1thessalonians      37   72.55%       14   27.45%\n",
      " 2thessalonians      18    50.0%       18    50.0%\n",
      "       1timothy      44   77.19%       13   22.81%\n",
      "       2timothy      33   64.71%       18   35.29%\n",
      "          titus      15    62.5%        9    37.5%\n",
      "       philemon       5   83.33%        1   16.67%\n",
      "        hebrews     137   61.43%       86   38.57%\n",
      "          james      63   69.23%       28   30.77%\n",
      "         1peter      42   65.62%       22   34.38%\n",
      "         2peter      44   83.02%        9   16.98%\n",
      "          1john      93   84.55%       17   15.45%\n",
      "          2john       6   66.67%        3   33.33%\n",
      "          3john      10   76.92%        3   23.08%\n",
      "           jude       9   56.25%        7   43.75%\n",
      "     revelation     312    65.0%      168    35.0%\n",
      "          TOTAL    5101   64.86%     2764   35.14%\n"
     ]
    }
   ],
   "source": [
    "def percent(amount, total):\n",
    "    return round((amount/total)*100,2)\n",
    "\n",
    "SV_total = 0\n",
    "VS_total = 0\n",
    "\n",
    "# Table Header\n",
    "print('{:>15}{:>14}{:>19}'.format('Book','SV','VS'))\n",
    "print('-'*55)\n",
    "\n",
    "for book in (F.book.v(b) for b in F.otype.s('book')): # follow the canonical order\n",
    "    total = wordOrderCounts[book]['total']\n",
    "    SV = wordOrderCounts[book]['SV']\n",
    "    VS = wordOrderCounts[book]['VS']\n",
    "    SV_total += SV\n",
    "    VS_total += VS\n",
    "    print('{:>15}   {:5}   {:5}%    {:5}   {:5}%'.format( book, SV, percent(SV,total), VS, percent(VS,total)))\n",
    "print('{:>15}   {:5}   {:5}%    {:5}   {:5}%'.format( 'TOTAL', \n",
    "                                                     SV_total, percent(SV_total,allresults), \n",
    "                                                     VS_total, percent(VS_total,allresults)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
